2022-10-31 11:00:36,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-31 11:00:36,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-31 11:00:36,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-31 11:00:36,777:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2022-10-31 11:00:37,863:WARNING:
'prophet' is a soft dependency and not included in the pycaret installation. Please run: `pip install prophet` to install.
2022-10-31 11:01:22,838:INFO:PyCaret RegressionExperiment
2022-10-31 11:01:22,839:INFO:Logging name: reg-default-name
2022-10-31 11:01:22,839:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-31 11:01:22,839:INFO:version 3.0.0.rc4
2022-10-31 11:01:22,840:INFO:Initializing setup()
2022-10-31 11:01:22,840:INFO:self.USI: e3e1
2022-10-31 11:01:22,840:INFO:self.variable_keys: {'X', '_ml_usecase', 'fold_groups_param', 'y_test', 'memory', 'variable_keys', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', '_all_metrics', 'display_container', 'transform_target_param', '_gpu_n_jobs_param', '_all_models_internal', '_all_models', 'fold_shuffle_param', 'y_train', 'data', 'transform_target_method_param', 'log_plots_param', 'master_model_container', 'exp_id', 'seed', 'html_param', 'X_train', 'pipeline', '_available_plots', 'USI', 'idx', 'n_jobs_param', 'y', 'logging_param', 'gpu_param'}
2022-10-31 11:01:22,840:INFO:Checking environment
2022-10-31 11:01:22,841:INFO:python_version: 3.7.13
2022-10-31 11:01:22,841:INFO:python_build: ('default', 'Oct 19 2022 10:19:43')
2022-10-31 11:01:22,841:INFO:machine: AMD64
2022-10-31 11:01:22,841:INFO:platform: Windows-10-10.0.19041-SP0
2022-10-31 11:01:22,842:INFO:Memory: svmem(total=17052463104, available=9419935744, percent=44.8, used=7632527360, free=9419935744)
2022-10-31 11:01:22,842:INFO:Physical Core: 4
2022-10-31 11:01:22,842:INFO:Logical Core: 8
2022-10-31 11:01:22,842:INFO:Checking libraries
2022-10-31 11:01:22,842:INFO:System:
2022-10-31 11:01:22,842:INFO:    python: 3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]
2022-10-31 11:01:22,843:INFO:executable: C:\Users\Raghuram\anaconda3\envs\py37\python.exe
2022-10-31 11:01:22,843:INFO:   machine: Windows-10-10.0.19041-SP0
2022-10-31 11:01:22,843:INFO:PyCaret required dependencies:
2022-10-31 11:01:22,843:INFO:                 pip: 22.2.2
2022-10-31 11:01:22,843:INFO:          setuptools: 65.4.0
2022-10-31 11:01:22,843:INFO:             pycaret: 3.0.0rc4
2022-10-31 11:01:22,843:INFO:             IPython: 7.34.0
2022-10-31 11:01:22,843:INFO:          ipywidgets: 8.0.2
2022-10-31 11:01:22,843:INFO:                tqdm: 4.64.1
2022-10-31 11:01:22,844:INFO:               numpy: 1.21.6
2022-10-31 11:01:22,844:INFO:              pandas: 1.3.5
2022-10-31 11:01:22,844:INFO:              jinja2: 3.1.2
2022-10-31 11:01:22,844:INFO:               scipy: 1.5.4
2022-10-31 11:01:22,844:INFO:              joblib: 1.2.0
2022-10-31 11:01:22,845:INFO:             sklearn: 1.0.2
2022-10-31 11:01:22,845:INFO:                pyod: 1.0.6
2022-10-31 11:01:22,845:INFO:            imblearn: 0.9.0
2022-10-31 11:01:22,845:INFO:   category_encoders: 2.5.1.post0
2022-10-31 11:01:22,846:INFO:            lightgbm: 3.3.3
2022-10-31 11:01:22,846:INFO:               numba: 0.55.2
2022-10-31 11:01:22,846:INFO:            requests: 2.28.1
2022-10-31 11:01:22,846:INFO:          matplotlib: 3.5.3
2022-10-31 11:01:22,847:INFO:          scikitplot: 0.3.7
2022-10-31 11:01:22,847:INFO:         yellowbrick: 1.5
2022-10-31 11:01:22,847:INFO:              plotly: 5.11.0
2022-10-31 11:01:22,847:INFO:             kaleido: 0.2.1
2022-10-31 11:01:22,847:INFO:         statsmodels: 0.13.2
2022-10-31 11:01:22,847:INFO:              sktime: 0.13.4
2022-10-31 11:01:22,847:INFO:               tbats: 1.1.1
2022-10-31 11:01:22,848:INFO:            pmdarima: 1.8.5
2022-10-31 11:01:22,848:INFO:              psutil: 5.9.3
2022-10-31 11:01:22,848:INFO:PyCaret optional dependencies:
2022-10-31 11:01:22,858:INFO:                shap: Not installed
2022-10-31 11:01:22,858:INFO:           interpret: Not installed
2022-10-31 11:01:22,859:INFO:                umap: 0.5.3
2022-10-31 11:01:22,859:INFO:    pandas_profiling: 3.4.0
2022-10-31 11:01:22,859:INFO:  explainerdashboard: Not installed
2022-10-31 11:01:22,859:INFO:             autoviz: Not installed
2022-10-31 11:01:22,859:INFO:           fairlearn: Not installed
2022-10-31 11:01:22,859:INFO:             xgboost: Not installed
2022-10-31 11:01:22,859:INFO:            catboost: Not installed
2022-10-31 11:01:22,859:INFO:              kmodes: 0.12.2
2022-10-31 11:01:22,859:INFO:             mlxtend: 0.19.0
2022-10-31 11:01:22,859:INFO:       statsforecast: Not installed
2022-10-31 11:01:22,859:INFO:        tune_sklearn: Not installed
2022-10-31 11:01:22,859:INFO:                 ray: Not installed
2022-10-31 11:01:22,859:INFO:            hyperopt: Not installed
2022-10-31 11:01:22,859:INFO:              optuna: Not installed
2022-10-31 11:01:22,859:INFO:               skopt: Not installed
2022-10-31 11:01:22,859:INFO:              mlflow: 1.30.0
2022-10-31 11:01:22,859:INFO:              gradio: Not installed
2022-10-31 11:01:22,859:INFO:             fastapi: Not installed
2022-10-31 11:01:22,859:INFO:             uvicorn: Not installed
2022-10-31 11:01:22,859:INFO:              m2cgen: Not installed
2022-10-31 11:01:22,859:INFO:           evidently: Not installed
2022-10-31 11:01:22,859:INFO:                nltk: 3.7
2022-10-31 11:01:22,860:INFO:            pyLDAvis: 3.2.2
2022-10-31 11:01:22,860:INFO:              gensim: 3.8.3
2022-10-31 11:01:22,860:INFO:               spacy: 2.3.8
2022-10-31 11:01:22,860:INFO:           wordcloud: 1.8.2.2
2022-10-31 11:01:22,860:INFO:            textblob: 0.17.1
2022-10-31 11:01:22,860:INFO:               fugue: Not installed
2022-10-31 11:01:22,860:INFO:           streamlit: 1.14.0
2022-10-31 11:01:22,860:INFO:             prophet: Not installed
2022-10-31 11:01:22,860:INFO:None
2022-10-31 11:01:22,860:INFO:Set up data.
2022-10-31 11:01:22,887:INFO:Set up train/test split.
2022-10-31 11:01:22,902:INFO:Set up index.
2022-10-31 11:01:22,902:INFO:Set up folding strategy.
2022-10-31 11:01:22,902:INFO:Assigning column types.
2022-10-31 11:01:22,918:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-31 11:01:22,918:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-31 11:01:22,918:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:01:22,934:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:22,987:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,035:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,035:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,057:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,058:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,062:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,067:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,126:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,170:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,171:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,171:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,172:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-31 11:01:23,176:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,181:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,239:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,284:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,285:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,285:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,290:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,295:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,348:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,397:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,397:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,398:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,398:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-31 11:01:23,407:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,465:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,508:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,509:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,509:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,519:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,576:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,620:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,621:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-31 11:01:23,690:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,734:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,734:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,734:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,854:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:01:23,909:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,909:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:23,909:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-31 11:01:23,982:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:24,029:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,029:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,098:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:01:24,132:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,132:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,132:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-31 11:01:24,261:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,261:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,363:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,363:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:24,363:INFO:Preparing preprocessing pipeline...
2022-10-31 11:01:24,363:INFO:Set up label encoding.
2022-10-31 11:01:24,363:INFO:Set up simple imputation.
2022-10-31 11:01:24,379:INFO:Set up encoding of ordinal features.
2022-10-31 11:01:24,379:INFO:Set up encoding of categorical features.
2022-10-31 11:01:24,379:INFO:Set up variance threshold.
2022-10-31 11:01:26,346:INFO:Finished creating preprocessing pipeline.
2022-10-31 11:01:26,361:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Age', 'RoomService', 'FoodCourt',
                                             'ShoppingMall', 'Spa', 'VRDeck'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(inclu...
                                    transformer=OneHotEncoder(cols=['HomePlanet',
                                                                    'Destination'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['PassengerId', 'Cabin', 'Name'],
                                    transformer=LeaveOneOutEncoder(cols=['PassengerId',
                                                                         'Cabin',
                                                                         'Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=3360))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-31 11:01:26,361:INFO:Creating final display dataframe.
2022-10-31 11:01:32,727:INFO:Setup display_container:                  Description             Value
0                 Session id              3360
1                     Target       Transported
2                Target type        Regression
3                 Data shape        (8693, 19)
4           Train data shape        (6085, 19)
5            Test data shape        (2608, 19)
6           Ordinal features                 2
7           Numeric features                 6
8       Categorical features                 7
9   Rows with missing values             24.0%
10                Preprocess              True
11           Imputation type            simple
12        Numeric imputation              mean
13    Categorical imputation          constant
14  Maximum one-hot encoding                 5
15           Encoding method              None
16    Low variance threshold                 0
17            Fold Generator             KFold
18               Fold Number                10
19                  CPU Jobs                -1
20                   Use GPU             False
21            Log Experiment             False
22           Experiment Name  reg-default-name
23                       USI              e3e1
2022-10-31 11:01:32,846:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:32,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:32,960:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:32,960:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:01:32,975:INFO:setup() successfully completed in 10.14s...............
2022-10-31 11:01:32,975:INFO:Initializing compare_models()
2022-10-31 11:01:32,975:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-31 11:01:32,975:INFO:Checking exceptions
2022-10-31 11:01:32,975:INFO:Preparing display monitor
2022-10-31 11:01:32,990:INFO:Initializing Linear Regression
2022-10-31 11:01:32,991:INFO:Total runtime is 1.735687255859375e-05 minutes
2022-10-31 11:01:32,991:INFO:SubProcess create_model() called ==================================
2022-10-31 11:01:32,991:INFO:Initializing create_model()
2022-10-31 11:01:32,991:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:01:32,991:INFO:Checking exceptions
2022-10-31 11:01:32,994:INFO:Importing libraries
2022-10-31 11:01:32,994:INFO:Copying training dataset
2022-10-31 11:01:32,998:INFO:Defining folds
2022-10-31 11:01:32,998:INFO:Declaring metric variables
2022-10-31 11:01:32,998:INFO:Importing untrained model
2022-10-31 11:01:32,999:INFO:Linear Regression Imported successfully
2022-10-31 11:01:32,999:INFO:Starting cross validation
2022-10-31 11:01:33,005:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:01:43,491:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24194797 -0.84419775 -0.60693611 -0.40250413 -0.38078738 -0.24163741
 -0.16921183 -0.1044453  -0.1032711  -0.09339279 -0.08550376 -0.08414727
 -0.05753256 -0.05487387 -0.03848074 -0.03501986 -0.01256979 -0.00582977
 -0.00403834  0.00838846  0.01732571  0.02044145  0.02498321  0.0259042
  0.03474579  0.05187124  0.05781138  0.06219944  0.06392184  0.09782585
  0.11172976  0.12673803  0.13431239  0.13534562  0.13742668  0.14325894
  0.14374922  0.14416599  0.14956382  0.15030022  0.15192921  0.15541783
  0.15647455  0.15699477  0.15699883  0.1580581   0.15945985  0.16203864
  0.16318305  0.16421437  0.16917848  0.17124075  0.17433662  0.17763702
  0.17825802  0.18686835  0.19049503  0.19169473  0.19211793  0.19284836
  0.19477492  0.19665897  0.19725865  0.19927061  0.19964648  0.19991583
  0.20005353  0.20507564  0.20683075  0.20734666  0.20902668  0.20949185
  0.21259572  0.21538975  0.21580861  0.21742839  0.21774558  0.21774654
  0.22214797  0.22366603  0.22428223  0.22458386  0.22471285  0.22554627
  0.22627782  0.22815116  0.23082553  0.23301337  0.23904117  0.23910414
  0.24100187  0.2410416   0.24143609  0.2417633   0.24548442  0.24693015
  0.24759788  0.24879089  0.25024618  0.2509244   0.2512435   0.25156084
  0.25787422  0.25880685  0.26157628  0.26167447  0.26452477  0.26487773
  0.26696599  0.26769566  0.2678095   0.26800434  0.26868639  0.26977468
  0.2699369   0.27031504  0.27045193  0.27069761  0.27156818  0.27173318
  0.27175027  0.27317142  0.27405799  0.27461939  0.28164931  0.28184533
  0.28241683  0.28267359  0.28356957  0.2842039   0.28455937  0.28456142
  0.28679553  0.28791818  0.28812227  0.28918756  0.289397    0.28962978
  0.29032217  0.29060298  0.29075738  0.29133833  0.29169289  0.29180384
  0.29259643  0.29392365  0.29445183  0.29516252  0.29582985  0.29674996
  0.29682996  0.29685291  0.29847564  0.29884191  0.29977104  0.29993794
  0.30363438  0.30419349  0.30445826  0.30622599  0.30645135  0.30672153
  0.30729403  0.30817941  0.30841695  0.30906594  0.30998953  0.31176733
  0.31329342  0.31423203  0.31433617  0.31437619  0.31456746  0.31465037
  0.31864183  0.31910114  0.32169752  0.32300928  0.32418067  0.32513643
  0.32619141  0.32640176  0.3267397   0.32720084  0.32761983  0.32781635
  0.32843027  0.32944887  0.33042621  0.33293227  0.33324225  0.33770183
  0.33845096  0.3386033   0.33957879  0.33958711  0.34037982  0.34046935
  0.34048039  0.3406224   0.34107704  0.34264339  0.34423549  0.34637294
  0.34686134  0.3476103   0.34810343  0.34858122  0.34864333  0.34920478
  0.35278689  0.35297722  0.35387322  0.35603449  0.35648441  0.35860548
  0.35930156  0.36042819  0.36199685  0.36265625  0.36372923  0.36374398
  0.36375204  0.36388844  0.36403059  0.36411462  0.36442531  0.36486431
  0.36739865  0.36797795  0.36817473  0.36950421  0.3731721   0.37374609
  0.37403703  0.37650864  0.37738133  0.37785964  0.38027268  0.38272166
  0.38507513  0.38509286  0.38536131  0.38685032  0.38710101  0.38746406
  0.38753897  0.38765696  0.38773251  0.38794028  0.38865106  0.38931515
  0.38941719  0.38949344  0.38983526  0.38989653  0.39127654  0.39153241
  0.39204988  0.39239655  0.39421461  0.39457767  0.39511083  0.39631528
  0.3970948   0.3973278   0.39914046  0.39932007  0.39989873  0.40042602
  0.40224528  0.40292443  0.4030223   0.40412152  0.40440031  0.40494198
  0.40694782  0.40765774  0.40880488  0.40906699  0.41031519  0.41079913
  0.41185754  0.41258532  0.41378836  0.41555966  0.41999468  0.42258202
  0.42349569  0.42405704  0.42484868  0.42771504  0.42825626  0.42840844
  0.43300722  0.4345335   0.43531996  0.43633591  0.43733194  0.43783528
  0.43789325  0.4399014   0.44026445  0.44154558  0.44172092  0.44263135
  0.44357427  0.44500686  0.44628604  0.44948108  0.44967225  0.4512558
  0.45138596  0.45198011  0.45212047  0.4531742   0.45570418  0.45634256
  0.45855144  0.46449103  0.46469634  0.46649949  0.47263431  0.47796143
  0.47843994  0.4788777   0.47990696  0.48089973  0.48389914  0.486715
  0.49340992  0.49399987  0.49411947  0.49764769  0.49886188  0.50025322
  0.5039352   0.50816465  0.50922259  0.51067708  0.51441435  0.5153858
  0.5173044   0.51767853  0.5225739   0.52968751  0.53834701  0.5384
  0.54137426  0.54255046  0.54389154  0.54628592  0.5472298   0.55714737
  0.55969032  0.56384887  0.56556066  0.56661469  0.56715291  0.56867475
  0.57054863  0.57650182  0.57810332  0.58018243  0.58196759  0.5840787
  0.58649233  0.58670999  0.58964322  0.59781519  0.598566    0.59891098
  0.60451127  0.61021644  0.61313819  0.61575047  0.61678481  0.62006977
  0.6202518   0.62152721  0.62197367  0.62286407  0.63237626  0.63447901
  0.63590205  0.63685022  0.64015242  0.64159262  0.64203299  0.64396382
  0.64870623  0.65246682  0.65344863  0.65581984  0.65768456  0.65819104
  0.65896527  0.66056224  0.66281146  0.66289945  0.66293344  0.66344296
  0.66530464  0.66560456  0.66995597  0.67072849  0.67241825  0.67478945
  0.67524136  0.67716066  0.67953186  0.67977293  0.6801658   0.68190306
  0.68214413  0.68451533  0.68472617  0.6849082   0.6867591   0.68688654
  0.68691391  0.68727941  0.68898268  0.69005801  0.69162894  0.69375907
  0.69874255  0.69913542  0.70111375  0.70288082  0.70324388  0.70387782
  0.70525203  0.70585615  0.70813628  0.70972316  0.71030879  0.7103235
  0.71035749  0.71041653  0.71099143  0.71173413  0.7131761   0.7174224
  0.71810504  0.71875351  0.72175165  0.72320445  0.72413241  0.72491099
  0.72927841  0.72956818  0.73049421  0.73151444  0.73164961  0.73589734
  0.73598842  0.73639202  0.7388017   0.74603915  0.74637312  0.74824803
  0.75021563  0.75061923  0.75130187  0.75367307  0.75536164  0.76010404
  0.76721765  0.76723235  0.76958885  0.77196005  0.77621338  0.77661138
  0.77670246  0.77709345  0.77900866  0.77907366  0.77951403  0.78360004
  0.78381606  0.78839594  0.78855847  0.79056662  0.79092967  0.79804328
  0.80041448  0.80278568  0.80515688  0.8085526   0.81077907  0.81227049
  0.81902104  0.82649771  0.82748337  0.82950285  0.83357732  0.84072492
  0.84309612  0.84546732  0.846902    0.85061611  0.85350472  0.85599348
  0.86217937  0.86379976  0.87367556  0.87568976  0.87802697  0.8816693
  0.88342796  0.8840405   0.88751178  0.88814572  0.88988298  0.89225419
  0.89522858  0.90242892  0.9088526   0.91375289  0.91833741  0.92134255
  0.92307982  0.93256462  0.93295562  0.93310749  0.93520671  0.9362597
  0.93730703  0.94204943  0.94268338  0.94342608  0.94442064  0.94679184
  0.94691776  0.94979698  0.95231581  0.95425928  0.95453939  0.95691059
  0.95731799  0.95928179  0.96002449  0.9711378   0.97287506  0.973509
  0.97434037  0.97588021  0.9824735   0.98299381  0.98536502  0.98773622
  0.99247862  0.99301909  0.99685797  0.9989243   0.99989215  1.00187236
  1.00196343  1.00486779  1.01519609  1.01563634  1.02702113  1.02737872
  1.02989755  1.03503555  1.0400163   1.04141377  1.04282787]

  UserWarning,

2022-10-31 11:01:43,538:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14373561e+00 -5.70012693e-01 -4.97240939e-01 -4.66859112e-01
 -3.51236114e-01 -1.10337661e-01 -1.09337646e-01 -9.72805052e-02
 -9.38809151e-02 -9.00388052e-02 -3.74152838e-02 -2.78680818e-02
 -1.68677049e-02  5.80980708e-04  3.97522753e-02  4.75240221e-02
  5.95697462e-02  6.00827058e-02  6.91305057e-02  7.62875938e-02
  9.19447980e-02  9.47890882e-02  1.01724313e-01  1.14038118e-01
  1.17769758e-01  1.29634607e-01  1.30390927e-01  1.37518191e-01
  1.40044133e-01  1.46840056e-01  1.50567796e-01  1.68955894e-01
  1.73091335e-01  1.75903598e-01  1.80631183e-01  1.85791350e-01
  1.89410975e-01  1.94198407e-01  1.94411578e-01  1.94663462e-01
  1.97955313e-01  1.98489324e-01  2.00742921e-01  2.05333219e-01
  2.05768764e-01  2.07154371e-01  2.09546518e-01  2.11620963e-01
  2.14412587e-01  2.16677509e-01  2.18654773e-01  2.19387307e-01
  2.19793876e-01  2.23214708e-01  2.25017147e-01  2.25780540e-01
  2.26410209e-01  2.27467991e-01  2.28574611e-01  2.30096944e-01
  2.30103192e-01  2.31049853e-01  2.33516468e-01  2.34412203e-01
  2.35118093e-01  2.35607656e-01  2.36201810e-01  2.37132159e-01
  2.38792758e-01  2.41769797e-01  2.41809652e-01  2.42253222e-01
  2.42564558e-01  2.44404383e-01  2.46463182e-01  2.47708173e-01
  2.48437349e-01  2.48469760e-01  2.49036108e-01  2.50007834e-01
  2.50486106e-01  2.50731258e-01  2.50790169e-01  2.51295706e-01
  2.51808917e-01  2.52514824e-01  2.52625007e-01  2.53239417e-01
  2.54453227e-01  2.54668436e-01  2.56430967e-01  2.56950434e-01
  2.57737196e-01  2.58427144e-01  2.58957890e-01  2.61262106e-01
  2.62929950e-01  2.62978031e-01  2.63466226e-01  2.65023467e-01
  2.65863430e-01  2.66027181e-01  2.66309582e-01  2.66450900e-01
  2.69356799e-01  2.71180945e-01  2.71709311e-01  2.74668161e-01
  2.75125605e-01  2.75892040e-01  2.77342410e-01  2.77965037e-01
  2.78303355e-01  2.79175077e-01  2.80325555e-01  2.80489069e-01
  2.81678691e-01  2.86308603e-01  2.86657415e-01  2.86753294e-01
  2.86896241e-01  2.87110300e-01  2.87282589e-01  2.89092692e-01
  2.91639286e-01  2.92844701e-01  2.93443186e-01  2.95562423e-01
  2.95701493e-01  2.96048772e-01  2.96719617e-01  2.98334748e-01
  3.00229862e-01  3.00369692e-01  3.00601679e-01  3.00974439e-01
  3.01088933e-01  3.01343613e-01  3.02937986e-01  3.03121506e-01
  3.05403691e-01  3.05414632e-01  3.05529991e-01  3.07689524e-01
  3.09544947e-01  3.10047745e-01  3.10363938e-01  3.10714214e-01
  3.13859107e-01  3.14024667e-01  3.15433581e-01  3.16210961e-01
  3.16785211e-01  3.18493464e-01  3.20398478e-01  3.22392993e-01
  3.22761073e-01  3.23963109e-01  3.24331503e-01  3.25592528e-01
  3.25825392e-01  3.25994370e-01  3.26321545e-01  3.26754065e-01
  3.27707429e-01  3.28520923e-01  3.29851171e-01  3.29976653e-01
  3.30325008e-01  3.30511310e-01  3.31729442e-01  3.32929170e-01
  3.33064942e-01  3.33296512e-01  3.34168210e-01  3.34218646e-01
  3.34635259e-01  3.35396290e-01  3.35643629e-01  3.36039765e-01
  3.36150160e-01  3.36891802e-01  3.37446884e-01  3.37882255e-01
  3.40408382e-01  3.41053935e-01  3.41082011e-01  3.41500931e-01
  3.41591697e-01  3.42528522e-01  3.42957555e-01  3.43754993e-01
  3.43915455e-01  3.44215465e-01  3.44272408e-01  3.44695459e-01
  3.45839873e-01  3.46658420e-01  3.47224615e-01  3.49957858e-01
  3.50816507e-01  3.50835830e-01  3.51353920e-01  3.53415829e-01
  3.54159389e-01  3.54671340e-01  3.54679234e-01  3.54730354e-01
  3.55028756e-01  3.55532046e-01  3.58903691e-01  3.58905253e-01
  3.59624043e-01  3.60258021e-01  3.60352543e-01  3.60508744e-01
  3.60839382e-01  3.60847096e-01  3.60890585e-01  3.60914819e-01
  3.61429526e-01  3.62122273e-01  3.65822213e-01  3.65879817e-01
  3.66574140e-01  3.67239735e-01  3.67305454e-01  3.67940663e-01
  3.67994460e-01  3.70652492e-01  3.72791133e-01  3.73594172e-01
  3.73769162e-01  3.73923530e-01  3.74251939e-01  3.75704487e-01
  3.76202457e-01  3.76668272e-01  3.77109405e-01  3.77461180e-01
  3.78448876e-01  3.78930912e-01  3.80190244e-01  3.80678646e-01
  3.83414975e-01  3.83622464e-01  3.84381580e-01  3.84592096e-01
  3.84636379e-01  3.85756355e-01  3.86159832e-01  3.87446522e-01
  3.87456680e-01  3.87462581e-01  3.88575281e-01  3.89720628e-01
  3.90388201e-01  3.90841828e-01  3.91399822e-01  3.91456461e-01
  3.91822474e-01  3.92674801e-01  3.95630522e-01  3.95728211e-01
  3.96089785e-01  3.96790038e-01  3.97232307e-01  3.99027577e-01
  3.99128385e-01  3.99351317e-01  3.99556535e-01  3.99984790e-01
  4.00363881e-01  4.06851515e-01  4.07451217e-01  4.08408973e-01
  4.08741084e-01  4.08792408e-01  4.08953202e-01  4.10106404e-01
  4.10388387e-01  4.10920551e-01  4.11272302e-01  4.12834909e-01
  4.15703785e-01  4.16872160e-01  4.17061353e-01  4.18505024e-01
  4.19389753e-01  4.20349354e-01  4.22435931e-01  4.25986124e-01
  4.26746878e-01  4.32780897e-01  4.32853956e-01  4.35582712e-01
  4.35918877e-01  4.36497022e-01  4.38896169e-01  4.42518521e-01
  4.43056993e-01  4.45314622e-01  4.45524588e-01  4.46884830e-01
  4.47578448e-01  4.47821143e-01  4.49353248e-01  4.50731881e-01
  4.52770461e-01  4.53372767e-01  4.54775889e-01  4.55612776e-01
  4.55659783e-01  4.57954765e-01  4.61524195e-01  4.62602499e-01
  4.63655507e-01  4.63828229e-01  4.63990786e-01  4.64908871e-01
  4.67342055e-01  4.69162274e-01  4.70031689e-01  4.73085721e-01
  4.73707842e-01  4.76755361e-01  4.79120954e-01  4.79583729e-01
  4.84090680e-01  4.92393654e-01  5.01303424e-01  5.05861966e-01
  5.08304252e-01  5.14990457e-01  5.17280650e-01  5.18935241e-01
  5.21014564e-01  5.22477892e-01  5.34237524e-01  5.36880137e-01
  5.39576851e-01  5.43621707e-01  5.46215414e-01  5.48224140e-01
  5.50916534e-01  5.56629317e-01  5.63039494e-01  5.64953191e-01
  5.67022123e-01  5.69885459e-01  5.74810619e-01  5.75315849e-01
  5.79608913e-01  5.82008060e-01  5.86263823e-01  5.86806355e-01
  5.88793534e-01  5.89569656e-01  5.94003796e-01  5.96402943e-01
  5.99267816e-01  6.02364511e-01  6.05992184e-01  6.13196973e-01
  6.15053588e-01  6.17374179e-01  6.20394414e-01  6.22792822e-01
  6.23132104e-01  6.25192708e-01  6.26081888e-01  6.27591855e-01
  6.29991002e-01  6.30561910e-01  6.30886054e-01  6.32390149e-01
  6.34338287e-01  6.34789297e-01  6.37188444e-01  6.37517802e-01
  6.39587591e-01  6.39916949e-01  6.43290078e-01  6.45226187e-01
  6.45297858e-01  6.46785032e-01  6.47727374e-01  6.50024481e-01
  6.51706589e-01  6.51825737e-01  6.53982473e-01  6.56381620e-01
  6.57239997e-01  6.58335702e-01  6.58520252e-01  6.58780767e-01
  6.58887538e-01  6.59305793e-01  6.59468244e-01  6.61179915e-01
  6.63133996e-01  6.63908420e-01  6.64104087e-01  6.65533143e-01
  6.65978209e-01  6.68377356e-01  6.70015716e-01  6.71439190e-01
  6.71909959e-01  6.72786366e-01  6.73175650e-01  6.75185513e-01
  6.75622720e-01  6.77973944e-01  6.80533607e-01  6.85044520e-01
  6.85171386e-01  6.86655505e-01  6.87125467e-01  6.87511978e-01
  6.88012063e-01  6.88884315e-01  6.91923761e-01  6.92172965e-01
  6.94767974e-01  6.95637074e-01  6.96716111e-01  6.96722056e-01
  6.97167121e-01  6.97491265e-01  6.99121203e-01  7.00458557e-01
  7.02490441e-01  7.03156235e-01  7.03919497e-01  7.05498018e-01
  7.08717791e-01  7.08773572e-01  7.09236872e-01  7.09604387e-01
  7.09649764e-01  7.09715914e-01  7.11116938e-01  7.13571867e-01
  7.14458427e-01  7.15915232e-01  7.15971014e-01  7.18314379e-01
  7.23168455e-01  7.27584221e-01  7.28352498e-01  7.29292577e-01
  7.30634259e-01  7.31196711e-01  7.33595858e-01  7.33679353e-01
  7.41485224e-01  7.43192446e-01  7.44704997e-01  7.44760779e-01
  7.49190949e-01  7.49503292e-01  7.50389887e-01  7.51332828e-01
  7.51980531e-01  7.56700733e-01  7.57587329e-01  7.59843138e-01
  7.59986476e-01  7.62689847e-01  7.65118402e-01  7.67513276e-01
  7.69583064e-01  7.71594533e-01  7.71982211e-01  7.73319476e-01
  7.74381358e-01  7.80031416e-01  7.81578800e-01  7.83977947e-01
  7.86377094e-01  7.88386957e-01  7.88776241e-01  7.89616543e-01
  7.91175388e-01  7.93574535e-01  7.93952331e-01  7.95973682e-01
  7.97983545e-01  7.98372829e-01  7.99315770e-01  8.03171123e-01
  8.05180986e-01  8.06175851e-01  8.06298017e-01  8.06513211e-01
  8.06568992e-01  8.07649386e-01  8.07969418e-01  8.09663658e-01
  8.10368565e-01  8.22688444e-01  8.28629476e-01  8.28800955e-01
  8.30213908e-01  8.36369899e-01  8.39158330e-01  8.40922639e-01
  8.44259265e-01  8.57450647e-01  8.80468856e-01  8.83962261e-01
  8.84225152e-01  8.86595776e-01  8.90735117e-01  8.97932559e-01
  9.00331706e-01  9.03458599e-01  9.05130000e-01  9.05468543e-01
  9.07529147e-01  9.08471489e-01  9.12327441e-01  9.16736451e-01
  9.17125735e-01  9.17342032e-01  9.19135598e-01  9.21381497e-01
  9.21534745e-01  9.26722324e-01  9.27293231e-01  9.29121471e-01
  9.31520618e-01  9.33919765e-01  9.34217957e-01  9.36318912e-01
  9.36535209e-01  9.38328775e-01  9.40479787e-01  9.41117206e-01
  9.49953008e-01  9.51037939e-01  9.51873658e-01  9.52723658e-01
  9.56023071e-01  9.57911236e-01  9.59921099e-01  9.63033674e-01
  9.64680439e-01  9.68698645e-01  9.69517687e-01  9.75230291e-01
  9.76172596e-01  9.76715129e-01  9.83856788e-01  9.84295011e-01
  9.88710864e-01  9.88801459e-01  9.91478647e-01  9.94423468e-01
  9.97099126e-01  1.00070660e+00  1.00363077e+00  1.00550489e+00
  1.00733443e+00  1.00790404e+00  1.01030319e+00  1.01698564e+00
  1.01831995e+00  1.02211860e+00  1.04396582e+00  1.05775952e+00]

  UserWarning,

2022-10-31 11:01:43,657:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77224717 -0.74862747 -0.50257446 -0.39064432 -0.27797477 -0.2575772
 -0.18786206 -0.16517645 -0.14688948 -0.08380591 -0.04005814 -0.00448239
  0.02125225  0.02372309  0.04330348  0.0709786   0.07553126  0.07662885
  0.10338273  0.10356763  0.11604264  0.11999216  0.1241977   0.12719506
  0.14068104  0.14102355  0.14505905  0.1540924   0.15874226  0.15999593
  0.16031228  0.16135147  0.16232583  0.16458831  0.16473908  0.16571599
  0.17340688  0.17952491  0.18359781  0.18503832  0.18623298  0.18672808
  0.18881302  0.19249899  0.19267569  0.1946815   0.19476261  0.19949249
  0.20000918  0.20175381  0.20280571  0.20710966  0.2157884   0.21708049
  0.2191776   0.21973637  0.21982091  0.22119713  0.22243484  0.22443534
  0.22643384  0.22749138  0.22860573  0.2325713   0.23277061  0.23541846
  0.23549136  0.23853315  0.23869463  0.2405921   0.24065688  0.24320639
  0.24322132  0.24342744  0.24459623  0.24774478  0.24779208  0.2481148
  0.24824068  0.25136634  0.25256218  0.25261197  0.25281521  0.25324136
  0.25449153  0.25488607  0.25580673  0.25608386  0.25712793  0.2595164
  0.26112881  0.26286558  0.26325697  0.26353501  0.26436498  0.26608131
  0.26655176  0.26783682  0.2696009   0.27032145  0.27034555  0.27044203
  0.27095939  0.27238098  0.27284223  0.27558105  0.27728976  0.27753638
  0.27869551  0.27955982  0.28018593  0.28060053  0.28119514  0.28268721
  0.282859    0.28332022  0.28402988  0.28480613  0.28546211  0.28549196
  0.28650053  0.28781687  0.28793453  0.28914477  0.29180416  0.29450022
  0.2948403   0.29524978  0.29676067  0.29682725  0.29705566  0.29735555
  0.29755529  0.29974094  0.30143192  0.30245402  0.30247274  0.30313988
  0.30453661  0.30474875  0.30593981  0.30677767  0.30870298  0.30881064
  0.3088274   0.30961326  0.31163279  0.31224999  0.31278855  0.31424541
  0.31443234  0.31530146  0.31543769  0.31546162  0.31661261  0.3171257
  0.31732583  0.3182638   0.32040045  0.32133473  0.32242321  0.32425895
  0.32610929  0.3262021   0.32643825  0.32680861  0.32870924  0.32965889
  0.32976931  0.32987434  0.33088922  0.33332625  0.3333389   0.33416345
  0.33516516  0.33540837  0.33542195  0.33636557  0.33685834  0.33715668
  0.33760065  0.33833001  0.33890728  0.33973435  0.33993323  0.34005312
  0.34047984  0.34125822  0.34225294  0.34321037  0.34445026  0.34460426
  0.34547874  0.34549347  0.3455912   0.34573359  0.3462022   0.34751411
  0.34891674  0.35108395  0.3511297   0.35117584  0.35198412  0.35243644
  0.35295734  0.35307571  0.35350711  0.35372173  0.35376534  0.35400469
  0.35420472  0.35561131  0.35590274  0.35621407  0.35644596  0.35651604
  0.3574543   0.35839882  0.35902469  0.36001987  0.36053098  0.3613415
  0.36164064  0.362688    0.36273803  0.36409052  0.3646359   0.36580315
  0.36728314  0.36804987  0.3685228   0.36947764  0.37009459  0.37127581
  0.37181819  0.37272167  0.37399051  0.37430092  0.37465202  0.37778656
  0.37788004  0.37831055  0.38035442  0.38101848  0.38108453  0.38124549
  0.38182749  0.38284276  0.38337605  0.38377627  0.38513473  0.38687263
  0.38705679  0.38734051  0.38846438  0.39104952  0.39216233  0.39375365
  0.39424167  0.39546889  0.39625894  0.39688631  0.39830052  0.39891115
  0.40168756  0.40179182  0.40217558  0.40264239  0.40268493  0.40331508
  0.4034817   0.40650615  0.40697684  0.40746486  0.40764088  0.40940135
  0.41124649  0.41204357  0.41384848  0.41444119  0.41551791  0.41633993
  0.41865442  0.41998981  0.42188996  0.4250019   0.42876614  0.43069886
  0.43105449  0.43428012  0.43452141  0.43569456  0.43789149  0.43838439
  0.44209567  0.44625054  0.44960676  0.45267189  0.45403136  0.45666809
  0.46285443  0.46388885  0.46411607  0.46528541  0.46562173  0.46750222
  0.46765569  0.46796348  0.46917211  0.4694428   0.47015967  0.47064228
  0.4795525   0.48524537  0.48897399  0.48974921  0.49469714  0.50089745
  0.5035524   0.50769368  0.5085044   0.51006409  0.51524094  0.51803263
  0.52112283  0.52265813  0.52969549  0.53388749  0.53430328  0.53671731
  0.53974391  0.54086825  0.54504344  0.54978995  0.55066786  0.55071968
  0.55073264  0.55322927  0.55323834  0.55796515  0.55990175  0.56202536
  0.56787261  0.56797407  0.56901327  0.56995927  0.57145529  0.57260391
  0.58047903  0.58053782  0.58191991  0.58340747  0.59073691  0.59323709
  0.59376101  0.5951431   0.6003109   0.60043237  0.6069842   0.60962883
  0.61211366  0.61227347  0.61457461  0.61756275  0.61894593  0.62285202
  0.62549666  0.62978476  0.63145962  0.63160972  0.63524872  0.63592739
  0.63607521  0.63947061  0.64121667  0.64137873  0.64665376  0.64744449
  0.6488991   0.6492984   0.64931265  0.65145502  0.65194303  0.65282362
  0.65376118  0.65458767  0.65645231  0.65987695  0.66252159  0.66516622
  0.66948388  0.6703667   0.6713107   0.67207012  0.67310014  0.67350865
  0.67432126  0.67448332  0.67491828  0.6749666   0.67574477  0.67838941
  0.67884767  0.68103405  0.68607711  0.68896796  0.68898221  0.69125273
  0.69470873  0.69486018  0.69502225  0.69533584  0.69651058  0.69675406
  0.69690187  0.69778246  0.69801218  0.69956076  0.70000477  0.7022054
  0.70468797  0.70484562  0.70485004  0.70748042  0.70749467  0.70750285
  0.70918061  0.70997725  0.71213592  0.71278395  0.71541434  0.71612865
  0.71629492  0.71708853  0.71807322  0.7205313   0.7219786   0.72287448
  0.7229712   0.72320043  0.7233625   0.72402194  0.72645115  0.72687348
  0.72865177  0.7299465   0.73054213  0.73129641  0.73174042  0.73207289
  0.73239604  0.73377898  0.73473178  0.73658569  0.73745203  0.73870451
  0.74009666  0.74534532  0.74538594  0.74692941  0.74700217  0.7475569
  0.74882301  0.75040819  0.75311826  0.75331985  0.75860913  0.76098674
  0.76125376  0.76238198  0.76380854  0.76518868  0.76551464  0.76704728
  0.76765691  0.76815927  0.76918768  0.77183231  0.77447695  0.77513639
  0.77518346  0.77787235  0.78770014  0.79034478  0.79298942  0.7978496
  0.80092333  0.80356797  0.8062126   0.81152099  0.8120135   0.81414652
  0.81479113  0.81482592  0.81943579  0.81989291  0.82311941  0.82423185
  0.82457725  0.82760599  0.82782796  0.83530362  0.83794826  0.84063267
  0.84803879  0.85068343  0.85332806  0.86068852  0.86417283  0.86424681
  0.86831799  0.87938482  0.89379274  0.89515866  0.90005364  0.90269828
  0.91062777  0.91230521  0.91264099  0.91592146  0.92106292  0.92370756
  0.92650001  0.92695141  0.92836647  0.93164147  0.93178929  0.93254005
  0.93472036  0.93707856  0.93957539  0.94486466  0.94501248  0.94934305
  0.95030175  0.95279857  0.95808785  0.9608803   0.96352494  0.9686664
  0.9714731   0.97230538  0.97552719  0.97660031  0.97842115  0.98188959
  0.98371042  0.98420827  0.98453423  0.98644905  0.98717886  0.98778494
  0.98792962  0.9898235   0.99230304  0.99321527  0.99462476  0.99511278
  0.99672567  1.00416056  1.00569133  1.00784795  1.01362524  1.0179429
  1.02288928  1.02420379  1.0393418   1.03995274  1.04967855  1.3159016
  1.44084446]

  UserWarning,

2022-10-31 11:01:43,681:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390677 -0.9404312  -0.590322   -0.54798138 -0.29263285 -0.20438721
 -0.1866832  -0.13798057 -0.12398716 -0.12304452 -0.1198953  -0.08278575
 -0.07046144 -0.05659303 -0.03509535 -0.02298702 -0.00978239 -0.00867918
 -0.00382677 -0.0026685   0.01637773  0.02021382  0.02536482  0.0286603
  0.02940902  0.03994095  0.0466716   0.0482134   0.04841486  0.05029733
  0.05073295  0.0719822   0.07856243  0.07874466  0.07996084  0.08056576
  0.08314081  0.08389327  0.09049631  0.09099067  0.09817093  0.101219
  0.10143571  0.10146786  0.10982714  0.11249419  0.14611001  0.15545589
  0.15665583  0.15985543  0.16139877  0.16279983  0.16686824  0.17524696
  0.18183328  0.18679973  0.18830515  0.19158659  0.19190296  0.19252427
  0.1926502   0.1951644   0.20279927  0.20425062  0.20733099  0.2074614
  0.21343426  0.21642321  0.22152616  0.22450732  0.2254441   0.2264667
  0.22749343  0.22963944  0.23249826  0.23352633  0.23594711  0.2364368
  0.23653825  0.23672313  0.24006843  0.24218397  0.24388555  0.24753795
  0.24923427  0.2493424   0.24944902  0.24982936  0.25171546  0.25178211
  0.25209328  0.25342516  0.25419332  0.25434203  0.25640774  0.25774491
  0.25815876  0.25831133  0.25931453  0.26218984  0.26220736  0.26361477
  0.26420538  0.2652288   0.26639281  0.26674639  0.26805315  0.2686495
  0.27090326  0.2751332   0.27573181  0.27579385  0.27583188  0.27714872
  0.27715522  0.27716429  0.27779335  0.27802854  0.27837289  0.27848577
  0.27910157  0.27930592  0.28126373  0.28155147  0.28181826  0.28260855
  0.28266623  0.28311093  0.28351711  0.28357219  0.28387128  0.2844636
  0.28725363  0.28759254  0.28846417  0.28895381  0.2890973   0.28915778
  0.28940858  0.28968461  0.29104614  0.29139901  0.29168013  0.29205209
  0.29239633  0.29287621  0.29320078  0.29656362  0.29709445  0.29730662
  0.29770865  0.29779807  0.2980921   0.30065335  0.30320717  0.30372513
  0.3060367   0.3065846   0.30782986  0.30783688  0.31016565  0.31217423
  0.31231064  0.31380226  0.31474381  0.31477784  0.31552171  0.31572159
  0.31644943  0.31673138  0.31727018  0.3180071   0.31925789  0.3198505
  0.32070667  0.32086699  0.32321097  0.32406432  0.32537297  0.32637709
  0.32709331  0.32934075  0.32943596  0.33042909  0.33074056  0.33403231
  0.33505735  0.33842389  0.33963374  0.34179339  0.34190047  0.342241
  0.34360276  0.34361855  0.34398     0.34737001  0.34746404  0.34843494
  0.34997945  0.35064859  0.35092599  0.35175     0.35408262  0.35469468
  0.35636466  0.3570634   0.35712433  0.35920264  0.35942988  0.36116294
  0.36166034  0.36407137  0.36737264  0.36947401  0.37051185  0.37078933
  0.37172624  0.37224861  0.37288026  0.37308115  0.37317483  0.37404887
  0.37503896  0.37512675  0.37597968  0.37657759  0.37766158  0.3779937
  0.37940359  0.38082543  0.38317591  0.3837317   0.38419512  0.38430621
  0.38567953  0.38619893  0.38629087  0.38655359  0.38674364  0.38970153
  0.38992702  0.39066709  0.39086599  0.39245987  0.39273984  0.39284276
  0.39371492  0.39417023  0.3946333   0.3948346   0.39591428  0.39797407
  0.39868131  0.39994992  0.40313597  0.4038835   0.40410309  0.40442945
  0.40470724  0.40652781  0.40849959  0.40871711  0.40880784  0.41013149
  0.4102992   0.4106173   0.41147821  0.41210082  0.41233539  0.41516063
  0.41838704  0.41960785  0.41970929  0.42105177  0.42106439  0.42210204
  0.42241033  0.42271587  0.42339005  0.42386382  0.42585448  0.42815084
  0.43081642  0.43255117  0.43260956  0.43425387  0.43530128  0.43775747
  0.44180064  0.44226667  0.44407468  0.44410794  0.44557073  0.44797072
  0.4479862   0.44862344  0.44888002  0.45467508  0.4551799   0.4567633
  0.45716664  0.45868611  0.45900235  0.46025031  0.46066686  0.46270305
  0.46366913  0.46405048  0.46699396  0.46817765  0.46830359  0.46920927
  0.47093077  0.47318923  0.47409716  0.47547221  0.47787205  0.48313319
  0.48640268  0.48737567  0.48822826  0.48912915  0.49198735  0.49322307
  0.49410273  0.49538602  0.49756572  0.5007252   0.50188517  0.50568469
  0.5103218   0.5135527   0.52288708  0.53062247  0.53311624  0.53575509
  0.53654311  0.53677499  0.53876682  0.54807891  0.55128789  0.55524213
  0.55556024  0.55649443  0.55711916  0.56127471  0.56709427  0.56985869
  0.5713711   0.57491537  0.57559213  0.57777783  0.58034736  0.58455071
  0.58704449  0.59173113  0.59475049  0.59701959  0.59951337  0.60073038
  0.60418282  0.60420954  0.60450093  0.6069947   0.6075276   0.6091971
  0.61198226  0.61590775  0.61732749  0.61788447  0.61946359  0.61950477
  0.62168996  0.62195737  0.62249575  0.62569813  0.62997159  0.63003628
  0.63193247  0.63766504  0.63809056  0.63912242  0.6394138   0.6416162
  0.64190758  0.64439034  0.64440136  0.64938891  0.65095373  0.65188269
  0.65458024  0.65464711  0.65687024  0.65711176  0.65804077  0.65936402
  0.66053455  0.66062203  0.6618578   0.66228401  0.66435158  0.66462222
  0.66488447  0.66655397  0.66684535  0.6684748   0.66933913  0.67361279
  0.67432668  0.67510811  0.67801215  0.67931424  0.68070742  0.68151663
  0.68211188  0.68213905  0.68375307  0.68928935  0.68981122  0.69178312
  0.69398552  0.69493568  0.69532028  0.69607114  0.69897307  0.70146685
  0.70175823  0.70301624  0.70455587  0.7064544   0.70674579  0.70704965
  0.70756749  0.70782812  0.70828617  0.70894818  0.70923956  0.71049758
  0.71144196  0.71292503  0.71464777  0.71797891  0.72237504  0.72598916
  0.72708082  0.72949364  0.73107407  0.73326735  0.73674274  0.7378628
  0.73949593  0.7404229   0.74196253  0.7428963   0.7441317   0.74541045
  0.74631679  0.74695008  0.74944386  0.75007991  0.75310816  0.75538556
  0.75730837  0.75866171  0.75941897  0.75949588  0.75986868  0.76280559
  0.76440652  0.7669003   0.76692747  0.76818549  0.76882112  0.76939407
  0.77017255  0.77064256  0.77090432  0.77188785  0.7751601   0.7766112
  0.77809241  0.77999391  0.78435674  0.78685051  0.78983437  0.79138078
  0.79682562  0.7993194   0.80057741  0.80250871  0.80650935  0.80680073
  0.80929451  0.80973072  0.81423893  0.82035195  0.8272934   0.82743799
  0.82969836  0.83228096  0.83798407  0.8451003   0.84665444  0.84670116
  0.84846272  0.85168872  0.85253248  0.85721873  0.85775861  0.8597125
  0.86719384  0.88041837  0.89267148  0.89462538  0.89465574  0.89975753
  0.90046464  0.90460049  0.9058585   0.90632902  0.90803418  0.90958805
  0.91446061  0.91583361  0.92205693  0.92704449  0.92801183  0.93203204
  0.93256493  0.93701959  0.93827761  0.94077138  0.94200715  0.94326516
  0.94450092  0.94574792  0.94575894  0.9469947   0.94724201  0.95600469
  0.95822782  0.9607216   0.96189213  0.96195736  0.96282009  0.96445114
  0.96513819  0.96570915  0.96763196  0.96837628  0.9694387   0.97069671
  0.9716418   0.97413487  0.97527446  0.97568426  0.97817804  0.98215489
  0.98316559  0.98524512  0.98815315  0.99064692  1.01059714  1.01309092
  1.0155847   1.0164329   1.01936097  1.02057225  1.04180496  1.06903635
  1.085963    1.09490337  1.18722084  1.64802087]

  UserWarning,

2022-10-31 11:01:43,693:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04602623 -0.84332998 -0.75815329 -0.72378269 -0.65507834 -0.57382694
 -0.51062582 -0.41031553 -0.39184235 -0.38498398 -0.35385629 -0.35319306
 -0.28530344 -0.28401411 -0.26001254 -0.24322189 -0.2379901  -0.22543094
 -0.18854615 -0.15641655 -0.13776864 -0.10165344 -0.08344072 -0.08151435
 -0.04285185 -0.03609068 -0.02911575 -0.02763678 -0.02334932 -0.00797357
 -0.00779856 -0.00316579  0.01604145  0.04585153  0.0459524   0.07021959
  0.07819721  0.07943861  0.08137006  0.09220573  0.09998941  0.10073869
  0.10184522  0.10397276  0.11101953  0.1121763   0.11357561  0.12020607
  0.12799487  0.14236152  0.15393916  0.15488832  0.16166636  0.16417037
  0.16570563  0.17273235  0.17380542  0.1738621   0.17461713  0.17496176
  0.17618227  0.17814193  0.18168691  0.18827384  0.19310441  0.19312773
  0.19396588  0.19761632  0.19763355  0.20016568  0.20069067  0.20082482
  0.20154775  0.20187227  0.20309592  0.21412437  0.21971231  0.21993504
  0.22045778  0.22116935  0.22349152  0.22352586  0.22408235  0.22513635
  0.22743463  0.22800574  0.23074463  0.23198206  0.23486439  0.23657057
  0.23713044  0.23871501  0.24032984  0.24190612  0.24206001  0.24286801
  0.24353644  0.24383856  0.24532357  0.2456746   0.25431716  0.25680667
  0.25738799  0.2574842   0.25918734  0.25951401  0.26046965  0.26069937
  0.2629975   0.26402185  0.26462454  0.26565811  0.2681493   0.26862298
  0.26890069  0.27031524  0.27033501  0.27162932  0.27198062  0.27280418
  0.27293207  0.27341383  0.27424996  0.27531101  0.27555241  0.27565565
  0.2757062   0.27621206  0.27701716  0.27896984  0.28035345  0.28041623
  0.28131452  0.28252967  0.2830827   0.28453233  0.28511573  0.28694259
  0.28884817  0.28974463  0.29007473  0.29034247  0.29069461  0.29136547
  0.29151674  0.29522217  0.29602975  0.29665803  0.2968958   0.29776048
  0.29794264  0.29795022  0.29942936  0.30393445  0.30417144  0.30506949
  0.30730329  0.30850285  0.3094855   0.31146253  0.31155781  0.3119878
  0.3129221   0.31311156  0.313976    0.31451404  0.31755496  0.31862324
  0.31917805  0.31933174  0.31949389  0.32128953  0.32159751  0.3222298
  0.32247224  0.32399357  0.32456226  0.32611124  0.32653783  0.32729271
  0.32864592  0.32870635  0.3292738   0.33001889  0.33133016  0.33403159
  0.33589042  0.33593041  0.33674533  0.33697648  0.33779593  0.33784999
  0.33973899  0.34087472  0.34201787  0.342062    0.34231484  0.34268263
  0.34375535  0.34424387  0.34570838  0.34655725  0.3466895   0.34741895
  0.34763344  0.34776778  0.34837625  0.34902319  0.34988584  0.35239226
  0.35281542  0.35349465  0.35375421  0.35431006  0.35459479  0.35609631
  0.35788084  0.35856433  0.35956163  0.35959866  0.3596282   0.35975679
  0.36009964  0.36083063  0.36117063  0.36159746  0.36164423  0.36309858
  0.36350201  0.36514151  0.36642129  0.36689233  0.36698951  0.36731718
  0.36921405  0.37033919  0.37053969  0.37108029  0.37131573  0.37204272
  0.37264345  0.37393736  0.37577995  0.37595509  0.37744343  0.37758908
  0.37956937  0.38020713  0.38072185  0.38079394  0.38145717  0.38252642
  0.38380803  0.38491735  0.38506762  0.38743638  0.38872018  0.38906008
  0.39590066  0.39673153  0.39702288  0.39770805  0.39857193  0.39938113
  0.39979639  0.39994833  0.40045592  0.40217352  0.40282843  0.40295157
  0.40409503  0.40533449  0.4053786   0.40642468  0.40661771  0.4080029
  0.40897683  0.41021143  0.41096418  0.41261215  0.41292924  0.41397726
  0.41483073  0.41543606  0.41555885  0.41633547  0.4165261   0.41669907
  0.41688605  0.41860875  0.42147668  0.42148263  0.42273286  0.42351973
  0.42380797  0.42405456  0.42432846  0.42495421  0.42723382  0.42809443
  0.42837999  0.42853252  0.43104441  0.43136426  0.43182125  0.43214816
  0.4376696   0.43966571  0.44060406  0.44358868  0.44490992  0.44491427
  0.44692314  0.44735294  0.44758772  0.44777822  0.46157271  0.46456497
  0.46697108  0.47076902  0.47192374  0.47410238  0.47856112  0.48115575
  0.49428084  0.49928423  0.50010041  0.50754113  0.5076128   0.50907679
  0.51080837  0.5116005   0.51618256  0.51697131  0.52241067  0.52429882
  0.52520582  0.52925899  0.5332023   0.53421672  0.53648455  0.53683084
  0.53879419  0.54054697  0.54174168  0.5434914   0.54418402  0.54422307
  0.54476223  0.5453905   0.54741263  0.54809167  0.56284935  0.56342035
  0.56442506  0.57078339  0.57132369  0.5741095   0.57521532  0.57585007
  0.57790672  0.58530687  0.58635527  0.59680694  0.59871994  0.60242816
  0.60354058  0.6044011   0.60554937  0.60722203  0.61184774  0.61437448
  0.61461679  0.61533424  0.61606412  0.61955251  0.62015489  0.62043123
  0.62197977  0.62292395  0.62505825  0.62538546  0.625693    0.62617908
  0.62846205  0.6291795   0.63367032  0.63400015  0.63476976  0.6351333
  0.6367692   0.63953826  0.63995395  0.64231686  0.64299622  0.64507636
  0.64579381  0.64760331  0.64905677  0.65338351  0.65491107  0.65615256
  0.66024273  0.66296948  0.66382497  0.66445972  0.66517717  0.66589662
  0.6665729   0.66762251  0.66999782  0.67020573  0.67024775  0.67044295
  0.67063762  0.67268922  0.67276687  0.67459364  0.67553593  0.67670991
  0.67685704  0.67805624  0.67815632  0.67830498  0.6791018   0.67979204
  0.68050303  0.68107403  0.68229146  0.68447562  0.68456053  0.68540639
  0.68733158  0.68930353  0.68985554  0.69151549  0.69215024  0.69261895
  0.69563674  0.69705359  0.69768834  0.69840579  0.69982264  0.70045739
  0.70156859  0.7025917   0.70322644  0.7039439   0.7044378   0.70536075
  0.70542449  0.70599549  0.70624542  0.7081298   0.70876454  0.71121494
  0.71187499  0.71225105  0.71254532  0.71430265  0.71447492  0.71455257
  0.7150201   0.71821801  0.71857978  0.71967296  0.72020012  0.72040303
  0.72057676  0.73116337  0.73305126  0.73601702  0.73646421  0.73944326
  0.73947404  0.74083466  0.74689652  0.74778119  0.74810669  0.74824872
  0.74929128  0.75055024  0.75378682  0.75421568  0.75428073  0.7588574
  0.75903633  0.76078621  0.76162645  0.76294756  0.7699336   0.77270265
  0.7758874   0.77624132  0.77824076  0.78031339  0.78118381  0.78243853
  0.78545775  0.78654791  0.78726736  0.79208602  0.80316222  0.80510234
  0.80593127  0.80774005  0.81772493  0.82049398  0.82191084  0.82254558
  0.82531463  0.82603209  0.83130863  0.83483315  0.8376022   0.83987735
  0.84192894  0.84689605  0.8481845   0.85813944  0.85975461  0.87198469
  0.87710898  0.8830609   0.888599    0.89967521  0.90039266  0.90220216
  0.90244426  0.90316171  0.90316371  0.90521331  0.90869982  0.91075142
  0.91564644  0.91598198  0.91905857  0.9273315   0.93290383  0.93331952
  0.93411519  0.93608857  0.93757564  0.93844193  0.94121098  0.94192844
  0.94340903  0.94746654  0.94951814  0.95171619  0.95228719  0.95300464
  0.95505624  0.95577369  0.9601563   0.96059434  0.96336339  0.96726559
  0.96954287  0.96961895  0.9744396   0.97515705  0.9799777   0.98274676
  0.98900231  0.99177136  0.99730947  1.00284757  1.00318234  1.01669283
  1.02133103  1.02499998  1.03174944  1.10853919  1.52274401]

  UserWarning,

2022-10-31 11:01:43,705:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81219705 -0.51089605 -0.43866517 -0.37893118 -0.34634484 -0.3359245
 -0.25773317 -0.25435879 -0.12485832 -0.11859412 -0.09815983 -0.08087424
 -0.07552297 -0.07194131 -0.03130715 -0.03060841 -0.02587335 -0.02055669
  0.01539194  0.02053805  0.02863662  0.03610948  0.03945943  0.04566762
  0.05250905  0.05257065  0.06292916  0.06551879  0.08744528  0.09413208
  0.10170225  0.10210926  0.10658488  0.1123544   0.11434204  0.11683864
  0.11766464  0.11808358  0.12737825  0.13843173  0.14029367  0.14259209
  0.15297841  0.15754003  0.16303453  0.1665088   0.17717489  0.17959636
  0.18250296  0.19469571  0.19505191  0.19783217  0.19817061  0.19834019
  0.19963234  0.20149143  0.20283392  0.20305354  0.20421738  0.20671652
  0.20948633  0.21053412  0.2146518   0.21662185  0.21752118  0.21839719
  0.2190275   0.2228486   0.22568004  0.2263259   0.23048969  0.2311156
  0.23172223  0.23583047  0.23787254  0.23839018  0.24051224  0.24082491
  0.24107988  0.24131903  0.24369319  0.24387164  0.24398317  0.24635369
  0.24810895  0.24893144  0.25117699  0.25138521  0.25203679  0.25284204
  0.25356743  0.25457878  0.25458084  0.25519708  0.25599929  0.25760307
  0.25769993  0.25925479  0.26037581  0.26088318  0.2620578   0.26217824
  0.26395018  0.26401469  0.26429811  0.26443887  0.26493539  0.26538749
  0.26848426  0.26871262  0.26889502  0.26897605  0.26911809  0.26966634
  0.27097376  0.27104305  0.27122971  0.27265608  0.27377168  0.27486936
  0.2773301   0.27754153  0.27908151  0.28053696  0.28185057  0.28310103
  0.28409925  0.28438621  0.28465877  0.28498257  0.28537244  0.28558351
  0.28672081  0.28717727  0.28745869  0.28758295  0.28789283  0.28952915
  0.2898024   0.28985368  0.28992554  0.2903031   0.2908729   0.29100535
  0.29114302  0.29295482  0.29354277  0.29412604  0.29431757  0.29596697
  0.29807516  0.29834155  0.29864787  0.29981589  0.30036115  0.3012905
  0.30278503  0.30280127  0.30315045  0.30436978  0.30440015  0.30504682
  0.30735261  0.30750092  0.31005774  0.31157789  0.31291965  0.3149045
  0.31500822  0.31521267  0.31748521  0.31854167  0.31926676  0.31940544
  0.31992948  0.32007662  0.32142702  0.32281632  0.32388934  0.32505618
  0.325394    0.32564456  0.32675348  0.32684679  0.32709985  0.33040602
  0.33096728  0.33214233  0.33329148  0.334534    0.33795799  0.3391392
  0.3392043   0.33940811  0.33979728  0.34061624  0.34077884  0.34121875
  0.34179887  0.3421378   0.34224776  0.34346589  0.34407164  0.34534393
  0.34580396  0.3459266   0.34883301  0.34929817  0.35137195  0.35256222
  0.3536592   0.35607378  0.35731553  0.35814705  0.35829731  0.35976451
  0.36077129  0.36185894  0.36188584  0.36237111  0.36284178  0.3632492
  0.36335432  0.36444259  0.36631566  0.36656523  0.36669753  0.36719791
  0.3677919   0.36789852  0.36991664  0.3700443   0.37142498  0.37181744
  0.37274885  0.37292875  0.37293194  0.37442781  0.3745111   0.37827511
  0.37839438  0.37952488  0.37972929  0.3812321   0.38133165  0.3815468
  0.38210745  0.38217755  0.38315143  0.38382605  0.38453131  0.38831188
  0.38933519  0.38960002  0.38969699  0.38971902  0.39007773  0.39070082
  0.3913012   0.39159494  0.39188358  0.39376085  0.39383279  0.39458935
  0.39524504  0.39629964  0.39710408  0.39799278  0.39914648  0.39939704
  0.40048156  0.40060111  0.40209658  0.40343072  0.40567489  0.40594098
  0.40628968  0.4063406   0.40936457  0.40979436  0.41049896  0.41097181
  0.41272646  0.41413823  0.41480637  0.41523148  0.41898439  0.42008924
  0.42027267  0.42443198  0.42456188  0.4251781   0.4253635   0.42545647
  0.42779654  0.42909323  0.43061683  0.43080229  0.43147292  0.43310381
  0.43403205  0.43403361  0.43609901  0.43683657  0.43859755  0.43860328
  0.43937632  0.44767248  0.4481551   0.45082585  0.45300545  0.4536478
  0.45515548  0.45681604  0.45923871  0.46310204  0.46624999  0.46664597
  0.46673787  0.46837651  0.46969947  0.47617474  0.47736303  0.48154006
  0.48155882  0.48750264  0.48763206  0.48864913  0.48875542  0.49102516
  0.49120188  0.49211861  0.49333622  0.50230752  0.50237791  0.504216
  0.50647707  0.50773491  0.51100701  0.51267951  0.51398465  0.5253638
  0.52770028  0.52868426  0.53104157  0.53413811  0.53880994  0.54291528
  0.55943054  0.56025454  0.56228717  0.56575812  0.56633673  0.56764064
  0.56885492  0.56956945  0.57072201  0.57285103  0.58099625  0.58108053
  0.58165692  0.58739783  0.58742916  0.59301327  0.59600274  0.59769819
  0.59778768  0.59858569  0.60111204  0.60179756  0.60438719  0.60697682
  0.60858853  0.60898387  0.6147015   0.61545565  0.61547371  0.61733533
  0.61890164  0.62251459  0.6236705   0.62525472  0.62573124  0.6294158
  0.62990378  0.63107315  0.63463532  0.63486333  0.63546273  0.63805236
  0.6408148   0.6414142   0.64323162  0.64519901  0.64582125  0.64699062
  0.65359013  0.65403246  0.65545656  0.65617976  0.65633026  0.65843951
  0.65872519  0.65883068  0.6595416   0.66093391  0.66135902  0.66394865
  0.66396147  0.66653828  0.66666706  0.66731049  0.6691279   0.67430716
  0.67666213  0.67689679  0.67814805  0.67948642  0.68024411  0.68025863
  0.68496166  0.68542878  0.68769763  0.69061715  0.69171136  0.69392129
  0.69517469  0.6966334   0.70097566  0.70108101  0.70279308  0.7040601
  0.70611071  0.70615492  0.70655208  0.70754722  0.70797234  0.70960233
  0.71056196  0.71071247  0.71306744  0.71512566  0.71541134  0.71564714
  0.71574122  0.71589172  0.71816226  0.723085    0.72423716  0.72537653
  0.72603833  0.72796616  0.72883987  0.72953001  0.73097651  0.73165708
  0.73314542  0.73491105  0.73691397  0.73710304  0.73714872  0.73982009
  0.7409143   0.74092577  0.74146892  0.74304417  0.74350393  0.74576368
  0.74609356  0.75127282  0.75150878  0.75172141  0.75660258  0.7590417
  0.75959632  0.76209537  0.76218595  0.76317892  0.76475674  0.76907034
  0.77198985  0.77383099  0.77457948  0.77538841  0.77716911  0.77878082
  0.77942885  0.77975873  0.78234836  0.78566505  0.78752762  0.78825468
  0.78913934  0.79026775  0.79270688  0.79358673  0.79496662  0.79529651
  0.79946138  0.80306539  0.80559407  0.80565502  0.81860316  0.82548394
  0.82652255  0.82735413  0.8286318   0.83106406  0.83429144  0.83614633
  0.83682012  0.83844738  0.84148471  0.8435871   0.84416957  0.85220739
  0.85907329  0.85983152  0.86235224  0.86276967  0.8694361   0.87450994
  0.88067654  0.88233454  0.89048433  0.89052853  0.89311816  0.89322446
  0.89829742  0.90621681  0.9075952   0.91653112  0.91715188  0.92650873
  0.92678333  0.92937296  0.93163271  0.93206889  0.93245499  0.93714185
  0.93875357  0.93973148  0.93988198  0.94232111  0.94491074  0.94619257
  0.94652245  0.94761954  0.94947443  0.95008999  0.95283012  0.95785888
  0.95863109  0.96026912  0.96044851  0.96577827  0.96821739  0.97252384
  0.97811085  0.97857591  0.9864953   0.98989385  0.99167455  0.99685381
  0.99944344  1.00100975  1.00160796  1.00706182  1.00721233  1.00780073
  1.01655162  1.02016047  1.02533973  1.02731379  1.03439045  1.06508329
  1.07392711]

  UserWarning,

2022-10-31 11:01:43,707:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60470879 -0.84478708 -0.31001272 -0.23181711 -0.22925958 -0.18388184
 -0.17375493 -0.15939072 -0.0589028  -0.05433673 -0.04762999 -0.03805126
 -0.01194197 -0.00576226  0.00481885  0.01000654  0.01190334  0.02147636
  0.06059361  0.07117711  0.07460485  0.08399168  0.1019461   0.10397485
  0.1127978   0.1184939   0.1231407   0.1234011   0.14262182  0.14327143
  0.14461337  0.14687813  0.15400446  0.16326069  0.16334379  0.17621338
  0.18122406  0.18711622  0.19040683  0.19182796  0.19249545  0.1950319
  0.19532825  0.19607945  0.20023442  0.2011216   0.20186613  0.20679127
  0.20997048  0.21001827  0.21073364  0.21229152  0.21408936  0.21443799
  0.21612068  0.21706476  0.21864666  0.22136675  0.22235533  0.22350263
  0.2240505   0.22494087  0.22612033  0.22727753  0.22801278  0.22816124
  0.22965419  0.23071314  0.23470041  0.23643053  0.2382808   0.23994042
  0.24060604  0.2409866   0.24617892  0.24673872  0.24711723  0.24804804
  0.24970946  0.25070528  0.25156132  0.25192229  0.2531271   0.25439267
  0.2547662   0.25533802  0.25540359  0.25625564  0.25641315  0.25662577
  0.25845769  0.25870054  0.25872585  0.25895     0.25989356  0.26261927
  0.26270334  0.26281105  0.26314879  0.26322826  0.26515705  0.26555267
  0.26597203  0.26640316  0.26642117  0.26652057  0.26808957  0.26874787
  0.26951459  0.26972458  0.27030322  0.27047059  0.27222882  0.27341672
  0.27375312  0.27512217  0.27535875  0.27552695  0.27649965  0.27814485
  0.28280997  0.28582948  0.28622529  0.28673768  0.28696301  0.28722409
  0.28755421  0.28823392  0.28915519  0.28942761  0.29053647  0.29074591
  0.29134011  0.29170649  0.29295831  0.29417098  0.29430165  0.29505041
  0.29546911  0.29600105  0.29833401  0.29963867  0.29971372  0.30023446
  0.30137032  0.30205436  0.3020922   0.30221455  0.30302158  0.30433206
  0.30586645  0.30784663  0.30878058  0.31213021  0.31241801  0.31308032
  0.31457202  0.3146049   0.31605988  0.31804905  0.31974926  0.3217056
  0.32225099  0.32245936  0.32307597  0.3238702   0.32449181  0.32457576
  0.32580563  0.32592995  0.32709836  0.32740353  0.32766754  0.3281436
  0.32869647  0.32925215  0.32984152  0.32997867  0.33089801  0.33093212
  0.33208825  0.33237908  0.33242087  0.33249107  0.33431356  0.33434605
  0.33445376  0.33515712  0.33752799  0.33856788  0.33861026  0.34026276
  0.34041652  0.34118112  0.34166332  0.34214162  0.34276107  0.34332645
  0.34369824  0.34446564  0.34618057  0.34693592  0.34742643  0.34822122
  0.3491309   0.34965011  0.35078526  0.35079828  0.35209405  0.3521503
  0.35255261  0.35416277  0.35486102  0.35492884  0.35651903  0.35693773
  0.35694874  0.35860531  0.3586325   0.3601311   0.36077102  0.3608579
  0.36135351  0.36265732  0.36353222  0.36622361  0.36754006  0.36816123
  0.37001079  0.37153022  0.3715447   0.37260543  0.37279561  0.37519972
  0.37606024  0.37659015  0.37771838  0.37853774  0.38060026  0.38100688
  0.38279326  0.3831032   0.38323119  0.38412235  0.38412839  0.38435305
  0.38537346  0.3856215   0.38586005  0.38595411  0.38825662  0.38884622
  0.38926689  0.38932228  0.38940774  0.38964807  0.39019835  0.39279244
  0.39457328  0.39461366  0.39472921  0.39518276  0.39712245  0.3974392
  0.39769251  0.39872363  0.39996338  0.40033605  0.40054594  0.4017234
  0.40219994  0.40305786  0.40312481  0.40364     0.40472246  0.40474401
  0.4049287   0.40646747  0.40694992  0.40725682  0.40785314  0.41230995
  0.41301391  0.41365165  0.41516554  0.41696125  0.41814221  0.41842242
  0.41844063  0.42032923  0.42170758  0.42233232  0.42300205  0.42321538
  0.42482828  0.42484058  0.4250305   0.42530722  0.42820911  0.4282697
  0.43094281  0.43424308  0.43428753  0.43776209  0.43807822  0.43838693
  0.43914666  0.44176836  0.44250557  0.44299156  0.44517127  0.44664696
  0.44840203  0.45684142  0.45835847  0.46567722  0.46668409  0.47157969
  0.47493696  0.47539092  0.47594528  0.47736355  0.47806417  0.47893596
  0.48153439  0.48201992  0.48331561  0.48626294  0.48725237  0.48833257
  0.49411924  0.49762476  0.50196857  0.50270286  0.50305404  0.50930593
  0.51009982  0.51383287  0.51480715  0.5178661   0.51786661  0.51906084
  0.52100075  0.52898606  0.52954023  0.5316103   0.53773601  0.53787236
  0.5411771   0.5435626   0.54499424  0.5470858   0.55488101  0.55903737
  0.56424387  0.57283897  0.57590675  0.57767038  0.57955207  0.58055019
  0.58226181  0.58456431  0.59250176  0.59890056  0.60131368  0.60206302
  0.6058214   0.60878054  0.61081671  0.61162427  0.61401459  0.6203174
  0.62052206  0.62459431  0.62522668  0.62562317  0.62596616  0.62698462
  0.62701508  0.62835647  0.62844833  0.63074679  0.63083864  0.63191233
  0.63353162  0.63486394  0.63556034  0.63971033  0.64314593  0.64350297
  0.64368205  0.6498693   0.65177012  0.65203531  0.65464992  0.65999437
  0.6603507   0.66182087  0.66354771  0.66660149  0.66796956  0.66797619
  0.6683941   0.66850834  0.67020157  0.67089263  0.67222251  0.67377244
  0.67473027  0.67616275  0.67855306  0.68177963  0.68273598  0.68333369
  0.68524054  0.68762482  0.68837496  0.68984116  0.69001514  0.69448111
  0.6948018   0.69958243  0.7024562   0.70436305  0.70723683  0.70930646
  0.71115188  0.71153399  0.71201746  0.71354219  0.71362445  0.71388342
  0.71391828  0.71419969  0.7159325   0.71601861  0.71631462  0.71710599
  0.71844893  0.71870494  0.71971737  0.7207439   0.72209298  0.72348556
  0.72447151  0.72615126  0.72788407  0.72826619  0.73027439  0.73065048
  0.73197623  0.73208734  0.73313868  0.73983564  0.73989104  0.74049314
  0.74235206  0.74401856  0.74518366  0.74711686  0.74717459  0.7493969
  0.75039463  0.75156181  0.75170061  0.75292081  0.75554559  0.75758233
  0.75895815  0.75933424  0.75934027  0.76134847  0.76173059  0.76481068
  0.76523674  0.76525008  0.77042023  0.77569035  0.77613793  0.7774172
  0.78047098  0.78246366  0.78286129  0.78525161  0.78715846  0.78764192
  0.78880784  0.78904775  0.79264191  0.79441522  0.79481286  0.79594645
  0.79838526  0.79959349  0.80198381  0.80375951  0.80644374  0.80777289
  0.80978215  0.81258498  0.81822651  0.818716    0.82061682  0.82110632
  0.82349663  0.83305789  0.83581436  0.85017606  0.85042245  0.85288212
  0.85856677  0.86916085  0.87450022  0.88044677  0.88379472  0.88406148
  0.89295926  0.89522973  0.89791387  0.90142605  0.90460867  0.90974032
  0.91035493  0.91513556  0.92367457  0.92947744  0.92992502  0.93186776
  0.93425807  0.93664839  0.9390387   0.94381933  0.94512243  0.94923233
  0.95229338  0.9552814   0.9557709   0.95816121  0.95946432  0.96006203
  0.96294184  0.9720136   0.9741079   0.97440391  0.97918454  0.98202243
  0.98396517  0.98575777  0.98606165  0.99087434  0.99352643  0.9990448
  0.99991404  1.00069737  1.00181923  1.00308768  1.00567291  1.00786831
  1.00996261  1.02393704  1.02762319  1.02859781  1.02905495  1.0315544
  1.05055272  1.05630696  1.06108759  1.06846241  1.11726747  1.26569606
  1.29631179  2.53663387]

  UserWarning,

2022-10-31 11:01:43,707:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86838184 -0.4162217  -0.34501471 -0.30478228 -0.1980063  -0.13967709
 -0.1284054  -0.06729697 -0.03472098 -0.0340365  -0.0263516  -0.01094653
  0.00368368  0.01329788  0.02390228  0.03834598  0.0394689   0.0411893
  0.04390586  0.05374874  0.05378996  0.05437416  0.06622262  0.08364501
  0.08592252  0.09823717  0.10052055  0.10327234  0.10686868  0.10845667
  0.10871827  0.12183695  0.12852057  0.13017266  0.13246469  0.13543075
  0.13742164  0.14068731  0.14243097  0.14966285  0.1514042   0.15235997
  0.15835264  0.16182235  0.16206588  0.16607312  0.16693317  0.16907892
  0.17143992  0.17191538  0.17529989  0.18353606  0.1848029   0.18592183
  0.18612489  0.18742978  0.19267727  0.19314548  0.19346119  0.19391005
  0.19686382  0.19858248  0.20074874  0.20136356  0.20293017  0.20784257
  0.21074845  0.21236285  0.2157182   0.21710681  0.21932425  0.22058348
  0.22071201  0.22171023  0.22453284  0.22726721  0.23147221  0.23619296
  0.23666677  0.2367486   0.23713181  0.24564271  0.24637143  0.24705837
  0.24716517  0.24733782  0.24876559  0.25070208  0.2523058   0.25257604
  0.25349147  0.25552383  0.25631956  0.25642644  0.25708058  0.25713867
  0.25725298  0.25973376  0.2612723   0.26197751  0.26299295  0.26367214
  0.26529006  0.26661749  0.26735088  0.26841313  0.27099379  0.27200248
  0.27216762  0.27328023  0.27386172  0.27444797  0.27477828  0.27491115
  0.27590316  0.2764938   0.27735432  0.27742492  0.27804886  0.28098029
  0.2812084   0.28203981  0.28384907  0.28405936  0.28418944  0.28442383
  0.28582102  0.28626392  0.28627727  0.28712702  0.28758147  0.28853118
  0.28963685  0.29017489  0.2910957   0.29113461  0.29373926  0.29559755
  0.29586983  0.29646019  0.29653764  0.29792669  0.29798855  0.29841489
  0.29881307  0.2989323   0.29897531  0.29927547  0.30023952  0.30118086
  0.30119702  0.3014272   0.30217187  0.30292266  0.30457278  0.30509954
  0.30524606  0.30716968  0.30943471  0.30975939  0.30978325  0.30998422
  0.31061164  0.31212364  0.31267834  0.31279543  0.31442427  0.31531979
  0.31794945  0.31966574  0.32078111  0.32403136  0.32475517  0.32597118
  0.32665424  0.32674068  0.32746541  0.32869246  0.32914194  0.33016377
  0.33173065  0.33179249  0.33210058  0.3334353   0.33348665  0.33420218
  0.33424985  0.33623247  0.33653521  0.3369212   0.33728725  0.3377365
  0.33990904  0.3436232   0.34405918  0.34530494  0.3459732   0.34665775
  0.34760875  0.34811931  0.34838183  0.35049004  0.35263057  0.35281272
  0.35291618  0.35331679  0.35430184  0.3544011   0.35482738  0.35519122
  0.35590937  0.35681416  0.35770588  0.35881161  0.35951474  0.3596768
  0.3609671   0.36155881  0.36518572  0.36734833  0.3678763   0.36886878
  0.36982598  0.37081889  0.37117558  0.37134577  0.37182285  0.37235353
  0.37439159  0.37558801  0.37589703  0.37659766  0.37701099  0.37733022
  0.37798038  0.37855727  0.37885702  0.38197758  0.38222373  0.38302288
  0.38373069  0.38410367  0.38507687  0.38564481  0.3858884   0.38615843
  0.38643171  0.38704934  0.38732772  0.38770219  0.3882477   0.38876961
  0.38963771  0.39159672  0.39167262  0.39243984  0.39248252  0.39260276
  0.39287303  0.39320678  0.3939828   0.39401     0.39483049  0.39530889
  0.39547806  0.39677376  0.39830616  0.39844022  0.3994639   0.39959623
  0.39975912  0.40074745  0.4027579   0.4037171   0.40497733  0.40729427
  0.40772722  0.40822889  0.40870319  0.40872175  0.40873924  0.41263459
  0.41459038  0.41519151  0.41550086  0.4158436   0.41916654  0.42148332
  0.42186905  0.4219193   0.42235729  0.42247928  0.42300731  0.42428966
  0.42447559  0.42456296  0.42540791  0.42790243  0.43025818  0.43231818
  0.43322566  0.43414956  0.43643378  0.43874422  0.44047568  0.4437993
  0.4502761   0.45261014  0.45498038  0.45566046  0.45641913  0.45667953
  0.45742384  0.45805621  0.45905775  0.45993453  0.4623745   0.46251939
  0.46259475  0.46341151  0.46437306  0.46439001  0.46511071  0.46583597
  0.46726244  0.46769118  0.46827309  0.46845944  0.46918477  0.47308764
  0.47333401  0.48026067  0.48233978  0.48731566  0.49028506  0.49375778
  0.49492652  0.4997721   0.50330516  0.50340724  0.50546938  0.50554842
  0.50608354  0.50657388  0.50818595  0.50891486  0.50923587  0.51197998
  0.51225881  0.5291019   0.52932203  0.53050889  0.5312162   0.53918324
  0.54764492  0.54961631  0.54982444  0.54989238  0.55133082  0.55202315
  0.55375432  0.55438344  0.55784819  0.56048257  0.56091576  0.56152305
  0.56253724  0.56287018  0.56345738  0.56432106  0.56526344  0.56598217
  0.56689217  0.56758693  0.5754692   0.57612056  0.58024547  0.5842794
  0.58482412  0.58602699  0.586598    0.58698223  0.58935932  0.5908411
  0.59514566  0.59719917  0.59786223  0.60057879  0.60870813  0.60872849
  0.60916717  0.61208692  0.61687818  0.61959475  0.62221127  0.62469955
  0.62629174  0.63172487  0.63251452  0.6344277   0.63552957  0.63589414
  0.63649552  0.63821275  0.63861071  0.64066422  0.64132727  0.64175131
  0.64364588  0.64517241  0.64636244  0.64947697  0.65194661  0.65219354
  0.6549101   0.66034323  0.66076072  0.66216618  0.66266184  0.6630598
  0.6651296   0.66577636  0.66775724  0.67054497  0.67081153  0.67120949
  0.6735281   0.67392606  0.67465667  0.675984    0.6778435   0.67935919
  0.68045047  0.68124461  0.68227917  0.68371399  0.68439436  0.68474693
  0.68711093  0.68726196  0.68858676  0.68982749  0.69115237  0.69254406
  0.6952254   0.69526062  0.69627074  0.69772839  0.69797719  0.70069375
  0.70314522  0.70334475  0.7033488   0.70578916  0.70587808  0.70612688
  0.70652484  0.70859465  0.70884345  0.71156001  0.71247052  0.71402778
  0.71424136  0.71467454  0.71674434  0.7167482   0.71699315  0.71700678
  0.72217747  0.72327345  0.7324171   0.73386266  0.73783947  0.74129947
  0.74452215  0.7451871   0.74653595  0.746889    0.74934313  0.74959193
  0.74960556  0.75123346  0.75169304  0.7517104   0.75177556  0.75329774
  0.75439193  0.75477626  0.75730844  0.76036525  0.76082627  0.76494712
  0.76590496  0.76859643  0.77405465  0.77483316  0.78220435  0.78227098
  0.78268451  0.78492091  0.79035405  0.79307061  0.79578718  0.79850374
  0.80078712  0.80122031  0.80393687  0.80544566  0.80937     0.81095021
  0.82023627  0.82246376  0.827988    0.82838596  0.831912    0.83568303
  0.83610247  0.83750372  0.83771686  0.83925222  0.84132203  0.84196879
  0.84468535  0.84619414  0.85380311  0.85410471  0.857835    0.86055156
  0.86175828  0.8630246   0.86370131  0.8637268   0.87120425  0.87353185
  0.8864019   0.88765782  0.88845796  0.89001433  0.89231683  0.90531182
  0.90541785  0.90760288  0.90904492  0.918265    0.91835392  0.92171725
  0.93193674  0.93465331  0.93486689  0.93530007  0.94164371  0.94280301
  0.94344977  0.94616634  0.95135254  0.95159947  0.96246573  0.96305529
  0.96444661  0.96807964  0.97424249  0.97540179  0.97666565  0.98371968
  0.98626805  0.99170119  0.99441775  0.99985088  1.00256745  1.00528401
  1.0088195   1.01071714  1.01594441  1.02115022  1.02158341  1.02712257
  1.02919237  1.0334274   1.05036606  1.07234552  1.07777865  1.08903399
  1.42095824  1.66752361]

  UserWarning,

2022-10-31 11:01:46,855:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4051563  -0.40513932 -0.35103044 -0.2330514  -0.21247933 -0.18264918
 -0.13817349 -0.12681862 -0.09967875 -0.03978759 -0.02461572 -0.02023458
 -0.01592225 -0.00524282  0.0222814   0.03979957  0.04002484  0.04368484
  0.05127418  0.05165241  0.07441814  0.08253408  0.0862313   0.089347
  0.10093851  0.10241407  0.10688991  0.11597369  0.12625719  0.12801135
  0.13247995  0.13357617  0.13383359  0.13594837  0.14739853  0.15127366
  0.15286655  0.15316628  0.15611028  0.16050176  0.16194962  0.16348284
  0.16542811  0.18669191  0.19146363  0.19262117  0.19475407  0.1948737
  0.19910781  0.20209062  0.20292288  0.20647704  0.20704483  0.20799744
  0.20845553  0.21020249  0.21329357  0.21357014  0.21434883  0.21581551
  0.21586655  0.21914479  0.21979914  0.22149076  0.22152114  0.22227012
  0.22232146  0.22291616  0.22428778  0.22864595  0.22884517  0.22894572
  0.23085042  0.23382831  0.23397248  0.23468876  0.23572877  0.23590029
  0.23632385  0.23652307  0.24145602  0.24183377  0.24215233  0.24332384
  0.24568205  0.24617115  0.24823232  0.24964627  0.25136275  0.251919
  0.25493376  0.25713651  0.25828862  0.25918749  0.2635571   0.26665266
  0.26726465  0.26802519  0.26902843  0.26923876  0.26929472  0.27100927
  0.27122636  0.27210955  0.27326415  0.2734712   0.2736799   0.2749884
  0.27675862  0.27788084  0.27969626  0.2819594   0.28215278  0.28233622
  0.28297377  0.28463041  0.28623232  0.28654467  0.28694381  0.28717355
  0.28759229  0.28791311  0.28961843  0.28964992  0.2898756   0.29030085
  0.29059693  0.29082643  0.29163415  0.29163953  0.29202848  0.29338239
  0.2939855   0.29655731  0.29656512  0.29658344  0.29716614  0.29737117
  0.29761043  0.29816707  0.29827158  0.29904856  0.29907447  0.29995422
  0.30065878  0.30130793  0.30192443  0.30215546  0.3022679   0.30314669
  0.30335369  0.30368624  0.30397468  0.30411493  0.30469389  0.30476432
  0.30530817  0.30626912  0.30705926  0.3083699   0.30951013  0.3100403
  0.31081157  0.31245291  0.31320944  0.31370859  0.31395819  0.31407625
  0.31654586  0.31742393  0.31751827  0.32014082  0.32077153  0.32269314
  0.32425021  0.32469806  0.32490107  0.32511623  0.32586379  0.32623024
  0.32658796  0.32692621  0.32845691  0.32877224  0.32987487  0.33019465
  0.33151717  0.33377394  0.33414388  0.33498874  0.33629318  0.3363925
  0.33691417  0.33753388  0.33757744  0.33787672  0.33935184  0.34125655
  0.34192837  0.34227128  0.34283387  0.34547919  0.34717693  0.34779053
  0.34782649  0.34807262  0.34879075  0.3489736   0.3497017   0.34976122
  0.34990619  0.34993565  0.35243988  0.35391699  0.35410907  0.35435556
  0.35485313  0.35512392  0.3556231   0.35615433  0.35694377  0.35727452
  0.35970756  0.36125935  0.36201449  0.36285703  0.36318316  0.36340267
  0.36416031  0.36447428  0.36489656  0.36510507  0.36525511  0.36720818
  0.36732218  0.36749937  0.37103736  0.3711649   0.37198539  0.37308854
  0.37389575  0.37412984  0.374578    0.37611352  0.37613771  0.37806883
  0.37985751  0.38017507  0.38086249  0.38097032  0.38309973  0.3835084
  0.38482274  0.38604648  0.38667511  0.38743639  0.38934425  0.39004224
  0.39026359  0.39106707  0.39112265  0.39121828  0.39140443  0.3922089
  0.39410376  0.39413604  0.3952324   0.39619881  0.39732136  0.39740915
  0.39743531  0.39873689  0.39893662  0.40127497  0.40280145  0.40331907
  0.40375748  0.40381306  0.40420302  0.40434282  0.40586565  0.40635114
  0.40705802  0.40888922  0.40975244  0.41026948  0.41199583  0.41578098
  0.41874656  0.41940299  0.42168507  0.42221054  0.42237937  0.42385883
  0.42501109  0.42658712  0.43001543  0.43168059  0.43402432  0.43425835
  0.43662006  0.44101641  0.44432751  0.44641932  0.4471961   0.44973419
  0.4501304   0.45194175  0.45227227  0.45275831  0.45481035  0.45734843
  0.4596998   0.46202019  0.46209408  0.46435636  0.46482729  0.46717024
  0.47115615  0.47393034  0.47405777  0.47474419  0.48148677  0.48350206
  0.48442394  0.49077086  0.49198973  0.49289988  0.4983851   0.51046369
  0.51099674  0.51366113  0.513837    0.51769008  0.51816754  0.52127698
  0.5215249   0.52258466  0.53138017  0.53391825  0.544015    0.54407058
  0.54733618  0.55003868  0.5551812   0.56030459  0.5623855   0.5704556
  0.57464328  0.57724513  0.58201411  0.58491554  0.5865934   0.58703272
  0.58724828  0.59247363  0.59447058  0.59729193  0.60056809  0.60164453
  0.60593644  0.60607118  0.60661231  0.607133    0.60943807  0.61168848
  0.61218967  0.61531636  0.61785444  0.61868269  0.62039253  0.62293061
  0.62565036  0.62883502  0.63054485  0.63456417  0.63562102  0.64069718
  0.64323526  0.64454142  0.64577334  0.64829514  0.64831143  0.65084951
  0.65284646  0.65338759  0.65794527  0.66096883  0.66161449  0.666078
  0.66694823  0.66852122  0.66861608  0.67047418  0.67115416  0.67138982
  0.67181718  0.67369224  0.67623033  0.67822728  0.67876841  0.67973995
  0.68003028  0.68107264  0.68133683  0.68467282  0.68487742  0.68584152
  0.68892074  0.69228707  0.69345577  0.69549443  0.69575048  0.69647941
  0.69653498  0.69663329  0.697198    0.69736323  0.69853193  0.69907306
  0.70107001  0.70161115  0.7036081   0.70414923  0.71176347  0.71305562
  0.71555834  0.7162985   0.71773831  0.71961337  0.71999326  0.72765498
  0.72886885  0.72892349  0.73220893  0.73266384  0.73267557  0.7353396
  0.73887939  0.73906866  0.73964937  0.74205146  0.7421256   0.74288348
  0.74720176  0.74732337  0.74795964  0.75008837  0.75475001  0.75557388
  0.7576846   0.75811197  0.76045652  0.76445     0.76750642  0.76826429
  0.76952564  0.77080238  0.7712826   0.77206372  0.7732374   0.77334046
  0.77587854  0.77841662  0.7809547   0.78335425  0.78397301  0.78603087
  0.78626652  0.78887498  0.78904917  0.79364511  0.79612762  0.79872128
  0.80040521  0.80125936  0.80149501  0.8040331   0.80627995  0.80736741
  0.80777695  0.81137868  0.81394977  0.81418542  0.81517863  0.81565678
  0.81679873  0.81897036  0.82031808  0.82052293  0.82179967  0.82539424
  0.83052782  0.831952    0.83225061  0.83422142  0.83673693  0.83775777
  0.83933059  0.83956624  0.84718049  0.84795257  0.85179688  0.85302874
  0.85325592  0.85733281  0.86212457  0.86318107  0.86566357  0.86696508
  0.86930398  0.87229265  0.87436624  0.88327522  0.88348572  0.89461532
  0.89612865  0.89871421  0.90379037  0.90594278  0.90632846  0.90656411
  0.91140462  0.91164028  0.9139427   0.91648078  0.91901887  0.92155695
  0.9217926   0.92433069  0.92657754  0.92663311  0.93170927  0.93194493
  0.93424736  0.93663939  0.93932352  0.93955918  0.9400394   0.94439968
  0.94693777  0.94717342  0.94822018  0.94947585  0.95201393  0.95831111
  0.96216626  0.96240191  0.96470434  0.96494     0.96709637  0.9697805
  0.97020935  0.97509232  0.97736174  0.9776304   0.98016849  0.98270657
  0.98318679  0.98524465  0.98778273  0.99252838  0.9928589   0.99539698
  0.99841528  1.00047314  1.0055493   1.01167222  1.01560678  1.02098168
  1.02324721  1.02829718  1.28353226]

  UserWarning,

2022-10-31 11:01:46,922:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94547024 -0.7769212  -0.475268   -0.39286633 -0.3342177  -0.31828377
 -0.31582689 -0.30841293 -0.2684237  -0.26011261 -0.25918253 -0.255448
 -0.12385182 -0.11839672 -0.10368724 -0.09856177 -0.05762328 -0.04689406
 -0.02913981 -0.02853047 -0.00235619  0.00617414  0.0230898   0.02551127
  0.0327106   0.03591053  0.05462452  0.06077473  0.06645141  0.066876
  0.08110093  0.08489302  0.08581859  0.09230081  0.09677853  0.1026558
  0.1111432   0.12642578  0.126697    0.12749689  0.13560586  0.14893953
  0.15158257  0.1586888   0.17082699  0.17288944  0.17438129  0.17638239
  0.1770858   0.17894021  0.18470933  0.19003952  0.19523459  0.2023678
  0.20553985  0.20557671  0.20615     0.20907266  0.2092559   0.21011132
  0.21157819  0.21158038  0.21170576  0.21228063  0.21544919  0.2160408
  0.21659591  0.21695481  0.21735582  0.21854989  0.22021886  0.22436046
  0.22471874  0.2335439   0.23539492  0.23585576  0.23690381  0.23883767
  0.2393378   0.24141753  0.24151799  0.24162932  0.24448089  0.24656035
  0.24764854  0.24807353  0.24853669  0.24948812  0.24999802  0.25010911
  0.25142998  0.25452029  0.25720127  0.25745161  0.25764395  0.26281678
  0.26283238  0.26291113  0.26425001  0.26439542  0.26501076  0.26550845
  0.26629931  0.26841303  0.26895505  0.27008826  0.27043693  0.2705421
  0.27167103  0.27207276  0.27239086  0.27315774  0.27346394  0.27384822
  0.27441927  0.27587736  0.27656424  0.27657219  0.27673234  0.27849738
  0.2787583   0.2812762   0.28195562  0.28268919  0.28348847  0.28350872
  0.28397096  0.2840047   0.28447047  0.28608999  0.28614168  0.28721282
  0.2877382   0.29007026  0.29010392  0.29021936  0.2910541   0.291314
  0.291468    0.29192382  0.29487624  0.2969387   0.29724117  0.29749065
  0.29749077  0.29787786  0.29799934  0.30171722  0.30328991  0.30389433
  0.3044576   0.30630384  0.30755444  0.31122846  0.31198961  0.31234061
  0.31247107  0.31295219  0.31307725  0.31343401  0.3139147   0.31453246
  0.31753968  0.31798459  0.31805659  0.31811404  0.31914702  0.31992056
  0.3211042   0.32153237  0.32643793  0.32736012  0.32806381  0.3283722
  0.32868585  0.3312465   0.33325703  0.33335466  0.33400093  0.33525222
  0.33567382  0.33668267  0.33761054  0.33946344  0.34021865  0.3404125
  0.34171032  0.34232112  0.3424479   0.34524721  0.34615581  0.34624389
  0.34706415  0.34707618  0.34777239  0.34831392  0.34888026  0.34894465
  0.35084192  0.35127049  0.35252923  0.35296862  0.35480602  0.35947403
  0.35978647  0.36358045  0.36434788  0.36655751  0.36743964  0.37213312
  0.37255604  0.37257191  0.37446411  0.3761311   0.37662853  0.37698291
  0.37725052  0.37791427  0.37832209  0.37895605  0.37973217  0.3798117
  0.37984411  0.38028643  0.38056454  0.38132042  0.3815238   0.38225617
  0.38303407  0.38342287  0.38458412  0.38465436  0.38510293  0.38555306
  0.38646736  0.38716559  0.38728324  0.38752793  0.3882337   0.38891002
  0.39014733  0.3905364   0.39075877  0.39272507  0.39335251  0.39343333
  0.39409365  0.39424075  0.39472553  0.39530645  0.39612324  0.39754191
  0.40189592  0.40314096  0.4038326   0.40464543  0.40485059  0.40638542
  0.40643241  0.40708489  0.40821777  0.40899031  0.40903223  0.40923198
  0.41084814  0.4112559   0.41158041  0.41240366  0.41246552  0.41354546
  0.41408385  0.41549468  0.41644835  0.41702303  0.41709515  0.41913417
  0.42123369  0.42229338  0.42242483  0.42517515  0.42533601  0.42555114
  0.42894292  0.43126092  0.43381942  0.43537043  0.43631764  0.43891745
  0.44265308  0.44411709  0.44590245  0.44593936  0.44618902  0.4492295
  0.44954016  0.45346589  0.45456103  0.45523738  0.45559959  0.45566281
  0.45687339  0.45703535  0.45711617  0.45806308  0.45830616  0.46195324
  0.46421528  0.46721648  0.48241399  0.48325441  0.48661998  0.48830335
  0.48907486  0.49500913  0.4957891   0.49802634  0.49852835  0.5006127
  0.50101285  0.50301471  0.50303699  0.50451729  0.50581233  0.50919945
  0.50998364  0.5124732   0.51345032  0.51968394  0.52140135  0.5317766
  0.53255673  0.53334799  0.53701013  0.54121616  0.54942401  0.55077029
  0.55232651  0.55515458  0.55520884  0.55533708  0.55533766  0.5625152
  0.56845103  0.56921866  0.57515943  0.57519063  0.57537321  0.57676845
  0.58196808  0.5845679   0.59104095  0.59144481  0.59194911  0.59289351
  0.59424395  0.59496717  0.59533187  0.60699338  0.60722231  0.60796625
  0.61219301  0.61836551  0.62876477  0.63206841  0.63299154  0.63396441
  0.63656422  0.63916404  0.63976269  0.64792872  0.65100065  0.65152654
  0.65216312  0.65307165  0.65345882  0.65468211  0.65476294  0.65510297
  0.65736275  0.65754316  0.65996257  0.66124982  0.66360844  0.66384945
  0.66384963  0.6648008   0.6651622   0.66620825  0.66762873  0.66776202
  0.66880807  0.67036183  0.67043018  0.67274122  0.67296165  0.67380863
  0.67556146  0.67660752  0.67720617  0.6788006   0.67920733  0.67944853
  0.6807611   0.68440696  0.68464816  0.68524681  0.6877106   0.6896066
  0.69339878  0.69480623  0.69504743  0.69746916  0.69947454  0.70000586
  0.70024706  0.70066737  0.70284687  0.70291918  0.70415944  0.70432419
  0.70495086  0.70520549  0.70667844  0.70675926  0.70780531  0.71040513
  0.71447788  0.71455871  0.71541855  0.71560476  0.71584596  0.72077949
  0.7224045   0.72445291  0.73380347  0.73724121  0.73854396  0.74114378
  0.74277462  0.74412191  0.74634341  0.74646651  0.74894323  0.75040446
  0.75154304  0.75200218  0.75406204  0.75407008  0.75996201  0.76004283
  0.76454212  0.76471611  0.7653891   0.76714194  0.76999141  0.77073255
  0.77234157  0.77384942  0.77461026  0.77921848  0.77973913  0.78034553
  0.780988    0.78315373  0.78562563  0.78605641  0.78794047  0.79054028
  0.79103937  0.79644373  0.79833973  0.80113483  0.80353936  0.80613918
  0.808739    0.81133881  0.81233972  0.81314583  0.81515867  0.81653845
  0.82433789  0.82663825  0.82866353  0.83452538  0.83943682  0.83972501
  0.83993679  0.84023007  0.84203154  0.85012427  0.85347686  0.85868322
  0.86354759  0.86520024  0.86572317  0.8719136   0.88157172  0.89531194
  0.90131309  0.90361963  0.90463959  0.90472041  0.90600766  0.90732023
  0.91511968  0.91531943  0.91616573  0.92023849  0.92031931  0.92129199
  0.92291912  0.92512601  0.93323757  0.93460564  0.93851246  0.94111784
  0.94363683  0.9450049   0.94589923  0.94631747  0.94651722  0.9475239
  0.94760472  0.94891728  0.95020453  0.952278    0.95280435  0.95436657
  0.95800398  0.96007745  0.9606038   0.96191637  0.96320361  0.96572261
  0.96971581  0.97231563  0.97293863  0.97307653  0.97491545  0.97690651
  0.97880251  0.98140233  0.98400214  0.98660196  0.98860325  0.98920178
  0.99180159  0.99700122  0.99907469  1.0034326   1.00480067  1.00597288
  1.00740049  1.01217185  1.01609426  1.01677283  1.0263601   1.03027248
  1.03631909  1.07315679  1.07389955  1.08187185  1.2273629   1.25953641
  1.27264062  1.33467606  1.39584365]

  UserWarning,

2022-10-31 11:01:46,922:INFO:Calculating mean and std
2022-10-31 11:01:46,922:INFO:Creating metrics dataframe
2022-10-31 11:01:46,938:INFO:Uploading results into container
2022-10-31 11:01:46,938:INFO:Uploading model into container now
2022-10-31 11:01:46,938:INFO:master_model_container: 1
2022-10-31 11:01:46,938:INFO:display_container: 2
2022-10-31 11:01:46,938:INFO:LinearRegression(n_jobs=-1)
2022-10-31 11:01:46,938:INFO:create_model() successfully completed......................................
2022-10-31 11:01:47,055:WARNING:create_model() for LinearRegression(n_jobs=-1) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:01:47,056:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:01:47,056:INFO:Initializing create_model()
2022-10-31 11:01:47,056:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:01:47,056:INFO:Checking exceptions
2022-10-31 11:01:47,056:INFO:Importing libraries
2022-10-31 11:01:47,056:INFO:Copying training dataset
2022-10-31 11:01:47,056:INFO:Defining folds
2022-10-31 11:01:47,056:INFO:Declaring metric variables
2022-10-31 11:01:47,056:INFO:Importing untrained model
2022-10-31 11:01:47,056:INFO:Linear Regression Imported successfully
2022-10-31 11:01:47,056:INFO:Starting cross validation
2022-10-31 11:01:47,056:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:01:49,613:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86838184 -0.4162217  -0.34501471 -0.30478228 -0.1980063  -0.13967709
 -0.1284054  -0.06729697 -0.03472098 -0.0340365  -0.0263516  -0.01094653
  0.00368368  0.01329788  0.02390228  0.03834598  0.0394689   0.0411893
  0.04390586  0.05374874  0.05378996  0.05437416  0.06622262  0.08364501
  0.08592252  0.09823717  0.10052055  0.10327234  0.10686868  0.10845667
  0.10871827  0.12183695  0.12852057  0.13017266  0.13246469  0.13543075
  0.13742164  0.14068731  0.14243097  0.14966285  0.1514042   0.15235997
  0.15835264  0.16182235  0.16206588  0.16607312  0.16693317  0.16907892
  0.17143992  0.17191538  0.17529989  0.18353606  0.1848029   0.18592183
  0.18612489  0.18742978  0.19267727  0.19314548  0.19346119  0.19391005
  0.19686382  0.19858248  0.20074874  0.20136356  0.20293017  0.20784257
  0.21074845  0.21236285  0.2157182   0.21710681  0.21932425  0.22058348
  0.22071201  0.22171023  0.22453284  0.22726721  0.23147221  0.23619296
  0.23666677  0.2367486   0.23713181  0.24564271  0.24637143  0.24705837
  0.24716517  0.24733782  0.24876559  0.25070208  0.2523058   0.25257604
  0.25349147  0.25552383  0.25631956  0.25642644  0.25708058  0.25713867
  0.25725298  0.25973376  0.2612723   0.26197751  0.26299295  0.26367214
  0.26529006  0.26661749  0.26735088  0.26841313  0.27099379  0.27200248
  0.27216762  0.27328023  0.27386172  0.27444797  0.27477828  0.27491115
  0.27590316  0.2764938   0.27735432  0.27742492  0.27804886  0.28098029
  0.2812084   0.28203981  0.28384907  0.28405936  0.28418944  0.28442383
  0.28582102  0.28626392  0.28627727  0.28712702  0.28758147  0.28853118
  0.28963685  0.29017489  0.2910957   0.29113461  0.29373926  0.29559755
  0.29586983  0.29646019  0.29653764  0.29792669  0.29798855  0.29841489
  0.29881307  0.2989323   0.29897531  0.29927547  0.30023952  0.30118086
  0.30119702  0.3014272   0.30217187  0.30292266  0.30457278  0.30509954
  0.30524606  0.30716968  0.30943471  0.30975939  0.30978325  0.30998422
  0.31061164  0.31212364  0.31267834  0.31279543  0.31442427  0.31531979
  0.31794945  0.31966574  0.32078111  0.32403136  0.32475517  0.32597118
  0.32665424  0.32674068  0.32746541  0.32869246  0.32914194  0.33016377
  0.33173065  0.33179249  0.33210058  0.3334353   0.33348665  0.33420218
  0.33424985  0.33623247  0.33653521  0.3369212   0.33728725  0.3377365
  0.33990904  0.3436232   0.34405918  0.34530494  0.3459732   0.34665775
  0.34760875  0.34811931  0.34838183  0.35049004  0.35263057  0.35281272
  0.35291618  0.35331679  0.35430184  0.3544011   0.35482738  0.35519122
  0.35590937  0.35681416  0.35770588  0.35881161  0.35951474  0.3596768
  0.3609671   0.36155881  0.36518572  0.36734833  0.3678763   0.36886878
  0.36982598  0.37081889  0.37117558  0.37134577  0.37182285  0.37235353
  0.37439159  0.37558801  0.37589703  0.37659766  0.37701099  0.37733022
  0.37798038  0.37855727  0.37885702  0.38197758  0.38222373  0.38302288
  0.38373069  0.38410367  0.38507687  0.38564481  0.3858884   0.38615843
  0.38643171  0.38704934  0.38732772  0.38770219  0.3882477   0.38876961
  0.38963771  0.39159672  0.39167262  0.39243984  0.39248252  0.39260276
  0.39287303  0.39320678  0.3939828   0.39401     0.39483049  0.39530889
  0.39547806  0.39677376  0.39830616  0.39844022  0.3994639   0.39959623
  0.39975912  0.40074745  0.4027579   0.4037171   0.40497733  0.40729427
  0.40772722  0.40822889  0.40870319  0.40872175  0.40873924  0.41263459
  0.41459038  0.41519151  0.41550086  0.4158436   0.41916654  0.42148332
  0.42186905  0.4219193   0.42235729  0.42247928  0.42300731  0.42428966
  0.42447559  0.42456296  0.42540791  0.42790243  0.43025818  0.43231818
  0.43322566  0.43414956  0.43643378  0.43874422  0.44047568  0.4437993
  0.4502761   0.45261014  0.45498038  0.45566046  0.45641913  0.45667953
  0.45742384  0.45805621  0.45905775  0.45993453  0.4623745   0.46251939
  0.46259475  0.46341151  0.46437306  0.46439001  0.46511071  0.46583597
  0.46726244  0.46769118  0.46827309  0.46845944  0.46918477  0.47308764
  0.47333401  0.48026067  0.48233978  0.48731566  0.49028506  0.49375778
  0.49492652  0.4997721   0.50330516  0.50340724  0.50546938  0.50554842
  0.50608354  0.50657388  0.50818595  0.50891486  0.50923587  0.51197998
  0.51225881  0.5291019   0.52932203  0.53050889  0.5312162   0.53918324
  0.54764492  0.54961631  0.54982444  0.54989238  0.55133082  0.55202315
  0.55375432  0.55438344  0.55784819  0.56048257  0.56091576  0.56152305
  0.56253724  0.56287018  0.56345738  0.56432106  0.56526344  0.56598217
  0.56689217  0.56758693  0.5754692   0.57612056  0.58024547  0.5842794
  0.58482412  0.58602699  0.586598    0.58698223  0.58935932  0.5908411
  0.59514566  0.59719917  0.59786223  0.60057879  0.60870813  0.60872849
  0.60916717  0.61208692  0.61687818  0.61959475  0.62221127  0.62469955
  0.62629174  0.63172487  0.63251452  0.6344277   0.63552957  0.63589414
  0.63649552  0.63821275  0.63861071  0.64066422  0.64132727  0.64175131
  0.64364588  0.64517241  0.64636244  0.64947697  0.65194661  0.65219354
  0.6549101   0.66034323  0.66076072  0.66216618  0.66266184  0.6630598
  0.6651296   0.66577636  0.66775724  0.67054497  0.67081153  0.67120949
  0.6735281   0.67392606  0.67465667  0.675984    0.6778435   0.67935919
  0.68045047  0.68124461  0.68227917  0.68371399  0.68439436  0.68474693
  0.68711093  0.68726196  0.68858676  0.68982749  0.69115237  0.69254406
  0.6952254   0.69526062  0.69627074  0.69772839  0.69797719  0.70069375
  0.70314522  0.70334475  0.7033488   0.70578916  0.70587808  0.70612688
  0.70652484  0.70859465  0.70884345  0.71156001  0.71247052  0.71402778
  0.71424136  0.71467454  0.71674434  0.7167482   0.71699315  0.71700678
  0.72217747  0.72327345  0.7324171   0.73386266  0.73783947  0.74129947
  0.74452215  0.7451871   0.74653595  0.746889    0.74934313  0.74959193
  0.74960556  0.75123346  0.75169304  0.7517104   0.75177556  0.75329774
  0.75439193  0.75477626  0.75730844  0.76036525  0.76082627  0.76494712
  0.76590496  0.76859643  0.77405465  0.77483316  0.78220435  0.78227098
  0.78268451  0.78492091  0.79035405  0.79307061  0.79578718  0.79850374
  0.80078712  0.80122031  0.80393687  0.80544566  0.80937     0.81095021
  0.82023627  0.82246376  0.827988    0.82838596  0.831912    0.83568303
  0.83610247  0.83750372  0.83771686  0.83925222  0.84132203  0.84196879
  0.84468535  0.84619414  0.85380311  0.85410471  0.857835    0.86055156
  0.86175828  0.8630246   0.86370131  0.8637268   0.87120425  0.87353185
  0.8864019   0.88765782  0.88845796  0.89001433  0.89231683  0.90531182
  0.90541785  0.90760288  0.90904492  0.918265    0.91835392  0.92171725
  0.93193674  0.93465331  0.93486689  0.93530007  0.94164371  0.94280301
  0.94344977  0.94616634  0.95135254  0.95159947  0.96246573  0.96305529
  0.96444661  0.96807964  0.97424249  0.97540179  0.97666565  0.98371968
  0.98626805  0.99170119  0.99441775  0.99985088  1.00256745  1.00528401
  1.0088195   1.01071714  1.01594441  1.02115022  1.02158341  1.02712257
  1.02919237  1.0334274   1.05036606  1.07234552  1.07777865  1.08903399
  1.42095824  1.66752361]

  UserWarning,

2022-10-31 11:01:49,613:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390677 -0.9404312  -0.590322   -0.54798138 -0.29263285 -0.20438721
 -0.1866832  -0.13798057 -0.12398716 -0.12304452 -0.1198953  -0.08278575
 -0.07046144 -0.05659303 -0.03509535 -0.02298702 -0.00978239 -0.00867918
 -0.00382677 -0.0026685   0.01637773  0.02021382  0.02536482  0.0286603
  0.02940902  0.03994095  0.0466716   0.0482134   0.04841486  0.05029733
  0.05073295  0.0719822   0.07856243  0.07874466  0.07996084  0.08056576
  0.08314081  0.08389327  0.09049631  0.09099067  0.09817093  0.101219
  0.10143571  0.10146786  0.10982714  0.11249419  0.14611001  0.15545589
  0.15665583  0.15985543  0.16139877  0.16279983  0.16686824  0.17524696
  0.18183328  0.18679973  0.18830515  0.19158659  0.19190296  0.19252427
  0.1926502   0.1951644   0.20279927  0.20425062  0.20733099  0.2074614
  0.21343426  0.21642321  0.22152616  0.22450732  0.2254441   0.2264667
  0.22749343  0.22963944  0.23249826  0.23352633  0.23594711  0.2364368
  0.23653825  0.23672313  0.24006843  0.24218397  0.24388555  0.24753795
  0.24923427  0.2493424   0.24944902  0.24982936  0.25171546  0.25178211
  0.25209328  0.25342516  0.25419332  0.25434203  0.25640774  0.25774491
  0.25815876  0.25831133  0.25931453  0.26218984  0.26220736  0.26361477
  0.26420538  0.2652288   0.26639281  0.26674639  0.26805315  0.2686495
  0.27090326  0.2751332   0.27573181  0.27579385  0.27583188  0.27714872
  0.27715522  0.27716429  0.27779335  0.27802854  0.27837289  0.27848577
  0.27910157  0.27930592  0.28126373  0.28155147  0.28181826  0.28260855
  0.28266623  0.28311093  0.28351711  0.28357219  0.28387128  0.2844636
  0.28725363  0.28759254  0.28846417  0.28895381  0.2890973   0.28915778
  0.28940858  0.28968461  0.29104614  0.29139901  0.29168013  0.29205209
  0.29239633  0.29287621  0.29320078  0.29656362  0.29709445  0.29730662
  0.29770865  0.29779807  0.2980921   0.30065335  0.30320717  0.30372513
  0.3060367   0.3065846   0.30782986  0.30783688  0.31016565  0.31217423
  0.31231064  0.31380226  0.31474381  0.31477784  0.31552171  0.31572159
  0.31644943  0.31673138  0.31727018  0.3180071   0.31925789  0.3198505
  0.32070667  0.32086699  0.32321097  0.32406432  0.32537297  0.32637709
  0.32709331  0.32934075  0.32943596  0.33042909  0.33074056  0.33403231
  0.33505735  0.33842389  0.33963374  0.34179339  0.34190047  0.342241
  0.34360276  0.34361855  0.34398     0.34737001  0.34746404  0.34843494
  0.34997945  0.35064859  0.35092599  0.35175     0.35408262  0.35469468
  0.35636466  0.3570634   0.35712433  0.35920264  0.35942988  0.36116294
  0.36166034  0.36407137  0.36737264  0.36947401  0.37051185  0.37078933
  0.37172624  0.37224861  0.37288026  0.37308115  0.37317483  0.37404887
  0.37503896  0.37512675  0.37597968  0.37657759  0.37766158  0.3779937
  0.37940359  0.38082543  0.38317591  0.3837317   0.38419512  0.38430621
  0.38567953  0.38619893  0.38629087  0.38655359  0.38674364  0.38970153
  0.38992702  0.39066709  0.39086599  0.39245987  0.39273984  0.39284276
  0.39371492  0.39417023  0.3946333   0.3948346   0.39591428  0.39797407
  0.39868131  0.39994992  0.40313597  0.4038835   0.40410309  0.40442945
  0.40470724  0.40652781  0.40849959  0.40871711  0.40880784  0.41013149
  0.4102992   0.4106173   0.41147821  0.41210082  0.41233539  0.41516063
  0.41838704  0.41960785  0.41970929  0.42105177  0.42106439  0.42210204
  0.42241033  0.42271587  0.42339005  0.42386382  0.42585448  0.42815084
  0.43081642  0.43255117  0.43260956  0.43425387  0.43530128  0.43775747
  0.44180064  0.44226667  0.44407468  0.44410794  0.44557073  0.44797072
  0.4479862   0.44862344  0.44888002  0.45467508  0.4551799   0.4567633
  0.45716664  0.45868611  0.45900235  0.46025031  0.46066686  0.46270305
  0.46366913  0.46405048  0.46699396  0.46817765  0.46830359  0.46920927
  0.47093077  0.47318923  0.47409716  0.47547221  0.47787205  0.48313319
  0.48640268  0.48737567  0.48822826  0.48912915  0.49198735  0.49322307
  0.49410273  0.49538602  0.49756572  0.5007252   0.50188517  0.50568469
  0.5103218   0.5135527   0.52288708  0.53062247  0.53311624  0.53575509
  0.53654311  0.53677499  0.53876682  0.54807891  0.55128789  0.55524213
  0.55556024  0.55649443  0.55711916  0.56127471  0.56709427  0.56985869
  0.5713711   0.57491537  0.57559213  0.57777783  0.58034736  0.58455071
  0.58704449  0.59173113  0.59475049  0.59701959  0.59951337  0.60073038
  0.60418282  0.60420954  0.60450093  0.6069947   0.6075276   0.6091971
  0.61198226  0.61590775  0.61732749  0.61788447  0.61946359  0.61950477
  0.62168996  0.62195737  0.62249575  0.62569813  0.62997159  0.63003628
  0.63193247  0.63766504  0.63809056  0.63912242  0.6394138   0.6416162
  0.64190758  0.64439034  0.64440136  0.64938891  0.65095373  0.65188269
  0.65458024  0.65464711  0.65687024  0.65711176  0.65804077  0.65936402
  0.66053455  0.66062203  0.6618578   0.66228401  0.66435158  0.66462222
  0.66488447  0.66655397  0.66684535  0.6684748   0.66933913  0.67361279
  0.67432668  0.67510811  0.67801215  0.67931424  0.68070742  0.68151663
  0.68211188  0.68213905  0.68375307  0.68928935  0.68981122  0.69178312
  0.69398552  0.69493568  0.69532028  0.69607114  0.69897307  0.70146685
  0.70175823  0.70301624  0.70455587  0.7064544   0.70674579  0.70704965
  0.70756749  0.70782812  0.70828617  0.70894818  0.70923956  0.71049758
  0.71144196  0.71292503  0.71464777  0.71797891  0.72237504  0.72598916
  0.72708082  0.72949364  0.73107407  0.73326735  0.73674274  0.7378628
  0.73949593  0.7404229   0.74196253  0.7428963   0.7441317   0.74541045
  0.74631679  0.74695008  0.74944386  0.75007991  0.75310816  0.75538556
  0.75730837  0.75866171  0.75941897  0.75949588  0.75986868  0.76280559
  0.76440652  0.7669003   0.76692747  0.76818549  0.76882112  0.76939407
  0.77017255  0.77064256  0.77090432  0.77188785  0.7751601   0.7766112
  0.77809241  0.77999391  0.78435674  0.78685051  0.78983437  0.79138078
  0.79682562  0.7993194   0.80057741  0.80250871  0.80650935  0.80680073
  0.80929451  0.80973072  0.81423893  0.82035195  0.8272934   0.82743799
  0.82969836  0.83228096  0.83798407  0.8451003   0.84665444  0.84670116
  0.84846272  0.85168872  0.85253248  0.85721873  0.85775861  0.8597125
  0.86719384  0.88041837  0.89267148  0.89462538  0.89465574  0.89975753
  0.90046464  0.90460049  0.9058585   0.90632902  0.90803418  0.90958805
  0.91446061  0.91583361  0.92205693  0.92704449  0.92801183  0.93203204
  0.93256493  0.93701959  0.93827761  0.94077138  0.94200715  0.94326516
  0.94450092  0.94574792  0.94575894  0.9469947   0.94724201  0.95600469
  0.95822782  0.9607216   0.96189213  0.96195736  0.96282009  0.96445114
  0.96513819  0.96570915  0.96763196  0.96837628  0.9694387   0.97069671
  0.9716418   0.97413487  0.97527446  0.97568426  0.97817804  0.98215489
  0.98316559  0.98524512  0.98815315  0.99064692  1.01059714  1.01309092
  1.0155847   1.0164329   1.01936097  1.02057225  1.04180496  1.06903635
  1.085963    1.09490337  1.18722084  1.64802087]

  UserWarning,

2022-10-31 11:01:49,628:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14373561e+00 -5.70012693e-01 -4.97240939e-01 -4.66859112e-01
 -3.51236114e-01 -1.10337661e-01 -1.09337646e-01 -9.72805052e-02
 -9.38809151e-02 -9.00388052e-02 -3.74152838e-02 -2.78680818e-02
 -1.68677049e-02  5.80980708e-04  3.97522753e-02  4.75240221e-02
  5.95697462e-02  6.00827058e-02  6.91305057e-02  7.62875938e-02
  9.19447980e-02  9.47890882e-02  1.01724313e-01  1.14038118e-01
  1.17769758e-01  1.29634607e-01  1.30390927e-01  1.37518191e-01
  1.40044133e-01  1.46840056e-01  1.50567796e-01  1.68955894e-01
  1.73091335e-01  1.75903598e-01  1.80631183e-01  1.85791350e-01
  1.89410975e-01  1.94198407e-01  1.94411578e-01  1.94663462e-01
  1.97955313e-01  1.98489324e-01  2.00742921e-01  2.05333219e-01
  2.05768764e-01  2.07154371e-01  2.09546518e-01  2.11620963e-01
  2.14412587e-01  2.16677509e-01  2.18654773e-01  2.19387307e-01
  2.19793876e-01  2.23214708e-01  2.25017147e-01  2.25780540e-01
  2.26410209e-01  2.27467991e-01  2.28574611e-01  2.30096944e-01
  2.30103192e-01  2.31049853e-01  2.33516468e-01  2.34412203e-01
  2.35118093e-01  2.35607656e-01  2.36201810e-01  2.37132159e-01
  2.38792758e-01  2.41769797e-01  2.41809652e-01  2.42253222e-01
  2.42564558e-01  2.44404383e-01  2.46463182e-01  2.47708173e-01
  2.48437349e-01  2.48469760e-01  2.49036108e-01  2.50007834e-01
  2.50486106e-01  2.50731258e-01  2.50790169e-01  2.51295706e-01
  2.51808917e-01  2.52514824e-01  2.52625007e-01  2.53239417e-01
  2.54453227e-01  2.54668436e-01  2.56430967e-01  2.56950434e-01
  2.57737196e-01  2.58427144e-01  2.58957890e-01  2.61262106e-01
  2.62929950e-01  2.62978031e-01  2.63466226e-01  2.65023467e-01
  2.65863430e-01  2.66027181e-01  2.66309582e-01  2.66450900e-01
  2.69356799e-01  2.71180945e-01  2.71709311e-01  2.74668161e-01
  2.75125605e-01  2.75892040e-01  2.77342410e-01  2.77965037e-01
  2.78303355e-01  2.79175077e-01  2.80325555e-01  2.80489069e-01
  2.81678691e-01  2.86308603e-01  2.86657415e-01  2.86753294e-01
  2.86896241e-01  2.87110300e-01  2.87282589e-01  2.89092692e-01
  2.91639286e-01  2.92844701e-01  2.93443186e-01  2.95562423e-01
  2.95701493e-01  2.96048772e-01  2.96719617e-01  2.98334748e-01
  3.00229862e-01  3.00369692e-01  3.00601679e-01  3.00974439e-01
  3.01088933e-01  3.01343613e-01  3.02937986e-01  3.03121506e-01
  3.05403691e-01  3.05414632e-01  3.05529991e-01  3.07689524e-01
  3.09544947e-01  3.10047745e-01  3.10363938e-01  3.10714214e-01
  3.13859107e-01  3.14024667e-01  3.15433581e-01  3.16210961e-01
  3.16785211e-01  3.18493464e-01  3.20398478e-01  3.22392993e-01
  3.22761073e-01  3.23963109e-01  3.24331503e-01  3.25592528e-01
  3.25825392e-01  3.25994370e-01  3.26321545e-01  3.26754065e-01
  3.27707429e-01  3.28520923e-01  3.29851171e-01  3.29976653e-01
  3.30325008e-01  3.30511310e-01  3.31729442e-01  3.32929170e-01
  3.33064942e-01  3.33296512e-01  3.34168210e-01  3.34218646e-01
  3.34635259e-01  3.35396290e-01  3.35643629e-01  3.36039765e-01
  3.36150160e-01  3.36891802e-01  3.37446884e-01  3.37882255e-01
  3.40408382e-01  3.41053935e-01  3.41082011e-01  3.41500931e-01
  3.41591697e-01  3.42528522e-01  3.42957555e-01  3.43754993e-01
  3.43915455e-01  3.44215465e-01  3.44272408e-01  3.44695459e-01
  3.45839873e-01  3.46658420e-01  3.47224615e-01  3.49957858e-01
  3.50816507e-01  3.50835830e-01  3.51353920e-01  3.53415829e-01
  3.54159389e-01  3.54671340e-01  3.54679234e-01  3.54730354e-01
  3.55028756e-01  3.55532046e-01  3.58903691e-01  3.58905253e-01
  3.59624043e-01  3.60258021e-01  3.60352543e-01  3.60508744e-01
  3.60839382e-01  3.60847096e-01  3.60890585e-01  3.60914819e-01
  3.61429526e-01  3.62122273e-01  3.65822213e-01  3.65879817e-01
  3.66574140e-01  3.67239735e-01  3.67305454e-01  3.67940663e-01
  3.67994460e-01  3.70652492e-01  3.72791133e-01  3.73594172e-01
  3.73769162e-01  3.73923530e-01  3.74251939e-01  3.75704487e-01
  3.76202457e-01  3.76668272e-01  3.77109405e-01  3.77461180e-01
  3.78448876e-01  3.78930912e-01  3.80190244e-01  3.80678646e-01
  3.83414975e-01  3.83622464e-01  3.84381580e-01  3.84592096e-01
  3.84636379e-01  3.85756355e-01  3.86159832e-01  3.87446522e-01
  3.87456680e-01  3.87462581e-01  3.88575281e-01  3.89720628e-01
  3.90388201e-01  3.90841828e-01  3.91399822e-01  3.91456461e-01
  3.91822474e-01  3.92674801e-01  3.95630522e-01  3.95728211e-01
  3.96089785e-01  3.96790038e-01  3.97232307e-01  3.99027577e-01
  3.99128385e-01  3.99351317e-01  3.99556535e-01  3.99984790e-01
  4.00363881e-01  4.06851515e-01  4.07451217e-01  4.08408973e-01
  4.08741084e-01  4.08792408e-01  4.08953202e-01  4.10106404e-01
  4.10388387e-01  4.10920551e-01  4.11272302e-01  4.12834909e-01
  4.15703785e-01  4.16872160e-01  4.17061353e-01  4.18505024e-01
  4.19389753e-01  4.20349354e-01  4.22435931e-01  4.25986124e-01
  4.26746878e-01  4.32780897e-01  4.32853956e-01  4.35582712e-01
  4.35918877e-01  4.36497022e-01  4.38896169e-01  4.42518521e-01
  4.43056993e-01  4.45314622e-01  4.45524588e-01  4.46884830e-01
  4.47578448e-01  4.47821143e-01  4.49353248e-01  4.50731881e-01
  4.52770461e-01  4.53372767e-01  4.54775889e-01  4.55612776e-01
  4.55659783e-01  4.57954765e-01  4.61524195e-01  4.62602499e-01
  4.63655507e-01  4.63828229e-01  4.63990786e-01  4.64908871e-01
  4.67342055e-01  4.69162274e-01  4.70031689e-01  4.73085721e-01
  4.73707842e-01  4.76755361e-01  4.79120954e-01  4.79583729e-01
  4.84090680e-01  4.92393654e-01  5.01303424e-01  5.05861966e-01
  5.08304252e-01  5.14990457e-01  5.17280650e-01  5.18935241e-01
  5.21014564e-01  5.22477892e-01  5.34237524e-01  5.36880137e-01
  5.39576851e-01  5.43621707e-01  5.46215414e-01  5.48224140e-01
  5.50916534e-01  5.56629317e-01  5.63039494e-01  5.64953191e-01
  5.67022123e-01  5.69885459e-01  5.74810619e-01  5.75315849e-01
  5.79608913e-01  5.82008060e-01  5.86263823e-01  5.86806355e-01
  5.88793534e-01  5.89569656e-01  5.94003796e-01  5.96402943e-01
  5.99267816e-01  6.02364511e-01  6.05992184e-01  6.13196973e-01
  6.15053588e-01  6.17374179e-01  6.20394414e-01  6.22792822e-01
  6.23132104e-01  6.25192708e-01  6.26081888e-01  6.27591855e-01
  6.29991002e-01  6.30561910e-01  6.30886054e-01  6.32390149e-01
  6.34338287e-01  6.34789297e-01  6.37188444e-01  6.37517802e-01
  6.39587591e-01  6.39916949e-01  6.43290078e-01  6.45226187e-01
  6.45297858e-01  6.46785032e-01  6.47727374e-01  6.50024481e-01
  6.51706589e-01  6.51825737e-01  6.53982473e-01  6.56381620e-01
  6.57239997e-01  6.58335702e-01  6.58520252e-01  6.58780767e-01
  6.58887538e-01  6.59305793e-01  6.59468244e-01  6.61179915e-01
  6.63133996e-01  6.63908420e-01  6.64104087e-01  6.65533143e-01
  6.65978209e-01  6.68377356e-01  6.70015716e-01  6.71439190e-01
  6.71909959e-01  6.72786366e-01  6.73175650e-01  6.75185513e-01
  6.75622720e-01  6.77973944e-01  6.80533607e-01  6.85044520e-01
  6.85171386e-01  6.86655505e-01  6.87125467e-01  6.87511978e-01
  6.88012063e-01  6.88884315e-01  6.91923761e-01  6.92172965e-01
  6.94767974e-01  6.95637074e-01  6.96716111e-01  6.96722056e-01
  6.97167121e-01  6.97491265e-01  6.99121203e-01  7.00458557e-01
  7.02490441e-01  7.03156235e-01  7.03919497e-01  7.05498018e-01
  7.08717791e-01  7.08773572e-01  7.09236872e-01  7.09604387e-01
  7.09649764e-01  7.09715914e-01  7.11116938e-01  7.13571867e-01
  7.14458427e-01  7.15915232e-01  7.15971014e-01  7.18314379e-01
  7.23168455e-01  7.27584221e-01  7.28352498e-01  7.29292577e-01
  7.30634259e-01  7.31196711e-01  7.33595858e-01  7.33679353e-01
  7.41485224e-01  7.43192446e-01  7.44704997e-01  7.44760779e-01
  7.49190949e-01  7.49503292e-01  7.50389887e-01  7.51332828e-01
  7.51980531e-01  7.56700733e-01  7.57587329e-01  7.59843138e-01
  7.59986476e-01  7.62689847e-01  7.65118402e-01  7.67513276e-01
  7.69583064e-01  7.71594533e-01  7.71982211e-01  7.73319476e-01
  7.74381358e-01  7.80031416e-01  7.81578800e-01  7.83977947e-01
  7.86377094e-01  7.88386957e-01  7.88776241e-01  7.89616543e-01
  7.91175388e-01  7.93574535e-01  7.93952331e-01  7.95973682e-01
  7.97983545e-01  7.98372829e-01  7.99315770e-01  8.03171123e-01
  8.05180986e-01  8.06175851e-01  8.06298017e-01  8.06513211e-01
  8.06568992e-01  8.07649386e-01  8.07969418e-01  8.09663658e-01
  8.10368565e-01  8.22688444e-01  8.28629476e-01  8.28800955e-01
  8.30213908e-01  8.36369899e-01  8.39158330e-01  8.40922639e-01
  8.44259265e-01  8.57450647e-01  8.80468856e-01  8.83962261e-01
  8.84225152e-01  8.86595776e-01  8.90735117e-01  8.97932559e-01
  9.00331706e-01  9.03458599e-01  9.05130000e-01  9.05468543e-01
  9.07529147e-01  9.08471489e-01  9.12327441e-01  9.16736451e-01
  9.17125735e-01  9.17342032e-01  9.19135598e-01  9.21381497e-01
  9.21534745e-01  9.26722324e-01  9.27293231e-01  9.29121471e-01
  9.31520618e-01  9.33919765e-01  9.34217957e-01  9.36318912e-01
  9.36535209e-01  9.38328775e-01  9.40479787e-01  9.41117206e-01
  9.49953008e-01  9.51037939e-01  9.51873658e-01  9.52723658e-01
  9.56023071e-01  9.57911236e-01  9.59921099e-01  9.63033674e-01
  9.64680439e-01  9.68698645e-01  9.69517687e-01  9.75230291e-01
  9.76172596e-01  9.76715129e-01  9.83856788e-01  9.84295011e-01
  9.88710864e-01  9.88801459e-01  9.91478647e-01  9.94423468e-01
  9.97099126e-01  1.00070660e+00  1.00363077e+00  1.00550489e+00
  1.00733443e+00  1.00790404e+00  1.01030319e+00  1.01698564e+00
  1.01831995e+00  1.02211860e+00  1.04396582e+00  1.05775952e+00]

  UserWarning,

2022-10-31 11:01:49,687:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24194797 -0.84419775 -0.60693611 -0.40250413 -0.38078738 -0.24163741
 -0.16921183 -0.1044453  -0.1032711  -0.09339279 -0.08550376 -0.08414727
 -0.05753256 -0.05487387 -0.03848074 -0.03501986 -0.01256979 -0.00582977
 -0.00403834  0.00838846  0.01732571  0.02044145  0.02498321  0.0259042
  0.03474579  0.05187124  0.05781138  0.06219944  0.06392184  0.09782585
  0.11172976  0.12673803  0.13431239  0.13534562  0.13742668  0.14325894
  0.14374922  0.14416599  0.14956382  0.15030022  0.15192921  0.15541783
  0.15647455  0.15699477  0.15699883  0.1580581   0.15945985  0.16203864
  0.16318305  0.16421437  0.16917848  0.17124075  0.17433662  0.17763702
  0.17825802  0.18686835  0.19049503  0.19169473  0.19211793  0.19284836
  0.19477492  0.19665897  0.19725865  0.19927061  0.19964648  0.19991583
  0.20005353  0.20507564  0.20683075  0.20734666  0.20902668  0.20949185
  0.21259572  0.21538975  0.21580861  0.21742839  0.21774558  0.21774654
  0.22214797  0.22366603  0.22428223  0.22458386  0.22471285  0.22554627
  0.22627782  0.22815116  0.23082553  0.23301337  0.23904117  0.23910414
  0.24100187  0.2410416   0.24143609  0.2417633   0.24548442  0.24693015
  0.24759788  0.24879089  0.25024618  0.2509244   0.2512435   0.25156084
  0.25787422  0.25880685  0.26157628  0.26167447  0.26452477  0.26487773
  0.26696599  0.26769566  0.2678095   0.26800434  0.26868639  0.26977468
  0.2699369   0.27031504  0.27045193  0.27069761  0.27156818  0.27173318
  0.27175027  0.27317142  0.27405799  0.27461939  0.28164931  0.28184533
  0.28241683  0.28267359  0.28356957  0.2842039   0.28455937  0.28456142
  0.28679553  0.28791818  0.28812227  0.28918756  0.289397    0.28962978
  0.29032217  0.29060298  0.29075738  0.29133833  0.29169289  0.29180384
  0.29259643  0.29392365  0.29445183  0.29516252  0.29582985  0.29674996
  0.29682996  0.29685291  0.29847564  0.29884191  0.29977104  0.29993794
  0.30363438  0.30419349  0.30445826  0.30622599  0.30645135  0.30672153
  0.30729403  0.30817941  0.30841695  0.30906594  0.30998953  0.31176733
  0.31329342  0.31423203  0.31433617  0.31437619  0.31456746  0.31465037
  0.31864183  0.31910114  0.32169752  0.32300928  0.32418067  0.32513643
  0.32619141  0.32640176  0.3267397   0.32720084  0.32761983  0.32781635
  0.32843027  0.32944887  0.33042621  0.33293227  0.33324225  0.33770183
  0.33845096  0.3386033   0.33957879  0.33958711  0.34037982  0.34046935
  0.34048039  0.3406224   0.34107704  0.34264339  0.34423549  0.34637294
  0.34686134  0.3476103   0.34810343  0.34858122  0.34864333  0.34920478
  0.35278689  0.35297722  0.35387322  0.35603449  0.35648441  0.35860548
  0.35930156  0.36042819  0.36199685  0.36265625  0.36372923  0.36374398
  0.36375204  0.36388844  0.36403059  0.36411462  0.36442531  0.36486431
  0.36739865  0.36797795  0.36817473  0.36950421  0.3731721   0.37374609
  0.37403703  0.37650864  0.37738133  0.37785964  0.38027268  0.38272166
  0.38507513  0.38509286  0.38536131  0.38685032  0.38710101  0.38746406
  0.38753897  0.38765696  0.38773251  0.38794028  0.38865106  0.38931515
  0.38941719  0.38949344  0.38983526  0.38989653  0.39127654  0.39153241
  0.39204988  0.39239655  0.39421461  0.39457767  0.39511083  0.39631528
  0.3970948   0.3973278   0.39914046  0.39932007  0.39989873  0.40042602
  0.40224528  0.40292443  0.4030223   0.40412152  0.40440031  0.40494198
  0.40694782  0.40765774  0.40880488  0.40906699  0.41031519  0.41079913
  0.41185754  0.41258532  0.41378836  0.41555966  0.41999468  0.42258202
  0.42349569  0.42405704  0.42484868  0.42771504  0.42825626  0.42840844
  0.43300722  0.4345335   0.43531996  0.43633591  0.43733194  0.43783528
  0.43789325  0.4399014   0.44026445  0.44154558  0.44172092  0.44263135
  0.44357427  0.44500686  0.44628604  0.44948108  0.44967225  0.4512558
  0.45138596  0.45198011  0.45212047  0.4531742   0.45570418  0.45634256
  0.45855144  0.46449103  0.46469634  0.46649949  0.47263431  0.47796143
  0.47843994  0.4788777   0.47990696  0.48089973  0.48389914  0.486715
  0.49340992  0.49399987  0.49411947  0.49764769  0.49886188  0.50025322
  0.5039352   0.50816465  0.50922259  0.51067708  0.51441435  0.5153858
  0.5173044   0.51767853  0.5225739   0.52968751  0.53834701  0.5384
  0.54137426  0.54255046  0.54389154  0.54628592  0.5472298   0.55714737
  0.55969032  0.56384887  0.56556066  0.56661469  0.56715291  0.56867475
  0.57054863  0.57650182  0.57810332  0.58018243  0.58196759  0.5840787
  0.58649233  0.58670999  0.58964322  0.59781519  0.598566    0.59891098
  0.60451127  0.61021644  0.61313819  0.61575047  0.61678481  0.62006977
  0.6202518   0.62152721  0.62197367  0.62286407  0.63237626  0.63447901
  0.63590205  0.63685022  0.64015242  0.64159262  0.64203299  0.64396382
  0.64870623  0.65246682  0.65344863  0.65581984  0.65768456  0.65819104
  0.65896527  0.66056224  0.66281146  0.66289945  0.66293344  0.66344296
  0.66530464  0.66560456  0.66995597  0.67072849  0.67241825  0.67478945
  0.67524136  0.67716066  0.67953186  0.67977293  0.6801658   0.68190306
  0.68214413  0.68451533  0.68472617  0.6849082   0.6867591   0.68688654
  0.68691391  0.68727941  0.68898268  0.69005801  0.69162894  0.69375907
  0.69874255  0.69913542  0.70111375  0.70288082  0.70324388  0.70387782
  0.70525203  0.70585615  0.70813628  0.70972316  0.71030879  0.7103235
  0.71035749  0.71041653  0.71099143  0.71173413  0.7131761   0.7174224
  0.71810504  0.71875351  0.72175165  0.72320445  0.72413241  0.72491099
  0.72927841  0.72956818  0.73049421  0.73151444  0.73164961  0.73589734
  0.73598842  0.73639202  0.7388017   0.74603915  0.74637312  0.74824803
  0.75021563  0.75061923  0.75130187  0.75367307  0.75536164  0.76010404
  0.76721765  0.76723235  0.76958885  0.77196005  0.77621338  0.77661138
  0.77670246  0.77709345  0.77900866  0.77907366  0.77951403  0.78360004
  0.78381606  0.78839594  0.78855847  0.79056662  0.79092967  0.79804328
  0.80041448  0.80278568  0.80515688  0.8085526   0.81077907  0.81227049
  0.81902104  0.82649771  0.82748337  0.82950285  0.83357732  0.84072492
  0.84309612  0.84546732  0.846902    0.85061611  0.85350472  0.85599348
  0.86217937  0.86379976  0.87367556  0.87568976  0.87802697  0.8816693
  0.88342796  0.8840405   0.88751178  0.88814572  0.88988298  0.89225419
  0.89522858  0.90242892  0.9088526   0.91375289  0.91833741  0.92134255
  0.92307982  0.93256462  0.93295562  0.93310749  0.93520671  0.9362597
  0.93730703  0.94204943  0.94268338  0.94342608  0.94442064  0.94679184
  0.94691776  0.94979698  0.95231581  0.95425928  0.95453939  0.95691059
  0.95731799  0.95928179  0.96002449  0.9711378   0.97287506  0.973509
  0.97434037  0.97588021  0.9824735   0.98299381  0.98536502  0.98773622
  0.99247862  0.99301909  0.99685797  0.9989243   0.99989215  1.00187236
  1.00196343  1.00486779  1.01519609  1.01563634  1.02702113  1.02737872
  1.02989755  1.03503555  1.0400163   1.04141377  1.04282787]

  UserWarning,

2022-10-31 11:01:49,734:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04602623 -0.84332998 -0.75815329 -0.72378269 -0.65507834 -0.57382694
 -0.51062582 -0.41031553 -0.39184235 -0.38498398 -0.35385629 -0.35319306
 -0.28530344 -0.28401411 -0.26001254 -0.24322189 -0.2379901  -0.22543094
 -0.18854615 -0.15641655 -0.13776864 -0.10165344 -0.08344072 -0.08151435
 -0.04285185 -0.03609068 -0.02911575 -0.02763678 -0.02334932 -0.00797357
 -0.00779856 -0.00316579  0.01604145  0.04585153  0.0459524   0.07021959
  0.07819721  0.07943861  0.08137006  0.09220573  0.09998941  0.10073869
  0.10184522  0.10397276  0.11101953  0.1121763   0.11357561  0.12020607
  0.12799487  0.14236152  0.15393916  0.15488832  0.16166636  0.16417037
  0.16570563  0.17273235  0.17380542  0.1738621   0.17461713  0.17496176
  0.17618227  0.17814193  0.18168691  0.18827384  0.19310441  0.19312773
  0.19396588  0.19761632  0.19763355  0.20016568  0.20069067  0.20082482
  0.20154775  0.20187227  0.20309592  0.21412437  0.21971231  0.21993504
  0.22045778  0.22116935  0.22349152  0.22352586  0.22408235  0.22513635
  0.22743463  0.22800574  0.23074463  0.23198206  0.23486439  0.23657057
  0.23713044  0.23871501  0.24032984  0.24190612  0.24206001  0.24286801
  0.24353644  0.24383856  0.24532357  0.2456746   0.25431716  0.25680667
  0.25738799  0.2574842   0.25918734  0.25951401  0.26046965  0.26069937
  0.2629975   0.26402185  0.26462454  0.26565811  0.2681493   0.26862298
  0.26890069  0.27031524  0.27033501  0.27162932  0.27198062  0.27280418
  0.27293207  0.27341383  0.27424996  0.27531101  0.27555241  0.27565565
  0.2757062   0.27621206  0.27701716  0.27896984  0.28035345  0.28041623
  0.28131452  0.28252967  0.2830827   0.28453233  0.28511573  0.28694259
  0.28884817  0.28974463  0.29007473  0.29034247  0.29069461  0.29136547
  0.29151674  0.29522217  0.29602975  0.29665803  0.2968958   0.29776048
  0.29794264  0.29795022  0.29942936  0.30393445  0.30417144  0.30506949
  0.30730329  0.30850285  0.3094855   0.31146253  0.31155781  0.3119878
  0.3129221   0.31311156  0.313976    0.31451404  0.31755496  0.31862324
  0.31917805  0.31933174  0.31949389  0.32128953  0.32159751  0.3222298
  0.32247224  0.32399357  0.32456226  0.32611124  0.32653783  0.32729271
  0.32864592  0.32870635  0.3292738   0.33001889  0.33133016  0.33403159
  0.33589042  0.33593041  0.33674533  0.33697648  0.33779593  0.33784999
  0.33973899  0.34087472  0.34201787  0.342062    0.34231484  0.34268263
  0.34375535  0.34424387  0.34570838  0.34655725  0.3466895   0.34741895
  0.34763344  0.34776778  0.34837625  0.34902319  0.34988584  0.35239226
  0.35281542  0.35349465  0.35375421  0.35431006  0.35459479  0.35609631
  0.35788084  0.35856433  0.35956163  0.35959866  0.3596282   0.35975679
  0.36009964  0.36083063  0.36117063  0.36159746  0.36164423  0.36309858
  0.36350201  0.36514151  0.36642129  0.36689233  0.36698951  0.36731718
  0.36921405  0.37033919  0.37053969  0.37108029  0.37131573  0.37204272
  0.37264345  0.37393736  0.37577995  0.37595509  0.37744343  0.37758908
  0.37956937  0.38020713  0.38072185  0.38079394  0.38145717  0.38252642
  0.38380803  0.38491735  0.38506762  0.38743638  0.38872018  0.38906008
  0.39590066  0.39673153  0.39702288  0.39770805  0.39857193  0.39938113
  0.39979639  0.39994833  0.40045592  0.40217352  0.40282843  0.40295157
  0.40409503  0.40533449  0.4053786   0.40642468  0.40661771  0.4080029
  0.40897683  0.41021143  0.41096418  0.41261215  0.41292924  0.41397726
  0.41483073  0.41543606  0.41555885  0.41633547  0.4165261   0.41669907
  0.41688605  0.41860875  0.42147668  0.42148263  0.42273286  0.42351973
  0.42380797  0.42405456  0.42432846  0.42495421  0.42723382  0.42809443
  0.42837999  0.42853252  0.43104441  0.43136426  0.43182125  0.43214816
  0.4376696   0.43966571  0.44060406  0.44358868  0.44490992  0.44491427
  0.44692314  0.44735294  0.44758772  0.44777822  0.46157271  0.46456497
  0.46697108  0.47076902  0.47192374  0.47410238  0.47856112  0.48115575
  0.49428084  0.49928423  0.50010041  0.50754113  0.5076128   0.50907679
  0.51080837  0.5116005   0.51618256  0.51697131  0.52241067  0.52429882
  0.52520582  0.52925899  0.5332023   0.53421672  0.53648455  0.53683084
  0.53879419  0.54054697  0.54174168  0.5434914   0.54418402  0.54422307
  0.54476223  0.5453905   0.54741263  0.54809167  0.56284935  0.56342035
  0.56442506  0.57078339  0.57132369  0.5741095   0.57521532  0.57585007
  0.57790672  0.58530687  0.58635527  0.59680694  0.59871994  0.60242816
  0.60354058  0.6044011   0.60554937  0.60722203  0.61184774  0.61437448
  0.61461679  0.61533424  0.61606412  0.61955251  0.62015489  0.62043123
  0.62197977  0.62292395  0.62505825  0.62538546  0.625693    0.62617908
  0.62846205  0.6291795   0.63367032  0.63400015  0.63476976  0.6351333
  0.6367692   0.63953826  0.63995395  0.64231686  0.64299622  0.64507636
  0.64579381  0.64760331  0.64905677  0.65338351  0.65491107  0.65615256
  0.66024273  0.66296948  0.66382497  0.66445972  0.66517717  0.66589662
  0.6665729   0.66762251  0.66999782  0.67020573  0.67024775  0.67044295
  0.67063762  0.67268922  0.67276687  0.67459364  0.67553593  0.67670991
  0.67685704  0.67805624  0.67815632  0.67830498  0.6791018   0.67979204
  0.68050303  0.68107403  0.68229146  0.68447562  0.68456053  0.68540639
  0.68733158  0.68930353  0.68985554  0.69151549  0.69215024  0.69261895
  0.69563674  0.69705359  0.69768834  0.69840579  0.69982264  0.70045739
  0.70156859  0.7025917   0.70322644  0.7039439   0.7044378   0.70536075
  0.70542449  0.70599549  0.70624542  0.7081298   0.70876454  0.71121494
  0.71187499  0.71225105  0.71254532  0.71430265  0.71447492  0.71455257
  0.7150201   0.71821801  0.71857978  0.71967296  0.72020012  0.72040303
  0.72057676  0.73116337  0.73305126  0.73601702  0.73646421  0.73944326
  0.73947404  0.74083466  0.74689652  0.74778119  0.74810669  0.74824872
  0.74929128  0.75055024  0.75378682  0.75421568  0.75428073  0.7588574
  0.75903633  0.76078621  0.76162645  0.76294756  0.7699336   0.77270265
  0.7758874   0.77624132  0.77824076  0.78031339  0.78118381  0.78243853
  0.78545775  0.78654791  0.78726736  0.79208602  0.80316222  0.80510234
  0.80593127  0.80774005  0.81772493  0.82049398  0.82191084  0.82254558
  0.82531463  0.82603209  0.83130863  0.83483315  0.8376022   0.83987735
  0.84192894  0.84689605  0.8481845   0.85813944  0.85975461  0.87198469
  0.87710898  0.8830609   0.888599    0.89967521  0.90039266  0.90220216
  0.90244426  0.90316171  0.90316371  0.90521331  0.90869982  0.91075142
  0.91564644  0.91598198  0.91905857  0.9273315   0.93290383  0.93331952
  0.93411519  0.93608857  0.93757564  0.93844193  0.94121098  0.94192844
  0.94340903  0.94746654  0.94951814  0.95171619  0.95228719  0.95300464
  0.95505624  0.95577369  0.9601563   0.96059434  0.96336339  0.96726559
  0.96954287  0.96961895  0.9744396   0.97515705  0.9799777   0.98274676
  0.98900231  0.99177136  0.99730947  1.00284757  1.00318234  1.01669283
  1.02133103  1.02499998  1.03174944  1.10853919  1.52274401]

  UserWarning,

2022-10-31 11:01:49,766:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60470879 -0.84478708 -0.31001272 -0.23181711 -0.22925958 -0.18388184
 -0.17375493 -0.15939072 -0.0589028  -0.05433673 -0.04762999 -0.03805126
 -0.01194197 -0.00576226  0.00481885  0.01000654  0.01190334  0.02147636
  0.06059361  0.07117711  0.07460485  0.08399168  0.1019461   0.10397485
  0.1127978   0.1184939   0.1231407   0.1234011   0.14262182  0.14327143
  0.14461337  0.14687813  0.15400446  0.16326069  0.16334379  0.17621338
  0.18122406  0.18711622  0.19040683  0.19182796  0.19249545  0.1950319
  0.19532825  0.19607945  0.20023442  0.2011216   0.20186613  0.20679127
  0.20997048  0.21001827  0.21073364  0.21229152  0.21408936  0.21443799
  0.21612068  0.21706476  0.21864666  0.22136675  0.22235533  0.22350263
  0.2240505   0.22494087  0.22612033  0.22727753  0.22801278  0.22816124
  0.22965419  0.23071314  0.23470041  0.23643053  0.2382808   0.23994042
  0.24060604  0.2409866   0.24617892  0.24673872  0.24711723  0.24804804
  0.24970946  0.25070528  0.25156132  0.25192229  0.2531271   0.25439267
  0.2547662   0.25533802  0.25540359  0.25625564  0.25641315  0.25662577
  0.25845769  0.25870054  0.25872585  0.25895     0.25989356  0.26261927
  0.26270334  0.26281105  0.26314879  0.26322826  0.26515705  0.26555267
  0.26597203  0.26640316  0.26642117  0.26652057  0.26808957  0.26874787
  0.26951459  0.26972458  0.27030322  0.27047059  0.27222882  0.27341672
  0.27375312  0.27512217  0.27535875  0.27552695  0.27649965  0.27814485
  0.28280997  0.28582948  0.28622529  0.28673768  0.28696301  0.28722409
  0.28755421  0.28823392  0.28915519  0.28942761  0.29053647  0.29074591
  0.29134011  0.29170649  0.29295831  0.29417098  0.29430165  0.29505041
  0.29546911  0.29600105  0.29833401  0.29963867  0.29971372  0.30023446
  0.30137032  0.30205436  0.3020922   0.30221455  0.30302158  0.30433206
  0.30586645  0.30784663  0.30878058  0.31213021  0.31241801  0.31308032
  0.31457202  0.3146049   0.31605988  0.31804905  0.31974926  0.3217056
  0.32225099  0.32245936  0.32307597  0.3238702   0.32449181  0.32457576
  0.32580563  0.32592995  0.32709836  0.32740353  0.32766754  0.3281436
  0.32869647  0.32925215  0.32984152  0.32997867  0.33089801  0.33093212
  0.33208825  0.33237908  0.33242087  0.33249107  0.33431356  0.33434605
  0.33445376  0.33515712  0.33752799  0.33856788  0.33861026  0.34026276
  0.34041652  0.34118112  0.34166332  0.34214162  0.34276107  0.34332645
  0.34369824  0.34446564  0.34618057  0.34693592  0.34742643  0.34822122
  0.3491309   0.34965011  0.35078526  0.35079828  0.35209405  0.3521503
  0.35255261  0.35416277  0.35486102  0.35492884  0.35651903  0.35693773
  0.35694874  0.35860531  0.3586325   0.3601311   0.36077102  0.3608579
  0.36135351  0.36265732  0.36353222  0.36622361  0.36754006  0.36816123
  0.37001079  0.37153022  0.3715447   0.37260543  0.37279561  0.37519972
  0.37606024  0.37659015  0.37771838  0.37853774  0.38060026  0.38100688
  0.38279326  0.3831032   0.38323119  0.38412235  0.38412839  0.38435305
  0.38537346  0.3856215   0.38586005  0.38595411  0.38825662  0.38884622
  0.38926689  0.38932228  0.38940774  0.38964807  0.39019835  0.39279244
  0.39457328  0.39461366  0.39472921  0.39518276  0.39712245  0.3974392
  0.39769251  0.39872363  0.39996338  0.40033605  0.40054594  0.4017234
  0.40219994  0.40305786  0.40312481  0.40364     0.40472246  0.40474401
  0.4049287   0.40646747  0.40694992  0.40725682  0.40785314  0.41230995
  0.41301391  0.41365165  0.41516554  0.41696125  0.41814221  0.41842242
  0.41844063  0.42032923  0.42170758  0.42233232  0.42300205  0.42321538
  0.42482828  0.42484058  0.4250305   0.42530722  0.42820911  0.4282697
  0.43094281  0.43424308  0.43428753  0.43776209  0.43807822  0.43838693
  0.43914666  0.44176836  0.44250557  0.44299156  0.44517127  0.44664696
  0.44840203  0.45684142  0.45835847  0.46567722  0.46668409  0.47157969
  0.47493696  0.47539092  0.47594528  0.47736355  0.47806417  0.47893596
  0.48153439  0.48201992  0.48331561  0.48626294  0.48725237  0.48833257
  0.49411924  0.49762476  0.50196857  0.50270286  0.50305404  0.50930593
  0.51009982  0.51383287  0.51480715  0.5178661   0.51786661  0.51906084
  0.52100075  0.52898606  0.52954023  0.5316103   0.53773601  0.53787236
  0.5411771   0.5435626   0.54499424  0.5470858   0.55488101  0.55903737
  0.56424387  0.57283897  0.57590675  0.57767038  0.57955207  0.58055019
  0.58226181  0.58456431  0.59250176  0.59890056  0.60131368  0.60206302
  0.6058214   0.60878054  0.61081671  0.61162427  0.61401459  0.6203174
  0.62052206  0.62459431  0.62522668  0.62562317  0.62596616  0.62698462
  0.62701508  0.62835647  0.62844833  0.63074679  0.63083864  0.63191233
  0.63353162  0.63486394  0.63556034  0.63971033  0.64314593  0.64350297
  0.64368205  0.6498693   0.65177012  0.65203531  0.65464992  0.65999437
  0.6603507   0.66182087  0.66354771  0.66660149  0.66796956  0.66797619
  0.6683941   0.66850834  0.67020157  0.67089263  0.67222251  0.67377244
  0.67473027  0.67616275  0.67855306  0.68177963  0.68273598  0.68333369
  0.68524054  0.68762482  0.68837496  0.68984116  0.69001514  0.69448111
  0.6948018   0.69958243  0.7024562   0.70436305  0.70723683  0.70930646
  0.71115188  0.71153399  0.71201746  0.71354219  0.71362445  0.71388342
  0.71391828  0.71419969  0.7159325   0.71601861  0.71631462  0.71710599
  0.71844893  0.71870494  0.71971737  0.7207439   0.72209298  0.72348556
  0.72447151  0.72615126  0.72788407  0.72826619  0.73027439  0.73065048
  0.73197623  0.73208734  0.73313868  0.73983564  0.73989104  0.74049314
  0.74235206  0.74401856  0.74518366  0.74711686  0.74717459  0.7493969
  0.75039463  0.75156181  0.75170061  0.75292081  0.75554559  0.75758233
  0.75895815  0.75933424  0.75934027  0.76134847  0.76173059  0.76481068
  0.76523674  0.76525008  0.77042023  0.77569035  0.77613793  0.7774172
  0.78047098  0.78246366  0.78286129  0.78525161  0.78715846  0.78764192
  0.78880784  0.78904775  0.79264191  0.79441522  0.79481286  0.79594645
  0.79838526  0.79959349  0.80198381  0.80375951  0.80644374  0.80777289
  0.80978215  0.81258498  0.81822651  0.818716    0.82061682  0.82110632
  0.82349663  0.83305789  0.83581436  0.85017606  0.85042245  0.85288212
  0.85856677  0.86916085  0.87450022  0.88044677  0.88379472  0.88406148
  0.89295926  0.89522973  0.89791387  0.90142605  0.90460867  0.90974032
  0.91035493  0.91513556  0.92367457  0.92947744  0.92992502  0.93186776
  0.93425807  0.93664839  0.9390387   0.94381933  0.94512243  0.94923233
  0.95229338  0.9552814   0.9557709   0.95816121  0.95946432  0.96006203
  0.96294184  0.9720136   0.9741079   0.97440391  0.97918454  0.98202243
  0.98396517  0.98575777  0.98606165  0.99087434  0.99352643  0.9990448
  0.99991404  1.00069737  1.00181923  1.00308768  1.00567291  1.00786831
  1.00996261  1.02393704  1.02762319  1.02859781  1.02905495  1.0315544
  1.05055272  1.05630696  1.06108759  1.06846241  1.11726747  1.26569606
  1.29631179  2.53663387]

  UserWarning,

2022-10-31 11:01:49,781:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81219705 -0.51089605 -0.43866517 -0.37893118 -0.34634484 -0.3359245
 -0.25773317 -0.25435879 -0.12485832 -0.11859412 -0.09815983 -0.08087424
 -0.07552297 -0.07194131 -0.03130715 -0.03060841 -0.02587335 -0.02055669
  0.01539194  0.02053805  0.02863662  0.03610948  0.03945943  0.04566762
  0.05250905  0.05257065  0.06292916  0.06551879  0.08744528  0.09413208
  0.10170225  0.10210926  0.10658488  0.1123544   0.11434204  0.11683864
  0.11766464  0.11808358  0.12737825  0.13843173  0.14029367  0.14259209
  0.15297841  0.15754003  0.16303453  0.1665088   0.17717489  0.17959636
  0.18250296  0.19469571  0.19505191  0.19783217  0.19817061  0.19834019
  0.19963234  0.20149143  0.20283392  0.20305354  0.20421738  0.20671652
  0.20948633  0.21053412  0.2146518   0.21662185  0.21752118  0.21839719
  0.2190275   0.2228486   0.22568004  0.2263259   0.23048969  0.2311156
  0.23172223  0.23583047  0.23787254  0.23839018  0.24051224  0.24082491
  0.24107988  0.24131903  0.24369319  0.24387164  0.24398317  0.24635369
  0.24810895  0.24893144  0.25117699  0.25138521  0.25203679  0.25284204
  0.25356743  0.25457878  0.25458084  0.25519708  0.25599929  0.25760307
  0.25769993  0.25925479  0.26037581  0.26088318  0.2620578   0.26217824
  0.26395018  0.26401469  0.26429811  0.26443887  0.26493539  0.26538749
  0.26848426  0.26871262  0.26889502  0.26897605  0.26911809  0.26966634
  0.27097376  0.27104305  0.27122971  0.27265608  0.27377168  0.27486936
  0.2773301   0.27754153  0.27908151  0.28053696  0.28185057  0.28310103
  0.28409925  0.28438621  0.28465877  0.28498257  0.28537244  0.28558351
  0.28672081  0.28717727  0.28745869  0.28758295  0.28789283  0.28952915
  0.2898024   0.28985368  0.28992554  0.2903031   0.2908729   0.29100535
  0.29114302  0.29295482  0.29354277  0.29412604  0.29431757  0.29596697
  0.29807516  0.29834155  0.29864787  0.29981589  0.30036115  0.3012905
  0.30278503  0.30280127  0.30315045  0.30436978  0.30440015  0.30504682
  0.30735261  0.30750092  0.31005774  0.31157789  0.31291965  0.3149045
  0.31500822  0.31521267  0.31748521  0.31854167  0.31926676  0.31940544
  0.31992948  0.32007662  0.32142702  0.32281632  0.32388934  0.32505618
  0.325394    0.32564456  0.32675348  0.32684679  0.32709985  0.33040602
  0.33096728  0.33214233  0.33329148  0.334534    0.33795799  0.3391392
  0.3392043   0.33940811  0.33979728  0.34061624  0.34077884  0.34121875
  0.34179887  0.3421378   0.34224776  0.34346589  0.34407164  0.34534393
  0.34580396  0.3459266   0.34883301  0.34929817  0.35137195  0.35256222
  0.3536592   0.35607378  0.35731553  0.35814705  0.35829731  0.35976451
  0.36077129  0.36185894  0.36188584  0.36237111  0.36284178  0.3632492
  0.36335432  0.36444259  0.36631566  0.36656523  0.36669753  0.36719791
  0.3677919   0.36789852  0.36991664  0.3700443   0.37142498  0.37181744
  0.37274885  0.37292875  0.37293194  0.37442781  0.3745111   0.37827511
  0.37839438  0.37952488  0.37972929  0.3812321   0.38133165  0.3815468
  0.38210745  0.38217755  0.38315143  0.38382605  0.38453131  0.38831188
  0.38933519  0.38960002  0.38969699  0.38971902  0.39007773  0.39070082
  0.3913012   0.39159494  0.39188358  0.39376085  0.39383279  0.39458935
  0.39524504  0.39629964  0.39710408  0.39799278  0.39914648  0.39939704
  0.40048156  0.40060111  0.40209658  0.40343072  0.40567489  0.40594098
  0.40628968  0.4063406   0.40936457  0.40979436  0.41049896  0.41097181
  0.41272646  0.41413823  0.41480637  0.41523148  0.41898439  0.42008924
  0.42027267  0.42443198  0.42456188  0.4251781   0.4253635   0.42545647
  0.42779654  0.42909323  0.43061683  0.43080229  0.43147292  0.43310381
  0.43403205  0.43403361  0.43609901  0.43683657  0.43859755  0.43860328
  0.43937632  0.44767248  0.4481551   0.45082585  0.45300545  0.4536478
  0.45515548  0.45681604  0.45923871  0.46310204  0.46624999  0.46664597
  0.46673787  0.46837651  0.46969947  0.47617474  0.47736303  0.48154006
  0.48155882  0.48750264  0.48763206  0.48864913  0.48875542  0.49102516
  0.49120188  0.49211861  0.49333622  0.50230752  0.50237791  0.504216
  0.50647707  0.50773491  0.51100701  0.51267951  0.51398465  0.5253638
  0.52770028  0.52868426  0.53104157  0.53413811  0.53880994  0.54291528
  0.55943054  0.56025454  0.56228717  0.56575812  0.56633673  0.56764064
  0.56885492  0.56956945  0.57072201  0.57285103  0.58099625  0.58108053
  0.58165692  0.58739783  0.58742916  0.59301327  0.59600274  0.59769819
  0.59778768  0.59858569  0.60111204  0.60179756  0.60438719  0.60697682
  0.60858853  0.60898387  0.6147015   0.61545565  0.61547371  0.61733533
  0.61890164  0.62251459  0.6236705   0.62525472  0.62573124  0.6294158
  0.62990378  0.63107315  0.63463532  0.63486333  0.63546273  0.63805236
  0.6408148   0.6414142   0.64323162  0.64519901  0.64582125  0.64699062
  0.65359013  0.65403246  0.65545656  0.65617976  0.65633026  0.65843951
  0.65872519  0.65883068  0.6595416   0.66093391  0.66135902  0.66394865
  0.66396147  0.66653828  0.66666706  0.66731049  0.6691279   0.67430716
  0.67666213  0.67689679  0.67814805  0.67948642  0.68024411  0.68025863
  0.68496166  0.68542878  0.68769763  0.69061715  0.69171136  0.69392129
  0.69517469  0.6966334   0.70097566  0.70108101  0.70279308  0.7040601
  0.70611071  0.70615492  0.70655208  0.70754722  0.70797234  0.70960233
  0.71056196  0.71071247  0.71306744  0.71512566  0.71541134  0.71564714
  0.71574122  0.71589172  0.71816226  0.723085    0.72423716  0.72537653
  0.72603833  0.72796616  0.72883987  0.72953001  0.73097651  0.73165708
  0.73314542  0.73491105  0.73691397  0.73710304  0.73714872  0.73982009
  0.7409143   0.74092577  0.74146892  0.74304417  0.74350393  0.74576368
  0.74609356  0.75127282  0.75150878  0.75172141  0.75660258  0.7590417
  0.75959632  0.76209537  0.76218595  0.76317892  0.76475674  0.76907034
  0.77198985  0.77383099  0.77457948  0.77538841  0.77716911  0.77878082
  0.77942885  0.77975873  0.78234836  0.78566505  0.78752762  0.78825468
  0.78913934  0.79026775  0.79270688  0.79358673  0.79496662  0.79529651
  0.79946138  0.80306539  0.80559407  0.80565502  0.81860316  0.82548394
  0.82652255  0.82735413  0.8286318   0.83106406  0.83429144  0.83614633
  0.83682012  0.83844738  0.84148471  0.8435871   0.84416957  0.85220739
  0.85907329  0.85983152  0.86235224  0.86276967  0.8694361   0.87450994
  0.88067654  0.88233454  0.89048433  0.89052853  0.89311816  0.89322446
  0.89829742  0.90621681  0.9075952   0.91653112  0.91715188  0.92650873
  0.92678333  0.92937296  0.93163271  0.93206889  0.93245499  0.93714185
  0.93875357  0.93973148  0.93988198  0.94232111  0.94491074  0.94619257
  0.94652245  0.94761954  0.94947443  0.95008999  0.95283012  0.95785888
  0.95863109  0.96026912  0.96044851  0.96577827  0.96821739  0.97252384
  0.97811085  0.97857591  0.9864953   0.98989385  0.99167455  0.99685381
  0.99944344  1.00100975  1.00160796  1.00706182  1.00721233  1.00780073
  1.01655162  1.02016047  1.02533973  1.02731379  1.03439045  1.06508329
  1.07392711]

  UserWarning,

2022-10-31 11:01:49,828:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77224717 -0.74862747 -0.50257446 -0.39064432 -0.27797477 -0.2575772
 -0.18786206 -0.16517645 -0.14688948 -0.08380591 -0.04005814 -0.00448239
  0.02125225  0.02372309  0.04330348  0.0709786   0.07553126  0.07662885
  0.10338273  0.10356763  0.11604264  0.11999216  0.1241977   0.12719506
  0.14068104  0.14102355  0.14505905  0.1540924   0.15874226  0.15999593
  0.16031228  0.16135147  0.16232583  0.16458831  0.16473908  0.16571599
  0.17340688  0.17952491  0.18359781  0.18503832  0.18623298  0.18672808
  0.18881302  0.19249899  0.19267569  0.1946815   0.19476261  0.19949249
  0.20000918  0.20175381  0.20280571  0.20710966  0.2157884   0.21708049
  0.2191776   0.21973637  0.21982091  0.22119713  0.22243484  0.22443534
  0.22643384  0.22749138  0.22860573  0.2325713   0.23277061  0.23541846
  0.23549136  0.23853315  0.23869463  0.2405921   0.24065688  0.24320639
  0.24322132  0.24342744  0.24459623  0.24774478  0.24779208  0.2481148
  0.24824068  0.25136634  0.25256218  0.25261197  0.25281521  0.25324136
  0.25449153  0.25488607  0.25580673  0.25608386  0.25712793  0.2595164
  0.26112881  0.26286558  0.26325697  0.26353501  0.26436498  0.26608131
  0.26655176  0.26783682  0.2696009   0.27032145  0.27034555  0.27044203
  0.27095939  0.27238098  0.27284223  0.27558105  0.27728976  0.27753638
  0.27869551  0.27955982  0.28018593  0.28060053  0.28119514  0.28268721
  0.282859    0.28332022  0.28402988  0.28480613  0.28546211  0.28549196
  0.28650053  0.28781687  0.28793453  0.28914477  0.29180416  0.29450022
  0.2948403   0.29524978  0.29676067  0.29682725  0.29705566  0.29735555
  0.29755529  0.29974094  0.30143192  0.30245402  0.30247274  0.30313988
  0.30453661  0.30474875  0.30593981  0.30677767  0.30870298  0.30881064
  0.3088274   0.30961326  0.31163279  0.31224999  0.31278855  0.31424541
  0.31443234  0.31530146  0.31543769  0.31546162  0.31661261  0.3171257
  0.31732583  0.3182638   0.32040045  0.32133473  0.32242321  0.32425895
  0.32610929  0.3262021   0.32643825  0.32680861  0.32870924  0.32965889
  0.32976931  0.32987434  0.33088922  0.33332625  0.3333389   0.33416345
  0.33516516  0.33540837  0.33542195  0.33636557  0.33685834  0.33715668
  0.33760065  0.33833001  0.33890728  0.33973435  0.33993323  0.34005312
  0.34047984  0.34125822  0.34225294  0.34321037  0.34445026  0.34460426
  0.34547874  0.34549347  0.3455912   0.34573359  0.3462022   0.34751411
  0.34891674  0.35108395  0.3511297   0.35117584  0.35198412  0.35243644
  0.35295734  0.35307571  0.35350711  0.35372173  0.35376534  0.35400469
  0.35420472  0.35561131  0.35590274  0.35621407  0.35644596  0.35651604
  0.3574543   0.35839882  0.35902469  0.36001987  0.36053098  0.3613415
  0.36164064  0.362688    0.36273803  0.36409052  0.3646359   0.36580315
  0.36728314  0.36804987  0.3685228   0.36947764  0.37009459  0.37127581
  0.37181819  0.37272167  0.37399051  0.37430092  0.37465202  0.37778656
  0.37788004  0.37831055  0.38035442  0.38101848  0.38108453  0.38124549
  0.38182749  0.38284276  0.38337605  0.38377627  0.38513473  0.38687263
  0.38705679  0.38734051  0.38846438  0.39104952  0.39216233  0.39375365
  0.39424167  0.39546889  0.39625894  0.39688631  0.39830052  0.39891115
  0.40168756  0.40179182  0.40217558  0.40264239  0.40268493  0.40331508
  0.4034817   0.40650615  0.40697684  0.40746486  0.40764088  0.40940135
  0.41124649  0.41204357  0.41384848  0.41444119  0.41551791  0.41633993
  0.41865442  0.41998981  0.42188996  0.4250019   0.42876614  0.43069886
  0.43105449  0.43428012  0.43452141  0.43569456  0.43789149  0.43838439
  0.44209567  0.44625054  0.44960676  0.45267189  0.45403136  0.45666809
  0.46285443  0.46388885  0.46411607  0.46528541  0.46562173  0.46750222
  0.46765569  0.46796348  0.46917211  0.4694428   0.47015967  0.47064228
  0.4795525   0.48524537  0.48897399  0.48974921  0.49469714  0.50089745
  0.5035524   0.50769368  0.5085044   0.51006409  0.51524094  0.51803263
  0.52112283  0.52265813  0.52969549  0.53388749  0.53430328  0.53671731
  0.53974391  0.54086825  0.54504344  0.54978995  0.55066786  0.55071968
  0.55073264  0.55322927  0.55323834  0.55796515  0.55990175  0.56202536
  0.56787261  0.56797407  0.56901327  0.56995927  0.57145529  0.57260391
  0.58047903  0.58053782  0.58191991  0.58340747  0.59073691  0.59323709
  0.59376101  0.5951431   0.6003109   0.60043237  0.6069842   0.60962883
  0.61211366  0.61227347  0.61457461  0.61756275  0.61894593  0.62285202
  0.62549666  0.62978476  0.63145962  0.63160972  0.63524872  0.63592739
  0.63607521  0.63947061  0.64121667  0.64137873  0.64665376  0.64744449
  0.6488991   0.6492984   0.64931265  0.65145502  0.65194303  0.65282362
  0.65376118  0.65458767  0.65645231  0.65987695  0.66252159  0.66516622
  0.66948388  0.6703667   0.6713107   0.67207012  0.67310014  0.67350865
  0.67432126  0.67448332  0.67491828  0.6749666   0.67574477  0.67838941
  0.67884767  0.68103405  0.68607711  0.68896796  0.68898221  0.69125273
  0.69470873  0.69486018  0.69502225  0.69533584  0.69651058  0.69675406
  0.69690187  0.69778246  0.69801218  0.69956076  0.70000477  0.7022054
  0.70468797  0.70484562  0.70485004  0.70748042  0.70749467  0.70750285
  0.70918061  0.70997725  0.71213592  0.71278395  0.71541434  0.71612865
  0.71629492  0.71708853  0.71807322  0.7205313   0.7219786   0.72287448
  0.7229712   0.72320043  0.7233625   0.72402194  0.72645115  0.72687348
  0.72865177  0.7299465   0.73054213  0.73129641  0.73174042  0.73207289
  0.73239604  0.73377898  0.73473178  0.73658569  0.73745203  0.73870451
  0.74009666  0.74534532  0.74538594  0.74692941  0.74700217  0.7475569
  0.74882301  0.75040819  0.75311826  0.75331985  0.75860913  0.76098674
  0.76125376  0.76238198  0.76380854  0.76518868  0.76551464  0.76704728
  0.76765691  0.76815927  0.76918768  0.77183231  0.77447695  0.77513639
  0.77518346  0.77787235  0.78770014  0.79034478  0.79298942  0.7978496
  0.80092333  0.80356797  0.8062126   0.81152099  0.8120135   0.81414652
  0.81479113  0.81482592  0.81943579  0.81989291  0.82311941  0.82423185
  0.82457725  0.82760599  0.82782796  0.83530362  0.83794826  0.84063267
  0.84803879  0.85068343  0.85332806  0.86068852  0.86417283  0.86424681
  0.86831799  0.87938482  0.89379274  0.89515866  0.90005364  0.90269828
  0.91062777  0.91230521  0.91264099  0.91592146  0.92106292  0.92370756
  0.92650001  0.92695141  0.92836647  0.93164147  0.93178929  0.93254005
  0.93472036  0.93707856  0.93957539  0.94486466  0.94501248  0.94934305
  0.95030175  0.95279857  0.95808785  0.9608803   0.96352494  0.9686664
  0.9714731   0.97230538  0.97552719  0.97660031  0.97842115  0.98188959
  0.98371042  0.98420827  0.98453423  0.98644905  0.98717886  0.98778494
  0.98792962  0.9898235   0.99230304  0.99321527  0.99462476  0.99511278
  0.99672567  1.00416056  1.00569133  1.00784795  1.01362524  1.0179429
  1.02288928  1.02420379  1.0393418   1.03995274  1.04967855  1.3159016
  1.44084446]

  UserWarning,

2022-10-31 11:01:50,829:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94547024 -0.7769212  -0.475268   -0.39286633 -0.3342177  -0.31828377
 -0.31582689 -0.30841293 -0.2684237  -0.26011261 -0.25918253 -0.255448
 -0.12385182 -0.11839672 -0.10368724 -0.09856177 -0.05762328 -0.04689406
 -0.02913981 -0.02853047 -0.00235619  0.00617414  0.0230898   0.02551127
  0.0327106   0.03591053  0.05462452  0.06077473  0.06645141  0.066876
  0.08110093  0.08489302  0.08581859  0.09230081  0.09677853  0.1026558
  0.1111432   0.12642578  0.126697    0.12749689  0.13560586  0.14893953
  0.15158257  0.1586888   0.17082699  0.17288944  0.17438129  0.17638239
  0.1770858   0.17894021  0.18470933  0.19003952  0.19523459  0.2023678
  0.20553985  0.20557671  0.20615     0.20907266  0.2092559   0.21011132
  0.21157819  0.21158038  0.21170576  0.21228063  0.21544919  0.2160408
  0.21659591  0.21695481  0.21735582  0.21854989  0.22021886  0.22436046
  0.22471874  0.2335439   0.23539492  0.23585576  0.23690381  0.23883767
  0.2393378   0.24141753  0.24151799  0.24162932  0.24448089  0.24656035
  0.24764854  0.24807353  0.24853669  0.24948812  0.24999802  0.25010911
  0.25142998  0.25452029  0.25720127  0.25745161  0.25764395  0.26281678
  0.26283238  0.26291113  0.26425001  0.26439542  0.26501076  0.26550845
  0.26629931  0.26841303  0.26895505  0.27008826  0.27043693  0.2705421
  0.27167103  0.27207276  0.27239086  0.27315774  0.27346394  0.27384822
  0.27441927  0.27587736  0.27656424  0.27657219  0.27673234  0.27849738
  0.2787583   0.2812762   0.28195562  0.28268919  0.28348847  0.28350872
  0.28397096  0.2840047   0.28447047  0.28608999  0.28614168  0.28721282
  0.2877382   0.29007026  0.29010392  0.29021936  0.2910541   0.291314
  0.291468    0.29192382  0.29487624  0.2969387   0.29724117  0.29749065
  0.29749077  0.29787786  0.29799934  0.30171722  0.30328991  0.30389433
  0.3044576   0.30630384  0.30755444  0.31122846  0.31198961  0.31234061
  0.31247107  0.31295219  0.31307725  0.31343401  0.3139147   0.31453246
  0.31753968  0.31798459  0.31805659  0.31811404  0.31914702  0.31992056
  0.3211042   0.32153237  0.32643793  0.32736012  0.32806381  0.3283722
  0.32868585  0.3312465   0.33325703  0.33335466  0.33400093  0.33525222
  0.33567382  0.33668267  0.33761054  0.33946344  0.34021865  0.3404125
  0.34171032  0.34232112  0.3424479   0.34524721  0.34615581  0.34624389
  0.34706415  0.34707618  0.34777239  0.34831392  0.34888026  0.34894465
  0.35084192  0.35127049  0.35252923  0.35296862  0.35480602  0.35947403
  0.35978647  0.36358045  0.36434788  0.36655751  0.36743964  0.37213312
  0.37255604  0.37257191  0.37446411  0.3761311   0.37662853  0.37698291
  0.37725052  0.37791427  0.37832209  0.37895605  0.37973217  0.3798117
  0.37984411  0.38028643  0.38056454  0.38132042  0.3815238   0.38225617
  0.38303407  0.38342287  0.38458412  0.38465436  0.38510293  0.38555306
  0.38646736  0.38716559  0.38728324  0.38752793  0.3882337   0.38891002
  0.39014733  0.3905364   0.39075877  0.39272507  0.39335251  0.39343333
  0.39409365  0.39424075  0.39472553  0.39530645  0.39612324  0.39754191
  0.40189592  0.40314096  0.4038326   0.40464543  0.40485059  0.40638542
  0.40643241  0.40708489  0.40821777  0.40899031  0.40903223  0.40923198
  0.41084814  0.4112559   0.41158041  0.41240366  0.41246552  0.41354546
  0.41408385  0.41549468  0.41644835  0.41702303  0.41709515  0.41913417
  0.42123369  0.42229338  0.42242483  0.42517515  0.42533601  0.42555114
  0.42894292  0.43126092  0.43381942  0.43537043  0.43631764  0.43891745
  0.44265308  0.44411709  0.44590245  0.44593936  0.44618902  0.4492295
  0.44954016  0.45346589  0.45456103  0.45523738  0.45559959  0.45566281
  0.45687339  0.45703535  0.45711617  0.45806308  0.45830616  0.46195324
  0.46421528  0.46721648  0.48241399  0.48325441  0.48661998  0.48830335
  0.48907486  0.49500913  0.4957891   0.49802634  0.49852835  0.5006127
  0.50101285  0.50301471  0.50303699  0.50451729  0.50581233  0.50919945
  0.50998364  0.5124732   0.51345032  0.51968394  0.52140135  0.5317766
  0.53255673  0.53334799  0.53701013  0.54121616  0.54942401  0.55077029
  0.55232651  0.55515458  0.55520884  0.55533708  0.55533766  0.5625152
  0.56845103  0.56921866  0.57515943  0.57519063  0.57537321  0.57676845
  0.58196808  0.5845679   0.59104095  0.59144481  0.59194911  0.59289351
  0.59424395  0.59496717  0.59533187  0.60699338  0.60722231  0.60796625
  0.61219301  0.61836551  0.62876477  0.63206841  0.63299154  0.63396441
  0.63656422  0.63916404  0.63976269  0.64792872  0.65100065  0.65152654
  0.65216312  0.65307165  0.65345882  0.65468211  0.65476294  0.65510297
  0.65736275  0.65754316  0.65996257  0.66124982  0.66360844  0.66384945
  0.66384963  0.6648008   0.6651622   0.66620825  0.66762873  0.66776202
  0.66880807  0.67036183  0.67043018  0.67274122  0.67296165  0.67380863
  0.67556146  0.67660752  0.67720617  0.6788006   0.67920733  0.67944853
  0.6807611   0.68440696  0.68464816  0.68524681  0.6877106   0.6896066
  0.69339878  0.69480623  0.69504743  0.69746916  0.69947454  0.70000586
  0.70024706  0.70066737  0.70284687  0.70291918  0.70415944  0.70432419
  0.70495086  0.70520549  0.70667844  0.70675926  0.70780531  0.71040513
  0.71447788  0.71455871  0.71541855  0.71560476  0.71584596  0.72077949
  0.7224045   0.72445291  0.73380347  0.73724121  0.73854396  0.74114378
  0.74277462  0.74412191  0.74634341  0.74646651  0.74894323  0.75040446
  0.75154304  0.75200218  0.75406204  0.75407008  0.75996201  0.76004283
  0.76454212  0.76471611  0.7653891   0.76714194  0.76999141  0.77073255
  0.77234157  0.77384942  0.77461026  0.77921848  0.77973913  0.78034553
  0.780988    0.78315373  0.78562563  0.78605641  0.78794047  0.79054028
  0.79103937  0.79644373  0.79833973  0.80113483  0.80353936  0.80613918
  0.808739    0.81133881  0.81233972  0.81314583  0.81515867  0.81653845
  0.82433789  0.82663825  0.82866353  0.83452538  0.83943682  0.83972501
  0.83993679  0.84023007  0.84203154  0.85012427  0.85347686  0.85868322
  0.86354759  0.86520024  0.86572317  0.8719136   0.88157172  0.89531194
  0.90131309  0.90361963  0.90463959  0.90472041  0.90600766  0.90732023
  0.91511968  0.91531943  0.91616573  0.92023849  0.92031931  0.92129199
  0.92291912  0.92512601  0.93323757  0.93460564  0.93851246  0.94111784
  0.94363683  0.9450049   0.94589923  0.94631747  0.94651722  0.9475239
  0.94760472  0.94891728  0.95020453  0.952278    0.95280435  0.95436657
  0.95800398  0.96007745  0.9606038   0.96191637  0.96320361  0.96572261
  0.96971581  0.97231563  0.97293863  0.97307653  0.97491545  0.97690651
  0.97880251  0.98140233  0.98400214  0.98660196  0.98860325  0.98920178
  0.99180159  0.99700122  0.99907469  1.0034326   1.00480067  1.00597288
  1.00740049  1.01217185  1.01609426  1.01677283  1.0263601   1.03027248
  1.03631909  1.07315679  1.07389955  1.08187185  1.2273629   1.25953641
  1.27264062  1.33467606  1.39584365]

  UserWarning,

2022-10-31 11:01:50,854:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4051563  -0.40513932 -0.35103044 -0.2330514  -0.21247933 -0.18264918
 -0.13817349 -0.12681862 -0.09967875 -0.03978759 -0.02461572 -0.02023458
 -0.01592225 -0.00524282  0.0222814   0.03979957  0.04002484  0.04368484
  0.05127418  0.05165241  0.07441814  0.08253408  0.0862313   0.089347
  0.10093851  0.10241407  0.10688991  0.11597369  0.12625719  0.12801135
  0.13247995  0.13357617  0.13383359  0.13594837  0.14739853  0.15127366
  0.15286655  0.15316628  0.15611028  0.16050176  0.16194962  0.16348284
  0.16542811  0.18669191  0.19146363  0.19262117  0.19475407  0.1948737
  0.19910781  0.20209062  0.20292288  0.20647704  0.20704483  0.20799744
  0.20845553  0.21020249  0.21329357  0.21357014  0.21434883  0.21581551
  0.21586655  0.21914479  0.21979914  0.22149076  0.22152114  0.22227012
  0.22232146  0.22291616  0.22428778  0.22864595  0.22884517  0.22894572
  0.23085042  0.23382831  0.23397248  0.23468876  0.23572877  0.23590029
  0.23632385  0.23652307  0.24145602  0.24183377  0.24215233  0.24332384
  0.24568205  0.24617115  0.24823232  0.24964627  0.25136275  0.251919
  0.25493376  0.25713651  0.25828862  0.25918749  0.2635571   0.26665266
  0.26726465  0.26802519  0.26902843  0.26923876  0.26929472  0.27100927
  0.27122636  0.27210955  0.27326415  0.2734712   0.2736799   0.2749884
  0.27675862  0.27788084  0.27969626  0.2819594   0.28215278  0.28233622
  0.28297377  0.28463041  0.28623232  0.28654467  0.28694381  0.28717355
  0.28759229  0.28791311  0.28961843  0.28964992  0.2898756   0.29030085
  0.29059693  0.29082643  0.29163415  0.29163953  0.29202848  0.29338239
  0.2939855   0.29655731  0.29656512  0.29658344  0.29716614  0.29737117
  0.29761043  0.29816707  0.29827158  0.29904856  0.29907447  0.29995422
  0.30065878  0.30130793  0.30192443  0.30215546  0.3022679   0.30314669
  0.30335369  0.30368624  0.30397468  0.30411493  0.30469389  0.30476432
  0.30530817  0.30626912  0.30705926  0.3083699   0.30951013  0.3100403
  0.31081157  0.31245291  0.31320944  0.31370859  0.31395819  0.31407625
  0.31654586  0.31742393  0.31751827  0.32014082  0.32077153  0.32269314
  0.32425021  0.32469806  0.32490107  0.32511623  0.32586379  0.32623024
  0.32658796  0.32692621  0.32845691  0.32877224  0.32987487  0.33019465
  0.33151717  0.33377394  0.33414388  0.33498874  0.33629318  0.3363925
  0.33691417  0.33753388  0.33757744  0.33787672  0.33935184  0.34125655
  0.34192837  0.34227128  0.34283387  0.34547919  0.34717693  0.34779053
  0.34782649  0.34807262  0.34879075  0.3489736   0.3497017   0.34976122
  0.34990619  0.34993565  0.35243988  0.35391699  0.35410907  0.35435556
  0.35485313  0.35512392  0.3556231   0.35615433  0.35694377  0.35727452
  0.35970756  0.36125935  0.36201449  0.36285703  0.36318316  0.36340267
  0.36416031  0.36447428  0.36489656  0.36510507  0.36525511  0.36720818
  0.36732218  0.36749937  0.37103736  0.3711649   0.37198539  0.37308854
  0.37389575  0.37412984  0.374578    0.37611352  0.37613771  0.37806883
  0.37985751  0.38017507  0.38086249  0.38097032  0.38309973  0.3835084
  0.38482274  0.38604648  0.38667511  0.38743639  0.38934425  0.39004224
  0.39026359  0.39106707  0.39112265  0.39121828  0.39140443  0.3922089
  0.39410376  0.39413604  0.3952324   0.39619881  0.39732136  0.39740915
  0.39743531  0.39873689  0.39893662  0.40127497  0.40280145  0.40331907
  0.40375748  0.40381306  0.40420302  0.40434282  0.40586565  0.40635114
  0.40705802  0.40888922  0.40975244  0.41026948  0.41199583  0.41578098
  0.41874656  0.41940299  0.42168507  0.42221054  0.42237937  0.42385883
  0.42501109  0.42658712  0.43001543  0.43168059  0.43402432  0.43425835
  0.43662006  0.44101641  0.44432751  0.44641932  0.4471961   0.44973419
  0.4501304   0.45194175  0.45227227  0.45275831  0.45481035  0.45734843
  0.4596998   0.46202019  0.46209408  0.46435636  0.46482729  0.46717024
  0.47115615  0.47393034  0.47405777  0.47474419  0.48148677  0.48350206
  0.48442394  0.49077086  0.49198973  0.49289988  0.4983851   0.51046369
  0.51099674  0.51366113  0.513837    0.51769008  0.51816754  0.52127698
  0.5215249   0.52258466  0.53138017  0.53391825  0.544015    0.54407058
  0.54733618  0.55003868  0.5551812   0.56030459  0.5623855   0.5704556
  0.57464328  0.57724513  0.58201411  0.58491554  0.5865934   0.58703272
  0.58724828  0.59247363  0.59447058  0.59729193  0.60056809  0.60164453
  0.60593644  0.60607118  0.60661231  0.607133    0.60943807  0.61168848
  0.61218967  0.61531636  0.61785444  0.61868269  0.62039253  0.62293061
  0.62565036  0.62883502  0.63054485  0.63456417  0.63562102  0.64069718
  0.64323526  0.64454142  0.64577334  0.64829514  0.64831143  0.65084951
  0.65284646  0.65338759  0.65794527  0.66096883  0.66161449  0.666078
  0.66694823  0.66852122  0.66861608  0.67047418  0.67115416  0.67138982
  0.67181718  0.67369224  0.67623033  0.67822728  0.67876841  0.67973995
  0.68003028  0.68107264  0.68133683  0.68467282  0.68487742  0.68584152
  0.68892074  0.69228707  0.69345577  0.69549443  0.69575048  0.69647941
  0.69653498  0.69663329  0.697198    0.69736323  0.69853193  0.69907306
  0.70107001  0.70161115  0.7036081   0.70414923  0.71176347  0.71305562
  0.71555834  0.7162985   0.71773831  0.71961337  0.71999326  0.72765498
  0.72886885  0.72892349  0.73220893  0.73266384  0.73267557  0.7353396
  0.73887939  0.73906866  0.73964937  0.74205146  0.7421256   0.74288348
  0.74720176  0.74732337  0.74795964  0.75008837  0.75475001  0.75557388
  0.7576846   0.75811197  0.76045652  0.76445     0.76750642  0.76826429
  0.76952564  0.77080238  0.7712826   0.77206372  0.7732374   0.77334046
  0.77587854  0.77841662  0.7809547   0.78335425  0.78397301  0.78603087
  0.78626652  0.78887498  0.78904917  0.79364511  0.79612762  0.79872128
  0.80040521  0.80125936  0.80149501  0.8040331   0.80627995  0.80736741
  0.80777695  0.81137868  0.81394977  0.81418542  0.81517863  0.81565678
  0.81679873  0.81897036  0.82031808  0.82052293  0.82179967  0.82539424
  0.83052782  0.831952    0.83225061  0.83422142  0.83673693  0.83775777
  0.83933059  0.83956624  0.84718049  0.84795257  0.85179688  0.85302874
  0.85325592  0.85733281  0.86212457  0.86318107  0.86566357  0.86696508
  0.86930398  0.87229265  0.87436624  0.88327522  0.88348572  0.89461532
  0.89612865  0.89871421  0.90379037  0.90594278  0.90632846  0.90656411
  0.91140462  0.91164028  0.9139427   0.91648078  0.91901887  0.92155695
  0.9217926   0.92433069  0.92657754  0.92663311  0.93170927  0.93194493
  0.93424736  0.93663939  0.93932352  0.93955918  0.9400394   0.94439968
  0.94693777  0.94717342  0.94822018  0.94947585  0.95201393  0.95831111
  0.96216626  0.96240191  0.96470434  0.96494     0.96709637  0.9697805
  0.97020935  0.97509232  0.97736174  0.9776304   0.98016849  0.98270657
  0.98318679  0.98524465  0.98778273  0.99252838  0.9928589   0.99539698
  0.99841528  1.00047314  1.0055493   1.01167222  1.01560678  1.02098168
  1.02324721  1.02829718  1.28353226]

  UserWarning,

2022-10-31 11:01:50,856:INFO:Calculating mean and std
2022-10-31 11:01:50,856:INFO:Creating metrics dataframe
2022-10-31 11:01:50,860:INFO:Uploading results into container
2022-10-31 11:01:50,860:INFO:Uploading model into container now
2022-10-31 11:01:50,861:INFO:master_model_container: 2
2022-10-31 11:01:50,861:INFO:display_container: 2
2022-10-31 11:01:50,861:INFO:LinearRegression(n_jobs=-1)
2022-10-31 11:01:50,861:INFO:create_model() successfully completed......................................
2022-10-31 11:01:50,965:ERROR:create_model() for LinearRegression(n_jobs=-1) raised an exception or returned all 0.0:
2022-10-31 11:01:50,965:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:01:50,965:INFO:Initializing Lasso Regression
2022-10-31 11:01:50,965:INFO:Total runtime is 0.2995858947436015 minutes
2022-10-31 11:01:50,965:INFO:SubProcess create_model() called ==================================
2022-10-31 11:01:50,965:INFO:Initializing create_model()
2022-10-31 11:01:50,965:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:01:50,965:INFO:Checking exceptions
2022-10-31 11:01:50,965:INFO:Importing libraries
2022-10-31 11:01:50,965:INFO:Copying training dataset
2022-10-31 11:01:50,981:INFO:Defining folds
2022-10-31 11:01:50,981:INFO:Declaring metric variables
2022-10-31 11:01:50,981:INFO:Importing untrained model
2022-10-31 11:01:50,981:INFO:Lasso Regression Imported successfully
2022-10-31 11:01:50,981:INFO:Starting cross validation
2022-10-31 11:01:50,981:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:01:52,902:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.29330418 -1.03166604 -0.7450064  -0.68425415 -0.2431175  -0.21543473
 -0.19660297 -0.17416203 -0.16564372 -0.1375903  -0.09207047 -0.06515115
 -0.05251586 -0.04488175 -0.04067885 -0.02536218 -0.00529496  0.00572611
  0.01761279  0.03125843  0.07187555  0.08373856  0.09530981  0.11789617
  0.12316403  0.12989934  0.13788894  0.14031497  0.15102376  0.17236063
  0.17569577  0.1818138   0.19611197  0.19796194  0.20285258  0.21312499
  0.21344638  0.21683833  0.2194052   0.22058546  0.22216095  0.22764117
  0.22928903  0.23667806  0.23844255  0.2449115   0.25285942  0.25749162
  0.25945025  0.26056856  0.27594781  0.27661978  0.27664458  0.27726147
  0.27929437  0.2960428   0.2992338   0.29939724  0.30199873  0.30830658
  0.30844463  0.31032518  0.31364131  0.33043404  0.33589002  0.33637687
  0.33759675  0.3387135   0.34001141  0.36283341  0.36463114  0.36982028
  0.37019616  0.37154075  0.37155054  0.3741184   0.37483002  0.37642702
  0.3838063   0.38584468  0.39092361  0.39119172  0.39253845  0.39275532
  0.39568784  0.39721619  0.39942803  0.39964903  0.40076072  0.40176142
  0.40349148  0.40355802  0.40480656  0.40521999  0.41086704  0.41697857
  0.41726004  0.41902425  0.42323119  0.42874507  0.43170172  0.43174694
  0.43238285  0.43338541  0.43474857  0.43714302  0.43729246  0.44300196
  0.44329587  0.4436425   0.44413752  0.44554833  0.44574594  0.44741292
  0.44799203  0.44860272  0.45096733  0.45183915  0.45297654  0.45493274
  0.45499797  0.45524021  0.45609232  0.45694282  0.45835933  0.45872131
  0.45917483  0.46043712  0.46138735  0.46182969  0.46305868  0.46321685
  0.46337455  0.46384413  0.46405302  0.46523009  0.4653523   0.46622379
  0.46707084  0.46787082  0.46801145  0.46862751  0.47141997  0.47201776
  0.47227428  0.47329321  0.47354193  0.47798522  0.47888379  0.48121178
  0.48278898  0.48408557  0.48574797  0.48923245  0.48987277  0.49256097
  0.49369571  0.4938317   0.49400738  0.49492336  0.49543782  0.49571961
  0.49576232  0.49656245  0.49759108  0.49866209  0.49877272  0.4990334
  0.49970981  0.49972035  0.49981992  0.50046481  0.5008489   0.5014439
  0.50169691  0.50204278  0.50268346  0.50296336  0.50315458  0.50332808
  0.50387203  0.50410186  0.50504599  0.50518456  0.50575129  0.50583334
  0.50626331  0.5079716   0.50798467  0.50883925  0.51051038  0.51087327
  0.51093749  0.51196442  0.51206224  0.5121235   0.51232691  0.51316585
  0.51411912  0.51526329  0.51543317  0.51600213  0.51645863  0.51691965
  0.51722406  0.51998926  0.5220852   0.52224448  0.52227767  0.52240847
  0.5228172   0.52293952  0.52326594  0.5233663   0.52467525  0.52494704
  0.52533963  0.52550119  0.52577338  0.52964309  0.53026639  0.53214334
  0.53341766  0.53365973  0.53471322  0.53586924  0.53622365  0.53626588
  0.53664348  0.53667023  0.53673232  0.53806762  0.53818759  0.53853583
  0.53874232  0.53901151  0.54039982  0.54259287  0.54347472  0.54472297
  0.54528253  0.54544378  0.54602202  0.54727694  0.54746554  0.54759951
  0.54818544  0.5482591   0.54887778  0.54909024  0.55026589  0.55051801
  0.55066265  0.55196653  0.55229023  0.55390431  0.55508889  0.55512755
  0.55534029  0.55545925  0.55621205  0.56039195  0.56108002  0.566573
  0.56768016  0.57122325  0.57277015  0.57402676  0.5744087   0.57615586
  0.57659089  0.5770235   0.57802418  0.58011112  0.58100107  0.58208579
  0.58270031  0.58353673  0.58410286  0.58427282  0.58439367  0.58609308
  0.58667161  0.58709605  0.58759678  0.58763875  0.5878416   0.58834476
  0.58845493  0.58996378  0.5899988   0.59004158  0.59152694  0.5915909
  0.5927535   0.59369918  0.59421192  0.5947561   0.59478414  0.59499119
  0.59594471  0.59666806  0.59855847  0.60030953  0.60291948  0.6031796
  0.60431776  0.60461288  0.60523942  0.60595876  0.60665775  0.60754195
  0.60922203  0.61094778  0.61325692  0.61688845  0.61761671  0.61866101
  0.6189907   0.6210288   0.62185741  0.62205773  0.6220827   0.62397218
  0.62720359  0.62752826  0.62792238  0.62828929  0.63379832  0.63658676
  0.63851627  0.63925789  0.64506342  0.65612168  0.66304344  0.66583446
  0.67052855  0.67169009  0.68222564  0.69311406  0.70911455  0.71831113
  0.72157055  0.73301673  0.74546296  0.76161311  0.80710681  0.80803571
  0.82144726  0.83546323  0.99240213  1.02263864  1.26145386  1.75370926]

  UserWarning,

2022-10-31 11:01:52,918:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86727901 -0.60546046 -0.47389979 -0.23729939 -0.21308689 -0.13843315
 -0.08545726 -0.06304646 -0.00818525  0.02109596  0.0583449   0.06145274
  0.0643398   0.073449    0.07409438  0.10980668  0.11797491  0.12055652
  0.12463834  0.15850252  0.20939299  0.21789437  0.22197343  0.23663063
  0.24446716  0.24474084  0.24676026  0.27185524  0.27758145  0.28377972
  0.28936111  0.29762513  0.30009852  0.30397348  0.30718748  0.30757493
  0.31075505  0.31672873  0.3169592   0.33091189  0.33258209  0.33526871
  0.33648786  0.34266901  0.34409647  0.34597598  0.34981158  0.35126019
  0.35386418  0.35557459  0.3569295   0.35814371  0.37076763  0.37174244
  0.37187013  0.37302123  0.37332847  0.3761071   0.38077206  0.38501846
  0.38790384  0.39487974  0.39534446  0.40010514  0.40280418  0.40391464
  0.40409005  0.40495332  0.40510065  0.40589526  0.40664436  0.40962339
  0.41246111  0.41428631  0.41451522  0.4147797   0.41525441  0.41578266
  0.41675345  0.41722953  0.4190168   0.42029143  0.42171349  0.42325654
  0.42558881  0.42569877  0.42699251  0.42924034  0.43153268  0.43308241
  0.43502219  0.43705712  0.43980775  0.44003937  0.44061888  0.44609876
  0.44645405  0.44742597  0.44794417  0.44823382  0.44844781  0.44922135
  0.44958715  0.45030647  0.45042477  0.45053597  0.45471108  0.45741304
  0.45741498  0.45803507  0.45882102  0.45973465  0.46043087  0.46301241
  0.46544732  0.4655334   0.46607617  0.46740472  0.46762823  0.46778242
  0.46798712  0.471862    0.47186525  0.47227371  0.47238437  0.47398706
  0.47454869  0.47494042  0.47547872  0.47581097  0.47752148  0.4790137
  0.48030104  0.48049331  0.4805131   0.48060799  0.48127125  0.48148909
  0.48184581  0.48214164  0.48280182  0.48368194  0.48454074  0.4858399
  0.48808016  0.49046565  0.49070822  0.49108446  0.49112478  0.49322239
  0.49735664  0.49737249  0.49770612  0.49779036  0.49791789  0.49810488
  0.4981306   0.49897473  0.49914366  0.50015863  0.50124322  0.5023524
  0.5032295   0.50376569  0.50381467  0.50431546  0.50464929  0.50465393
  0.50488814  0.50546138  0.5056947   0.50581498  0.50597506  0.50635191
  0.50675955  0.50730465  0.50754475  0.50834386  0.50851384  0.50951607
  0.51007306  0.51009647  0.51015689  0.51021285  0.51077121  0.51172121
  0.51224056  0.51242821  0.51303993  0.5134123   0.51390255  0.51459807
  0.51470552  0.51477171  0.51506073  0.51537776  0.51599951  0.51751451
  0.51784143  0.51861374  0.51914759  0.51938545  0.51946159  0.52110744
  0.52444264  0.52445472  0.52467005  0.52558021  0.52864705  0.5292248
  0.53042431  0.53128262  0.53142132  0.53238561  0.53363461  0.53429167
  0.5345932   0.53561208  0.53633595  0.53676477  0.53709772  0.53719234
  0.53812594  0.53871528  0.53948313  0.53966421  0.53976808  0.54012867
  0.54027233  0.54093936  0.54095806  0.54139627  0.54228328  0.54331751
  0.54378894  0.54465538  0.54467998  0.54526754  0.54530229  0.54675973
  0.54677274  0.54730822  0.54752901  0.54776536  0.54932682  0.55040714
  0.55176611  0.55193658  0.55281956  0.55385893  0.55400212  0.55401788
  0.55413584  0.55511726  0.55611999  0.55705614  0.55757551  0.56122854
  0.56181966  0.56240723  0.56375654  0.56456368  0.56473132  0.56477691
  0.56572673  0.56735417  0.56842554  0.56971022  0.57157105  0.57304598
  0.57385014  0.57393593  0.57394016  0.57412057  0.57426674  0.57438849
  0.57478515  0.57575019  0.57730426  0.57834548  0.57885981  0.5797896
  0.58129348  0.58152719  0.58561163  0.58566209  0.58579197  0.5867572
  0.58704147  0.58772187  0.58908452  0.58952784  0.58963318  0.58987146
  0.59081859  0.59088953  0.59128309  0.59154191  0.59202499  0.59235445
  0.59303591  0.59305581  0.59307752  0.59385016  0.59386135  0.59427341
  0.59474066  0.59505888  0.59529387  0.59576296  0.59583187  0.5959326
  0.59594566  0.596764    0.59702823  0.59741216  0.59779005  0.59886826
  0.59900416  0.59951509  0.59960211  0.6002573   0.60042105  0.60076926
  0.601055    0.60120223  0.60164923  0.60168566  0.60183713  0.60319967
  0.60402314  0.60544217  0.60561646  0.60574344  0.60575205  0.60636972
  0.60662278  0.60673375  0.60733503  0.60764399  0.60885203  0.61001014
  0.61142527  0.61233044  0.61260205  0.61370628  0.61808619  0.61963868
  0.62108734  0.62150881  0.62197879  0.62236065  0.62838597  0.62939224
  0.63330071  0.63834074  0.63946027  0.66166912  0.66718799  0.66785607
  0.67093106  0.67495755  0.67500903  0.68547407  0.6906534   0.69141915
  0.69655125  0.7081462   0.71854329  0.73066242  0.74160373  0.80074043
  0.81517163  0.85333321  0.86987872  0.87669708  0.89119712  0.98784503
  1.08909057  1.34231483  1.64968016]

  UserWarning,

2022-10-31 11:01:53,027:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.32298262e+00 -1.14693527e+00 -9.10642055e-01 -5.37323703e-01
 -4.88895383e-01 -2.35047104e-01 -1.94431627e-01 -1.35214811e-01
 -1.26755657e-01 -1.15933377e-01 -1.11946402e-01 -1.04453297e-01
 -9.87312480e-02 -3.97665190e-02 -3.35036672e-02 -2.21674778e-02
  8.63509810e-04  5.39583102e-02  6.41630056e-02  7.82167961e-02
  7.83770974e-02  9.91815332e-02  1.03111640e-01  1.06219089e-01
  1.11217076e-01  1.12956020e-01  1.13361804e-01  1.23135509e-01
  1.31724036e-01  1.40844386e-01  1.43123572e-01  1.43507767e-01
  1.47937760e-01  1.55943293e-01  1.56070621e-01  1.59976587e-01
  1.96863556e-01  1.97382511e-01  2.00966680e-01  2.03969306e-01
  2.12593819e-01  2.21201977e-01  2.23567906e-01  2.26422207e-01
  2.27230551e-01  2.43841240e-01  2.48183118e-01  2.49235718e-01
  2.61032738e-01  2.65151957e-01  2.71017715e-01  2.71628507e-01
  2.71686509e-01  2.71698960e-01  2.75893544e-01  2.85402953e-01
  2.91143647e-01  2.92863756e-01  3.02165227e-01  3.08640148e-01
  3.15880553e-01  3.16648444e-01  3.17711356e-01  3.19070349e-01
  3.19528397e-01  3.22198449e-01  3.34647394e-01  3.35966200e-01
  3.36742284e-01  3.37527334e-01  3.39551843e-01  3.41338185e-01
  3.48344651e-01  3.52793938e-01  3.54835766e-01  3.58937150e-01
  3.60881327e-01  3.70476633e-01  3.70839329e-01  3.70908277e-01
  3.72023832e-01  3.75930116e-01  3.80580894e-01  3.81414440e-01
  3.81444395e-01  3.85929797e-01  3.86599299e-01  3.87076658e-01
  3.90942900e-01  3.91070788e-01  3.94157929e-01  3.95823357e-01
  3.97284177e-01  3.97420469e-01  3.97438973e-01  3.99386465e-01
  4.04845695e-01  4.09601485e-01  4.09682016e-01  4.11291464e-01
  4.11977012e-01  4.13389126e-01  4.19798595e-01  4.20129215e-01
  4.21899939e-01  4.25287853e-01  4.29126841e-01  4.30175645e-01
  4.31638648e-01  4.32312513e-01  4.32779365e-01  4.33841124e-01
  4.34981646e-01  4.37893473e-01  4.39032550e-01  4.40861880e-01
  4.40992047e-01  4.43046824e-01  4.47070429e-01  4.48279329e-01
  4.48418176e-01  4.48633879e-01  4.49601624e-01  4.49988294e-01
  4.52062154e-01  4.53337109e-01  4.54063086e-01  4.55381841e-01
  4.55496170e-01  4.57532441e-01  4.57863391e-01  4.60117028e-01
  4.60590220e-01  4.60910022e-01  4.61201696e-01  4.61439092e-01
  4.61883188e-01  4.64756652e-01  4.66431374e-01  4.66769689e-01
  4.67760458e-01  4.67865340e-01  4.67899899e-01  4.67978482e-01
  4.69143648e-01  4.69525293e-01  4.70517548e-01  4.72388976e-01
  4.78281937e-01  4.78785740e-01  4.81064008e-01  4.82638049e-01
  4.83278046e-01  4.83279646e-01  4.83379335e-01  4.84031373e-01
  4.84737960e-01  4.87082654e-01  4.87126751e-01  4.87400885e-01
  4.87604000e-01  4.87801803e-01  4.88198886e-01  4.89234030e-01
  4.89484680e-01  4.89914934e-01  4.90421507e-01  4.91322906e-01
  4.91676771e-01  4.91734087e-01  4.92059300e-01  4.92531031e-01
  4.94200463e-01  4.94699980e-01  4.94833440e-01  4.95411050e-01
  4.95673370e-01  4.95914988e-01  4.97191956e-01  4.97389057e-01
  4.97402922e-01  4.97527009e-01  4.97704875e-01  4.98416272e-01
  4.98578271e-01  4.99191488e-01  4.99814834e-01  5.00463407e-01
  5.02089521e-01  5.02758146e-01  5.03646798e-01  5.03701832e-01
  5.03988661e-01  5.04392851e-01  5.04502161e-01  5.04697760e-01
  5.05507403e-01  5.05515531e-01  5.05773064e-01  5.05805601e-01
  5.06152425e-01  5.06418002e-01  5.06517269e-01  5.06600850e-01
  5.06676616e-01  5.07405364e-01  5.08387565e-01  5.08430237e-01
  5.08711503e-01  5.08977082e-01  5.09042750e-01  5.09402132e-01
  5.09858484e-01  5.10204245e-01  5.10852451e-01  5.11394132e-01
  5.12088791e-01  5.14358338e-01  5.15723442e-01  5.17815766e-01
  5.18026926e-01  5.18579609e-01  5.20196271e-01  5.21187825e-01
  5.22817532e-01  5.23200305e-01  5.23564474e-01  5.23990266e-01
  5.24303184e-01  5.26612806e-01  5.28316001e-01  5.28937905e-01
  5.30877255e-01  5.31181980e-01  5.31357548e-01  5.33248521e-01
  5.33642122e-01  5.34209743e-01  5.34688559e-01  5.34936963e-01
  5.35887665e-01  5.36321851e-01  5.36641184e-01  5.38095399e-01
  5.38641062e-01  5.39689636e-01  5.40114261e-01  5.41182410e-01
  5.41220889e-01  5.41789694e-01  5.42010183e-01  5.42260383e-01
  5.42411760e-01  5.44101781e-01  5.44184929e-01  5.45225520e-01
  5.45861996e-01  5.46244984e-01  5.46648956e-01  5.49073951e-01
  5.49121980e-01  5.50722043e-01  5.53112202e-01  5.53236809e-01
  5.57240010e-01  5.59314499e-01  5.60622025e-01  5.60988450e-01
  5.63800724e-01  5.64020448e-01  5.64636065e-01  5.66839390e-01
  5.67201369e-01  5.67790931e-01  5.68275984e-01  5.69329282e-01
  5.70116850e-01  5.70918646e-01  5.71522057e-01  5.72105293e-01
  5.73049199e-01  5.74699528e-01  5.74766027e-01  5.75771852e-01
  5.75871068e-01  5.76174222e-01  5.78781339e-01  5.79580767e-01
  5.80133387e-01  5.80659171e-01  5.80875816e-01  5.82693425e-01
  5.82967037e-01  5.87214167e-01  5.87749899e-01  5.87994711e-01
  5.89270139e-01  5.89517730e-01  5.90150156e-01  5.90295111e-01
  5.91222719e-01  5.91532277e-01  5.91631746e-01  5.93717624e-01
  5.94391556e-01  5.94431032e-01  5.95229955e-01  5.95399272e-01
  5.97964337e-01  5.97988741e-01  6.01134195e-01  6.01803650e-01
  6.03027761e-01  6.03264449e-01  6.03646921e-01  6.05539241e-01
  6.05929842e-01  6.05972858e-01  6.08385832e-01  6.09452020e-01
  6.11732481e-01  6.20483176e-01  6.21532897e-01  6.21773148e-01
  6.24770739e-01  6.29841094e-01  6.30364477e-01  6.50877247e-01
  6.71488215e-01  6.90959231e-01  7.12519360e-01  7.30655949e-01
  7.56845391e-01  7.64401726e-01  7.90805845e-01  8.07723271e-01
  8.17967955e-01  8.46724328e-01  8.75699150e-01  9.85329866e-01
  1.07419787e+00]

  UserWarning,

2022-10-31 11:01:53,088:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.93288327 -0.62610006 -0.5128991  -0.51159255 -0.47170674 -0.47166676
 -0.46632139 -0.28185105 -0.19544766 -0.18018499 -0.13762782 -0.12688003
 -0.11165285 -0.07104704 -0.05788086 -0.03186774 -0.01305357 -0.00274536
  0.0263031   0.04100971  0.04127721  0.04203264  0.06513166  0.06771981
  0.11243006  0.12021201  0.12650654  0.12961872  0.12977309  0.15464679
  0.17109092  0.1952807   0.20502869  0.21301461  0.21772492  0.23713104
  0.24047156  0.2446072   0.25066615  0.25421072  0.26036938  0.26109159
  0.26249836  0.26264868  0.26484496  0.26650402  0.27004248  0.27095116
  0.27326294  0.28228711  0.28982742  0.29098918  0.2956728   0.30008476
  0.30137113  0.30842144  0.31001409  0.31700472  0.3229846   0.32345469
  0.32923989  0.33580961  0.33964345  0.35148371  0.35182456  0.35361405
  0.35608554  0.35674187  0.35858142  0.35883556  0.36674521  0.36900913
  0.37298012  0.37533672  0.37833591  0.38103902  0.38350412  0.38447566
  0.38496022  0.3877299   0.38833609  0.38900997  0.39003213  0.3901644
  0.39030923  0.39471855  0.39913085  0.40034573  0.40238296  0.40609033
  0.40632858  0.4073506   0.40790143  0.40962851  0.41226842  0.41325683
  0.41502438  0.41550158  0.41710263  0.41838708  0.41841126  0.4190137
  0.4269999   0.42937957  0.4295321   0.43176321  0.43180331  0.43707751
  0.43732665  0.4395641   0.44055996  0.44227159  0.44377607  0.44411121
  0.44464986  0.44702402  0.44838386  0.44929249  0.45093788  0.45205121
  0.45404611  0.45467331  0.45589145  0.45592753  0.45718416  0.45789516
  0.45806159  0.46031731  0.46272407  0.46443248  0.46542224  0.46567292
  0.46607701  0.46664118  0.46803221  0.46945825  0.47066013  0.47093365
  0.47195719  0.472622    0.47306389  0.473638    0.474592    0.47602163
  0.47685636  0.47751056  0.47821804  0.47834278  0.47868735  0.48004277
  0.48092201  0.48157849  0.48449492  0.48593637  0.48617351  0.48636735
  0.48703668  0.48811249  0.48828138  0.48945913  0.48956168  0.49073113
  0.49075698  0.4912749   0.49156563  0.49177598  0.49231762  0.49249735
  0.49281717  0.49281877  0.49292475  0.49391456  0.49422213  0.49447972
  0.49465257  0.49493898  0.49582704  0.4970141   0.49719475  0.4977081
  0.49799535  0.49888389  0.49919404  0.49943157  0.49961349  0.50067212
  0.50148616  0.50212875  0.50238287  0.50250495  0.5027378   0.50430344
  0.50527526  0.50530776  0.50547632  0.50568168  0.50582985  0.50586884
  0.50630073  0.50695237  0.5072638   0.50745405  0.50776045  0.50956601
  0.50983351  0.50992827  0.51000801  0.51015677  0.510536    0.51094632
  0.51106888  0.51186776  0.51312884  0.51481304  0.51490632  0.51539908
  0.51555495  0.51614523  0.51614624  0.51683632  0.51689852  0.51728823
  0.51756675  0.5176115   0.51957176  0.52089318  0.52097025  0.52179895
  0.52212308  0.52311934  0.52503678  0.52515388  0.52539665  0.52586647
  0.52656621  0.52842923  0.5295741   0.53127876  0.53161689  0.53409249
  0.53513105  0.53516212  0.53530773  0.53690443  0.53694827  0.53739635
  0.54241683  0.54287682  0.5428861   0.54309968  0.54315015  0.54448776
  0.54703538  0.54708399  0.54833379  0.54890305  0.5515196   0.55188051
  0.55291974  0.55634289  0.55957348  0.56093382  0.56170283  0.5618306
  0.56272157  0.56363732  0.56502868  0.5651414   0.56606328  0.56665795
  0.57033748  0.57137416  0.574005    0.57459813  0.57465297  0.57473999
  0.57738357  0.57755588  0.57943752  0.58053516  0.58130861  0.58167106
  0.58246296  0.58256689  0.58386689  0.58506561  0.5851024   0.58585128
  0.58586395  0.58611824  0.58616332  0.58648698  0.58668137  0.58680021
  0.58688013  0.58728627  0.58741225  0.58757154  0.58757636  0.58792139
  0.58805708  0.58889321  0.58949596  0.58998803  0.59087649  0.59100063
  0.59119361  0.59120613  0.59127364  0.59182975  0.59219358  0.59223678
  0.59279281  0.59305635  0.59317817  0.59847802  0.59945575  0.59992936
  0.60076751  0.60076988  0.60098817  0.60339606  0.60346114  0.6046389
  0.60499719  0.60527801  0.60603642  0.60633574  0.6065232   0.60770971
  0.61196888  0.61330587  0.61344974  0.61439364  0.61579153  0.61651661
  0.61758364  0.61952017  0.62909147  0.63018159  0.63603739  0.63793313
  0.64055688  0.6411195   0.64160787  0.64372013  0.64409542  0.6453145
  0.64913023  0.6528099   0.65944705  0.66429865  0.67233568  0.67441222
  0.67732725  0.67777859  0.68216642  0.69734014  0.70009867  0.70571015
  0.72984242  0.75289528  0.75442772  0.7578294   0.83331827  0.84311358
  0.91165921  1.07847902]

  UserWarning,

2022-10-31 11:01:53,091:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.84498156 -0.80812469 -0.7129961  -0.46855304 -0.33921246 -0.22149497
 -0.1103665  -0.10524839 -0.08920578 -0.06201914  0.04327635  0.05209869
  0.07213756  0.08348136  0.09254532  0.10587048  0.11365544  0.1143196
  0.12362274  0.12921201  0.13651414  0.14886693  0.16299682  0.16642151
  0.17209564  0.18748147  0.19165136  0.19233081  0.20038684  0.20709045
  0.22513423  0.23125286  0.23207598  0.23508851  0.24606353  0.25732274
  0.259784    0.26116264  0.26219726  0.26313752  0.26419105  0.26729903
  0.27077958  0.27425663  0.27782277  0.30146681  0.30252519  0.31438139
  0.32482707  0.32722915  0.32761499  0.32830949  0.33157891  0.33995102
  0.34315258  0.34810525  0.35121048  0.35179959  0.3531071   0.3565305
  0.35692     0.35987123  0.37102411  0.37518883  0.37521498  0.3765418
  0.37677059  0.3810456   0.38571266  0.38808065  0.38859933  0.39113025
  0.39775889  0.39820966  0.39938224  0.40121283  0.40264641  0.40609692
  0.4066978   0.40701413  0.40716467  0.41248979  0.41374627  0.4161869
  0.41713319  0.41738577  0.4208277   0.42145383  0.42450108  0.42786197
  0.42990679  0.43474282  0.43478599  0.43735976  0.4388871   0.4407177
  0.44127334  0.44153849  0.44159316  0.44268762  0.442753    0.4449628
  0.44528603  0.44569437  0.44570822  0.44580444  0.44864956  0.44877865
  0.44943325  0.44973466  0.45058095  0.4538704   0.45440799  0.45520596
  0.45704044  0.45754304  0.45879768  0.46021993  0.46179473  0.46489283
  0.46540896  0.46584109  0.46805974  0.46962552  0.4699828   0.47009238
  0.47183556  0.47227879  0.47263568  0.47296725  0.47305589  0.47537104
  0.47595788  0.47721964  0.47745     0.48110589  0.48130438  0.48257192
  0.48470708  0.4862276   0.48661344  0.48694359  0.48794871  0.48910468
  0.48922079  0.48945868  0.4896191   0.48983748  0.48999765  0.49198707
  0.4920543   0.49263272  0.49285718  0.49316227  0.4941708   0.49585961
  0.49594606  0.49639943  0.49695331  0.49695669  0.49732902  0.49757019
  0.4979124   0.49811171  0.49880607  0.49881514  0.49884373  0.49902909
  0.49940972  0.4998529   0.50015549  0.50038064  0.50044254  0.50081016
  0.50110285  0.50227362  0.50292807  0.50296411  0.5031314   0.50338799
  0.504134    0.50434827  0.50445557  0.50607726  0.50650146  0.50651256
  0.50779102  0.50817412  0.51003204  0.51005257  0.51006691  0.51041283
  0.51049622  0.51134444  0.51212441  0.51232606  0.51326176  0.51339689
  0.5134761   0.51447838  0.51483326  0.51549725  0.51641589  0.51668996
  0.51673833  0.51747342  0.5177337   0.51782982  0.51912481  0.51939247
  0.52065904  0.52109072  0.52132611  0.52202355  0.52228452  0.52305777
  0.52364161  0.52671208  0.52801085  0.52831557  0.52903647  0.53026347
  0.53133749  0.5321379   0.53301898  0.53460459  0.53482586  0.53767786
  0.5382054   0.53829588  0.54021548  0.5415782   0.54169991  0.5418844
  0.54307462  0.5437026   0.54452295  0.54468382  0.54552562  0.5470328
  0.54727631  0.54783166  0.54927651  0.54952604  0.5498962   0.55009083
  0.55108077  0.55182499  0.552612    0.55323273  0.55427078  0.55440364
  0.55517142  0.55535237  0.55650416  0.55715154  0.55718756  0.55783247
  0.55946098  0.56060266  0.56333534  0.56395427  0.564826    0.56567896
  0.56676483  0.56903184  0.56957027  0.56973081  0.57004752  0.57011075
  0.57209744  0.57250876  0.57336515  0.57385511  0.57534553  0.5756981
  0.57577411  0.57717957  0.5779674   0.57824793  0.57841233  0.57847888
  0.57853176  0.57918938  0.57935131  0.57994314  0.5800436   0.58014811
  0.58061969  0.5808583   0.58155715  0.58160707  0.58233269  0.58233562
  0.58286491  0.58394314  0.58465306  0.58524857  0.5857265   0.58615933
  0.58630142  0.58778929  0.59005321  0.59154331  0.59202424  0.59202622
  0.59205856  0.59210244  0.59257254  0.59341153  0.59398266  0.59556731
  0.5956017   0.59598672  0.59609395  0.59677072  0.59782712  0.59927073
  0.60008663  0.60078458  0.60152696  0.60191465  0.60213035  0.60410097
  0.60462444  0.60621549  0.60872639  0.60890315  0.6096106   0.61320881
  0.6174308   0.61814853  0.61875618  0.61909196  0.62107054  0.62781654
  0.63259905  0.63330759  0.63794204  0.64194603  0.64361509  0.65094
  0.66406393  0.67963259  0.69648994  0.70284854  0.71407688  0.7388936
  0.74728341  0.75735846  0.81954539  0.91567318  1.26722099]

  UserWarning,

2022-10-31 11:01:53,102:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.05643431e+00 -9.40757678e-01 -9.19675294e-01 -8.80987749e-01
 -8.58341136e-01 -8.12998977e-01 -5.48805676e-01 -4.87779836e-01
 -4.87246794e-01 -4.70060416e-01 -4.50976685e-01 -4.26237058e-01
 -4.15942470e-01 -3.97687356e-01 -3.80029708e-01 -3.41429888e-01
 -3.18537682e-01 -3.13148259e-01 -2.91642812e-01 -2.83978283e-01
 -1.86813386e-01 -1.59465941e-01 -1.57412350e-01 -1.28586721e-01
 -9.14396130e-02 -7.92826309e-04  9.16470923e-03  2.66565740e-02
  3.50876600e-02  4.82535268e-02  6.89648023e-02  7.22508014e-02
  8.35081507e-02  8.35701980e-02  1.18007804e-01  1.38402929e-01
  1.42884749e-01  1.52260554e-01  1.65142434e-01  1.83791147e-01
  1.94865165e-01  1.96782306e-01  2.01444476e-01  2.06190166e-01
  2.11123372e-01  2.24443377e-01  2.25117395e-01  2.28444635e-01
  2.47220042e-01  2.50319585e-01  2.53644669e-01  2.54060399e-01
  2.54864026e-01  2.60766622e-01  2.66673224e-01  2.67469866e-01
  2.78037607e-01  2.79741354e-01  2.83460501e-01  2.92147763e-01
  2.97008771e-01  3.01405619e-01  3.07868882e-01  3.17805391e-01
  3.18836119e-01  3.19950102e-01  3.27621004e-01  3.31551957e-01
  3.33610840e-01  3.45301549e-01  3.47820670e-01  3.49557548e-01
  3.52804203e-01  3.53012495e-01  3.55944796e-01  3.63635800e-01
  3.63758046e-01  3.65424809e-01  3.66693482e-01  3.71870816e-01
  3.73471585e-01  3.76964526e-01  3.82482083e-01  3.85614263e-01
  3.92461514e-01  3.93507410e-01  3.94094113e-01  3.94134404e-01
  3.94499579e-01  3.95797108e-01  3.97197880e-01  3.98119029e-01
  4.00550367e-01  4.01378516e-01  4.03161127e-01  4.05451078e-01
  4.07868365e-01  4.09045559e-01  4.10548035e-01  4.11187798e-01
  4.12930660e-01  4.13274492e-01  4.16034926e-01  4.17366572e-01
  4.19917192e-01  4.22843084e-01  4.23245840e-01  4.25915810e-01
  4.26025859e-01  4.27200048e-01  4.28847391e-01  4.29521250e-01
  4.30755550e-01  4.33039608e-01  4.38259691e-01  4.39753835e-01
  4.40484222e-01  4.40899853e-01  4.42984265e-01  4.43175511e-01
  4.43632646e-01  4.47294887e-01  4.47925863e-01  4.48690230e-01
  4.48734809e-01  4.48759867e-01  4.49341526e-01  4.49514135e-01
  4.50933875e-01  4.51670405e-01  4.55462052e-01  4.57534572e-01
  4.58020442e-01  4.58956188e-01  4.59124695e-01  4.60981169e-01
  4.61384066e-01  4.64308359e-01  4.64509735e-01  4.66056736e-01
  4.66622133e-01  4.67515259e-01  4.67587976e-01  4.68922804e-01
  4.69457215e-01  4.69509216e-01  4.69512195e-01  4.70073679e-01
  4.70133876e-01  4.70772041e-01  4.70942569e-01  4.71427336e-01
  4.73113475e-01  4.74895079e-01  4.75338646e-01  4.76058763e-01
  4.78251780e-01  4.79720014e-01  4.79775463e-01  4.80355774e-01
  4.83975745e-01  4.84995039e-01  4.86177540e-01  4.86670255e-01
  4.87690687e-01  4.87699694e-01  4.88394710e-01  4.88647905e-01
  4.88821141e-01  4.88822358e-01  4.88999356e-01  4.90056052e-01
  4.90139821e-01  4.91634553e-01  4.92060998e-01  4.92273582e-01
  4.92566911e-01  4.92665324e-01  4.92731066e-01  4.92747486e-01
  4.92929210e-01  4.93561195e-01  4.93623948e-01  4.93660813e-01
  4.94196000e-01  4.94723820e-01  4.94731532e-01  4.95240116e-01
  4.95460410e-01  4.95474002e-01  4.95674376e-01  4.97202498e-01
  4.98632518e-01  4.99753848e-01  5.00333001e-01  5.00789586e-01
  5.01052765e-01  5.01255955e-01  5.03291816e-01  5.03953042e-01
  5.03999950e-01  5.04623706e-01  5.05683128e-01  5.06601239e-01
  5.06897113e-01  5.06925709e-01  5.07744098e-01  5.07757000e-01
  5.08514503e-01  5.09227661e-01  5.09466085e-01  5.10286654e-01
  5.10417002e-01  5.11192174e-01  5.11317149e-01  5.11746933e-01
  5.13406821e-01  5.13745286e-01  5.14392856e-01  5.15954067e-01
  5.17520975e-01  5.17665722e-01  5.17684060e-01  5.18596659e-01
  5.19614171e-01  5.20745865e-01  5.20851747e-01  5.23626825e-01
  5.23921562e-01  5.24296769e-01  5.26541446e-01  5.28597032e-01
  5.29299016e-01  5.29527776e-01  5.29700823e-01  5.31862045e-01
  5.32469592e-01  5.35175147e-01  5.37830310e-01  5.37998081e-01
  5.38434981e-01  5.39152006e-01  5.39294726e-01  5.39333448e-01
  5.39427476e-01  5.39521214e-01  5.39553898e-01  5.39937656e-01
  5.42205257e-01  5.46502530e-01  5.46640401e-01  5.46727959e-01
  5.47348554e-01  5.47937915e-01  5.48317437e-01  5.48874167e-01
  5.50163880e-01  5.50249247e-01  5.50338425e-01  5.50798143e-01
  5.51326625e-01  5.51665128e-01  5.54699281e-01  5.55237642e-01
  5.56166487e-01  5.57278756e-01  5.57574964e-01  5.58540328e-01
  5.62443153e-01  5.62604534e-01  5.63141527e-01  5.64737268e-01
  5.66072405e-01  5.67868833e-01  5.68984833e-01  5.69331423e-01
  5.69526481e-01  5.70415179e-01  5.70458776e-01  5.72568195e-01
  5.73204594e-01  5.74918296e-01  5.74972215e-01  5.76160887e-01
  5.76525033e-01  5.77053651e-01  5.77070550e-01  5.78787786e-01
  5.78910721e-01  5.79294523e-01  5.80204611e-01  5.81290508e-01
  5.84252048e-01  5.84497246e-01  5.84863137e-01  5.85568906e-01
  5.85940788e-01  5.87053827e-01  5.87430563e-01  5.87886273e-01
  5.89018727e-01  5.89289164e-01  5.90287718e-01  5.90891175e-01
  5.91082584e-01  5.92267815e-01  5.92682056e-01  5.92960405e-01
  5.92993425e-01  5.93048828e-01  5.93099047e-01  5.93559154e-01
  5.93638682e-01  5.94195235e-01  5.94580073e-01  5.94618754e-01
  5.94693423e-01  5.95230209e-01  5.95888288e-01  5.95912083e-01
  5.95955024e-01  5.97672880e-01  5.97778119e-01  5.98708020e-01
  5.99849195e-01  5.99884708e-01  6.00902210e-01  6.01105162e-01
  6.01721332e-01  6.02850441e-01  6.04912260e-01  6.04969935e-01
  6.05427247e-01  6.06098620e-01  6.07271708e-01  6.08306944e-01
  6.08857348e-01  6.09324316e-01  6.09934313e-01  6.10433401e-01
  6.11178186e-01  6.12606540e-01  6.13783066e-01  6.16219374e-01
  6.17069730e-01  6.19333388e-01  6.19746543e-01  6.20097820e-01
  6.20410102e-01  6.26576907e-01  6.29097429e-01  6.29811303e-01
  6.30209035e-01  6.30276286e-01  6.31750450e-01  6.33898957e-01
  6.35793085e-01  6.44456300e-01  6.46373552e-01  6.47060920e-01
  6.51385326e-01  6.72144984e-01  6.72955737e-01  6.79462638e-01
  6.94755646e-01  6.97889957e-01  6.98032325e-01  7.04443372e-01
  7.24856912e-01  7.25642149e-01  7.42060494e-01  7.81146365e-01
  8.71872049e-01  8.96976172e-01  8.98137627e-01]

  UserWarning,

2022-10-31 11:01:53,134:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.71658835 -1.05224453 -0.50640103 -0.36357846 -0.34833985 -0.26562872
 -0.26412456 -0.19125029 -0.17388545 -0.16840684 -0.07660678 -0.04399083
 -0.03144485 -0.02998972  0.00985612  0.02035821  0.06398315  0.07270781
  0.10184401  0.12524941  0.1341679   0.13871333  0.13904839  0.15281397
  0.15332141  0.17685059  0.20916854  0.21958746  0.22008375  0.22754796
  0.23982164  0.25722633  0.26408839  0.26836347  0.27848746  0.29054215
  0.29572073  0.31085964  0.31336404  0.31822117  0.32271018  0.3233871
  0.3274149   0.33923375  0.35147114  0.3531175   0.35631493  0.35936932
  0.36036864  0.36857411  0.37039345  0.3738961   0.37413577  0.3741407
  0.37481296  0.37801649  0.38229458  0.38414511  0.38665874  0.38788223
  0.38861376  0.38908916  0.38957947  0.39936079  0.39981436  0.40050172
  0.40197316  0.40380279  0.40486784  0.4059541   0.40876086  0.41214805
  0.41334044  0.41398045  0.41530603  0.42170342  0.42332641  0.42393185
  0.42416145  0.42510596  0.42542596  0.42877491  0.42926443  0.42960177
  0.43027488  0.4312313   0.43166583  0.43486962  0.43676209  0.43851566
  0.43930724  0.44162463  0.44318963  0.44384445  0.44427526  0.44457533
  0.44494351  0.44538018  0.44600419  0.44661522  0.44956384  0.44975602
  0.45007961  0.45155231  0.45596886  0.45704471  0.45962798  0.46024896
  0.46139741  0.461419    0.46188948  0.46189699  0.46241798  0.46275497
  0.46309072  0.46366111  0.4658738   0.46689793  0.46735035  0.46856328
  0.46895006  0.46901002  0.46967429  0.46983537  0.47057395  0.47102045
  0.47148592  0.47256491  0.47300811  0.47492309  0.47514284  0.47687318
  0.47726046  0.47772606  0.47903803  0.47954948  0.48038774  0.48140088
  0.48187141  0.48209663  0.48384761  0.48384845  0.48674518  0.48702181
  0.48747873  0.48788121  0.48868746  0.489002    0.48968989  0.49049979
  0.49074973  0.49255061  0.49470214  0.49534944  0.49578452  0.49644198
  0.49759496  0.49799836  0.49888204  0.49909845  0.50011422  0.50048793
  0.5006287   0.50072122  0.50180089  0.50244789  0.5025129   0.50283188
  0.50406426  0.50417894  0.50459019  0.50492659  0.50632898  0.50698581
  0.50718603  0.5074945   0.50759026  0.50830552  0.50861093  0.50978217
  0.51005513  0.51136584  0.5115364   0.51176004  0.51183774  0.51190971
  0.51257039  0.51341424  0.51431886  0.51439817  0.51480307  0.51517626
  0.51605919  0.51634497  0.51642085  0.51673378  0.51676219  0.51696244
  0.51750076  0.51840458  0.51846813  0.51985588  0.5200448   0.52276937
  0.52317489  0.52460585  0.5254186   0.52547356  0.52553813  0.52597237
  0.52753468  0.52907479  0.52908857  0.52994189  0.53181669  0.53210527
  0.53249536  0.53297033  0.53558652  0.53579874  0.53588249  0.5374174
  0.53811588  0.53816685  0.53817084  0.53913923  0.53926941  0.54243954
  0.54478666  0.54515939  0.54575428  0.54602904  0.54608796  0.54616905
  0.54826659  0.54855131  0.5489714   0.54973112  0.54975813  0.55025481
  0.55090494  0.55121528  0.55156982  0.55373267  0.5538029   0.55393842
  0.55469654  0.5547755   0.55691394  0.55756208  0.55839977  0.55978281
  0.55992939  0.56095675  0.56218847  0.56253423  0.56395479  0.56530029
  0.56669688  0.56691585  0.56772002  0.56794097  0.56801576  0.56909089
  0.57137289  0.57356909  0.5739202   0.57604593  0.57734187  0.57793902
  0.57817703  0.5781877   0.57820043  0.57860433  0.58045973  0.58098663
  0.58162136  0.58176504  0.5818128   0.58244076  0.58335401  0.58346204
  0.58374546  0.58379768  0.58386963  0.58426791  0.58467169  0.58477474
  0.58533485  0.58559611  0.58602544  0.58635348  0.58652146  0.58673673
  0.58681053  0.58686504  0.58698622  0.58724257  0.58777454  0.58787093
  0.58789069  0.58829797  0.58882836  0.589336    0.58962733  0.59018071
  0.59135199  0.59161392  0.59161537  0.59194148  0.59204076  0.59265431
  0.59302801  0.5953971   0.59548903  0.59694019  0.59705011  0.59753267
  0.59818741  0.59847128  0.60007041  0.600178    0.6004988   0.60071833
  0.60274688  0.6040085   0.60487186  0.60571585  0.60649583  0.60793091
  0.60853742  0.60952337  0.60997329  0.61143345  0.61380922  0.61472869
  0.61520531  0.61532353  0.61645137  0.61793148  0.62236307  0.6237991
  0.63069929  0.63580388  0.63705093  0.64258797  0.65229429  0.65365329
  0.66057903  0.66510145  0.67839137  0.67928993  0.68100968  0.68129538
  0.68814317  0.70661336  0.70819028  0.71179531  0.73201404  0.73930696
  0.74507494  0.76767219  0.79212654  0.80449827  0.86286563  0.87318495
  0.91768837  0.99211196  1.01944274  1.04878537  1.17297327  1.26810547]

  UserWarning,

2022-10-31 11:01:53,149:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.87926866 -0.82676955 -0.81692981 -0.31138186 -0.25953871 -0.2486295
 -0.2223754  -0.19600323 -0.15404287 -0.13464015 -0.10783177 -0.07626552
 -0.06056967 -0.05239197  0.00832641  0.02724752  0.04800214  0.0490216
  0.05774884  0.06012254  0.06472197  0.0652068   0.07197608  0.09611262
  0.10606838  0.11129921  0.13176168  0.13997696  0.17055726  0.18824532
  0.19090058  0.19517729  0.19786975  0.20837463  0.22355125  0.22423717
  0.23223353  0.25797492  0.26730086  0.2673904   0.27198522  0.27660891
  0.28228501  0.28594752  0.29118209  0.30470867  0.31226716  0.32287617
  0.32544754  0.32602261  0.32805191  0.3345593   0.3398576   0.35228426
  0.35381452  0.35854043  0.35864871  0.36249722  0.36398327  0.37964287
  0.38146775  0.38240697  0.3831471   0.38412194  0.3890206   0.39918976
  0.40040202  0.40093064  0.40435633  0.40649518  0.40912029  0.40926515
  0.4108038   0.41117515  0.41447081  0.41706772  0.41980263  0.42228744
  0.42283435  0.42431613  0.42478972  0.42494241  0.42748669  0.43078905
  0.43427849  0.43490237  0.43641549  0.43704114  0.43714411  0.43733065
  0.4391536   0.43988197  0.44101129  0.44252499  0.44263016  0.44373126
  0.44387795  0.44736164  0.44768408  0.44773501  0.44811513  0.44839938
  0.44870144  0.44903943  0.44937813  0.44959717  0.4506776   0.45076763
  0.45156612  0.45378294  0.45438546  0.45552309  0.45652277  0.46107084
  0.46120831  0.46129546  0.46140282  0.46162011  0.46219755  0.46522368
  0.46526441  0.46709445  0.46958888  0.47478515  0.47660221  0.47886727
  0.48317049  0.48386858  0.48454674  0.48486772  0.48631465  0.48689208
  0.48716085  0.48733439  0.48789572  0.48851231  0.48855484  0.49020463
  0.49150715  0.49156853  0.49399411  0.49412029  0.49442686  0.49453139
  0.49529532  0.49550468  0.49571446  0.49622378  0.4981621   0.49826785
  0.49888443  0.49926127  0.49927025  0.49943372  0.49958879  0.49970197
  0.50003357  0.5008592   0.50276434  0.50283677  0.5029965   0.50316079
  0.50339318  0.50352624  0.50379041  0.50380316  0.50381543  0.50436184
  0.50443557  0.50488458  0.50493795  0.50514351  0.50567538  0.50574162
  0.50821705  0.50903392  0.50989627  0.50993266  0.5104229   0.51205306
  0.51232981  0.51371107  0.51541955  0.51622255  0.51709628  0.51954704
  0.51961412  0.52036732  0.52038687  0.52160517  0.52198024  0.52222183
  0.52314819  0.52324722  0.52351525  0.5239659   0.52443793  0.52477341
  0.52487359  0.52516781  0.5258934   0.52782231  0.52813508  0.52869731
  0.53030292  0.530483    0.53102388  0.5325828   0.5333289   0.53398685
  0.5352897   0.53572749  0.53618945  0.53673375  0.53691827  0.53749634
  0.538716    0.53953978  0.54063435  0.54149272  0.54167079  0.54286231
  0.54319265  0.54379244  0.54379362  0.54443891  0.54485526  0.54515843
  0.54565559  0.54637644  0.54663128  0.54673206  0.54802965  0.54840957
  0.54843945  0.54913081  0.55099818  0.55233868  0.5524359   0.55312991
  0.55595981  0.55719459  0.5574292   0.55746702  0.55754095  0.55832634
  0.55936177  0.55971307  0.55997912  0.56000893  0.56033762  0.56044156
  0.56154065  0.56192561  0.56287731  0.5642592   0.56550187  0.56616535
  0.5679226   0.56996712  0.57008156  0.57061751  0.57068229  0.57134558
  0.57169368  0.57215458  0.57263232  0.57425149  0.57469819  0.57484454
  0.57665972  0.57678682  0.57803993  0.57829369  0.57856052  0.58024182
  0.58166458  0.58180792  0.58182083  0.58287623  0.58427593  0.58512907
  0.58545053  0.58552006  0.58562419  0.58574431  0.58613652  0.5862429
  0.58762993  0.58831537  0.58921294  0.58955124  0.58977469  0.59105946
  0.59144176  0.59245003  0.5929675   0.59305032  0.5933258   0.59348261
  0.59465519  0.59473513  0.59504537  0.5951558   0.59607032  0.59612245
  0.59626205  0.59633541  0.59633657  0.59641543  0.59699391  0.59757759
  0.59846922  0.59877548  0.59912138  0.5998953   0.5999467   0.60004031
  0.60161266  0.60228169  0.60374363  0.60503578  0.60742698  0.60749206
  0.60773416  0.60949888  0.61023169  0.61029556  0.611056    0.61134449
  0.61215902  0.61539864  0.61567892  0.61575018  0.61645431  0.61670915
  0.62188665  0.62208279  0.6284688   0.62894448  0.62972692  0.64179142
  0.64888248  0.64968378  0.65998356  0.66660966  0.66926256  0.66950888
  0.67530469  0.68464946  0.69744264  0.69951128  0.70855904  0.71823755
  0.72353127  0.75214631  0.76125526  0.90661368  0.96705537  1.29580256
  1.49622138]

  UserWarning,

2022-10-31 11:01:53,918:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.81086413 -0.55712169 -0.3786604  -0.37554805 -0.33271647 -0.30563916
 -0.22381383 -0.22312883 -0.21527679 -0.21503886 -0.1450292  -0.09360072
 -0.06630164 -0.01639945  0.00182671  0.01059172  0.03071634  0.05528528
  0.07288228  0.07607143  0.09295392  0.10406195  0.10975075  0.11079798
  0.11088073  0.11541638  0.13470791  0.1363936   0.15154197  0.15755705
  0.15850278  0.17051869  0.1980804   0.20187111  0.21302191  0.22597411
  0.23382921  0.24223486  0.2524866   0.27466895  0.27714939  0.28431532
  0.28566697  0.28919646  0.30603408  0.31825661  0.32912383  0.33082275
  0.33120862  0.33311101  0.33997711  0.34061757  0.35114952  0.35150684
  0.35182226  0.35255078  0.35682742  0.35710012  0.35906775  0.35917835
  0.35946953  0.36120698  0.36248112  0.36326658  0.36482592  0.36546902
  0.36777906  0.36807794  0.36850376  0.36971766  0.37174896  0.3737223
  0.37524473  0.37713715  0.38176857  0.38822312  0.38898177  0.38941375
  0.39221963  0.39222999  0.3925813   0.39475895  0.39540739  0.39592148
  0.39703982  0.4017364   0.4028714   0.40500006  0.40692579  0.40727174
  0.40890921  0.40925344  0.40955646  0.41053788  0.41112892  0.41159739
  0.41736946  0.42265852  0.42339695  0.42440485  0.4277828   0.43037347
  0.43186882  0.4327956   0.43455865  0.43455973  0.43576655  0.43850337
  0.44126957  0.44244707  0.44268067  0.44371812  0.44371914  0.443738
  0.44531049  0.44549479  0.44686792  0.44709921  0.44889574  0.44916382
  0.4492175   0.45435848  0.45437717  0.45581573  0.45606882  0.45729576
  0.45733742  0.4574681   0.46539168  0.46557757  0.46571316  0.46677242
  0.46745175  0.46970738  0.4710218   0.47188428  0.47547459  0.47577706
  0.47648506  0.47853304  0.48021318  0.48195503  0.48308626  0.48332603
  0.48417651  0.48424056  0.48444146  0.48498152  0.48542295  0.4862096
  0.48940812  0.48942544  0.48996089  0.4900849   0.49065542  0.491755
  0.49176937  0.49272483  0.49392842  0.49587959  0.49662505  0.49707955
  0.49712667  0.49846484  0.49861089  0.49861622  0.49908709  0.49973904
  0.49975679  0.49985379  0.50008309  0.50017015  0.50052259  0.5009124
  0.50091519  0.50112453  0.5020304   0.50209157  0.50278847  0.50293209
  0.50380513  0.50389017  0.50425413  0.504255    0.50475293  0.50540066
  0.50577869  0.50652499  0.50853526  0.50945611  0.51043768  0.51080546
  0.51144873  0.51199254  0.51217611  0.51236259  0.51247511  0.51344897
  0.51359598  0.5139347   0.51475454  0.51624431  0.51678766  0.5174113
  0.51899378  0.52061816  0.52068247  0.52136499  0.52418668  0.52443089
  0.52504009  0.52573748  0.52667002  0.52877818  0.52929025  0.53153547
  0.53184601  0.53251903  0.53505788  0.53584608  0.53616348  0.53661526
  0.53899024  0.54064426  0.54108936  0.5418959   0.54218697  0.54296159
  0.54359895  0.54628485  0.54847748  0.54866726  0.54921592  0.55056159
  0.55060847  0.55197682  0.55290022  0.55330117  0.55361735  0.55365497
  0.55408842  0.55408997  0.55498693  0.55533538  0.55614093  0.55638861
  0.55894683  0.55960999  0.55976362  0.56129932  0.56180278  0.56315055
  0.56341445  0.56347523  0.56364849  0.56449943  0.56527396  0.56684486
  0.56759833  0.5680684   0.5686972   0.56890916  0.57144978  0.5730054
  0.5738201   0.57386828  0.5743402   0.57467076  0.57495715  0.57522556
  0.57829329  0.58125094  0.5816725   0.58403124  0.58465692  0.58470334
  0.58557117  0.58568873  0.58584379  0.58611604  0.58635695  0.58668616
  0.58672431  0.58721169  0.58726989  0.58783776  0.58802852  0.58877051
  0.58890541  0.58950892  0.5895354   0.59057608  0.59108561  0.59326561
  0.59354285  0.59565651  0.59594373  0.59616123  0.59692277  0.59886643
  0.59990584  0.60074845  0.60123224  0.60125891  0.60283054  0.60326986
  0.6056502   0.60572725  0.60837327  0.60903466  0.60970474  0.61236693
  0.61310348  0.61715583  0.62169965  0.62217351  0.62371449  0.62738972
  0.62785961  0.63429912  0.63435806  0.6400153   0.64219006  0.64718053
  0.65546579  0.65602627  0.65964323  0.66110094  0.66497362  0.66732865
  0.6707752   0.67703752  0.68124528  0.68939185  0.69665081  0.71983487
  0.72448974  0.73125062  0.73232447  0.76546393  0.79143354  0.81639607
  0.95578116  1.25058347]

  UserWarning,

2022-10-31 11:01:53,949:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.44697827 -1.21938845 -0.92533014 -0.43016458 -0.42215156 -0.42027057
 -0.41455029 -0.36215817 -0.3571773  -0.33220425 -0.3272233  -0.24637658
 -0.13517517 -0.11806068 -0.10299131 -0.06521008 -0.04831927 -0.04411553
 -0.01428642  0.04498356  0.06866683  0.10185125  0.11111154  0.12122367
  0.12396726  0.12717732  0.15710561  0.15892877  0.17228906  0.18213534
  0.1903246   0.19205232  0.20150146  0.20357052  0.20462911  0.22889761
  0.22953945  0.23219292  0.23229017  0.24287274  0.24836447  0.24968786
  0.2586764   0.26783933  0.26939111  0.2807779   0.28104975  0.28708727
  0.29432847  0.29973458  0.30473849  0.3063426   0.31765458  0.32255634
  0.32485949  0.32528785  0.32581553  0.3273602   0.33558335  0.34313026
  0.34563131  0.35158761  0.35568767  0.35639307  0.35949192  0.36245843
  0.36500407  0.36776334  0.37774443  0.38199847  0.38295343  0.38399327
  0.38731791  0.3879114   0.39306418  0.39472182  0.39739377  0.3976859
  0.40091709  0.40251917  0.40471316  0.40577663  0.40769904  0.41035375
  0.41222239  0.41438651  0.41477982  0.42227703  0.42287384  0.42523845
  0.42700416  0.42824703  0.42853082  0.42855666  0.42970106  0.43082915
  0.43173211  0.4322028   0.43248712  0.43815944  0.43959764  0.44026403
  0.44269488  0.44304919  0.44307958  0.44675766  0.44781607  0.44855238
  0.44992217  0.4503272   0.45042315  0.45335264  0.45364507  0.4556591
  0.45677267  0.45735028  0.4582434   0.45970319  0.46060375  0.4609589
  0.46123702  0.4612471   0.46387773  0.46429101  0.46570759  0.46624569
  0.46669685  0.46733157  0.46783375  0.46912974  0.46913756  0.46927178
  0.47061098  0.47065921  0.47350049  0.47438331  0.47482863  0.47492847
  0.4754573   0.47550405  0.47637594  0.47856265  0.48057536  0.48321629
  0.48450654  0.484668    0.48496504  0.48663828  0.48668233  0.48859217
  0.48922667  0.49090993  0.4913059   0.4916398   0.49276512  0.49333573
  0.49339568  0.49342003  0.49507564  0.49513707  0.4952632   0.49564739
  0.49712299  0.49740859  0.49785896  0.49928083  0.4998813   0.50042024
  0.50066775  0.50206046  0.5021735   0.50238792  0.50308012  0.50359426
  0.50390038  0.50397731  0.5044369   0.50505506  0.50541919  0.50580195
  0.50671906  0.50767977  0.50798317  0.50884628  0.50939486  0.51029733
  0.51074342  0.51084882  0.5108697   0.51119512  0.51130607  0.5145456
  0.5158202   0.51660561  0.51678855  0.51756594  0.51758448  0.51835463
  0.51919383  0.51943597  0.52097441  0.52147487  0.52265646  0.52315217
  0.52565961  0.52687997  0.52850279  0.52854489  0.52875642  0.52924952
  0.53069639  0.5324196   0.53273345  0.53328928  0.5333069   0.5335939
  0.53407536  0.53486396  0.53680414  0.53908669  0.54083705  0.54141161
  0.5414565   0.54213251  0.54214734  0.54261642  0.5427166   0.54279608
  0.54305851  0.54329205  0.543416    0.54344255  0.54366535  0.54366838
  0.54488715  0.54520971  0.54618222  0.54622082  0.54670115  0.54733615
  0.5477114   0.54987153  0.55105024  0.55220953  0.55260875  0.55643334
  0.55665511  0.55808135  0.558411    0.56081125  0.56130071  0.56413187
  0.56447294  0.56495792  0.5658742   0.56669329  0.57035832  0.57048437
  0.5714597   0.57227426  0.57245044  0.57348254  0.5740746   0.57535052
  0.57548639  0.57554305  0.57561547  0.57577634  0.57594032  0.57603557
  0.5770008   0.57708146  0.58015392  0.58132509  0.58144563  0.58239765
  0.58262635  0.58264348  0.5831198   0.58335523  0.58370717  0.58497899
  0.58508524  0.58592483  0.58600575  0.58631877  0.58670645  0.58682674
  0.5880457   0.58834527  0.58935187  0.59029648  0.59081767  0.59129383
  0.59146478  0.591507    0.59282442  0.59338413  0.59362413  0.59444165
  0.59476799  0.59693581  0.59844334  0.59879538  0.59990761  0.60075113
  0.60224661  0.60249609  0.60297277  0.60441483  0.60477271  0.60538919
  0.60615824  0.60668242  0.60672573  0.607796    0.60808982  0.60962011
  0.61187188  0.61496035  0.61732268  0.62077339  0.6211242   0.62423666
  0.62576939  0.62991353  0.63001123  0.64442797  0.6466743   0.65000405
  0.65195606  0.671308    0.67414988  0.67810686  0.69849739  0.70915151
  0.71024571  0.71754272  0.75813809  0.75953856  0.78394043  0.81151864
  0.82553401  0.90019652  1.12517194  1.12656343  1.13846243  1.32050034]

  UserWarning,

2022-10-31 11:01:53,949:INFO:Calculating mean and std
2022-10-31 11:01:53,949:INFO:Creating metrics dataframe
2022-10-31 11:01:53,949:INFO:Uploading results into container
2022-10-31 11:01:53,949:INFO:Uploading model into container now
2022-10-31 11:01:53,949:INFO:master_model_container: 3
2022-10-31 11:01:53,949:INFO:display_container: 2
2022-10-31 11:01:53,949:INFO:Lasso(random_state=3360)
2022-10-31 11:01:53,949:INFO:create_model() successfully completed......................................
2022-10-31 11:01:54,083:WARNING:create_model() for Lasso(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:01:54,083:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:01:54,084:INFO:Initializing create_model()
2022-10-31 11:01:54,084:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:01:54,084:INFO:Checking exceptions
2022-10-31 11:01:54,086:INFO:Importing libraries
2022-10-31 11:01:54,086:INFO:Copying training dataset
2022-10-31 11:01:54,086:INFO:Defining folds
2022-10-31 11:01:54,086:INFO:Declaring metric variables
2022-10-31 11:01:54,086:INFO:Importing untrained model
2022-10-31 11:01:54,086:INFO:Lasso Regression Imported successfully
2022-10-31 11:01:54,086:INFO:Starting cross validation
2022-10-31 11:01:54,086:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:01:56,101:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.29330418 -1.03166604 -0.7450064  -0.68425415 -0.2431175  -0.21543473
 -0.19660297 -0.17416203 -0.16564372 -0.1375903  -0.09207047 -0.06515115
 -0.05251586 -0.04488175 -0.04067885 -0.02536218 -0.00529496  0.00572611
  0.01761279  0.03125843  0.07187555  0.08373856  0.09530981  0.11789617
  0.12316403  0.12989934  0.13788894  0.14031497  0.15102376  0.17236063
  0.17569577  0.1818138   0.19611197  0.19796194  0.20285258  0.21312499
  0.21344638  0.21683833  0.2194052   0.22058546  0.22216095  0.22764117
  0.22928903  0.23667806  0.23844255  0.2449115   0.25285942  0.25749162
  0.25945025  0.26056856  0.27594781  0.27661978  0.27664458  0.27726147
  0.27929437  0.2960428   0.2992338   0.29939724  0.30199873  0.30830658
  0.30844463  0.31032518  0.31364131  0.33043404  0.33589002  0.33637687
  0.33759675  0.3387135   0.34001141  0.36283341  0.36463114  0.36982028
  0.37019616  0.37154075  0.37155054  0.3741184   0.37483002  0.37642702
  0.3838063   0.38584468  0.39092361  0.39119172  0.39253845  0.39275532
  0.39568784  0.39721619  0.39942803  0.39964903  0.40076072  0.40176142
  0.40349148  0.40355802  0.40480656  0.40521999  0.41086704  0.41697857
  0.41726004  0.41902425  0.42323119  0.42874507  0.43170172  0.43174694
  0.43238285  0.43338541  0.43474857  0.43714302  0.43729246  0.44300196
  0.44329587  0.4436425   0.44413752  0.44554833  0.44574594  0.44741292
  0.44799203  0.44860272  0.45096733  0.45183915  0.45297654  0.45493274
  0.45499797  0.45524021  0.45609232  0.45694282  0.45835933  0.45872131
  0.45917483  0.46043712  0.46138735  0.46182969  0.46305868  0.46321685
  0.46337455  0.46384413  0.46405302  0.46523009  0.4653523   0.46622379
  0.46707084  0.46787082  0.46801145  0.46862751  0.47141997  0.47201776
  0.47227428  0.47329321  0.47354193  0.47798522  0.47888379  0.48121178
  0.48278898  0.48408557  0.48574797  0.48923245  0.48987277  0.49256097
  0.49369571  0.4938317   0.49400738  0.49492336  0.49543782  0.49571961
  0.49576232  0.49656245  0.49759108  0.49866209  0.49877272  0.4990334
  0.49970981  0.49972035  0.49981992  0.50046481  0.5008489   0.5014439
  0.50169691  0.50204278  0.50268346  0.50296336  0.50315458  0.50332808
  0.50387203  0.50410186  0.50504599  0.50518456  0.50575129  0.50583334
  0.50626331  0.5079716   0.50798467  0.50883925  0.51051038  0.51087327
  0.51093749  0.51196442  0.51206224  0.5121235   0.51232691  0.51316585
  0.51411912  0.51526329  0.51543317  0.51600213  0.51645863  0.51691965
  0.51722406  0.51998926  0.5220852   0.52224448  0.52227767  0.52240847
  0.5228172   0.52293952  0.52326594  0.5233663   0.52467525  0.52494704
  0.52533963  0.52550119  0.52577338  0.52964309  0.53026639  0.53214334
  0.53341766  0.53365973  0.53471322  0.53586924  0.53622365  0.53626588
  0.53664348  0.53667023  0.53673232  0.53806762  0.53818759  0.53853583
  0.53874232  0.53901151  0.54039982  0.54259287  0.54347472  0.54472297
  0.54528253  0.54544378  0.54602202  0.54727694  0.54746554  0.54759951
  0.54818544  0.5482591   0.54887778  0.54909024  0.55026589  0.55051801
  0.55066265  0.55196653  0.55229023  0.55390431  0.55508889  0.55512755
  0.55534029  0.55545925  0.55621205  0.56039195  0.56108002  0.566573
  0.56768016  0.57122325  0.57277015  0.57402676  0.5744087   0.57615586
  0.57659089  0.5770235   0.57802418  0.58011112  0.58100107  0.58208579
  0.58270031  0.58353673  0.58410286  0.58427282  0.58439367  0.58609308
  0.58667161  0.58709605  0.58759678  0.58763875  0.5878416   0.58834476
  0.58845493  0.58996378  0.5899988   0.59004158  0.59152694  0.5915909
  0.5927535   0.59369918  0.59421192  0.5947561   0.59478414  0.59499119
  0.59594471  0.59666806  0.59855847  0.60030953  0.60291948  0.6031796
  0.60431776  0.60461288  0.60523942  0.60595876  0.60665775  0.60754195
  0.60922203  0.61094778  0.61325692  0.61688845  0.61761671  0.61866101
  0.6189907   0.6210288   0.62185741  0.62205773  0.6220827   0.62397218
  0.62720359  0.62752826  0.62792238  0.62828929  0.63379832  0.63658676
  0.63851627  0.63925789  0.64506342  0.65612168  0.66304344  0.66583446
  0.67052855  0.67169009  0.68222564  0.69311406  0.70911455  0.71831113
  0.72157055  0.73301673  0.74546296  0.76161311  0.80710681  0.80803571
  0.82144726  0.83546323  0.99240213  1.02263864  1.26145386  1.75370926]

  UserWarning,

2022-10-31 11:01:56,101:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.84498156 -0.80812469 -0.7129961  -0.46855304 -0.33921246 -0.22149497
 -0.1103665  -0.10524839 -0.08920578 -0.06201914  0.04327635  0.05209869
  0.07213756  0.08348136  0.09254532  0.10587048  0.11365544  0.1143196
  0.12362274  0.12921201  0.13651414  0.14886693  0.16299682  0.16642151
  0.17209564  0.18748147  0.19165136  0.19233081  0.20038684  0.20709045
  0.22513423  0.23125286  0.23207598  0.23508851  0.24606353  0.25732274
  0.259784    0.26116264  0.26219726  0.26313752  0.26419105  0.26729903
  0.27077958  0.27425663  0.27782277  0.30146681  0.30252519  0.31438139
  0.32482707  0.32722915  0.32761499  0.32830949  0.33157891  0.33995102
  0.34315258  0.34810525  0.35121048  0.35179959  0.3531071   0.3565305
  0.35692     0.35987123  0.37102411  0.37518883  0.37521498  0.3765418
  0.37677059  0.3810456   0.38571266  0.38808065  0.38859933  0.39113025
  0.39775889  0.39820966  0.39938224  0.40121283  0.40264641  0.40609692
  0.4066978   0.40701413  0.40716467  0.41248979  0.41374627  0.4161869
  0.41713319  0.41738577  0.4208277   0.42145383  0.42450108  0.42786197
  0.42990679  0.43474282  0.43478599  0.43735976  0.4388871   0.4407177
  0.44127334  0.44153849  0.44159316  0.44268762  0.442753    0.4449628
  0.44528603  0.44569437  0.44570822  0.44580444  0.44864956  0.44877865
  0.44943325  0.44973466  0.45058095  0.4538704   0.45440799  0.45520596
  0.45704044  0.45754304  0.45879768  0.46021993  0.46179473  0.46489283
  0.46540896  0.46584109  0.46805974  0.46962552  0.4699828   0.47009238
  0.47183556  0.47227879  0.47263568  0.47296725  0.47305589  0.47537104
  0.47595788  0.47721964  0.47745     0.48110589  0.48130438  0.48257192
  0.48470708  0.4862276   0.48661344  0.48694359  0.48794871  0.48910468
  0.48922079  0.48945868  0.4896191   0.48983748  0.48999765  0.49198707
  0.4920543   0.49263272  0.49285718  0.49316227  0.4941708   0.49585961
  0.49594606  0.49639943  0.49695331  0.49695669  0.49732902  0.49757019
  0.4979124   0.49811171  0.49880607  0.49881514  0.49884373  0.49902909
  0.49940972  0.4998529   0.50015549  0.50038064  0.50044254  0.50081016
  0.50110285  0.50227362  0.50292807  0.50296411  0.5031314   0.50338799
  0.504134    0.50434827  0.50445557  0.50607726  0.50650146  0.50651256
  0.50779102  0.50817412  0.51003204  0.51005257  0.51006691  0.51041283
  0.51049622  0.51134444  0.51212441  0.51232606  0.51326176  0.51339689
  0.5134761   0.51447838  0.51483326  0.51549725  0.51641589  0.51668996
  0.51673833  0.51747342  0.5177337   0.51782982  0.51912481  0.51939247
  0.52065904  0.52109072  0.52132611  0.52202355  0.52228452  0.52305777
  0.52364161  0.52671208  0.52801085  0.52831557  0.52903647  0.53026347
  0.53133749  0.5321379   0.53301898  0.53460459  0.53482586  0.53767786
  0.5382054   0.53829588  0.54021548  0.5415782   0.54169991  0.5418844
  0.54307462  0.5437026   0.54452295  0.54468382  0.54552562  0.5470328
  0.54727631  0.54783166  0.54927651  0.54952604  0.5498962   0.55009083
  0.55108077  0.55182499  0.552612    0.55323273  0.55427078  0.55440364
  0.55517142  0.55535237  0.55650416  0.55715154  0.55718756  0.55783247
  0.55946098  0.56060266  0.56333534  0.56395427  0.564826    0.56567896
  0.56676483  0.56903184  0.56957027  0.56973081  0.57004752  0.57011075
  0.57209744  0.57250876  0.57336515  0.57385511  0.57534553  0.5756981
  0.57577411  0.57717957  0.5779674   0.57824793  0.57841233  0.57847888
  0.57853176  0.57918938  0.57935131  0.57994314  0.5800436   0.58014811
  0.58061969  0.5808583   0.58155715  0.58160707  0.58233269  0.58233562
  0.58286491  0.58394314  0.58465306  0.58524857  0.5857265   0.58615933
  0.58630142  0.58778929  0.59005321  0.59154331  0.59202424  0.59202622
  0.59205856  0.59210244  0.59257254  0.59341153  0.59398266  0.59556731
  0.5956017   0.59598672  0.59609395  0.59677072  0.59782712  0.59927073
  0.60008663  0.60078458  0.60152696  0.60191465  0.60213035  0.60410097
  0.60462444  0.60621549  0.60872639  0.60890315  0.6096106   0.61320881
  0.6174308   0.61814853  0.61875618  0.61909196  0.62107054  0.62781654
  0.63259905  0.63330759  0.63794204  0.64194603  0.64361509  0.65094
  0.66406393  0.67963259  0.69648994  0.70284854  0.71407688  0.7388936
  0.74728341  0.75735846  0.81954539  0.91567318  1.26722099]

  UserWarning,

2022-10-31 11:01:56,148:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.32298262e+00 -1.14693527e+00 -9.10642055e-01 -5.37323703e-01
 -4.88895383e-01 -2.35047104e-01 -1.94431627e-01 -1.35214811e-01
 -1.26755657e-01 -1.15933377e-01 -1.11946402e-01 -1.04453297e-01
 -9.87312480e-02 -3.97665190e-02 -3.35036672e-02 -2.21674778e-02
  8.63509810e-04  5.39583102e-02  6.41630056e-02  7.82167961e-02
  7.83770974e-02  9.91815332e-02  1.03111640e-01  1.06219089e-01
  1.11217076e-01  1.12956020e-01  1.13361804e-01  1.23135509e-01
  1.31724036e-01  1.40844386e-01  1.43123572e-01  1.43507767e-01
  1.47937760e-01  1.55943293e-01  1.56070621e-01  1.59976587e-01
  1.96863556e-01  1.97382511e-01  2.00966680e-01  2.03969306e-01
  2.12593819e-01  2.21201977e-01  2.23567906e-01  2.26422207e-01
  2.27230551e-01  2.43841240e-01  2.48183118e-01  2.49235718e-01
  2.61032738e-01  2.65151957e-01  2.71017715e-01  2.71628507e-01
  2.71686509e-01  2.71698960e-01  2.75893544e-01  2.85402953e-01
  2.91143647e-01  2.92863756e-01  3.02165227e-01  3.08640148e-01
  3.15880553e-01  3.16648444e-01  3.17711356e-01  3.19070349e-01
  3.19528397e-01  3.22198449e-01  3.34647394e-01  3.35966200e-01
  3.36742284e-01  3.37527334e-01  3.39551843e-01  3.41338185e-01
  3.48344651e-01  3.52793938e-01  3.54835766e-01  3.58937150e-01
  3.60881327e-01  3.70476633e-01  3.70839329e-01  3.70908277e-01
  3.72023832e-01  3.75930116e-01  3.80580894e-01  3.81414440e-01
  3.81444395e-01  3.85929797e-01  3.86599299e-01  3.87076658e-01
  3.90942900e-01  3.91070788e-01  3.94157929e-01  3.95823357e-01
  3.97284177e-01  3.97420469e-01  3.97438973e-01  3.99386465e-01
  4.04845695e-01  4.09601485e-01  4.09682016e-01  4.11291464e-01
  4.11977012e-01  4.13389126e-01  4.19798595e-01  4.20129215e-01
  4.21899939e-01  4.25287853e-01  4.29126841e-01  4.30175645e-01
  4.31638648e-01  4.32312513e-01  4.32779365e-01  4.33841124e-01
  4.34981646e-01  4.37893473e-01  4.39032550e-01  4.40861880e-01
  4.40992047e-01  4.43046824e-01  4.47070429e-01  4.48279329e-01
  4.48418176e-01  4.48633879e-01  4.49601624e-01  4.49988294e-01
  4.52062154e-01  4.53337109e-01  4.54063086e-01  4.55381841e-01
  4.55496170e-01  4.57532441e-01  4.57863391e-01  4.60117028e-01
  4.60590220e-01  4.60910022e-01  4.61201696e-01  4.61439092e-01
  4.61883188e-01  4.64756652e-01  4.66431374e-01  4.66769689e-01
  4.67760458e-01  4.67865340e-01  4.67899899e-01  4.67978482e-01
  4.69143648e-01  4.69525293e-01  4.70517548e-01  4.72388976e-01
  4.78281937e-01  4.78785740e-01  4.81064008e-01  4.82638049e-01
  4.83278046e-01  4.83279646e-01  4.83379335e-01  4.84031373e-01
  4.84737960e-01  4.87082654e-01  4.87126751e-01  4.87400885e-01
  4.87604000e-01  4.87801803e-01  4.88198886e-01  4.89234030e-01
  4.89484680e-01  4.89914934e-01  4.90421507e-01  4.91322906e-01
  4.91676771e-01  4.91734087e-01  4.92059300e-01  4.92531031e-01
  4.94200463e-01  4.94699980e-01  4.94833440e-01  4.95411050e-01
  4.95673370e-01  4.95914988e-01  4.97191956e-01  4.97389057e-01
  4.97402922e-01  4.97527009e-01  4.97704875e-01  4.98416272e-01
  4.98578271e-01  4.99191488e-01  4.99814834e-01  5.00463407e-01
  5.02089521e-01  5.02758146e-01  5.03646798e-01  5.03701832e-01
  5.03988661e-01  5.04392851e-01  5.04502161e-01  5.04697760e-01
  5.05507403e-01  5.05515531e-01  5.05773064e-01  5.05805601e-01
  5.06152425e-01  5.06418002e-01  5.06517269e-01  5.06600850e-01
  5.06676616e-01  5.07405364e-01  5.08387565e-01  5.08430237e-01
  5.08711503e-01  5.08977082e-01  5.09042750e-01  5.09402132e-01
  5.09858484e-01  5.10204245e-01  5.10852451e-01  5.11394132e-01
  5.12088791e-01  5.14358338e-01  5.15723442e-01  5.17815766e-01
  5.18026926e-01  5.18579609e-01  5.20196271e-01  5.21187825e-01
  5.22817532e-01  5.23200305e-01  5.23564474e-01  5.23990266e-01
  5.24303184e-01  5.26612806e-01  5.28316001e-01  5.28937905e-01
  5.30877255e-01  5.31181980e-01  5.31357548e-01  5.33248521e-01
  5.33642122e-01  5.34209743e-01  5.34688559e-01  5.34936963e-01
  5.35887665e-01  5.36321851e-01  5.36641184e-01  5.38095399e-01
  5.38641062e-01  5.39689636e-01  5.40114261e-01  5.41182410e-01
  5.41220889e-01  5.41789694e-01  5.42010183e-01  5.42260383e-01
  5.42411760e-01  5.44101781e-01  5.44184929e-01  5.45225520e-01
  5.45861996e-01  5.46244984e-01  5.46648956e-01  5.49073951e-01
  5.49121980e-01  5.50722043e-01  5.53112202e-01  5.53236809e-01
  5.57240010e-01  5.59314499e-01  5.60622025e-01  5.60988450e-01
  5.63800724e-01  5.64020448e-01  5.64636065e-01  5.66839390e-01
  5.67201369e-01  5.67790931e-01  5.68275984e-01  5.69329282e-01
  5.70116850e-01  5.70918646e-01  5.71522057e-01  5.72105293e-01
  5.73049199e-01  5.74699528e-01  5.74766027e-01  5.75771852e-01
  5.75871068e-01  5.76174222e-01  5.78781339e-01  5.79580767e-01
  5.80133387e-01  5.80659171e-01  5.80875816e-01  5.82693425e-01
  5.82967037e-01  5.87214167e-01  5.87749899e-01  5.87994711e-01
  5.89270139e-01  5.89517730e-01  5.90150156e-01  5.90295111e-01
  5.91222719e-01  5.91532277e-01  5.91631746e-01  5.93717624e-01
  5.94391556e-01  5.94431032e-01  5.95229955e-01  5.95399272e-01
  5.97964337e-01  5.97988741e-01  6.01134195e-01  6.01803650e-01
  6.03027761e-01  6.03264449e-01  6.03646921e-01  6.05539241e-01
  6.05929842e-01  6.05972858e-01  6.08385832e-01  6.09452020e-01
  6.11732481e-01  6.20483176e-01  6.21532897e-01  6.21773148e-01
  6.24770739e-01  6.29841094e-01  6.30364477e-01  6.50877247e-01
  6.71488215e-01  6.90959231e-01  7.12519360e-01  7.30655949e-01
  7.56845391e-01  7.64401726e-01  7.90805845e-01  8.07723271e-01
  8.17967955e-01  8.46724328e-01  8.75699150e-01  9.85329866e-01
  1.07419787e+00]

  UserWarning,

2022-10-31 11:01:56,148:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86727901 -0.60546046 -0.47389979 -0.23729939 -0.21308689 -0.13843315
 -0.08545726 -0.06304646 -0.00818525  0.02109596  0.0583449   0.06145274
  0.0643398   0.073449    0.07409438  0.10980668  0.11797491  0.12055652
  0.12463834  0.15850252  0.20939299  0.21789437  0.22197343  0.23663063
  0.24446716  0.24474084  0.24676026  0.27185524  0.27758145  0.28377972
  0.28936111  0.29762513  0.30009852  0.30397348  0.30718748  0.30757493
  0.31075505  0.31672873  0.3169592   0.33091189  0.33258209  0.33526871
  0.33648786  0.34266901  0.34409647  0.34597598  0.34981158  0.35126019
  0.35386418  0.35557459  0.3569295   0.35814371  0.37076763  0.37174244
  0.37187013  0.37302123  0.37332847  0.3761071   0.38077206  0.38501846
  0.38790384  0.39487974  0.39534446  0.40010514  0.40280418  0.40391464
  0.40409005  0.40495332  0.40510065  0.40589526  0.40664436  0.40962339
  0.41246111  0.41428631  0.41451522  0.4147797   0.41525441  0.41578266
  0.41675345  0.41722953  0.4190168   0.42029143  0.42171349  0.42325654
  0.42558881  0.42569877  0.42699251  0.42924034  0.43153268  0.43308241
  0.43502219  0.43705712  0.43980775  0.44003937  0.44061888  0.44609876
  0.44645405  0.44742597  0.44794417  0.44823382  0.44844781  0.44922135
  0.44958715  0.45030647  0.45042477  0.45053597  0.45471108  0.45741304
  0.45741498  0.45803507  0.45882102  0.45973465  0.46043087  0.46301241
  0.46544732  0.4655334   0.46607617  0.46740472  0.46762823  0.46778242
  0.46798712  0.471862    0.47186525  0.47227371  0.47238437  0.47398706
  0.47454869  0.47494042  0.47547872  0.47581097  0.47752148  0.4790137
  0.48030104  0.48049331  0.4805131   0.48060799  0.48127125  0.48148909
  0.48184581  0.48214164  0.48280182  0.48368194  0.48454074  0.4858399
  0.48808016  0.49046565  0.49070822  0.49108446  0.49112478  0.49322239
  0.49735664  0.49737249  0.49770612  0.49779036  0.49791789  0.49810488
  0.4981306   0.49897473  0.49914366  0.50015863  0.50124322  0.5023524
  0.5032295   0.50376569  0.50381467  0.50431546  0.50464929  0.50465393
  0.50488814  0.50546138  0.5056947   0.50581498  0.50597506  0.50635191
  0.50675955  0.50730465  0.50754475  0.50834386  0.50851384  0.50951607
  0.51007306  0.51009647  0.51015689  0.51021285  0.51077121  0.51172121
  0.51224056  0.51242821  0.51303993  0.5134123   0.51390255  0.51459807
  0.51470552  0.51477171  0.51506073  0.51537776  0.51599951  0.51751451
  0.51784143  0.51861374  0.51914759  0.51938545  0.51946159  0.52110744
  0.52444264  0.52445472  0.52467005  0.52558021  0.52864705  0.5292248
  0.53042431  0.53128262  0.53142132  0.53238561  0.53363461  0.53429167
  0.5345932   0.53561208  0.53633595  0.53676477  0.53709772  0.53719234
  0.53812594  0.53871528  0.53948313  0.53966421  0.53976808  0.54012867
  0.54027233  0.54093936  0.54095806  0.54139627  0.54228328  0.54331751
  0.54378894  0.54465538  0.54467998  0.54526754  0.54530229  0.54675973
  0.54677274  0.54730822  0.54752901  0.54776536  0.54932682  0.55040714
  0.55176611  0.55193658  0.55281956  0.55385893  0.55400212  0.55401788
  0.55413584  0.55511726  0.55611999  0.55705614  0.55757551  0.56122854
  0.56181966  0.56240723  0.56375654  0.56456368  0.56473132  0.56477691
  0.56572673  0.56735417  0.56842554  0.56971022  0.57157105  0.57304598
  0.57385014  0.57393593  0.57394016  0.57412057  0.57426674  0.57438849
  0.57478515  0.57575019  0.57730426  0.57834548  0.57885981  0.5797896
  0.58129348  0.58152719  0.58561163  0.58566209  0.58579197  0.5867572
  0.58704147  0.58772187  0.58908452  0.58952784  0.58963318  0.58987146
  0.59081859  0.59088953  0.59128309  0.59154191  0.59202499  0.59235445
  0.59303591  0.59305581  0.59307752  0.59385016  0.59386135  0.59427341
  0.59474066  0.59505888  0.59529387  0.59576296  0.59583187  0.5959326
  0.59594566  0.596764    0.59702823  0.59741216  0.59779005  0.59886826
  0.59900416  0.59951509  0.59960211  0.6002573   0.60042105  0.60076926
  0.601055    0.60120223  0.60164923  0.60168566  0.60183713  0.60319967
  0.60402314  0.60544217  0.60561646  0.60574344  0.60575205  0.60636972
  0.60662278  0.60673375  0.60733503  0.60764399  0.60885203  0.61001014
  0.61142527  0.61233044  0.61260205  0.61370628  0.61808619  0.61963868
  0.62108734  0.62150881  0.62197879  0.62236065  0.62838597  0.62939224
  0.63330071  0.63834074  0.63946027  0.66166912  0.66718799  0.66785607
  0.67093106  0.67495755  0.67500903  0.68547407  0.6906534   0.69141915
  0.69655125  0.7081462   0.71854329  0.73066242  0.74160373  0.80074043
  0.81517163  0.85333321  0.86987872  0.87669708  0.89119712  0.98784503
  1.08909057  1.34231483  1.64968016]

  UserWarning,

2022-10-31 11:01:56,179:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.71658835 -1.05224453 -0.50640103 -0.36357846 -0.34833985 -0.26562872
 -0.26412456 -0.19125029 -0.17388545 -0.16840684 -0.07660678 -0.04399083
 -0.03144485 -0.02998972  0.00985612  0.02035821  0.06398315  0.07270781
  0.10184401  0.12524941  0.1341679   0.13871333  0.13904839  0.15281397
  0.15332141  0.17685059  0.20916854  0.21958746  0.22008375  0.22754796
  0.23982164  0.25722633  0.26408839  0.26836347  0.27848746  0.29054215
  0.29572073  0.31085964  0.31336404  0.31822117  0.32271018  0.3233871
  0.3274149   0.33923375  0.35147114  0.3531175   0.35631493  0.35936932
  0.36036864  0.36857411  0.37039345  0.3738961   0.37413577  0.3741407
  0.37481296  0.37801649  0.38229458  0.38414511  0.38665874  0.38788223
  0.38861376  0.38908916  0.38957947  0.39936079  0.39981436  0.40050172
  0.40197316  0.40380279  0.40486784  0.4059541   0.40876086  0.41214805
  0.41334044  0.41398045  0.41530603  0.42170342  0.42332641  0.42393185
  0.42416145  0.42510596  0.42542596  0.42877491  0.42926443  0.42960177
  0.43027488  0.4312313   0.43166583  0.43486962  0.43676209  0.43851566
  0.43930724  0.44162463  0.44318963  0.44384445  0.44427526  0.44457533
  0.44494351  0.44538018  0.44600419  0.44661522  0.44956384  0.44975602
  0.45007961  0.45155231  0.45596886  0.45704471  0.45962798  0.46024896
  0.46139741  0.461419    0.46188948  0.46189699  0.46241798  0.46275497
  0.46309072  0.46366111  0.4658738   0.46689793  0.46735035  0.46856328
  0.46895006  0.46901002  0.46967429  0.46983537  0.47057395  0.47102045
  0.47148592  0.47256491  0.47300811  0.47492309  0.47514284  0.47687318
  0.47726046  0.47772606  0.47903803  0.47954948  0.48038774  0.48140088
  0.48187141  0.48209663  0.48384761  0.48384845  0.48674518  0.48702181
  0.48747873  0.48788121  0.48868746  0.489002    0.48968989  0.49049979
  0.49074973  0.49255061  0.49470214  0.49534944  0.49578452  0.49644198
  0.49759496  0.49799836  0.49888204  0.49909845  0.50011422  0.50048793
  0.5006287   0.50072122  0.50180089  0.50244789  0.5025129   0.50283188
  0.50406426  0.50417894  0.50459019  0.50492659  0.50632898  0.50698581
  0.50718603  0.5074945   0.50759026  0.50830552  0.50861093  0.50978217
  0.51005513  0.51136584  0.5115364   0.51176004  0.51183774  0.51190971
  0.51257039  0.51341424  0.51431886  0.51439817  0.51480307  0.51517626
  0.51605919  0.51634497  0.51642085  0.51673378  0.51676219  0.51696244
  0.51750076  0.51840458  0.51846813  0.51985588  0.5200448   0.52276937
  0.52317489  0.52460585  0.5254186   0.52547356  0.52553813  0.52597237
  0.52753468  0.52907479  0.52908857  0.52994189  0.53181669  0.53210527
  0.53249536  0.53297033  0.53558652  0.53579874  0.53588249  0.5374174
  0.53811588  0.53816685  0.53817084  0.53913923  0.53926941  0.54243954
  0.54478666  0.54515939  0.54575428  0.54602904  0.54608796  0.54616905
  0.54826659  0.54855131  0.5489714   0.54973112  0.54975813  0.55025481
  0.55090494  0.55121528  0.55156982  0.55373267  0.5538029   0.55393842
  0.55469654  0.5547755   0.55691394  0.55756208  0.55839977  0.55978281
  0.55992939  0.56095675  0.56218847  0.56253423  0.56395479  0.56530029
  0.56669688  0.56691585  0.56772002  0.56794097  0.56801576  0.56909089
  0.57137289  0.57356909  0.5739202   0.57604593  0.57734187  0.57793902
  0.57817703  0.5781877   0.57820043  0.57860433  0.58045973  0.58098663
  0.58162136  0.58176504  0.5818128   0.58244076  0.58335401  0.58346204
  0.58374546  0.58379768  0.58386963  0.58426791  0.58467169  0.58477474
  0.58533485  0.58559611  0.58602544  0.58635348  0.58652146  0.58673673
  0.58681053  0.58686504  0.58698622  0.58724257  0.58777454  0.58787093
  0.58789069  0.58829797  0.58882836  0.589336    0.58962733  0.59018071
  0.59135199  0.59161392  0.59161537  0.59194148  0.59204076  0.59265431
  0.59302801  0.5953971   0.59548903  0.59694019  0.59705011  0.59753267
  0.59818741  0.59847128  0.60007041  0.600178    0.6004988   0.60071833
  0.60274688  0.6040085   0.60487186  0.60571585  0.60649583  0.60793091
  0.60853742  0.60952337  0.60997329  0.61143345  0.61380922  0.61472869
  0.61520531  0.61532353  0.61645137  0.61793148  0.62236307  0.6237991
  0.63069929  0.63580388  0.63705093  0.64258797  0.65229429  0.65365329
  0.66057903  0.66510145  0.67839137  0.67928993  0.68100968  0.68129538
  0.68814317  0.70661336  0.70819028  0.71179531  0.73201404  0.73930696
  0.74507494  0.76767219  0.79212654  0.80449827  0.86286563  0.87318495
  0.91768837  0.99211196  1.01944274  1.04878537  1.17297327  1.26810547]

  UserWarning,

2022-10-31 11:01:56,179:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.93288327 -0.62610006 -0.5128991  -0.51159255 -0.47170674 -0.47166676
 -0.46632139 -0.28185105 -0.19544766 -0.18018499 -0.13762782 -0.12688003
 -0.11165285 -0.07104704 -0.05788086 -0.03186774 -0.01305357 -0.00274536
  0.0263031   0.04100971  0.04127721  0.04203264  0.06513166  0.06771981
  0.11243006  0.12021201  0.12650654  0.12961872  0.12977309  0.15464679
  0.17109092  0.1952807   0.20502869  0.21301461  0.21772492  0.23713104
  0.24047156  0.2446072   0.25066615  0.25421072  0.26036938  0.26109159
  0.26249836  0.26264868  0.26484496  0.26650402  0.27004248  0.27095116
  0.27326294  0.28228711  0.28982742  0.29098918  0.2956728   0.30008476
  0.30137113  0.30842144  0.31001409  0.31700472  0.3229846   0.32345469
  0.32923989  0.33580961  0.33964345  0.35148371  0.35182456  0.35361405
  0.35608554  0.35674187  0.35858142  0.35883556  0.36674521  0.36900913
  0.37298012  0.37533672  0.37833591  0.38103902  0.38350412  0.38447566
  0.38496022  0.3877299   0.38833609  0.38900997  0.39003213  0.3901644
  0.39030923  0.39471855  0.39913085  0.40034573  0.40238296  0.40609033
  0.40632858  0.4073506   0.40790143  0.40962851  0.41226842  0.41325683
  0.41502438  0.41550158  0.41710263  0.41838708  0.41841126  0.4190137
  0.4269999   0.42937957  0.4295321   0.43176321  0.43180331  0.43707751
  0.43732665  0.4395641   0.44055996  0.44227159  0.44377607  0.44411121
  0.44464986  0.44702402  0.44838386  0.44929249  0.45093788  0.45205121
  0.45404611  0.45467331  0.45589145  0.45592753  0.45718416  0.45789516
  0.45806159  0.46031731  0.46272407  0.46443248  0.46542224  0.46567292
  0.46607701  0.46664118  0.46803221  0.46945825  0.47066013  0.47093365
  0.47195719  0.472622    0.47306389  0.473638    0.474592    0.47602163
  0.47685636  0.47751056  0.47821804  0.47834278  0.47868735  0.48004277
  0.48092201  0.48157849  0.48449492  0.48593637  0.48617351  0.48636735
  0.48703668  0.48811249  0.48828138  0.48945913  0.48956168  0.49073113
  0.49075698  0.4912749   0.49156563  0.49177598  0.49231762  0.49249735
  0.49281717  0.49281877  0.49292475  0.49391456  0.49422213  0.49447972
  0.49465257  0.49493898  0.49582704  0.4970141   0.49719475  0.4977081
  0.49799535  0.49888389  0.49919404  0.49943157  0.49961349  0.50067212
  0.50148616  0.50212875  0.50238287  0.50250495  0.5027378   0.50430344
  0.50527526  0.50530776  0.50547632  0.50568168  0.50582985  0.50586884
  0.50630073  0.50695237  0.5072638   0.50745405  0.50776045  0.50956601
  0.50983351  0.50992827  0.51000801  0.51015677  0.510536    0.51094632
  0.51106888  0.51186776  0.51312884  0.51481304  0.51490632  0.51539908
  0.51555495  0.51614523  0.51614624  0.51683632  0.51689852  0.51728823
  0.51756675  0.5176115   0.51957176  0.52089318  0.52097025  0.52179895
  0.52212308  0.52311934  0.52503678  0.52515388  0.52539665  0.52586647
  0.52656621  0.52842923  0.5295741   0.53127876  0.53161689  0.53409249
  0.53513105  0.53516212  0.53530773  0.53690443  0.53694827  0.53739635
  0.54241683  0.54287682  0.5428861   0.54309968  0.54315015  0.54448776
  0.54703538  0.54708399  0.54833379  0.54890305  0.5515196   0.55188051
  0.55291974  0.55634289  0.55957348  0.56093382  0.56170283  0.5618306
  0.56272157  0.56363732  0.56502868  0.5651414   0.56606328  0.56665795
  0.57033748  0.57137416  0.574005    0.57459813  0.57465297  0.57473999
  0.57738357  0.57755588  0.57943752  0.58053516  0.58130861  0.58167106
  0.58246296  0.58256689  0.58386689  0.58506561  0.5851024   0.58585128
  0.58586395  0.58611824  0.58616332  0.58648698  0.58668137  0.58680021
  0.58688013  0.58728627  0.58741225  0.58757154  0.58757636  0.58792139
  0.58805708  0.58889321  0.58949596  0.58998803  0.59087649  0.59100063
  0.59119361  0.59120613  0.59127364  0.59182975  0.59219358  0.59223678
  0.59279281  0.59305635  0.59317817  0.59847802  0.59945575  0.59992936
  0.60076751  0.60076988  0.60098817  0.60339606  0.60346114  0.6046389
  0.60499719  0.60527801  0.60603642  0.60633574  0.6065232   0.60770971
  0.61196888  0.61330587  0.61344974  0.61439364  0.61579153  0.61651661
  0.61758364  0.61952017  0.62909147  0.63018159  0.63603739  0.63793313
  0.64055688  0.6411195   0.64160787  0.64372013  0.64409542  0.6453145
  0.64913023  0.6528099   0.65944705  0.66429865  0.67233568  0.67441222
  0.67732725  0.67777859  0.68216642  0.69734014  0.70009867  0.70571015
  0.72984242  0.75289528  0.75442772  0.7578294   0.83331827  0.84311358
  0.91165921  1.07847902]

  UserWarning,

2022-10-31 11:01:56,195:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.87926866 -0.82676955 -0.81692981 -0.31138186 -0.25953871 -0.2486295
 -0.2223754  -0.19600323 -0.15404287 -0.13464015 -0.10783177 -0.07626552
 -0.06056967 -0.05239197  0.00832641  0.02724752  0.04800214  0.0490216
  0.05774884  0.06012254  0.06472197  0.0652068   0.07197608  0.09611262
  0.10606838  0.11129921  0.13176168  0.13997696  0.17055726  0.18824532
  0.19090058  0.19517729  0.19786975  0.20837463  0.22355125  0.22423717
  0.23223353  0.25797492  0.26730086  0.2673904   0.27198522  0.27660891
  0.28228501  0.28594752  0.29118209  0.30470867  0.31226716  0.32287617
  0.32544754  0.32602261  0.32805191  0.3345593   0.3398576   0.35228426
  0.35381452  0.35854043  0.35864871  0.36249722  0.36398327  0.37964287
  0.38146775  0.38240697  0.3831471   0.38412194  0.3890206   0.39918976
  0.40040202  0.40093064  0.40435633  0.40649518  0.40912029  0.40926515
  0.4108038   0.41117515  0.41447081  0.41706772  0.41980263  0.42228744
  0.42283435  0.42431613  0.42478972  0.42494241  0.42748669  0.43078905
  0.43427849  0.43490237  0.43641549  0.43704114  0.43714411  0.43733065
  0.4391536   0.43988197  0.44101129  0.44252499  0.44263016  0.44373126
  0.44387795  0.44736164  0.44768408  0.44773501  0.44811513  0.44839938
  0.44870144  0.44903943  0.44937813  0.44959717  0.4506776   0.45076763
  0.45156612  0.45378294  0.45438546  0.45552309  0.45652277  0.46107084
  0.46120831  0.46129546  0.46140282  0.46162011  0.46219755  0.46522368
  0.46526441  0.46709445  0.46958888  0.47478515  0.47660221  0.47886727
  0.48317049  0.48386858  0.48454674  0.48486772  0.48631465  0.48689208
  0.48716085  0.48733439  0.48789572  0.48851231  0.48855484  0.49020463
  0.49150715  0.49156853  0.49399411  0.49412029  0.49442686  0.49453139
  0.49529532  0.49550468  0.49571446  0.49622378  0.4981621   0.49826785
  0.49888443  0.49926127  0.49927025  0.49943372  0.49958879  0.49970197
  0.50003357  0.5008592   0.50276434  0.50283677  0.5029965   0.50316079
  0.50339318  0.50352624  0.50379041  0.50380316  0.50381543  0.50436184
  0.50443557  0.50488458  0.50493795  0.50514351  0.50567538  0.50574162
  0.50821705  0.50903392  0.50989627  0.50993266  0.5104229   0.51205306
  0.51232981  0.51371107  0.51541955  0.51622255  0.51709628  0.51954704
  0.51961412  0.52036732  0.52038687  0.52160517  0.52198024  0.52222183
  0.52314819  0.52324722  0.52351525  0.5239659   0.52443793  0.52477341
  0.52487359  0.52516781  0.5258934   0.52782231  0.52813508  0.52869731
  0.53030292  0.530483    0.53102388  0.5325828   0.5333289   0.53398685
  0.5352897   0.53572749  0.53618945  0.53673375  0.53691827  0.53749634
  0.538716    0.53953978  0.54063435  0.54149272  0.54167079  0.54286231
  0.54319265  0.54379244  0.54379362  0.54443891  0.54485526  0.54515843
  0.54565559  0.54637644  0.54663128  0.54673206  0.54802965  0.54840957
  0.54843945  0.54913081  0.55099818  0.55233868  0.5524359   0.55312991
  0.55595981  0.55719459  0.5574292   0.55746702  0.55754095  0.55832634
  0.55936177  0.55971307  0.55997912  0.56000893  0.56033762  0.56044156
  0.56154065  0.56192561  0.56287731  0.5642592   0.56550187  0.56616535
  0.5679226   0.56996712  0.57008156  0.57061751  0.57068229  0.57134558
  0.57169368  0.57215458  0.57263232  0.57425149  0.57469819  0.57484454
  0.57665972  0.57678682  0.57803993  0.57829369  0.57856052  0.58024182
  0.58166458  0.58180792  0.58182083  0.58287623  0.58427593  0.58512907
  0.58545053  0.58552006  0.58562419  0.58574431  0.58613652  0.5862429
  0.58762993  0.58831537  0.58921294  0.58955124  0.58977469  0.59105946
  0.59144176  0.59245003  0.5929675   0.59305032  0.5933258   0.59348261
  0.59465519  0.59473513  0.59504537  0.5951558   0.59607032  0.59612245
  0.59626205  0.59633541  0.59633657  0.59641543  0.59699391  0.59757759
  0.59846922  0.59877548  0.59912138  0.5998953   0.5999467   0.60004031
  0.60161266  0.60228169  0.60374363  0.60503578  0.60742698  0.60749206
  0.60773416  0.60949888  0.61023169  0.61029556  0.611056    0.61134449
  0.61215902  0.61539864  0.61567892  0.61575018  0.61645431  0.61670915
  0.62188665  0.62208279  0.6284688   0.62894448  0.62972692  0.64179142
  0.64888248  0.64968378  0.65998356  0.66660966  0.66926256  0.66950888
  0.67530469  0.68464946  0.69744264  0.69951128  0.70855904  0.71823755
  0.72353127  0.75214631  0.76125526  0.90661368  0.96705537  1.29580256
  1.49622138]

  UserWarning,

2022-10-31 11:01:56,210:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.05643431e+00 -9.40757678e-01 -9.19675294e-01 -8.80987749e-01
 -8.58341136e-01 -8.12998977e-01 -5.48805676e-01 -4.87779836e-01
 -4.87246794e-01 -4.70060416e-01 -4.50976685e-01 -4.26237058e-01
 -4.15942470e-01 -3.97687356e-01 -3.80029708e-01 -3.41429888e-01
 -3.18537682e-01 -3.13148259e-01 -2.91642812e-01 -2.83978283e-01
 -1.86813386e-01 -1.59465941e-01 -1.57412350e-01 -1.28586721e-01
 -9.14396130e-02 -7.92826309e-04  9.16470923e-03  2.66565740e-02
  3.50876600e-02  4.82535268e-02  6.89648023e-02  7.22508014e-02
  8.35081507e-02  8.35701980e-02  1.18007804e-01  1.38402929e-01
  1.42884749e-01  1.52260554e-01  1.65142434e-01  1.83791147e-01
  1.94865165e-01  1.96782306e-01  2.01444476e-01  2.06190166e-01
  2.11123372e-01  2.24443377e-01  2.25117395e-01  2.28444635e-01
  2.47220042e-01  2.50319585e-01  2.53644669e-01  2.54060399e-01
  2.54864026e-01  2.60766622e-01  2.66673224e-01  2.67469866e-01
  2.78037607e-01  2.79741354e-01  2.83460501e-01  2.92147763e-01
  2.97008771e-01  3.01405619e-01  3.07868882e-01  3.17805391e-01
  3.18836119e-01  3.19950102e-01  3.27621004e-01  3.31551957e-01
  3.33610840e-01  3.45301549e-01  3.47820670e-01  3.49557548e-01
  3.52804203e-01  3.53012495e-01  3.55944796e-01  3.63635800e-01
  3.63758046e-01  3.65424809e-01  3.66693482e-01  3.71870816e-01
  3.73471585e-01  3.76964526e-01  3.82482083e-01  3.85614263e-01
  3.92461514e-01  3.93507410e-01  3.94094113e-01  3.94134404e-01
  3.94499579e-01  3.95797108e-01  3.97197880e-01  3.98119029e-01
  4.00550367e-01  4.01378516e-01  4.03161127e-01  4.05451078e-01
  4.07868365e-01  4.09045559e-01  4.10548035e-01  4.11187798e-01
  4.12930660e-01  4.13274492e-01  4.16034926e-01  4.17366572e-01
  4.19917192e-01  4.22843084e-01  4.23245840e-01  4.25915810e-01
  4.26025859e-01  4.27200048e-01  4.28847391e-01  4.29521250e-01
  4.30755550e-01  4.33039608e-01  4.38259691e-01  4.39753835e-01
  4.40484222e-01  4.40899853e-01  4.42984265e-01  4.43175511e-01
  4.43632646e-01  4.47294887e-01  4.47925863e-01  4.48690230e-01
  4.48734809e-01  4.48759867e-01  4.49341526e-01  4.49514135e-01
  4.50933875e-01  4.51670405e-01  4.55462052e-01  4.57534572e-01
  4.58020442e-01  4.58956188e-01  4.59124695e-01  4.60981169e-01
  4.61384066e-01  4.64308359e-01  4.64509735e-01  4.66056736e-01
  4.66622133e-01  4.67515259e-01  4.67587976e-01  4.68922804e-01
  4.69457215e-01  4.69509216e-01  4.69512195e-01  4.70073679e-01
  4.70133876e-01  4.70772041e-01  4.70942569e-01  4.71427336e-01
  4.73113475e-01  4.74895079e-01  4.75338646e-01  4.76058763e-01
  4.78251780e-01  4.79720014e-01  4.79775463e-01  4.80355774e-01
  4.83975745e-01  4.84995039e-01  4.86177540e-01  4.86670255e-01
  4.87690687e-01  4.87699694e-01  4.88394710e-01  4.88647905e-01
  4.88821141e-01  4.88822358e-01  4.88999356e-01  4.90056052e-01
  4.90139821e-01  4.91634553e-01  4.92060998e-01  4.92273582e-01
  4.92566911e-01  4.92665324e-01  4.92731066e-01  4.92747486e-01
  4.92929210e-01  4.93561195e-01  4.93623948e-01  4.93660813e-01
  4.94196000e-01  4.94723820e-01  4.94731532e-01  4.95240116e-01
  4.95460410e-01  4.95474002e-01  4.95674376e-01  4.97202498e-01
  4.98632518e-01  4.99753848e-01  5.00333001e-01  5.00789586e-01
  5.01052765e-01  5.01255955e-01  5.03291816e-01  5.03953042e-01
  5.03999950e-01  5.04623706e-01  5.05683128e-01  5.06601239e-01
  5.06897113e-01  5.06925709e-01  5.07744098e-01  5.07757000e-01
  5.08514503e-01  5.09227661e-01  5.09466085e-01  5.10286654e-01
  5.10417002e-01  5.11192174e-01  5.11317149e-01  5.11746933e-01
  5.13406821e-01  5.13745286e-01  5.14392856e-01  5.15954067e-01
  5.17520975e-01  5.17665722e-01  5.17684060e-01  5.18596659e-01
  5.19614171e-01  5.20745865e-01  5.20851747e-01  5.23626825e-01
  5.23921562e-01  5.24296769e-01  5.26541446e-01  5.28597032e-01
  5.29299016e-01  5.29527776e-01  5.29700823e-01  5.31862045e-01
  5.32469592e-01  5.35175147e-01  5.37830310e-01  5.37998081e-01
  5.38434981e-01  5.39152006e-01  5.39294726e-01  5.39333448e-01
  5.39427476e-01  5.39521214e-01  5.39553898e-01  5.39937656e-01
  5.42205257e-01  5.46502530e-01  5.46640401e-01  5.46727959e-01
  5.47348554e-01  5.47937915e-01  5.48317437e-01  5.48874167e-01
  5.50163880e-01  5.50249247e-01  5.50338425e-01  5.50798143e-01
  5.51326625e-01  5.51665128e-01  5.54699281e-01  5.55237642e-01
  5.56166487e-01  5.57278756e-01  5.57574964e-01  5.58540328e-01
  5.62443153e-01  5.62604534e-01  5.63141527e-01  5.64737268e-01
  5.66072405e-01  5.67868833e-01  5.68984833e-01  5.69331423e-01
  5.69526481e-01  5.70415179e-01  5.70458776e-01  5.72568195e-01
  5.73204594e-01  5.74918296e-01  5.74972215e-01  5.76160887e-01
  5.76525033e-01  5.77053651e-01  5.77070550e-01  5.78787786e-01
  5.78910721e-01  5.79294523e-01  5.80204611e-01  5.81290508e-01
  5.84252048e-01  5.84497246e-01  5.84863137e-01  5.85568906e-01
  5.85940788e-01  5.87053827e-01  5.87430563e-01  5.87886273e-01
  5.89018727e-01  5.89289164e-01  5.90287718e-01  5.90891175e-01
  5.91082584e-01  5.92267815e-01  5.92682056e-01  5.92960405e-01
  5.92993425e-01  5.93048828e-01  5.93099047e-01  5.93559154e-01
  5.93638682e-01  5.94195235e-01  5.94580073e-01  5.94618754e-01
  5.94693423e-01  5.95230209e-01  5.95888288e-01  5.95912083e-01
  5.95955024e-01  5.97672880e-01  5.97778119e-01  5.98708020e-01
  5.99849195e-01  5.99884708e-01  6.00902210e-01  6.01105162e-01
  6.01721332e-01  6.02850441e-01  6.04912260e-01  6.04969935e-01
  6.05427247e-01  6.06098620e-01  6.07271708e-01  6.08306944e-01
  6.08857348e-01  6.09324316e-01  6.09934313e-01  6.10433401e-01
  6.11178186e-01  6.12606540e-01  6.13783066e-01  6.16219374e-01
  6.17069730e-01  6.19333388e-01  6.19746543e-01  6.20097820e-01
  6.20410102e-01  6.26576907e-01  6.29097429e-01  6.29811303e-01
  6.30209035e-01  6.30276286e-01  6.31750450e-01  6.33898957e-01
  6.35793085e-01  6.44456300e-01  6.46373552e-01  6.47060920e-01
  6.51385326e-01  6.72144984e-01  6.72955737e-01  6.79462638e-01
  6.94755646e-01  6.97889957e-01  6.98032325e-01  7.04443372e-01
  7.24856912e-01  7.25642149e-01  7.42060494e-01  7.81146365e-01
  8.71872049e-01  8.96976172e-01  8.98137627e-01]

  UserWarning,

2022-10-31 11:01:57,024:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.81086413 -0.55712169 -0.3786604  -0.37554805 -0.33271647 -0.30563916
 -0.22381383 -0.22312883 -0.21527679 -0.21503886 -0.1450292  -0.09360072
 -0.06630164 -0.01639945  0.00182671  0.01059172  0.03071634  0.05528528
  0.07288228  0.07607143  0.09295392  0.10406195  0.10975075  0.11079798
  0.11088073  0.11541638  0.13470791  0.1363936   0.15154197  0.15755705
  0.15850278  0.17051869  0.1980804   0.20187111  0.21302191  0.22597411
  0.23382921  0.24223486  0.2524866   0.27466895  0.27714939  0.28431532
  0.28566697  0.28919646  0.30603408  0.31825661  0.32912383  0.33082275
  0.33120862  0.33311101  0.33997711  0.34061757  0.35114952  0.35150684
  0.35182226  0.35255078  0.35682742  0.35710012  0.35906775  0.35917835
  0.35946953  0.36120698  0.36248112  0.36326658  0.36482592  0.36546902
  0.36777906  0.36807794  0.36850376  0.36971766  0.37174896  0.3737223
  0.37524473  0.37713715  0.38176857  0.38822312  0.38898177  0.38941375
  0.39221963  0.39222999  0.3925813   0.39475895  0.39540739  0.39592148
  0.39703982  0.4017364   0.4028714   0.40500006  0.40692579  0.40727174
  0.40890921  0.40925344  0.40955646  0.41053788  0.41112892  0.41159739
  0.41736946  0.42265852  0.42339695  0.42440485  0.4277828   0.43037347
  0.43186882  0.4327956   0.43455865  0.43455973  0.43576655  0.43850337
  0.44126957  0.44244707  0.44268067  0.44371812  0.44371914  0.443738
  0.44531049  0.44549479  0.44686792  0.44709921  0.44889574  0.44916382
  0.4492175   0.45435848  0.45437717  0.45581573  0.45606882  0.45729576
  0.45733742  0.4574681   0.46539168  0.46557757  0.46571316  0.46677242
  0.46745175  0.46970738  0.4710218   0.47188428  0.47547459  0.47577706
  0.47648506  0.47853304  0.48021318  0.48195503  0.48308626  0.48332603
  0.48417651  0.48424056  0.48444146  0.48498152  0.48542295  0.4862096
  0.48940812  0.48942544  0.48996089  0.4900849   0.49065542  0.491755
  0.49176937  0.49272483  0.49392842  0.49587959  0.49662505  0.49707955
  0.49712667  0.49846484  0.49861089  0.49861622  0.49908709  0.49973904
  0.49975679  0.49985379  0.50008309  0.50017015  0.50052259  0.5009124
  0.50091519  0.50112453  0.5020304   0.50209157  0.50278847  0.50293209
  0.50380513  0.50389017  0.50425413  0.504255    0.50475293  0.50540066
  0.50577869  0.50652499  0.50853526  0.50945611  0.51043768  0.51080546
  0.51144873  0.51199254  0.51217611  0.51236259  0.51247511  0.51344897
  0.51359598  0.5139347   0.51475454  0.51624431  0.51678766  0.5174113
  0.51899378  0.52061816  0.52068247  0.52136499  0.52418668  0.52443089
  0.52504009  0.52573748  0.52667002  0.52877818  0.52929025  0.53153547
  0.53184601  0.53251903  0.53505788  0.53584608  0.53616348  0.53661526
  0.53899024  0.54064426  0.54108936  0.5418959   0.54218697  0.54296159
  0.54359895  0.54628485  0.54847748  0.54866726  0.54921592  0.55056159
  0.55060847  0.55197682  0.55290022  0.55330117  0.55361735  0.55365497
  0.55408842  0.55408997  0.55498693  0.55533538  0.55614093  0.55638861
  0.55894683  0.55960999  0.55976362  0.56129932  0.56180278  0.56315055
  0.56341445  0.56347523  0.56364849  0.56449943  0.56527396  0.56684486
  0.56759833  0.5680684   0.5686972   0.56890916  0.57144978  0.5730054
  0.5738201   0.57386828  0.5743402   0.57467076  0.57495715  0.57522556
  0.57829329  0.58125094  0.5816725   0.58403124  0.58465692  0.58470334
  0.58557117  0.58568873  0.58584379  0.58611604  0.58635695  0.58668616
  0.58672431  0.58721169  0.58726989  0.58783776  0.58802852  0.58877051
  0.58890541  0.58950892  0.5895354   0.59057608  0.59108561  0.59326561
  0.59354285  0.59565651  0.59594373  0.59616123  0.59692277  0.59886643
  0.59990584  0.60074845  0.60123224  0.60125891  0.60283054  0.60326986
  0.6056502   0.60572725  0.60837327  0.60903466  0.60970474  0.61236693
  0.61310348  0.61715583  0.62169965  0.62217351  0.62371449  0.62738972
  0.62785961  0.63429912  0.63435806  0.6400153   0.64219006  0.64718053
  0.65546579  0.65602627  0.65964323  0.66110094  0.66497362  0.66732865
  0.6707752   0.67703752  0.68124528  0.68939185  0.69665081  0.71983487
  0.72448974  0.73125062  0.73232447  0.76546393  0.79143354  0.81639607
  0.95578116  1.25058347]

  UserWarning,

2022-10-31 11:01:57,055:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.44697827 -1.21938845 -0.92533014 -0.43016458 -0.42215156 -0.42027057
 -0.41455029 -0.36215817 -0.3571773  -0.33220425 -0.3272233  -0.24637658
 -0.13517517 -0.11806068 -0.10299131 -0.06521008 -0.04831927 -0.04411553
 -0.01428642  0.04498356  0.06866683  0.10185125  0.11111154  0.12122367
  0.12396726  0.12717732  0.15710561  0.15892877  0.17228906  0.18213534
  0.1903246   0.19205232  0.20150146  0.20357052  0.20462911  0.22889761
  0.22953945  0.23219292  0.23229017  0.24287274  0.24836447  0.24968786
  0.2586764   0.26783933  0.26939111  0.2807779   0.28104975  0.28708727
  0.29432847  0.29973458  0.30473849  0.3063426   0.31765458  0.32255634
  0.32485949  0.32528785  0.32581553  0.3273602   0.33558335  0.34313026
  0.34563131  0.35158761  0.35568767  0.35639307  0.35949192  0.36245843
  0.36500407  0.36776334  0.37774443  0.38199847  0.38295343  0.38399327
  0.38731791  0.3879114   0.39306418  0.39472182  0.39739377  0.3976859
  0.40091709  0.40251917  0.40471316  0.40577663  0.40769904  0.41035375
  0.41222239  0.41438651  0.41477982  0.42227703  0.42287384  0.42523845
  0.42700416  0.42824703  0.42853082  0.42855666  0.42970106  0.43082915
  0.43173211  0.4322028   0.43248712  0.43815944  0.43959764  0.44026403
  0.44269488  0.44304919  0.44307958  0.44675766  0.44781607  0.44855238
  0.44992217  0.4503272   0.45042315  0.45335264  0.45364507  0.4556591
  0.45677267  0.45735028  0.4582434   0.45970319  0.46060375  0.4609589
  0.46123702  0.4612471   0.46387773  0.46429101  0.46570759  0.46624569
  0.46669685  0.46733157  0.46783375  0.46912974  0.46913756  0.46927178
  0.47061098  0.47065921  0.47350049  0.47438331  0.47482863  0.47492847
  0.4754573   0.47550405  0.47637594  0.47856265  0.48057536  0.48321629
  0.48450654  0.484668    0.48496504  0.48663828  0.48668233  0.48859217
  0.48922667  0.49090993  0.4913059   0.4916398   0.49276512  0.49333573
  0.49339568  0.49342003  0.49507564  0.49513707  0.4952632   0.49564739
  0.49712299  0.49740859  0.49785896  0.49928083  0.4998813   0.50042024
  0.50066775  0.50206046  0.5021735   0.50238792  0.50308012  0.50359426
  0.50390038  0.50397731  0.5044369   0.50505506  0.50541919  0.50580195
  0.50671906  0.50767977  0.50798317  0.50884628  0.50939486  0.51029733
  0.51074342  0.51084882  0.5108697   0.51119512  0.51130607  0.5145456
  0.5158202   0.51660561  0.51678855  0.51756594  0.51758448  0.51835463
  0.51919383  0.51943597  0.52097441  0.52147487  0.52265646  0.52315217
  0.52565961  0.52687997  0.52850279  0.52854489  0.52875642  0.52924952
  0.53069639  0.5324196   0.53273345  0.53328928  0.5333069   0.5335939
  0.53407536  0.53486396  0.53680414  0.53908669  0.54083705  0.54141161
  0.5414565   0.54213251  0.54214734  0.54261642  0.5427166   0.54279608
  0.54305851  0.54329205  0.543416    0.54344255  0.54366535  0.54366838
  0.54488715  0.54520971  0.54618222  0.54622082  0.54670115  0.54733615
  0.5477114   0.54987153  0.55105024  0.55220953  0.55260875  0.55643334
  0.55665511  0.55808135  0.558411    0.56081125  0.56130071  0.56413187
  0.56447294  0.56495792  0.5658742   0.56669329  0.57035832  0.57048437
  0.5714597   0.57227426  0.57245044  0.57348254  0.5740746   0.57535052
  0.57548639  0.57554305  0.57561547  0.57577634  0.57594032  0.57603557
  0.5770008   0.57708146  0.58015392  0.58132509  0.58144563  0.58239765
  0.58262635  0.58264348  0.5831198   0.58335523  0.58370717  0.58497899
  0.58508524  0.58592483  0.58600575  0.58631877  0.58670645  0.58682674
  0.5880457   0.58834527  0.58935187  0.59029648  0.59081767  0.59129383
  0.59146478  0.591507    0.59282442  0.59338413  0.59362413  0.59444165
  0.59476799  0.59693581  0.59844334  0.59879538  0.59990761  0.60075113
  0.60224661  0.60249609  0.60297277  0.60441483  0.60477271  0.60538919
  0.60615824  0.60668242  0.60672573  0.607796    0.60808982  0.60962011
  0.61187188  0.61496035  0.61732268  0.62077339  0.6211242   0.62423666
  0.62576939  0.62991353  0.63001123  0.64442797  0.6466743   0.65000405
  0.65195606  0.671308    0.67414988  0.67810686  0.69849739  0.70915151
  0.71024571  0.71754272  0.75813809  0.75953856  0.78394043  0.81151864
  0.82553401  0.90019652  1.12517194  1.12656343  1.13846243  1.32050034]

  UserWarning,

2022-10-31 11:01:57,056:INFO:Calculating mean and std
2022-10-31 11:01:57,057:INFO:Creating metrics dataframe
2022-10-31 11:01:57,060:INFO:Uploading results into container
2022-10-31 11:01:57,061:INFO:Uploading model into container now
2022-10-31 11:01:57,061:INFO:master_model_container: 4
2022-10-31 11:01:57,061:INFO:display_container: 2
2022-10-31 11:01:57,061:INFO:Lasso(random_state=3360)
2022-10-31 11:01:57,061:INFO:create_model() successfully completed......................................
2022-10-31 11:01:57,163:ERROR:create_model() for Lasso(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:01:57,163:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:01:57,163:INFO:Initializing Ridge Regression
2022-10-31 11:01:57,163:INFO:Total runtime is 0.40288212696711223 minutes
2022-10-31 11:01:57,163:INFO:SubProcess create_model() called ==================================
2022-10-31 11:01:57,163:INFO:Initializing create_model()
2022-10-31 11:01:57,163:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:01:57,163:INFO:Checking exceptions
2022-10-31 11:01:57,163:INFO:Importing libraries
2022-10-31 11:01:57,163:INFO:Copying training dataset
2022-10-31 11:01:57,178:INFO:Defining folds
2022-10-31 11:01:57,178:INFO:Declaring metric variables
2022-10-31 11:01:57,178:INFO:Importing untrained model
2022-10-31 11:01:57,178:INFO:Ridge Regression Imported successfully
2022-10-31 11:01:57,178:INFO:Starting cross validation
2022-10-31 11:01:57,178:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:01:59,082:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390238 -0.94068138 -0.59039351 -0.54800603 -0.2923101  -0.20442241
 -0.18673878 -0.13766655 -0.12368684 -0.1229897  -0.11954131 -0.082832
 -0.07060698 -0.05659686 -0.03475587 -0.02264261 -0.00978007 -0.00835976
 -0.00379795 -0.00264154  0.01643107  0.02032372  0.02550704  0.0289719
  0.0297027   0.0398841   0.04684666  0.0485153   0.04878315  0.05033827
  0.05077262  0.07210069  0.07884441  0.07889112  0.08002797  0.08060359
  0.08312576  0.08389765  0.09060725  0.09100597  0.09817914  0.10122391
  0.10141383  0.10146693  0.10989965  0.11276695  0.1462199   0.15577902
  0.15680997  0.15992609  0.16147785  0.16281593  0.16699417  0.17533857
  0.18208666  0.1865269   0.18830391  0.19171318  0.19201892  0.19252136
  0.19264413  0.19531549  0.20279589  0.20440353  0.20745636  0.20757594
  0.21381247  0.21657506  0.22166777  0.22462254  0.22555997  0.22659418
  0.22762555  0.22974752  0.23263708  0.23362877  0.23608238  0.23649436
  0.23680519  0.23680935  0.24010608  0.24227293  0.24398271  0.24763293
  0.24936121  0.24946242  0.24957677  0.25001755  0.25182062  0.25188801
  0.25218524  0.25356623  0.25432629  0.25447183  0.25652695  0.25784277
  0.25824781  0.2584321   0.25943078  0.26228661  0.26230867  0.26370758
  0.264161    0.26535651  0.26649497  0.26686591  0.26814665  0.26872205
  0.27097539  0.27522453  0.27581312  0.27598637  0.27614541  0.27714866
  0.27717887  0.27726808  0.27790961  0.27810845  0.27849585  0.27861386
  0.27919126  0.27941918  0.28135384  0.28163707  0.28194836  0.28262725
  0.28270126  0.28323316  0.28350502  0.28369268  0.28393871  0.28461925
  0.28726054  0.28774501  0.28857861  0.28906715  0.28922072  0.28923696
  0.28926936  0.28973227  0.29116352  0.29143527  0.2917996   0.29237111
  0.29273177  0.29293694  0.29319411  0.29667924  0.29717859  0.29742343
  0.29773972  0.29794897  0.29820386  0.30076293  0.30328755  0.30371555
  0.30582644  0.30667923  0.30792731  0.3079519   0.31027189  0.31229562
  0.31234644  0.313971    0.31485181  0.31487146  0.31563973  0.31568065
  0.31652425  0.31685144  0.31738182  0.31811635  0.31937362  0.31996687
  0.32086371  0.32096172  0.32332013  0.3241807   0.32549635  0.32641425
  0.32696395  0.32944396  0.32953163  0.33046865  0.33073403  0.33402479
  0.3350993   0.33849932  0.33971889  0.34183451  0.34202581  0.34235356
  0.34367108  0.34371638  0.3441251   0.34747837  0.34761572  0.3485237
  0.3501147   0.35071691  0.3509965   0.35176133  0.35415312  0.3547383
  0.35645317  0.35725069  0.35739681  0.359342    0.35958306  0.36124617
  0.3617      0.36420165  0.36738471  0.36972985  0.37055815  0.37084793
  0.37187639  0.37225961  0.37274614  0.37299814  0.37317093  0.37408596
  0.37526777  0.3753091   0.37602986  0.37664353  0.37744813  0.37779649
  0.37918917  0.38097438  0.38293689  0.38379856  0.38434054  0.38468515
  0.38578378  0.38640923  0.38652625  0.38668772  0.38685956  0.38980847
  0.38981654  0.39076944  0.39098512  0.39261538  0.39285399  0.39294508
  0.39380817  0.39431187  0.39465883  0.39491855  0.39600661  0.39794339
  0.39877823  0.40009136  0.4032336   0.4039943   0.40421763  0.40445651
  0.40461833  0.40661201  0.40857064  0.40882125  0.40889026  0.41022065
  0.41039489  0.41071209  0.41153264  0.41222968  0.41236817  0.41517973
  0.41849879  0.41949474  0.41966936  0.42084214  0.42110268  0.42218505
  0.42249706  0.42286666  0.42349751  0.42369172  0.4262253   0.42801232
  0.43088259  0.4323794   0.43274693  0.43431047  0.43528282  0.43781189
  0.44193535  0.4423481   0.44413204  0.44415387  0.445687    0.44805845
  0.44806444  0.44874592  0.44899982  0.45472038  0.45523191  0.45689234
  0.45696922  0.45864397  0.45882985  0.46031974  0.4607299   0.46273945
  0.46373995  0.46403832  0.46698028  0.4681826   0.46841684  0.46924249
  0.47095146  0.47319105  0.47420433  0.47562929  0.47794027  0.48323429
  0.48650383  0.48737441  0.48831112  0.4892338   0.49183463  0.49327754
  0.49415473  0.49545201  0.49768     0.50073218  0.50178183  0.50576093
  0.51006567  0.51360878  0.52296292  0.53068924  0.53318208  0.53607595
  0.53658246  0.53661545  0.5387873   0.54813906  0.55117226  0.55530036
  0.55561756  0.55656971  0.55707403  0.56124489  0.56704535  0.56982037
  0.57138339  0.57485815  0.57529249  0.57791909  0.58031617  0.58449517
  0.586988    0.59182648  0.59473633  0.59695932  0.59945216  0.60065692
  0.60411648  0.60412062  0.60443782  0.60693065  0.60746094  0.60910214
  0.61191631  0.61588471  0.61730233  0.61765986  0.6193948   0.61949977
  0.62164916  0.62188763  0.62258484  0.62567029  0.62989642  0.63006503
  0.63185896  0.63758935  0.63813496  0.63901611  0.63933745  0.64150894
  0.64183028  0.64430364  0.64432311  0.64930878  0.65094917  0.65180161
  0.65448806  0.65457662  0.65678727  0.65699622  0.65807761  0.6592801
  0.66057044  0.66058939  0.66177293  0.6622595   0.66426576  0.66454795
  0.66479605  0.66643725  0.66675859  0.66833865  0.66925142  0.67356437
  0.67423709  0.67489584  0.67766632  0.67922275  0.68072655  0.68139424
  0.68203685  0.68231511  0.68359297  0.68919407  0.68970489  0.6916869
  0.69385839  0.6949609   0.69530223  0.69606454  0.69884406  0.70133689
  0.70165823  0.70296752  0.70447232  0.70632255  0.70664389  0.70696516
  0.70744983  0.70745262  0.7082785   0.70881538  0.70913672  0.71044601
  0.71130821  0.71256595  0.71425916  0.7179245   0.72219854  0.72567271
  0.72705593  0.72940063  0.73093366  0.73323648  0.73612871  0.73749426
  0.73965022  0.74035998  0.74186479  0.74285713  0.7439972   0.74534565
  0.74613429  0.74685045  0.74934328  0.75001411  0.75312646  0.75531697
  0.7570686   0.75870234  0.75931461  0.75939827  0.7598554   0.76275794
  0.76430027  0.7667931   0.76707136  0.76838065  0.76869449  0.76928593
  0.7697706   0.77038339  0.77082193  0.77177876  0.77475627  0.77672135
  0.77796918  0.77985038  0.78424292  0.78673575  0.79013325  0.79102254
  0.79670707  0.7991999   0.80050919  0.802069    0.80635706  0.8066784
  0.80917123  0.80966698  0.81417472  0.81994849  0.82711452  0.82718914
  0.82953727  0.83210018  0.83790166  0.84498798  0.84656369  0.84667014
  0.84849443  0.85154935  0.85209478  0.85702849  0.85734096  0.85952133
  0.86699982  0.87997734  0.89224059  0.89442096  0.89455492  0.89948124
  0.9004586   0.90439228  0.90570158  0.90635198  0.90787721  0.90937795
  0.91458364  0.9156729   0.9218421   0.92682776  0.92785374  0.93181343
  0.93234372  0.93679909  0.93810838  0.94060121  0.94178475  0.94309404
  0.94427758  0.9455674   0.94558687  0.94677041  0.94684461  0.95584038
  0.95805103  0.96054386  0.9617274   0.9618342   0.96279884  0.96422023
  0.96478832  0.96552952  0.96728115  0.96808492  0.96920589  0.97051518
  0.9714154   0.97387021  0.9750004   0.97550084  0.97799367  0.98174425
  0.98297934  0.9847083   0.987965    0.99045783  1.01040048  1.01289331
  1.01538614  1.01619498  1.01909008  1.0203718   1.04152556  1.06854153
  1.08598462  1.09493281  1.18725434  1.64839536]

  UserWarning,

2022-10-31 11:01:59,176:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86821138 -0.41627215 -0.34521038 -0.30481588 -0.19782507 -0.13964246
 -0.1281651  -0.06704292 -0.03453797 -0.0339275  -0.02631181 -0.01070708
  0.00356016  0.01370993  0.02414879  0.0382608   0.03973397  0.04143897
  0.04415541  0.05337656  0.05403384  0.05446404  0.06648931  0.08377465
  0.0860325   0.09848429  0.10076863  0.10336003  0.10705908  0.10864966
  0.10882053  0.12199957  0.12859439  0.13028449  0.1324365   0.13552094
  0.13741546  0.14078979  0.14252533  0.14979687  0.15153475  0.15243743
  0.15849283  0.16198053  0.16202134  0.16607999  0.16684354  0.16920514
  0.1715289   0.17197011  0.17532646  0.18366279  0.1849292   0.18605475
  0.18627298  0.18755957  0.19269061  0.19326164  0.19358822  0.19396053
  0.19701206  0.19873624  0.20089365  0.2014707   0.20304726  0.20799749
  0.21074237  0.21249663  0.21582978  0.21711386  0.21947729  0.22068715
  0.22083743  0.22186356  0.22467495  0.22740963  0.2318154   0.23632626
  0.23679977  0.23681653  0.23724062  0.24577864  0.24649391  0.24717756
  0.24732385  0.24747054  0.24885615  0.25083531  0.25217726  0.25271118
  0.25361266  0.25569707  0.25645269  0.25654834  0.25723125  0.25724374
  0.25739408  0.26029156  0.26141662  0.26197689  0.26336421  0.26380849
  0.26530415  0.2666759   0.26745797  0.26854789  0.27112649  0.27184765
  0.27230229  0.27341336  0.27399045  0.27459095  0.2746045   0.2750008
  0.27596792  0.27646421  0.27748549  0.27755789  0.27819372  0.27905374
  0.28132266  0.28217493  0.28384679  0.28417371  0.28419253  0.28455635
  0.28596017  0.28626828  0.28641089  0.28727788  0.28772456  0.28867535
  0.28947631  0.29031261  0.29113033  0.29123184  0.29387024  0.29574084
  0.29600249  0.29600498  0.29650499  0.29661227  0.29812019  0.29856178
  0.29895597  0.2990051   0.29906444  0.2994074   0.30022008  0.30128197
  0.30130718  0.30158787  0.30229833  0.3029501   0.30470122  0.30524084
  0.30538603  0.30715464  0.30958633  0.3098442   0.30986773  0.31000137
  0.31044458  0.3122555   0.31252366  0.31296428  0.31455832  0.31545083
  0.31795735  0.31979716  0.32090439  0.32416614  0.32489346  0.32612783
  0.32668508  0.32679876  0.3275928   0.32875768  0.32917443  0.33018053
  0.33166212  0.33193306  0.33224848  0.3336653   0.33377245  0.33423312
  0.33435125  0.33638113  0.33667021  0.33697776  0.3374412   0.33787422
  0.33998777  0.34391286  0.34411963  0.34543857  0.34618201  0.34680002
  0.34741487  0.34809419  0.34839042  0.35063692  0.35266755  0.35277781
  0.35321635  0.35344775  0.35444916  0.35468736  0.35509563  0.35533942
  0.35572312  0.35694262  0.35761805  0.35884104  0.35932223  0.35982161
  0.36112101  0.36159656  0.36532921  0.36747725  0.36800901  0.36890293
  0.36897545  0.3690361   0.36985858  0.37158115  0.37179997  0.3721529
  0.37451858  0.375747    0.3759972   0.37672556  0.37714185  0.37745973
  0.37806958  0.37870943  0.37899194  0.38197021  0.38226954  0.3831097
  0.38386152  0.38425172  0.38511204  0.38569773  0.38605738  0.38646684
  0.38655209  0.38707277  0.38717177  0.387836    0.38858677  0.38882192
  0.38976855  0.39156562  0.3917005   0.39257416  0.39261172  0.39275338
  0.39300626  0.39353892  0.39405559  0.39410252  0.39479472  0.39543248
  0.39560659  0.39673122  0.39741888  0.39843915  0.39859087  0.39924543
  0.39989539  0.40105478  0.4029168   0.40384563  0.405067    0.40734106
  0.40785401  0.40846004  0.40873971  0.40879756  0.40887283  0.41273014
  0.41471781  0.41526513  0.41546092  0.41593278  0.41921446  0.41993248
  0.42157375  0.42199789  0.42232859  0.42233     0.42313974  0.42430646
  0.4243376   0.42452501  0.42557826  0.42787873  0.43033607  0.43228667
  0.43321595  0.43420015  0.43655775  0.4385803   0.44044565  0.44376745
  0.45065052  0.45257668  0.45507439  0.45571728  0.45617002  0.45708732
  0.45750051  0.45810202  0.45917392  0.45993357  0.46242753  0.46256278
  0.46291935  0.46352131  0.46441319  0.46449616  0.46517004  0.46590216
  0.46733772  0.46770572  0.46831257  0.46857797  0.46942458  0.47318592
  0.47340512  0.480319    0.48241784  0.48735243  0.49035016  0.49380098
  0.49487597  0.49967813  0.50362289  0.50378825  0.50541271  0.50559135
  0.50601319  0.50674206  0.50812916  0.5089786   0.50928168  0.51203831
  0.51230714  0.52911327  0.5293766   0.53083045  0.53130848  0.53924615
  0.54770049  0.54967982  0.54988034  0.54989401  0.55122978  0.55178767
  0.55375823  0.55454207  0.55790214  0.5605456   0.5609777   0.56141414
  0.56261324  0.56286128  0.56348123  0.56422932  0.56528826  0.56603693
  0.56689515  0.56760555  0.57557322  0.57620649  0.58030712  0.58430337
  0.58455884  0.58607904  0.58646267  0.5871394   0.58941093  0.59086212
  0.59516915  0.59723153  0.59788559  0.60060203  0.60859863  0.60875137
  0.60917329  0.6122039   0.6169007   0.61961714  0.62226893  0.62473765
  0.62631602  0.63174891  0.6325453   0.63458493  0.63555795  0.6359158
  0.63651715  0.6380751   0.63863225  0.64069463  0.64134869  0.64170768
  0.64350799  0.645191    0.64622444  0.64949802  0.65195829  0.65221447
  0.65493091  0.65888463  0.6603638   0.66223129  0.6625231   0.66308024
  0.66509628  0.66579669  0.66778595  0.67063096  0.67067243  0.67122958
  0.67338887  0.67394602  0.67451474  0.67596872  0.67774106  0.67937891
  0.68042538  0.6811061   0.68213955  0.68370014  0.68425465  0.68471274
  0.68697109  0.68727206  0.68882913  0.68968754  0.69141814  0.69240398
  0.69512043  0.69524547  0.69633484  0.69769361  0.69783687  0.70055331
  0.70317284  0.70338169  0.70359515  0.70581616  0.70584294  0.7059862
  0.70654335  0.70855938  0.70870265  0.71141909  0.7123285   0.71399227
  0.71426057  0.71469268  0.71670118  0.71670872  0.71685198  0.71696104
  0.7221416   0.72336284  0.73244581  0.73358195  0.73562886  0.74099026
  0.74453043  0.74514678  0.74662851  0.74684192  0.74930604  0.7494493
  0.74955836  0.75133624  0.75168596  0.75170072  0.752058    0.75321745
  0.75429084  0.75473893  0.75716653  0.76009233  0.7607693   0.76515855
  0.76585703  0.76842715  0.77400636  0.77486695  0.78215569  0.78238388
  0.78294051  0.78487213  0.79030502  0.79302146  0.79573791  0.79845435
  0.80073869  0.8011708   0.80388724  0.80569415  0.80932013  0.81091746
  0.8201859   0.82254156  0.82777809  0.82833524  0.83207417  0.83550078
  0.83605246  0.83742463  0.83753075  0.83920101  0.84121705  0.84191745
  0.8446339   0.84644081  0.85372329  0.85396073  0.85778402  0.86050046
  0.86140807  0.86325805  0.86364901  0.86392274  0.87109793  0.87359528
  0.88632062  0.88787597  0.88845624  0.88982966  0.89217896  0.90533572
  0.90550217  0.90779593  0.90896158  0.91819076  0.91821754  0.92163439
  0.93179976  0.9345162   0.9347845   0.93521661  0.94155891  0.94266553
  0.94336594  0.94608238  0.95125909  0.95151527  0.96238105  0.96329526
  0.96437031  0.96795099  0.97415623  0.97526286  0.97652885  0.98358143
  0.98612864  0.99156152  0.99427797  0.99971086  1.0024273   1.00514374
  1.00843476  1.01057663  1.01572614  1.0210103   1.02144241  1.02670884
  1.02872488  1.03108617  1.05008332  1.07207105  1.07750394  1.08931065
  1.42131732  1.66807926]

  UserWarning,

2022-10-31 11:01:59,207:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24183509 -0.84358835 -0.60779896 -0.40205737 -0.38098777 -0.24130505
 -0.16903573 -0.10412578 -0.1026627  -0.09316583 -0.0852385  -0.0838159
 -0.05696668 -0.05604889 -0.03817931 -0.0346327  -0.01272463 -0.00509218
 -0.00398535  0.00833456  0.01736338  0.02076489  0.02525201  0.02581818
  0.0346664   0.05212424  0.05790097  0.06254634  0.06419381  0.09776756
  0.11011049  0.12700351  0.13444562  0.13555979  0.13749801  0.14333475
  0.14382151  0.1443721   0.149705    0.15056642  0.15205104  0.15551499
  0.15642883  0.15714386  0.15729845  0.15843314  0.15959143  0.16199522
  0.16336114  0.1644459   0.16940908  0.17129427  0.17449514  0.17768058
  0.17846609  0.18545851  0.19063058  0.19205521  0.19206591  0.19289549
  0.19480863  0.19681325  0.19747908  0.19942795  0.19968021  0.19991517
  0.20015902  0.20528002  0.20701622  0.20756361  0.20918455  0.20940038
  0.21270638  0.21542776  0.21612778  0.21756955  0.21789302  0.21796969
  0.22184351  0.22387663  0.22427741  0.22479225  0.22485697  0.22573355
  0.22640745  0.2282721   0.23073503  0.23271575  0.23924773  0.23931668
  0.24116574  0.24124408  0.24139332  0.24176657  0.24565927  0.24709226
  0.24779692  0.24873449  0.24895562  0.25109198  0.25131323  0.25173081
  0.25805143  0.25900158  0.26161667  0.26169258  0.26464396  0.26498152
  0.26695328  0.26788627  0.26792807  0.26815738  0.26888991  0.26974758
  0.27008491  0.27031724  0.27062009  0.27075694  0.27170273  0.27170824
  0.27189025  0.27323759  0.27423203  0.27477952  0.28178445  0.28195256
  0.28262638  0.28283717  0.28347751  0.28432881  0.28470898  0.28476411
  0.28718718  0.28807304  0.2883099   0.28925918  0.28955232  0.28977657
  0.29044622  0.29078763  0.29097713  0.29144472  0.29168391  0.29197198
  0.29246794  0.29261996  0.29461222  0.29534189  0.29583932  0.2964791
  0.29698032  0.29700417  0.29863452  0.29886471  0.29988796  0.30010004
  0.3038121   0.30439778  0.30453868  0.3064334   0.30649863  0.30687634
  0.30744376  0.30835508  0.30859675  0.3091419   0.31016547  0.31182656
  0.31347459  0.31441145  0.31450786  0.31451328  0.31468919  0.31470566
  0.3188211   0.31927063  0.32180234  0.32305458  0.32434957  0.32528303
  0.32636359  0.32647415  0.32693671  0.32733209  0.32770605  0.32796052
  0.32797346  0.32841973  0.33045964  0.33307105  0.3333686   0.33785393
  0.33850679  0.3387073   0.33959857  0.33974763  0.34053869  0.34056
  0.3406441   0.34066796  0.34122872  0.34248802  0.34433934  0.34650434
  0.34696803  0.3479533   0.34856833  0.34864106  0.34876319  0.34919704
  0.35285472  0.3530347   0.35404096  0.35608758  0.35652192  0.35905888
  0.35943734  0.36059817  0.36218923  0.36288163  0.36371488  0.36388996
  0.36390611  0.36402332  0.3642047   0.36426665  0.36444849  0.36489494
  0.36762207  0.36800665  0.36835271  0.36958802  0.37242447  0.37329012
  0.37411848  0.37659775  0.37741555  0.37787771  0.38042741  0.38286788
  0.38360399  0.3852381   0.38556744  0.38695424  0.38724646  0.38760832
  0.38767837  0.3876984   0.38793766  0.38806019  0.3887164   0.389461
  0.389586    0.38959509  0.38977785  0.38997854  0.3913961   0.39163422
  0.39221737  0.3924217   0.3936078   0.39435713  0.39471898  0.396469
  0.39708305  0.397337    0.39916568  0.39945942  0.40003729  0.40057472
  0.40220461  0.40287131  0.40306309  0.40411378  0.40451107  0.40509021
  0.40704773  0.40768785  0.40891308  0.4089403   0.41041282  0.41086472
  0.41187618  0.41269019  0.41379964  0.41570327  0.420109    0.4226369
  0.42366888  0.42416741  0.4249445   0.42773872  0.42835044  0.42849755
  0.43310219  0.4345458   0.43558763  0.43668711  0.43742601  0.43756828
  0.43797843  0.4399868   0.44034865  0.44167862  0.44183024  0.44266976
  0.4437785   0.4450891   0.44639289  0.4491468   0.44958773  0.45134258
  0.4514936   0.45202363  0.45219976  0.45326083  0.45581137  0.4562795
  0.45865823  0.46457549  0.46480838  0.46650995  0.47312418  0.47806018
  0.47853305  0.47882455  0.47976471  0.48090982  0.48386324  0.48673369
  0.49348351  0.49412711  0.49421153  0.49768457  0.49895198  0.50004364
  0.50404734  0.50822788  0.50929891  0.51076636  0.51449513  0.51561799
  0.51743338  0.51782419  0.52265418  0.52976484  0.53843123  0.53850867
  0.54145829  0.54271561  0.54373765  0.54635639  0.54729607  0.55715444
  0.55975102  0.56392227  0.56566582  0.5664396   0.56726363  0.56874055
  0.57072127  0.57645654  0.57657582  0.58005685  0.58184448  0.58411536
  0.58491294  0.58658492  0.5900587   0.59787523  0.59843602  0.59889971
  0.60489137  0.61027757  0.61312104  0.61561015  0.61683699  0.62002672
  0.6202317   0.62157743  0.62204409  0.62272082  0.63241214  0.63445302
  0.63601575  0.63682324  0.63998936  0.64156368  0.64200387  0.6439339
  0.64867434  0.65254886  0.65341479  0.65578501  0.65716294  0.65776258
  0.65815523  0.66052545  0.66261345  0.66265271  0.66289567  0.66334391
  0.66526589  0.66558713  0.66991728  0.67056542  0.67237655  0.67474677
  0.67513727  0.67711699  0.67948721  0.67960611  0.68008269  0.68185743
  0.68197633  0.68434655  0.68461816  0.68482314  0.68653265  0.68671677
  0.68692721  0.68719336  0.68868588  0.6900237   0.69145721  0.69370853
  0.69856787  0.69904446  0.70093809  0.70282756  0.70318942  0.7037849
  0.70519778  0.70567853  0.7079597   0.70999659  0.71001786  0.710214
  0.71024337  0.71030008  0.71089556  0.71161725  0.71333391  0.71735403
  0.71800622  0.71858525  0.72172218  0.72287475  0.7244229   0.72501106
  0.72874614  0.72920513  0.72938074  0.73157535  0.73163783  0.73618495
  0.736274    0.73631579  0.73744398  0.74578817  0.7461438   0.7481669
  0.75049532  0.75053712  0.75118931  0.75355953  0.75527756  0.760018
  0.76690316  0.76712866  0.76949888  0.7718691   0.77608551  0.77652049
  0.77660954  0.77701984  0.77897976  0.77916619  0.77941995  0.78323301
  0.78372021  0.78846065  0.78853775  0.79046901  0.79083087  0.79794153
  0.80031175  0.80268197  0.80505219  0.80870982  0.81074723  0.81216285
  0.81891166  0.82638418  0.82703652  0.82934988  0.83321262  0.8406055
  0.84297572  0.84534594  0.84704418  0.85024436  0.85380397  0.85588243
  0.86187226  0.86368169  0.87357287  0.87581501  0.87790302  0.88149473
  0.88351837  0.88386495  0.8873839   0.88797938  0.88975412  0.89212434
  0.89483836  0.90228223  0.90871589  0.91357518  0.91819677  0.92116247
  0.92293721  0.93241809  0.93282839  0.93292452  0.93502194  0.9360608
  0.93715853  0.94189897  0.94249446  0.94321614  0.94426919  0.94663941
  0.94703834  0.94960512  0.95219233  0.95434556  0.95474296  0.95671578
  0.95717591  0.959086    0.95980769  0.9709371   0.97271184  0.97330732
  0.97415781  0.97567754  0.9821275   0.98278821  0.98515843  0.98752865
  0.99226909  0.99307394  0.99664767  0.99850205  0.999701    1.00166092
  1.00174997  1.00300901  1.01491824  1.01502838  1.02669043  1.0269447
  1.02953191  1.03471952  1.03960828  1.04187577  1.04241869]

  UserWarning,

2022-10-31 11:01:59,223:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14440674e+00 -5.70398127e-01 -4.96868754e-01 -4.67186848e-01
 -3.51418341e-01 -1.10199882e-01 -1.09194418e-01 -9.71994367e-02
 -9.37784574e-02 -8.96426914e-02 -3.74703666e-02 -2.77463776e-02
 -1.65385417e-02  4.16379104e-04  3.95158615e-02  4.75904049e-02
  5.95440796e-02  6.03586598e-02  6.92239115e-02  7.63013497e-02
  9.22543302e-02  9.49175593e-02  1.01658979e-01  1.14106431e-01
  1.18460576e-01  1.29956757e-01  1.30384367e-01  1.37624291e-01
  1.40113571e-01  1.46908348e-01  1.50673654e-01  1.68784034e-01
  1.72833293e-01  1.75701547e-01  1.80723116e-01  1.86069152e-01
  1.89510201e-01  1.94340280e-01  1.94718865e-01  1.94770969e-01
  1.98018796e-01  1.98617572e-01  2.00882539e-01  2.05370441e-01
  2.05912942e-01  2.07240504e-01  2.09598904e-01  2.11753472e-01
  2.14402255e-01  2.16761577e-01  2.18779903e-01  2.19524130e-01
  2.19671061e-01  2.23274527e-01  2.25128643e-01  2.25918644e-01
  2.26830311e-01  2.27601481e-01  2.28580324e-01  2.30138633e-01
  2.30227445e-01  2.31139280e-01  2.33878505e-01  2.34534380e-01
  2.35203066e-01  2.35628663e-01  2.36309875e-01  2.37370949e-01
  2.38871504e-01  2.41887123e-01  2.41947574e-01  2.42348063e-01
  2.42594571e-01  2.44759945e-01  2.46573170e-01  2.47760526e-01
  2.48562046e-01  2.48601472e-01  2.49133680e-01  2.50091709e-01
  2.50619840e-01  2.50822148e-01  2.50875160e-01  2.51340152e-01
  2.51939287e-01  2.52611875e-01  2.52633668e-01  2.53316397e-01
  2.54545149e-01  2.54796552e-01  2.56333176e-01  2.57083610e-01
  2.57803970e-01  2.58504973e-01  2.59085323e-01  2.61398861e-01
  2.63105748e-01  2.63128029e-01  2.63588167e-01  2.65116643e-01
  2.65975615e-01  2.66023589e-01  2.66437617e-01  2.66584860e-01
  2.69471393e-01  2.71307162e-01  2.71894109e-01  2.74782856e-01
  2.75105201e-01  2.76250626e-01  2.77449273e-01  2.78090414e-01
  2.78405643e-01  2.79172294e-01  2.80450675e-01  2.80613880e-01
  2.81802722e-01  2.86358571e-01  2.86783517e-01  2.86878981e-01
  2.87019347e-01  2.87201887e-01  2.87404905e-01  2.89207973e-01
  2.91761715e-01  2.92967913e-01  2.93837665e-01  2.95684090e-01
  2.95884059e-01  2.96047770e-01  2.96843022e-01  2.98401895e-01
  3.00006711e-01  3.00390201e-01  3.00735643e-01  3.01097868e-01
  3.01175439e-01  3.01452982e-01  3.03065953e-01  3.03174182e-01
  3.05529035e-01  3.05581244e-01  3.05824350e-01  3.07833882e-01
  3.09554500e-01  3.10152056e-01  3.10476996e-01  3.10836346e-01
  3.13853047e-01  3.14144235e-01  3.15574590e-01  3.16389201e-01
  3.16916900e-01  3.18656272e-01  3.20457792e-01  3.22466426e-01
  3.22931855e-01  3.24137526e-01  3.24474158e-01  3.25359776e-01
  3.25943268e-01  3.26105442e-01  3.26462235e-01  3.26886941e-01
  3.27767355e-01  3.28842466e-01  3.30025416e-01  3.30119757e-01
  3.30430271e-01  3.30557615e-01  3.31581112e-01  3.32985373e-01
  3.33166521e-01  3.33197874e-01  3.34294642e-01  3.34317028e-01
  3.34761712e-01  3.35531985e-01  3.35730926e-01  3.36213046e-01
  3.36283840e-01  3.37017130e-01  3.37565671e-01  3.37846254e-01
  3.40484784e-01  3.41104113e-01  3.41200206e-01  3.41700917e-01
  3.41883249e-01  3.42759823e-01  3.43126643e-01  3.43928408e-01
  3.43989153e-01  3.44330106e-01  3.44339962e-01  3.44820318e-01
  3.45952323e-01  3.46830564e-01  3.47397672e-01  3.49893365e-01
  3.50765923e-01  3.50902311e-01  3.51525535e-01  3.53477338e-01
  3.54297714e-01  3.54768100e-01  3.54781361e-01  3.54848854e-01
  3.55150942e-01  3.55625873e-01  3.58614292e-01  3.58978202e-01
  3.59732537e-01  3.60385750e-01  3.60392536e-01  3.60618008e-01
  3.60921501e-01  3.60959108e-01  3.60997038e-01  3.61012305e-01
  3.61541533e-01  3.62292607e-01  3.65949449e-01  3.65988446e-01
  3.66679938e-01  3.67070827e-01  3.67429016e-01  3.68092602e-01
  3.68116287e-01  3.70808087e-01  3.72774716e-01  3.73715338e-01
  3.73766211e-01  3.74037627e-01  3.74322321e-01  3.75464561e-01
  3.76363386e-01  3.76724045e-01  3.77169882e-01  3.77544785e-01
  3.78522096e-01  3.79069464e-01  3.80373492e-01  3.80797574e-01
  3.83728207e-01  3.83748275e-01  3.84464772e-01  3.84702358e-01
  3.84795453e-01  3.85864632e-01  3.86229266e-01  3.87520715e-01
  3.87565711e-01  3.87598263e-01  3.88650915e-01  3.89698317e-01
  3.90505514e-01  3.90925836e-01  3.91513004e-01  3.91557585e-01
  3.91749231e-01  3.92762978e-01  3.95631772e-01  3.96140062e-01
  3.96249028e-01  3.96878255e-01  3.97174113e-01  3.98933920e-01
  3.99165409e-01  3.99201682e-01  3.99718661e-01  4.00099900e-01
  4.00465472e-01  4.06946380e-01  4.07572622e-01  4.08120356e-01
  4.08951884e-01  4.08953360e-01  4.09025190e-01  4.10114440e-01
  4.10480012e-01  4.10992929e-01  4.11352385e-01  4.12835326e-01
  4.15861402e-01  4.16954688e-01  4.17139795e-01  4.18934206e-01
  4.19511207e-01  4.20418020e-01  4.22449255e-01  4.26155014e-01
  4.26809009e-01  4.32875024e-01  4.32895576e-01  4.35749400e-01
  4.35995572e-01  4.36499002e-01  4.38897599e-01  4.42413045e-01
  4.43197084e-01  4.45334549e-01  4.45597448e-01  4.47052273e-01
  4.47742383e-01  4.47989188e-01  4.49425302e-01  4.50806728e-01
  4.53094268e-01  4.53336636e-01  4.54938173e-01  4.55681037e-01
  4.55807485e-01  4.57969950e-01  4.61247615e-01  4.62569586e-01
  4.63908922e-01  4.64054500e-01  4.64075398e-01  4.64796610e-01
  4.67063838e-01  4.69327805e-01  4.69778670e-01  4.73043982e-01
  4.73818090e-01  4.76829073e-01  4.78863073e-01  4.79643425e-01
  4.84141485e-01  4.92525204e-01  5.01335080e-01  5.05846171e-01
  5.08273642e-01  5.15021513e-01  5.17320015e-01  5.18944461e-01
  5.20980944e-01  5.22520612e-01  5.34272739e-01  5.36882508e-01
  5.39669663e-01  5.43568585e-01  5.46218219e-01  5.48189865e-01
  5.50983602e-01  5.56665500e-01  5.63001295e-01  5.64899683e-01
  5.66979994e-01  5.69863391e-01  5.74750341e-01  5.75169268e-01
  5.79547534e-01  5.81946130e-01  5.86234670e-01  5.86743324e-01
  5.88814267e-01  5.89710743e-01  5.93939113e-01  5.96337710e-01
  5.99589476e-01  6.02359365e-01  6.05872374e-01  6.13127886e-01
  6.15017829e-01  6.17398409e-01  6.20323676e-01  6.22822549e-01
  6.23044312e-01  6.25120869e-01  6.26109845e-01  6.27519466e-01
  6.29918062e-01  6.30488839e-01  6.30812036e-01  6.32316659e-01
  6.34302301e-01  6.34715255e-01  6.37113852e-01  6.37436141e-01
  6.39512448e-01  6.39834738e-01  6.43296289e-01  6.45152990e-01
  6.45301032e-01  6.46708238e-01  6.47654286e-01  6.49950183e-01
  6.51710226e-01  6.51731672e-01  6.53904028e-01  6.56302625e-01
  6.57205005e-01  6.58221723e-01  6.58467361e-01  6.58701221e-01
  6.58900359e-01  6.59121375e-01  6.59385514e-01  6.61099818e-01
  6.63018916e-01  6.63820704e-01  6.63918568e-01  6.65417513e-01
  6.65897011e-01  6.68295608e-01  6.69736211e-01  6.71339441e-01
  6.72136060e-01  6.72763352e-01  6.73092801e-01  6.75161949e-01
  6.75709394e-01  6.77889994e-01  6.80407310e-01  6.84996027e-01
  6.85085784e-01  6.87004882e-01  6.87026717e-01  6.87875237e-01
  6.87935542e-01  6.89508841e-01  6.91802075e-01  6.92105660e-01
  6.94680170e-01  6.95605791e-01  6.96599268e-01  6.96665813e-01
  6.97078767e-01  6.97401964e-01  6.98997865e-01  7.00416130e-01
  7.02296113e-01  7.03031008e-01  7.03795058e-01  7.05716412e-01
  7.08592251e-01  7.08742301e-01  7.09078949e-01  7.09522911e-01
  7.09614684e-01  7.09688348e-01  7.10990848e-01  7.13539494e-01
  7.14335492e-01  7.15788041e-01  7.15938090e-01  7.18186638e-01
  7.23133880e-01  7.27579294e-01  7.28232185e-01  7.29263520e-01
  7.30502818e-01  7.31110280e-01  7.33477869e-01  7.33508877e-01
  7.41695361e-01  7.43103263e-01  7.44571200e-01  7.44721250e-01
  7.49141905e-01  7.49368393e-01  7.50299053e-01  7.50854936e-01
  7.52001812e-01  7.56564183e-01  7.57494843e-01  7.59797122e-01
  7.59893439e-01  7.62516424e-01  7.65076568e-01  7.67411518e-01
  7.69487826e-01  7.71080810e-01  7.71886422e-01  7.72914394e-01
  7.74285019e-01  7.79983362e-01  7.81480809e-01  7.83879405e-01
  7.86278002e-01  7.88347150e-01  7.88676598e-01  7.89519947e-01
  7.91075195e-01  7.93473791e-01  7.93650079e-01  7.95872388e-01
  7.97941536e-01  7.98270985e-01  7.98826868e-01  8.03068178e-01
  8.05137326e-01  8.06022657e-01  8.06064992e-01  8.06068045e-01
  8.06172707e-01  8.07646660e-01  8.07865371e-01  8.09615154e-01
  8.10263968e-01  8.22580148e-01  8.28364263e-01  8.28494747e-01
  8.30031457e-01  8.36319081e-01  8.39047127e-01  8.41046551e-01
  8.44505566e-01  8.57733519e-01  8.80243422e-01  8.83680911e-01
  8.83993529e-01  8.86322220e-01  8.90505026e-01  8.97700816e-01
  9.00099412e-01  9.03099279e-01  9.04896605e-01  9.05218645e-01
  9.07295202e-01  9.08241249e-01  9.12092395e-01  9.16560140e-01
  9.16889588e-01  9.17158357e-01  9.18958736e-01  9.21178128e-01
  9.21357333e-01  9.26483975e-01  9.27054751e-01  9.28882571e-01
  9.31281168e-01  9.33679764e-01  9.34034608e-01  9.36078361e-01
  9.36347130e-01  9.38147509e-01  9.40202782e-01  9.40875554e-01
  9.49511947e-01  9.50793138e-01  9.51289266e-01  9.52539088e-01
  9.55830751e-01  9.57665730e-01  9.59734878e-01  9.62786120e-01
  9.64324643e-01  9.68415165e-01  9.69329264e-01  9.74876060e-01
  9.76016400e-01  9.76525054e-01  9.83570794e-01  9.83786264e-01
  9.88518037e-01  9.88538187e-01  9.91281379e-01  9.94064832e-01
  9.96868875e-01  1.00051102e+00  1.00332977e+00  1.00530821e+00
  1.00679476e+00  1.00770681e+00  1.01010541e+00  1.01672313e+00
  1.01782770e+00  1.02151578e+00  1.04414545e+00  1.05763568e+00]

  UserWarning,

2022-10-31 11:01:59,270:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.8121776  -0.51042801 -0.43857643 -0.37875454 -0.34646639 -0.33611543
 -0.25820646 -0.25427287 -0.1244459  -0.11857419 -0.098142   -0.08089684
 -0.07596489 -0.07183464 -0.03123232 -0.03036819 -0.02577378 -0.02057771
  0.01537521  0.02039074  0.02862751  0.03636212  0.03941127  0.0458492
  0.05259651  0.05286681  0.0632208   0.06580929  0.08687643  0.09416882
  0.10168896  0.1023761   0.10671195  0.1123313   0.11388465  0.11705447
  0.11773155  0.11830758  0.1276529   0.13778122  0.14045524  0.14274367
  0.15303885  0.15761774  0.16315835  0.16668586  0.17719605  0.17972168
  0.18259177  0.19469971  0.19521333  0.19793981  0.19847268  0.19847723
  0.19995416  0.20153432  0.20295622  0.20314009  0.20428385  0.20703571
  0.20953979  0.2105567   0.21471205  0.21680139  0.21766874  0.21849781
  0.21914725  0.22291434  0.22580711  0.22645418  0.23062485  0.23117544
  0.23188606  0.2359714   0.23800307  0.23851277  0.24059144  0.24097421
  0.24120797  0.2414421   0.24383681  0.24388561  0.24402964  0.2464873
  0.24823882  0.24905483  0.25129665  0.25153566  0.2521694   0.25296419
  0.25366842  0.25454554  0.2547018   0.25472008  0.25609699  0.2577383
  0.25776343  0.25937772  0.26052535  0.26096089  0.26160631  0.26214852
  0.26412075  0.26412995  0.26442714  0.2646031   0.26495839  0.26548357
  0.26823195  0.26862074  0.26901811  0.26907091  0.26927     0.26976173
  0.27106212  0.27113092  0.2713493   0.27275498  0.27390173  0.27515875
  0.27746312  0.27766168  0.27917161  0.28064345  0.28196273  0.28314144
  0.28419725  0.28446274  0.28474808  0.28506085  0.28540905  0.28548557
  0.28685665  0.28726362  0.28756844  0.28769986  0.28800445  0.28964018
  0.28991158  0.28993661  0.28999328  0.29041162  0.29100725  0.29112907
  0.29127704  0.29299459  0.29325762  0.29423683  0.29440634  0.29583168
  0.29818308  0.29850379  0.29875783  0.29990791  0.3004815   0.30141194
  0.30289606  0.30293872  0.30326306  0.30441956  0.30451323  0.30522213
  0.30744501  0.30762839  0.31001312  0.3116546   0.31303716  0.31495961
  0.31515252  0.31529337  0.31758235  0.31867993  0.31932256  0.31951928
  0.32008229  0.3201346   0.32156176  0.32294174  0.32394105  0.32517317
  0.3255033   0.32573454  0.32681277  0.32697229  0.32720409  0.33045667
  0.33097905  0.33221073  0.33363525  0.33465475  0.33776426  0.33914424
  0.33925025  0.3395397   0.33979445  0.34073986  0.34102976  0.34132211
  0.34164854  0.34210402  0.34219192  0.34357039  0.34413168  0.34540801
  0.34593051  0.34606002  0.3489704   0.3494161   0.35152699  0.35267078
  0.35371418  0.35619936  0.35734232  0.35822239  0.35871597  0.35988074
  0.36090509  0.36168184  0.36197028  0.36234561  0.36295015  0.36336892
  0.36343005  0.36458149  0.36641059  0.36670377  0.3668069   0.36734602
  0.36757265  0.36794999  0.37003593  0.37017176  0.37150485  0.37225982
  0.37280561  0.37304875  0.37306115  0.37449889  0.37462098  0.37840486
  0.37851097  0.37930317  0.37972552  0.38129514  0.38147262  0.38161399
  0.3818138   0.38229247  0.38331284  0.38393062  0.38477085  0.38842199
  0.38936149  0.38943659  0.38963706  0.38984426  0.39020393  0.39083258
  0.39116788  0.3916961   0.39197586  0.39382556  0.3939367   0.39442164
  0.39523851  0.39641522  0.39720208  0.39809454  0.39890685  0.39955985
  0.40058043  0.40069758  0.40221718  0.4035333   0.40573994  0.40591201
  0.4063951   0.40662493  0.40941745  0.40982967  0.41087279  0.41106728
  0.41278767  0.41420309  0.41486206  0.41532155  0.41907373  0.42022549
  0.42040342  0.42422132  0.42465807  0.42518721  0.42541398  0.42554594
  0.42792365  0.4292381   0.43075571  0.43091702  0.43161103  0.43324584
  0.43413353  0.43441523  0.43621888  0.43690445  0.43863715  0.43872346
  0.43949041  0.44802531  0.44827914  0.45079457  0.45340337  0.45373479
  0.45527003  0.45692684  0.45922468  0.46311241  0.46632588  0.46678154
  0.46679472  0.46849084  0.46977376  0.47615804  0.47735177  0.4815557
  0.48156063  0.48695214  0.48725186  0.48871353  0.48886921  0.49107612
  0.49130445  0.49218556  0.49343811  0.50238814  0.50247916  0.50431393
  0.50650931  0.50783775  0.51101607  0.51281625  0.5141289   0.52541149
  0.52770396  0.52883476  0.53113421  0.53414443  0.53884873  0.54300283
  0.55939992  0.56033532  0.56236683  0.56574059  0.56617066  0.56771074
  0.56884747  0.56952561  0.57069665  0.57310502  0.58083907  0.58105063
  0.58166905  0.58736063  0.58743351  0.59311848  0.59586112  0.59778749
  0.59781722  0.59853346  0.60121295  0.60175859  0.60434709  0.60693558
  0.60855709  0.6089949   0.61476358  0.61498897  0.61549196  0.61728957
  0.61886345  0.62246656  0.62351448  0.62524442  0.62578311  0.62879529
  0.62912599  0.63033187  0.63463141  0.6348027   0.63540903  0.63799753
  0.64070807  0.64131441  0.64317452  0.64512638  0.64576302  0.6469689
  0.6535285   0.6539279   0.65544736  0.656117    0.65630636  0.65837651
  0.658768    0.65894746  0.65943387  0.6608345   0.66129399  0.66388248
  0.6639842   0.66647098  0.66671472  0.66719936  0.66905947  0.67423647
  0.67662647  0.67682496  0.67803293  0.67941346  0.68014184  0.68024265
  0.68497546  0.68537516  0.68757834  0.69049582  0.6916863   0.69316874
  0.69513379  0.69664889  0.7008498   0.70103845  0.70270992  0.70402244
  0.70602679  0.7060893   0.7065043   0.70742742  0.70788691  0.70927292
  0.7104754   0.71066477  0.71305478  0.71493191  0.71532341  0.71565239
  0.71565968  0.71584176  0.71838776  0.72295839  0.72414554  0.72533674
  0.7263295   0.72792523  0.72878423  0.72953837  0.73100407  0.73167109
  0.73310223  0.7348027   0.73689954  0.73703624  0.73719655  0.73967723
  0.74086771  0.74120755  0.74168422  0.74327714  0.74345621  0.74571572
  0.7460447   0.75109214  0.75122169  0.75162121  0.75658805  0.75898718
  0.75980369  0.76212193  0.76239218  0.76308254  0.76435971  0.76901218
  0.77192966  0.77384196  0.77451815  0.77532673  0.77710665  0.77872816
  0.77936616  0.77969514  0.78228364  0.7855998   0.78746063  0.78818829
  0.78908214  0.79023849  0.79263762  0.79331515  0.79489713  0.79522612
  0.79940753  0.8029916   0.8055801   0.80567612  0.81852258  0.82573275
  0.82647743  0.82721435  0.82854757  0.83066586  0.83424291  0.83599444
  0.83692743  0.8384053   0.84135955  0.84314885  0.84407854  0.85226904
  0.85818484  0.8596999   0.86225688  0.86260706  0.86909     0.874578
  0.88058402  0.88212787  0.89035285  0.89041536  0.89294134  0.89319322
  0.89811833  0.90607318  0.90753972  0.91648968  0.91696546  0.92632166
  0.92659178  0.92918028  0.93143979  0.93202065  0.93229795  0.93694576
  0.93856727  0.93953426  0.93972362  0.94212275  0.94471125  0.94600378
  0.94633276  0.94741623  0.94916776  0.94988824  0.9526661   0.95765373
  0.95838211  0.9601026   0.96024222  0.96560858  0.96800771  0.97235946
  0.97783057  0.97836169  0.98631654  0.98971361  0.99149353  0.99667052
  0.99925902  1.00083291  1.00138803  1.00683514  1.00702451  1.00763141
  1.0162969   1.01996698  1.02514397  1.02701199  1.03420199  1.06515679
  1.0736049 ]

  UserWarning,

2022-10-31 11:01:59,283:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04576395 -0.84334916 -0.75821979 -0.72363098 -0.65553711 -0.57325674
 -0.51160024 -0.41039642 -0.39170596 -0.3849893  -0.35372235 -0.35334356
 -0.28544321 -0.2838158  -0.26010511 -0.24325735 -0.23801796 -0.22537232
 -0.18853982 -0.15661789 -0.13777648 -0.10153598 -0.08333086 -0.08122746
 -0.04275516 -0.03576546 -0.02895222 -0.02763272 -0.02334923 -0.00769664
 -0.00768045 -0.00313222  0.01634239  0.04581685  0.04626373  0.07018373
  0.07822252  0.07932295  0.08166283  0.09234299  0.099909    0.10104962
  0.10247218  0.1040501   0.11115111  0.11227514  0.11366854  0.1204501
  0.12806844  0.14249199  0.15411982  0.1549199   0.16183051  0.16425373
  0.16586597  0.17284873  0.17313285  0.17379239  0.17470806  0.17512655
  0.17630738  0.17825028  0.18235185  0.18839961  0.19243783  0.19323794
  0.19406019  0.19763888  0.19767733  0.20010856  0.2005508   0.20083531
  0.20155492  0.202045    0.20314781  0.21423743  0.21988116  0.22009586
  0.22056446  0.22127864  0.22366294  0.22373495  0.22422472  0.22528556
  0.22751419  0.2281136   0.23080115  0.23188298  0.23516173  0.23669142
  0.23727767  0.23905959  0.24043564  0.24202472  0.24212963  0.24294582
  0.24355527  0.24391793  0.24541152  0.24587494  0.25445347  0.25688798
  0.2575293   0.25761257  0.25933049  0.25961358  0.26062371  0.2610093
  0.26313188  0.26419606  0.26472293  0.26579137  0.26823625  0.26875927
  0.26900374  0.27043595  0.27048642  0.27174661  0.27182171  0.2729215
  0.27304888  0.27347465  0.27437458  0.27511096  0.27545311  0.27566489
  0.27580783  0.27632442  0.27714521  0.27916446  0.28044675  0.28049711
  0.281416    0.28264487  0.2831702   0.28462596  0.28523469  0.28708988
  0.28896627  0.28985677  0.29021082  0.29045875  0.29052883  0.29148324
  0.29165402  0.29533435  0.29587409  0.29677696  0.29704647  0.29768646
  0.29788322  0.29802647  0.29958323  0.30400942  0.30428549  0.30514162
  0.30740174  0.30862751  0.30959548  0.31146874  0.31167516  0.31210898
  0.31301033  0.31318383  0.31409041  0.31464295  0.31768655  0.3185926
  0.31927975  0.31935993  0.31963308  0.3213337   0.32163668  0.32236317
  0.32262487  0.32411217  0.32471125  0.32625244  0.3267074   0.32738779
  0.32876825  0.32885245  0.32938667  0.33013598  0.331448    0.33407701
  0.33603248  0.33603415  0.33687615  0.33704801  0.33794607  0.33797161
  0.33984041  0.34104186  0.34207887  0.34229228  0.34244317  0.34274906
  0.34349014  0.34435669  0.34603068  0.34668594  0.3467658   0.34680073
  0.34767272  0.34786616  0.34834763  0.34914067  0.34999894  0.35249762
  0.35293112  0.35349361  0.35359353  0.35431256  0.35487116  0.35624203
  0.35802725  0.35867054  0.3596784   0.35972539  0.35977385  0.3598778
  0.36020146  0.36100289  0.36122292  0.36164953  0.36169872  0.36322911
  0.36361873  0.36524541  0.3665012   0.36690826  0.36709665  0.36744629
  0.36928847  0.37046884  0.37066449  0.37100151  0.37145184  0.37209647
  0.37272494  0.37408093  0.37582791  0.37637502  0.37756162  0.37770151
  0.3796467   0.38005063  0.38021473  0.38090915  0.38158494  0.38264878
  0.38394328  0.38465365  0.38518007  0.38755401  0.38882622  0.38916873
  0.39595234  0.39688246  0.39716486  0.39784986  0.39872873  0.39948461
  0.39978284  0.39989713  0.40058687  0.40152548  0.40284823  0.40301531
  0.40419452  0.40533393  0.40543258  0.40654426  0.40672173  0.40771722
  0.40903549  0.41031222  0.41099717  0.41272056  0.41303781  0.41407531
  0.41493671  0.41556724  0.41565408  0.41644011  0.41650389  0.4166582
  0.41704871  0.41870166  0.42155445  0.421602    0.42303419  0.42359091
  0.42380063  0.42391187  0.42442221  0.42504876  0.42727686  0.42844808
  0.42846857  0.42859382  0.43118501  0.43133551  0.43186306  0.43223074
  0.43753167  0.43976291  0.44078693  0.4436121   0.4446136   0.44497116
  0.44703918  0.4474698   0.44770093  0.44784264  0.46148903  0.46464328
  0.46707501  0.47074276  0.47197012  0.47416727  0.47847295  0.48118343
  0.49435459  0.49927452  0.50010721  0.50759843  0.50767711  0.50916238
  0.51089362  0.51124302  0.51617641  0.51703198  0.52257033  0.52429273
  0.52563486  0.52904145  0.53322366  0.53418099  0.53643481  0.53675352
  0.53880292  0.54088509  0.54173161  0.54376846  0.54407104  0.54412128
  0.54485582  0.54538154  0.54746465  0.54816059  0.56291086  0.56348041
  0.56466786  0.57080124  0.57133835  0.57412539  0.57515466  0.57582653
  0.57789631  0.58535039  0.58632788  0.59686768  0.5987208   0.60244581
  0.60350378  0.60443003  0.60548992  0.60718458  0.61180696  0.61367205
  0.61457469  0.61532715  0.61619185  0.61964679  0.62011014  0.62050046
  0.62216806  0.62287786  0.62497373  0.62531571  0.62564559  0.62612261
  0.62841332  0.62916578  0.63368787  0.63394877  0.63471801  0.63521862
  0.63671649  0.63948422  0.64000161  0.64229775  0.64287971  0.64501967
  0.64577213  0.64758016  0.64889863  0.65332285  0.65491389  0.65609057
  0.66018748  0.66222657  0.66372189  0.66439375  0.66514622  0.66587646
  0.66660004  0.66678159  0.6699292   0.67021667  0.67026275  0.67048051
  0.67065559  0.67267085  0.67269693  0.67446291  0.67546465  0.67670652
  0.67679383  0.6779692   0.67803753  0.67823238  0.67905956  0.67975409
  0.68043056  0.68100011  0.68231598  0.68442776  0.6845203   0.68529414
  0.6872658   0.6892772   0.6894181   0.69139914  0.69207101  0.69256029
  0.6955912   0.6969346   0.69760646  0.69835892  0.69970232  0.70037419
  0.70074677  0.70247005  0.70314191  0.70389438  0.70425314  0.70523777
  0.70534009  0.70590964  0.7061971   0.7080055   0.70867736  0.71036726
  0.71178861  0.71219755  0.71249236  0.71421281  0.7144742   0.71450028
  0.71496528  0.71849882  0.71875306  0.7197573   0.71978354  0.72041669
  0.72051274  0.73140558  0.73291503  0.73555637  0.73648125  0.73939552
  0.73940981  0.74074851  0.74675366  0.74771299  0.748152    0.74817799
  0.74882544  0.75048071  0.75371344  0.75407221  0.75447925  0.75878389
  0.75926224  0.76064759  0.76155161  0.76288079  0.76985479  0.77262252
  0.77590764  0.77615949  0.77815797  0.77935432  0.78118298  0.7822054
  0.78526966  0.78646115  0.78719139  0.7919966   0.8030675   0.80510208
  0.80583523  0.80797911  0.81765859  0.82042632  0.82176972  0.82244158
  0.82520931  0.82596177  0.83127896  0.83462372  0.83739144  0.8398004
  0.84181566  0.84678156  0.84810358  0.85797678  0.85953325  0.87181541
  0.87689601  0.88288631  0.88842176  0.89949266  0.90024513  0.90205315
  0.90226039  0.90299064  0.90301285  0.90502811  0.9085483   0.91056357
  0.91571645  0.91576914  0.91886674  0.92681691  0.93270537  0.93322276
  0.9338166   0.93599049  0.9375122   0.93824082  0.94100855  0.94176101
  0.94320672  0.94729646  0.94931173  0.9515099   0.95207945  0.95283192
  0.95484718  0.95559964  0.95958438  0.96038263  0.96315035  0.96718794
  0.96943827  0.96945603  0.97422126  0.97497372  0.97975671  0.98252443
  0.98881235  0.99158008  0.99711553  1.00265098  1.00289098  1.01648961
  1.0210932   1.02479279  1.03143947  1.10844157  1.52238031]

  UserWarning,

2022-10-31 11:01:59,299:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77228358 -0.7487938  -0.50256376 -0.39033036 -0.2778407  -0.2574729
 -0.18776664 -0.16519221 -0.14686488 -0.08370009 -0.04002725 -0.00415394
  0.02138541  0.02365913  0.04334183  0.07131719  0.07554375  0.07687967
  0.10313715  0.10368789  0.11547808  0.11998871  0.12449625  0.12755704
  0.14071677  0.14113545  0.14522287  0.15426807  0.15856108  0.16000496
  0.16049131  0.16141065  0.16241606  0.16474468  0.16491056  0.16603467
  0.17330939  0.17965285  0.18376471  0.18516582  0.18625331  0.18690255
  0.18880126  0.19240744  0.19262447  0.19450087  0.19480334  0.19961942
  0.20016595  0.20175998  0.20273684  0.2071651   0.21592827  0.21710112
  0.21891808  0.2193353   0.21985619  0.22136587  0.22251318  0.22444037
  0.22629036  0.2276559   0.22876743  0.23257912  0.23288302  0.23539049
  0.23557115  0.23865175  0.23885095  0.24071243  0.24082565  0.24335323
  0.24347127  0.24355794  0.24475613  0.24789355  0.24794945  0.24824692
  0.2483612   0.25153525  0.2520258   0.25251184  0.25272532  0.25272827
  0.25463526  0.25504038  0.25592031  0.25620071  0.25726643  0.25966545
  0.26127994  0.26302073  0.26353568  0.2636932   0.26449401  0.26609917
  0.26663649  0.26797539  0.26954919  0.27021924  0.27037966  0.27049822
  0.27055446  0.27249222  0.27298647  0.27564286  0.27733318  0.27767768
  0.27883317  0.27966486  0.28032522  0.28075446  0.28132586  0.28280743
  0.28300734  0.28345951  0.28417459  0.28480351  0.28553871  0.28562874
  0.2866382   0.28793797  0.28807114  0.28928181  0.29194347  0.29441355
  0.29498794  0.29552182  0.29689317  0.29698174  0.29721087  0.29752227
  0.29767967  0.2997456   0.30160267  0.30171318  0.30261137  0.30327958
  0.30469337  0.30489201  0.30609312  0.30690713  0.3088409   0.30889029
  0.30895287  0.30976807  0.31168559  0.31202871  0.31224605  0.31440003
  0.31449985  0.31454123  0.31547866  0.31574061  0.31674312  0.31729724
  0.31746624  0.31841372  0.3204219   0.32149024  0.32254831  0.32414454
  0.32616836  0.32644901  0.32673651  0.32694476  0.32878139  0.3298106
  0.32991399  0.33003228  0.33090949  0.33255524  0.33345646  0.33431832
  0.33520734  0.33555863  0.33570679  0.33574038  0.336984    0.33747364
  0.33773197  0.3384887   0.33905841  0.33987707  0.33998249  0.34020637
  0.34063843  0.34139182  0.3423818   0.34333841  0.34452026  0.34463328
  0.3449466   0.34550193  0.3456298   0.34588499  0.34635217  0.3476643
  0.34906509  0.35121051  0.35125654  0.35126197  0.3522651   0.35248799
  0.35298315  0.3532297   0.3536275   0.35391267  0.35401819  0.35415778
  0.3542441   0.35572806  0.35585849  0.35614636  0.35648579  0.35658042
  0.35750298  0.35852609  0.35917159  0.36004971  0.36068355  0.36148367
  0.36163237  0.36274103  0.36287709  0.36415048  0.36477862  0.36595058
  0.36730576  0.36834582  0.36865637  0.36960187  0.37021391  0.37141674
  0.37196766  0.37277299  0.37411724  0.37427452  0.3746745   0.3779031
  0.3779087   0.37837364  0.38039049  0.38115308  0.38119648  0.38136179
  0.38208895  0.38317967  0.38351794  0.38388404  0.38527512  0.38700472
  0.38719199  0.38746798  0.38859812  0.39119906  0.39227417  0.3938859
  0.39437253  0.39551027  0.39640057  0.39701642  0.39844079  0.39902133
  0.40181756  0.40189952  0.40230419  0.40276427  0.40280068  0.40345767
  0.40355755  0.40663019  0.40710534  0.40759197  0.40764466  0.40951076
  0.4112803   0.41230668  0.41388075  0.4144456   0.41588354  0.41642048
  0.41880436  0.41991394  0.42206715  0.42511054  0.42891452  0.43074906
  0.43110833  0.43438199  0.43462554  0.43577332  0.43794657  0.43853528
  0.44212923  0.44635503  0.44950224  0.4525212   0.45400575  0.45656637
  0.46298873  0.463912    0.46407999  0.465199    0.46570252  0.46757365
  0.46778987  0.46800835  0.46926265  0.46940396  0.47030653  0.47071954
  0.47967311  0.48512067  0.48901919  0.48990838  0.49476564  0.50091578
  0.50363321  0.5077012   0.50853499  0.51009923  0.51526103  0.51808077
  0.52117471  0.52263698  0.52926361  0.53306274  0.53426794  0.53675412
  0.53983563  0.54084722  0.54506716  0.54982149  0.55073659  0.55074031
  0.55078738  0.55325806  0.55326186  0.558       0.55980074  0.56199834
  0.56706083  0.56794292  0.5681989   0.56993001  0.57150478  0.5725739
  0.58041501  0.58050556  0.58184578  0.58336373  0.59075454  0.59325792
  0.59372501  0.59506523  0.60026059  0.600353    0.60694446  0.60958835
  0.61211256  0.61223223  0.61460651  0.61752001  0.61880913  0.62280779
  0.62545168  0.62970475  0.63045091  0.63169682  0.63520761  0.63590232
  0.63602724  0.63942168  0.6411901   0.64125175  0.6466028   0.64761399
  0.64886604  0.64918342  0.64924668  0.65140394  0.65189057  0.6527518
  0.65371483  0.65453446  0.65635515  0.65982224  0.66246613  0.66511002
  0.66929369  0.67067547  0.67129591  0.67208917  0.67304169  0.67353835
  0.67426915  0.67433081  0.67482846  0.67486595  0.67568558  0.67832947
  0.67865792  0.68097336  0.68608324  0.68884176  0.68890502  0.69127922
  0.69441441  0.69481843  0.69488009  0.69538825  0.69647356  0.69671177
  0.69683669  0.69769791  0.69805315  0.69941732  0.69980904  0.70206121
  0.70464344  0.7047051   0.70483594  0.70734899  0.70741225  0.70753916
  0.70903109  0.70993122  0.71217449  0.71263677  0.71534392  0.71592191
  0.71620514  0.71699401  0.71792454  0.72054862  0.72203635  0.72272569
  0.72284919  0.72298704  0.72315066  0.72321232  0.72624793  0.7267807
  0.7285001   0.72985367  0.73064469  0.73114399  0.73153571  0.73221845
  0.73227739  0.73372622  0.73479907  0.73643177  0.73735625  0.73865524
  0.74000014  0.74528792  0.74534437  0.74694567  0.74695757  0.74754616
  0.74861037  0.74999774  0.75300984  0.75321959  0.75850737  0.7605733
  0.76115126  0.76232237  0.76394512  0.76502792  0.76545289  0.76706954
  0.76750917  0.76809678  0.76908292  0.77172681  0.77414542  0.7743707
  0.7748986   0.77776514  0.78759015  0.79023404  0.79287793  0.79752609
  0.8008096   0.80345348  0.80609737  0.81103495  0.81147918  0.81402904
  0.81458268  0.81492468  0.81931682  0.81974326  0.82301823  0.82411542
  0.82447968  0.82751126  0.82757694  0.83518016  0.83782405  0.84038142
  0.84791297  0.85055686  0.85320075  0.86066491  0.86413802  0.86420379
  0.86814899  0.87920598  0.89348377  0.8948855   0.89987566  0.90251955
  0.91058206  0.911991    0.91248355  0.915739    0.92090185  0.92354574
  0.92611037  0.92631455  0.92810132  0.93147741  0.93160233  0.93235288
  0.9345231   0.93689011  0.93940908  0.94469686  0.94482178  0.94914777
  0.95010956  0.95262852  0.9579163   0.96068511  0.963329    0.96849186
  0.97119741  0.97198082  0.97533325  0.97642353  0.97808823  0.98171131
  0.98337601  0.98393022  0.9843552   0.98620372  0.98699909  0.98745246
  0.98774963  0.98964297  0.99211395  0.99275831  0.99444412  0.99493075
  0.99638422  1.00393256  1.00550631  1.00766357  1.01343798  1.01762165
  1.02270727  1.02401354  1.0390815   1.04001109  1.04934832  1.31599104
  1.4412643 ]

  UserWarning,

2022-10-31 11:01:59,299:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60508693 -0.84479933 -0.30974899 -0.23191767 -0.22954605 -0.18414641
 -0.17372977 -0.1594233  -0.05902957 -0.05426967 -0.04767941 -0.03812311
 -0.01198321 -0.00580603  0.00506848  0.01022864  0.0119005   0.02152776
  0.06077911  0.07138445  0.07486234  0.08407977  0.1021523   0.10388578
  0.11272059  0.11870033  0.12322437  0.12338536  0.14286832  0.14331972
  0.14478233  0.14684391  0.15411811  0.16334936  0.16335126  0.17634752
  0.18154092  0.18697855  0.19046337  0.19191789  0.19244467  0.19515
  0.19539995  0.19626254  0.19994919  0.2011712   0.20178419  0.20651002
  0.21003292  0.21016734  0.21095665  0.21240059  0.21416641  0.21456688
  0.21631924  0.21715158  0.2187957   0.22141918  0.22240285  0.22354364
  0.22410837  0.22509116  0.22626449  0.2273236   0.22801716  0.22868629
  0.22969882  0.23105392  0.23471397  0.23651003  0.23836106  0.24004096
  0.24072867  0.24098137  0.24637706  0.24685186  0.2472285   0.24814033
  0.24977021  0.25073328  0.25162175  0.252013    0.25322293  0.25449149
  0.25484169  0.25533009  0.25555024  0.25643243  0.25646815  0.25668463
  0.25848347  0.25879343  0.25882009  0.25889309  0.25997563  0.26268357
  0.26278858  0.26286388  0.26322638  0.26333313  0.26518808  0.26557543
  0.26605198  0.26648903  0.26650239  0.26663048  0.26818148  0.26884271
  0.26972873  0.26978282  0.27039891  0.27053538  0.27226794  0.27344077
  0.27400242  0.27516435  0.27539307  0.27561736  0.27662005  0.27819056
  0.28284964  0.28618616  0.28630062  0.28675516  0.28703452  0.28728947
  0.28758018  0.28835862  0.28921458  0.28963102  0.29065677  0.29080225
  0.2914089   0.29148103  0.29300564  0.29423071  0.29435771  0.29507579
  0.29567364  0.29600443  0.29835522  0.29941549  0.29973124  0.30029163
  0.30144328  0.30211322  0.3021623   0.30229547  0.30307949  0.30440711
  0.30597503  0.3079508   0.30887431  0.31193067  0.31218835  0.31313272
  0.31464783  0.31467222  0.31615055  0.31813219  0.31990763  0.32183073
  0.32247763  0.32284465  0.32316486  0.32395621  0.32463199  0.32463664
  0.32587527  0.32596479  0.32692297  0.3274513   0.32773723  0.32784967
  0.32868571  0.32931706  0.32986574  0.33000771  0.33076699  0.33110834
  0.33204486  0.33227974  0.33239781  0.33247205  0.33441892  0.33454049
  0.33481358  0.33520014  0.33783958  0.33868899  0.33870143  0.34037051
  0.34043295  0.34129425  0.34176111  0.34204223  0.34276011  0.34339421
  0.34372063  0.34454204  0.34622072  0.34699101  0.34747562  0.34829149
  0.34914073  0.34972732  0.35089694  0.35090224  0.35179359  0.3518882
  0.35226344  0.35426153  0.35493237  0.35504417  0.3565644   0.35697945
  0.35700511  0.35867228  0.35882496  0.36017101  0.36063059  0.36075482
  0.36126955  0.36275974  0.36352498  0.36630978  0.3676357   0.36823476
  0.37007405  0.37155024  0.37161903  0.37258447  0.37270003  0.37526396
  0.37610352  0.37663747  0.37756666  0.37859571  0.38098876  0.38105053
  0.38286133  0.3831841   0.38326542  0.38412461  0.3841775   0.38429577
  0.3855181   0.38565272  0.3859339   0.38603085  0.38829031  0.38891715
  0.38903632  0.3893212   0.38941051  0.38978762  0.39021244  0.39281462
  0.39462481  0.39466016  0.39485637  0.39520192  0.39729647  0.39749885
  0.39775314  0.39879486  0.39997652  0.40040324  0.40058213  0.40178594
  0.40227729  0.40306147  0.40320389  0.40326256  0.40475112  0.40478243
  0.40499983  0.40653126  0.40736884  0.40737349  0.40785503  0.41240544
  0.41307098  0.41366496  0.41520764  0.41696789  0.41822693  0.41842607
  0.41848168  0.42036732  0.4217445   0.42240965  0.42306912  0.42319929
  0.42483802  0.42494039  0.42508243  0.42508298  0.42796232  0.42829807
  0.43085685  0.43428454  0.43430798  0.43792004  0.43817081  0.43848772
  0.43919533  0.44181725  0.4424542   0.44304564  0.44569198  0.44670809
  0.44831427  0.4568416   0.45812738  0.46574518  0.46717768  0.47132268
  0.47489451  0.47537106  0.47595149  0.47740819  0.47809843  0.47909553
  0.4816589   0.48206984  0.48337562  0.48594075  0.48729182  0.48763497
  0.49461469  0.4976028   0.50203715  0.50274269  0.50317489  0.5094315
  0.51009365  0.5138764   0.51484181  0.51794792  0.51817967  0.51916769
  0.52101881  0.52924777  0.52962602  0.53169643  0.53774941  0.53789505
  0.54106795  0.54333738  0.54505207  0.54713377  0.55491586  0.55907028
  0.56424076  0.57296915  0.575604    0.57773855  0.57926292  0.58055598
  0.58255962  0.58463653  0.59249248  0.59895805  0.60129468  0.60204168
  0.60576014  0.60909908  0.61056439  0.61159088  0.61397818  0.62034302
  0.62049123  0.62458111  0.62517285  0.6256793   0.62591468  0.62696841
  0.62704838  0.62830198  0.62835563  0.63068928  0.63074293  0.63196711
  0.63353736  0.63481503  0.63572461  0.63964538  0.64307279  0.64352786
  0.64369908  0.64978768  0.65174486  0.65209067  0.65456228  0.66005706
  0.66006017  0.66172418  0.66346264  0.66649878  0.66782995  0.66813183
  0.66829298  0.66839237  0.67014442  0.67084326  0.67209406  0.67366069
  0.67522214  0.67604799  0.67843529  0.68147362  0.68261678  0.68320989
  0.68510347  0.68755436  0.68828746  0.68972294  0.68994166  0.69395958
  0.69465267  0.69942727  0.70230829  0.70420187  0.70708289  0.7087771
  0.71116697  0.71136377  0.71185749  0.71344932  0.71355427  0.71381466
  0.71396717  0.71460155  0.71584294  0.71594157  0.71613837  0.71652834
  0.71827756  0.71852567  0.71964299  0.72079757  0.72193949  0.72330027
  0.72443463  0.72653805  0.72787807  0.72807487  0.73026537  0.73052577
  0.73160693  0.7319052   0.73290312  0.73938188  0.73981457  0.74086185
  0.74215056  0.74399606  0.74505418  0.74744176  0.74759363  0.74936377
  0.75039029  0.75139205  0.751605    0.75301521  0.75546953  0.75731422
  0.75891297  0.75910978  0.75917337  0.76130027  0.76149708  0.76492195
  0.76496984  0.76561021  0.77041935  0.77562407  0.77607108  0.77736252
  0.78039867  0.78240949  0.78278597  0.78517327  0.78706685  0.78756057
  0.78860671  0.78876527  0.79266665  0.79434599  0.79472247  0.7959537
  0.79834988  0.79949707  0.80188437  0.80361297  0.80596589  0.80751112
  0.80988582  0.81267199  0.81816535  0.81859548  0.82055265  0.82098278
  0.82337008  0.83291928  0.8358256   0.85005595  0.85022213  0.85274877
  0.85864505  0.86875021  0.87439227  0.88037157  0.88340819  0.88394147
  0.89284182  0.8950825   0.89783515  0.90124432  0.90469743  0.90954307
  0.91020177  0.91497637  0.92346944  0.92930018  0.92974718  0.93168748
  0.93407478  0.93646208  0.93884938  0.94362398  0.94498804  0.94899032
  0.95214995  0.95513035  0.95556048  0.95794778  0.95931185  0.95990495
  0.96272238  0.97184145  0.97393333  0.97422875  0.97900336  0.98183766
  0.98377796  0.98557215  0.98579991  0.99099877  0.99332716  0.99855331
  0.99969359  1.00048906  1.00151093  1.00287636  1.00584109  1.00765096
  1.00974283  1.02371321  1.02734111  1.02834119  1.02909178  1.03128338
  1.05043562  1.05598871  1.06076331  1.0679845   1.11743892  1.26601968
  1.29653749  2.53563957]

  UserWarning,

2022-10-31 11:02:00,129:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.40549538 -0.40486112 -0.35109427 -0.23301428 -0.21260413 -0.1826596
 -0.13818525 -0.12710741 -0.09976673 -0.03948601 -0.02482887 -0.02018988
 -0.01562095 -0.00525573  0.02259929  0.03982186  0.04035425  0.0439559
  0.05126041  0.05171112  0.0746957   0.08231324  0.08628288  0.0893107
  0.10094365  0.10253246  0.10701834  0.11591937  0.12629156  0.12804584
  0.13259527  0.13354262  0.13412142  0.13605349  0.1475015   0.15158946
  0.15296912  0.15320132  0.15594778  0.16062221  0.16159806  0.16349381
  0.16555719  0.18681709  0.19156253  0.19260703  0.1947289   0.19500973
  0.19937274  0.20225709  0.20305167  0.20661084  0.20682444  0.20807625
  0.20857904  0.21032523  0.21344087  0.21369484  0.2144149   0.21596934
  0.21612537  0.21923755  0.21991459  0.22156907  0.22157172  0.22229169
  0.2224078   0.22304893  0.22441526  0.22877274  0.22897221  0.22897307
  0.23093195  0.2339594   0.23406357  0.23481269  0.23581429  0.2360135
  0.2364467   0.23664996  0.24173953  0.24178999  0.24228054  0.24339402
  0.2457538   0.24620689  0.24830351  0.24977404  0.25147951  0.25201866
  0.25501993  0.25724686  0.25818594  0.25932014  0.26366351  0.26674896
  0.26735689  0.2681555   0.26915166  0.26933057  0.26943039  0.27111536
  0.27135087  0.27219456  0.27332982  0.27356701  0.27371672  0.27514844
  0.27684964  0.2780035   0.27979011  0.28183447  0.28185512  0.28245855
  0.28309673  0.2847501   0.2863581   0.28666735  0.28706529  0.28729274
  0.2876582   0.2880358   0.2897362   0.28977261  0.28998619  0.29039683
  0.29069908  0.29094603  0.2917576   0.29176626  0.29217022  0.29349811
  0.29408378  0.29667645  0.29670615  0.29673978  0.29729256  0.29743053
  0.29770866  0.29824622  0.29839186  0.29916961  0.29919402  0.30007433
  0.30077823  0.3013935   0.30199138  0.30199307  0.30227201  0.30298883
  0.30360159  0.30381029  0.3040175   0.3042178   0.30463924  0.30481505
  0.30542346  0.30639812  0.3072116   0.3084898   0.30962824  0.31002676
  0.31092986  0.31253037  0.31335788  0.31383015  0.31402836  0.31427348
  0.31666439  0.31745101  0.31764655  0.32026634  0.32089651  0.32275618
  0.32408388  0.3248136   0.32496792  0.32541805  0.32572432  0.32606603
  0.32671229  0.32705492  0.32862183  0.32894523  0.32993876  0.33036351
  0.33162501  0.333895    0.3344303   0.33511034  0.33651258  0.33653619
  0.33702656  0.33722282  0.33764608  0.33799702  0.33942295  0.34136651
  0.3419137   0.34236502  0.34290092  0.34554771  0.34697034  0.34794464
  0.34800874  0.34806046  0.34895599  0.34907902  0.3495418   0.34972702
  0.34980816  0.35009415  0.3525333   0.35397111  0.35421744  0.35442319
  0.35497098  0.35524832  0.3556941   0.35628165  0.3570556   0.35739023
  0.35981593  0.36136834  0.36210702  0.36298106  0.36329216  0.3635125
  0.3643286   0.36459767  0.36506025  0.3653516   0.36536292  0.36732938
  0.36741177  0.36760742  0.37127411  0.37138453  0.37152194  0.37318129
  0.37400638  0.37429065  0.37456001  0.37629602  0.37645992  0.37813391
  0.37998074  0.38014434  0.38108887  0.38116182  0.38314981  0.38362656
  0.38494236  0.38616425  0.38679431  0.3875942   0.38945801  0.3901613
  0.39053302  0.39118423  0.39123964  0.39133664  0.39152403  0.39228319
  0.39420338  0.39421832  0.39552849  0.39631502  0.39736855  0.39752966
  0.3975393   0.39885272  0.39919224  0.40139041  0.40290101  0.40341654
  0.4038727   0.4039281   0.40431989  0.40444086  0.40598698  0.40646579
  0.40704558  0.40900349  0.40985254  0.41038841  0.41196689  0.41611249
  0.41881355  0.41948629  0.42178143  0.42219059  0.42247392  0.42404135
  0.42508959  0.42664066  0.43004421  0.43199162  0.43428458  0.43429415
  0.43678702  0.44112303  0.44426196  0.4465125   0.44735689  0.44989459
  0.44990092  0.45187503  0.45243228  0.45252793  0.45496997  0.45750767
  0.45968145  0.46202581  0.46208886  0.4643984   0.46496311  0.46710119
  0.47129032  0.4739087   0.47412221  0.47481174  0.48157236  0.48358719
  0.48441397  0.49087997  0.49233036  0.49301054  0.49849305  0.51069768
  0.51128015  0.51388769  0.51392191  0.51768221  0.51814306  0.52135498
  0.52156768  0.52253148  0.53148306  0.53402075  0.54411611  0.54417152
  0.54736799  0.55006863  0.55525995  0.56038059  0.56245684  0.57043961
  0.57474328  0.577175    0.58205943  0.58506262  0.58663726  0.5872553
  0.5872787   0.59240116  0.59438248  0.59731272  0.60051484  0.60161759
  0.60568191  0.60597735  0.60653373  0.6071714   0.60949328  0.61160911
  0.61209388  0.61524039  0.61777808  0.61867126  0.62031578  0.62285347
  0.62561218  0.62882203  0.63046655  0.63470871  0.63554193  0.64061732
  0.64315501  0.64452733  0.6456927   0.6482304   0.64838881  0.65076809
  0.65274941  0.65330578  0.65792138  0.66056068  0.66160018  0.66599424
  0.66714715  0.66826271  0.66853194  0.67045604  0.67106963  0.67135765
  0.67173995  0.67360732  0.67614501  0.67812634  0.67868271  0.67939538
  0.67994074  0.68104102  0.68135755  0.68465127  0.68477788  0.68573941
  0.68883348  0.69226435  0.69335249  0.69545492  0.69563031  0.69639115
  0.69644656  0.69670581  0.69711688  0.69733973  0.69842788  0.69898425
  0.70096557  0.70152194  0.70350326  0.70405963  0.71167271  0.71270851
  0.71554155  0.71619173  0.71770644  0.71957381  0.71990367  0.72756919
  0.72887699  0.72895126  0.73199304  0.73258917  0.73268144  0.7351359
  0.73885194  0.73915448  0.73922915  0.7418558   0.74202275  0.74279535
  0.74693118  0.74728015  0.74787073  0.74980329  0.75483342  0.75548381
  0.7576392   0.7580215   0.76036206  0.76435492  0.76723272  0.76817227
  0.76960679  0.77070996  0.77121057  0.77214448  0.77313981  0.77324766
  0.77578535  0.77832304  0.78086074  0.78331219  0.78389903  0.78593612
  0.78622414  0.78871546  0.78897442  0.7935492   0.79603149  0.79862458
  0.80035671  0.80116228  0.8014503   0.80398799  0.80618226  0.80726751
  0.80790479  0.81095487  0.81385074  0.81413876  0.8147506   0.81567718
  0.81676953  0.81887072  0.81996193  0.82064866  0.82175184  0.82503731
  0.83053973  0.83171084  0.83190261  0.8337941   0.83663457  0.8376125
  0.83922767  0.83951568  0.84712876  0.8477474   0.85163033  0.85282279
  0.85302669  0.85727953  0.86222577  0.86297356  0.86545584  0.86701314
  0.86892393  0.87185949  0.87414411  0.88315136  0.8832751   0.89439025
  0.89591111  0.89850125  0.90357664  0.90567514  0.90611433  0.90640235
  0.91118972  0.91147774  0.91372741  0.9162651   0.91880279  0.92134049
  0.92162851  0.9241662   0.92636047  0.92641587  0.93149126  0.93177928
  0.93402895  0.93642195  0.93910434  0.93939235  0.93989296  0.94417972
  0.94671741  0.94700543  0.94788042  0.94925511  0.9517928   0.95812354
  0.96194357  0.96223159  0.96448126  0.96476928  0.96687427  0.96955665
  0.96994538  0.97492005  0.97681155  0.97745774  0.97999544  0.98253313
  0.98303373  0.98507082  0.98760851  0.99212665  0.9926839   0.99522159
  0.99825989  1.00029698  1.00537236  1.01132274  1.01525391  1.02098421
  1.02300953  1.02794237  1.28351232]

  UserWarning,

2022-10-31 11:02:00,160:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94588565 -0.77727197 -0.47565439 -0.39285328 -0.33391399 -0.31797557
 -0.31537212 -0.30828983 -0.26836208 -0.25985396 -0.25899128 -0.25549365
 -0.1240723  -0.11841169 -0.10358507 -0.09830363 -0.05744355 -0.04693365
 -0.0292156  -0.02892426 -0.00201809  0.00648351  0.02309888  0.02581329
  0.03282859  0.03620779  0.05466422  0.06077021  0.06656182  0.06702878
  0.08121338  0.08493697  0.08583782  0.09244132  0.09706522  0.10289261
  0.11108173  0.12658984  0.1268368   0.12754332  0.13566329  0.14906557
  0.15196698  0.15895679  0.17079591  0.17242304  0.17452383  0.17649313
  0.17714351  0.17911148  0.18484923  0.19018789  0.19539106  0.20242528
  0.20558385  0.2056833   0.20621969  0.20918107  0.20939491  0.21018678
  0.21162605  0.21171809  0.21185075  0.21239972  0.21555219  0.21619719
  0.21646353  0.21711645  0.21751815  0.2186626   0.22033353  0.22437602
  0.22491418  0.23361704  0.23549686  0.23599469  0.23696735  0.23897691
  0.23942881  0.24154655  0.24163589  0.24164409  0.2446316   0.24656641
  0.24777079  0.24845523  0.24868366  0.24961201  0.25010709  0.25015193
  0.25145716  0.25468348  0.257334    0.2575669   0.25778608  0.26289283
  0.26297567  0.26303567  0.26415559  0.26435008  0.26510261  0.26567617
  0.26639543  0.26854239  0.26910837  0.27016245  0.27068233  0.2707573
  0.27181289  0.27218138  0.27251714  0.27330069  0.27361604  0.27397517
  0.27452643  0.275931    0.27670286  0.27671454  0.27679948  0.27860116
  0.27889296  0.28140762  0.28216092  0.28275527  0.28360062  0.28362816
  0.28379269  0.28408496  0.28460086  0.28616624  0.28628987  0.28731391
  0.2878076   0.29015921  0.29024346  0.29033971  0.29117493  0.2913321
  0.29161228  0.29205317  0.29521043  0.29690321  0.29732838  0.29759487
  0.29761159  0.29771755  0.29810946  0.30183584  0.30341189  0.30400443
  0.30402029  0.30642901  0.30767269  0.3112514   0.31192871  0.31205703
  0.31255091  0.31305583  0.31318724  0.31354832  0.31403606  0.31494962
  0.3174336   0.31808922  0.31815498  0.3182134   0.3192459   0.32005625
  0.32124619  0.32159704  0.32657281  0.32746898  0.32762349  0.3285217
  0.3287782   0.33129934  0.33336467  0.33337749  0.3341557   0.33536867
  0.33578004  0.33679284  0.33774462  0.33957982  0.34033558  0.34053864
  0.34149333  0.34243696  0.34256778  0.34525111  0.34625361  0.34635853
  0.34683705  0.34711638  0.34786127  0.34860901  0.34903809  0.34907297
  0.35094995  0.35134711  0.35268745  0.35309941  0.35488541  0.35929282
  0.35990749  0.36370362  0.36409746  0.36667944  0.36754425  0.37233168
  0.37269469  0.37270394  0.37450325  0.37607724  0.37674068  0.37709392
  0.3776036   0.37798113  0.37870633  0.37907762  0.3797822   0.37989007
  0.37992384  0.38007515  0.38039288  0.38144769  0.38161024  0.38209292
  0.38314179  0.38352182  0.3847417   0.38498081  0.38540777  0.38565985
  0.38657452  0.38703333  0.38735934  0.38763063  0.38833904  0.38902429
  0.39025303  0.39065622  0.39085856  0.39281366  0.39345572  0.39353629
  0.39419662  0.39432139  0.39484539  0.39536218  0.39617224  0.39764119
  0.40201495  0.40317422  0.40393078  0.40442424  0.40476198  0.40651889
  0.40652941  0.4072342   0.4083153   0.40908769  0.40912803  0.40927083
  0.41091954  0.41131232  0.41156656  0.41224125  0.4124991   0.41364638
  0.41420307  0.41559055  0.41644041  0.41677403  0.41710223  0.41913122
  0.42133585  0.4221319   0.42251581  0.42524987  0.42536601  0.42562301
  0.42904496  0.43130934  0.43385538  0.43536875  0.43642546  0.43902408
  0.44301149  0.44422133  0.44600723  0.4460192   0.44654676  0.44929776
  0.44956748  0.45351254  0.45456571  0.45536872  0.45563243  0.45572388
  0.4569548   0.45713388  0.45721445  0.45814755  0.45849114  0.4620268
  0.46431281  0.46698757  0.48250318  0.48345794  0.48639841  0.48848555
  0.48898747  0.49507422  0.49617324  0.49767172  0.49858343  0.50069355
  0.50106681  0.50310665  0.50340089  0.5045241   0.50589079  0.50917469
  0.51000635  0.51237509  0.51363934  0.51972864  0.52143823  0.53180516
  0.53256183  0.53340322  0.53707428  0.54155778  0.54947334  0.55077074
  0.5523727   0.55518398  0.55526465  0.55531159  0.55534038  0.56256838
  0.5684567   0.56923109  0.57509475  0.57514129  0.5753981   0.57675698
  0.58195423  0.58455285  0.59070459  0.59159806  0.59194109  0.59295657
  0.59427161  0.59494735  0.59550787  0.60699474  0.60723411  0.60794047
  0.61219199  0.61833497  0.62872946  0.63203187  0.63298098  0.63392671
  0.63652533  0.63912396  0.63972233  0.6478547   0.65101339  0.65156452
  0.65211708  0.65278595  0.6534025   0.65463513  0.6547157   0.65497019
  0.65731432  0.65750506  0.65991295  0.66122414  0.66352701  0.66370895
  0.66382276  0.66480626  0.6651102   0.66612564  0.66761805  0.66770882
  0.66872426  0.67024318  0.67030744  0.67223575  0.67290607  0.67322818
  0.67550469  0.67652013  0.67711851  0.67882073  0.67911876  0.6794145
  0.68070194  0.68431601  0.68461175  0.68521013  0.68761842  0.68951325
  0.69339113  0.6947105   0.69500625  0.69780873  0.69941952  0.69990775
  0.70020349  0.70068571  0.70267884  0.70280212  0.70408956  0.70428634
  0.705105    0.70521075  0.70660761  0.70668818  0.70770362  0.71030224
  0.71440349  0.71448405  0.71540008  0.71549949  0.71579524  0.7208114
  0.72238187  0.72410752  0.73368986  0.73725248  0.73848534  0.74108397
  0.74260208  0.74400379  0.74612123  0.74628122  0.74887984  0.75016689
  0.75147846  0.75188023  0.75397167  0.75399652  0.75989128  0.75997185
  0.76444744  0.76447158  0.7647937   0.76707021  0.76993881  0.77065176
  0.77226746  0.77348137  0.77463626  0.77931266  0.77931432  0.78038544
  0.78057084  0.78315659  0.78552971  0.78608764  0.7878592   0.79045782
  0.79106648  0.79635886  0.7982537   0.8007788   0.80345094  0.80604957
  0.80864819  0.81124682  0.81249979  0.81318357  0.81483674  0.81644406
  0.82423994  0.82646936  0.8285098   0.83439205  0.83949428  0.8395893
  0.83983168  0.84036033  0.84165927  0.84998379  0.85306921  0.85871974
  0.86366738  0.86539124  0.86557554  0.87175572  0.88143726  0.89514333
  0.90143986  0.90350984  0.90447433  0.9045549   0.90586608  0.90715352
  0.91494939  0.91509219  0.91596483  0.92006607  0.92014664  0.92097856
  0.92274526  0.92500777  0.93305919  0.93445094  0.93855402  0.94093563
  0.94345369  0.94484544  0.94572524  0.94613288  0.94627568  0.9473635
  0.94744406  0.9487315   0.95004269  0.95191966  0.95264131  0.95419873
  0.95783856  0.95971553  0.96043718  0.96172462  0.96303581  0.96555387
  0.96952049  0.97211912  0.97270865  0.97274234  0.97471774  0.97673272
  0.97862755  0.98122618  0.9838248   0.98642342  0.98821393  0.98902205
  0.99162067  0.99681792  0.99869489  1.00322204  1.00461379  1.00569072
  1.00721242  1.01163284  1.01588809  1.01623171  1.02599232  1.02987838
  1.03539775  1.07276755  1.07362844  1.08139384  1.22725658  1.25929725
  1.27239887  1.3342835   1.39575305]

  UserWarning,

2022-10-31 11:02:00,160:INFO:Calculating mean and std
2022-10-31 11:02:00,160:INFO:Creating metrics dataframe
2022-10-31 11:02:00,176:INFO:Uploading results into container
2022-10-31 11:02:00,176:INFO:Uploading model into container now
2022-10-31 11:02:00,176:INFO:master_model_container: 5
2022-10-31 11:02:00,176:INFO:display_container: 2
2022-10-31 11:02:00,176:INFO:Ridge(random_state=3360)
2022-10-31 11:02:00,176:INFO:create_model() successfully completed......................................
2022-10-31 11:02:00,282:WARNING:create_model() for Ridge(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:00,282:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:00,282:INFO:Initializing create_model()
2022-10-31 11:02:00,282:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:00,282:INFO:Checking exceptions
2022-10-31 11:02:00,282:INFO:Importing libraries
2022-10-31 11:02:00,282:INFO:Copying training dataset
2022-10-31 11:02:00,282:INFO:Defining folds
2022-10-31 11:02:00,282:INFO:Declaring metric variables
2022-10-31 11:02:00,282:INFO:Importing untrained model
2022-10-31 11:02:00,282:INFO:Ridge Regression Imported successfully
2022-10-31 11:02:00,282:INFO:Starting cross validation
2022-10-31 11:02:00,298:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:02,406:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390238 -0.94068138 -0.59039351 -0.54800603 -0.2923101  -0.20442241
 -0.18673878 -0.13766655 -0.12368684 -0.1229897  -0.11954131 -0.082832
 -0.07060698 -0.05659686 -0.03475587 -0.02264261 -0.00978007 -0.00835976
 -0.00379795 -0.00264154  0.01643107  0.02032372  0.02550704  0.0289719
  0.0297027   0.0398841   0.04684666  0.0485153   0.04878315  0.05033827
  0.05077262  0.07210069  0.07884441  0.07889112  0.08002797  0.08060359
  0.08312576  0.08389765  0.09060725  0.09100597  0.09817914  0.10122391
  0.10141383  0.10146693  0.10989965  0.11276695  0.1462199   0.15577902
  0.15680997  0.15992609  0.16147785  0.16281593  0.16699417  0.17533857
  0.18208666  0.1865269   0.18830391  0.19171318  0.19201892  0.19252136
  0.19264413  0.19531549  0.20279589  0.20440353  0.20745636  0.20757594
  0.21381247  0.21657506  0.22166777  0.22462254  0.22555997  0.22659418
  0.22762555  0.22974752  0.23263708  0.23362877  0.23608238  0.23649436
  0.23680519  0.23680935  0.24010608  0.24227293  0.24398271  0.24763293
  0.24936121  0.24946242  0.24957677  0.25001755  0.25182062  0.25188801
  0.25218524  0.25356623  0.25432629  0.25447183  0.25652695  0.25784277
  0.25824781  0.2584321   0.25943078  0.26228661  0.26230867  0.26370758
  0.264161    0.26535651  0.26649497  0.26686591  0.26814665  0.26872205
  0.27097539  0.27522453  0.27581312  0.27598637  0.27614541  0.27714866
  0.27717887  0.27726808  0.27790961  0.27810845  0.27849585  0.27861386
  0.27919126  0.27941918  0.28135384  0.28163707  0.28194836  0.28262725
  0.28270126  0.28323316  0.28350502  0.28369268  0.28393871  0.28461925
  0.28726054  0.28774501  0.28857861  0.28906715  0.28922072  0.28923696
  0.28926936  0.28973227  0.29116352  0.29143527  0.2917996   0.29237111
  0.29273177  0.29293694  0.29319411  0.29667924  0.29717859  0.29742343
  0.29773972  0.29794897  0.29820386  0.30076293  0.30328755  0.30371555
  0.30582644  0.30667923  0.30792731  0.3079519   0.31027189  0.31229562
  0.31234644  0.313971    0.31485181  0.31487146  0.31563973  0.31568065
  0.31652425  0.31685144  0.31738182  0.31811635  0.31937362  0.31996687
  0.32086371  0.32096172  0.32332013  0.3241807   0.32549635  0.32641425
  0.32696395  0.32944396  0.32953163  0.33046865  0.33073403  0.33402479
  0.3350993   0.33849932  0.33971889  0.34183451  0.34202581  0.34235356
  0.34367108  0.34371638  0.3441251   0.34747837  0.34761572  0.3485237
  0.3501147   0.35071691  0.3509965   0.35176133  0.35415312  0.3547383
  0.35645317  0.35725069  0.35739681  0.359342    0.35958306  0.36124617
  0.3617      0.36420165  0.36738471  0.36972985  0.37055815  0.37084793
  0.37187639  0.37225961  0.37274614  0.37299814  0.37317093  0.37408596
  0.37526777  0.3753091   0.37602986  0.37664353  0.37744813  0.37779649
  0.37918917  0.38097438  0.38293689  0.38379856  0.38434054  0.38468515
  0.38578378  0.38640923  0.38652625  0.38668772  0.38685956  0.38980847
  0.38981654  0.39076944  0.39098512  0.39261538  0.39285399  0.39294508
  0.39380817  0.39431187  0.39465883  0.39491855  0.39600661  0.39794339
  0.39877823  0.40009136  0.4032336   0.4039943   0.40421763  0.40445651
  0.40461833  0.40661201  0.40857064  0.40882125  0.40889026  0.41022065
  0.41039489  0.41071209  0.41153264  0.41222968  0.41236817  0.41517973
  0.41849879  0.41949474  0.41966936  0.42084214  0.42110268  0.42218505
  0.42249706  0.42286666  0.42349751  0.42369172  0.4262253   0.42801232
  0.43088259  0.4323794   0.43274693  0.43431047  0.43528282  0.43781189
  0.44193535  0.4423481   0.44413204  0.44415387  0.445687    0.44805845
  0.44806444  0.44874592  0.44899982  0.45472038  0.45523191  0.45689234
  0.45696922  0.45864397  0.45882985  0.46031974  0.4607299   0.46273945
  0.46373995  0.46403832  0.46698028  0.4681826   0.46841684  0.46924249
  0.47095146  0.47319105  0.47420433  0.47562929  0.47794027  0.48323429
  0.48650383  0.48737441  0.48831112  0.4892338   0.49183463  0.49327754
  0.49415473  0.49545201  0.49768     0.50073218  0.50178183  0.50576093
  0.51006567  0.51360878  0.52296292  0.53068924  0.53318208  0.53607595
  0.53658246  0.53661545  0.5387873   0.54813906  0.55117226  0.55530036
  0.55561756  0.55656971  0.55707403  0.56124489  0.56704535  0.56982037
  0.57138339  0.57485815  0.57529249  0.57791909  0.58031617  0.58449517
  0.586988    0.59182648  0.59473633  0.59695932  0.59945216  0.60065692
  0.60411648  0.60412062  0.60443782  0.60693065  0.60746094  0.60910214
  0.61191631  0.61588471  0.61730233  0.61765986  0.6193948   0.61949977
  0.62164916  0.62188763  0.62258484  0.62567029  0.62989642  0.63006503
  0.63185896  0.63758935  0.63813496  0.63901611  0.63933745  0.64150894
  0.64183028  0.64430364  0.64432311  0.64930878  0.65094917  0.65180161
  0.65448806  0.65457662  0.65678727  0.65699622  0.65807761  0.6592801
  0.66057044  0.66058939  0.66177293  0.6622595   0.66426576  0.66454795
  0.66479605  0.66643725  0.66675859  0.66833865  0.66925142  0.67356437
  0.67423709  0.67489584  0.67766632  0.67922275  0.68072655  0.68139424
  0.68203685  0.68231511  0.68359297  0.68919407  0.68970489  0.6916869
  0.69385839  0.6949609   0.69530223  0.69606454  0.69884406  0.70133689
  0.70165823  0.70296752  0.70447232  0.70632255  0.70664389  0.70696516
  0.70744983  0.70745262  0.7082785   0.70881538  0.70913672  0.71044601
  0.71130821  0.71256595  0.71425916  0.7179245   0.72219854  0.72567271
  0.72705593  0.72940063  0.73093366  0.73323648  0.73612871  0.73749426
  0.73965022  0.74035998  0.74186479  0.74285713  0.7439972   0.74534565
  0.74613429  0.74685045  0.74934328  0.75001411  0.75312646  0.75531697
  0.7570686   0.75870234  0.75931461  0.75939827  0.7598554   0.76275794
  0.76430027  0.7667931   0.76707136  0.76838065  0.76869449  0.76928593
  0.7697706   0.77038339  0.77082193  0.77177876  0.77475627  0.77672135
  0.77796918  0.77985038  0.78424292  0.78673575  0.79013325  0.79102254
  0.79670707  0.7991999   0.80050919  0.802069    0.80635706  0.8066784
  0.80917123  0.80966698  0.81417472  0.81994849  0.82711452  0.82718914
  0.82953727  0.83210018  0.83790166  0.84498798  0.84656369  0.84667014
  0.84849443  0.85154935  0.85209478  0.85702849  0.85734096  0.85952133
  0.86699982  0.87997734  0.89224059  0.89442096  0.89455492  0.89948124
  0.9004586   0.90439228  0.90570158  0.90635198  0.90787721  0.90937795
  0.91458364  0.9156729   0.9218421   0.92682776  0.92785374  0.93181343
  0.93234372  0.93679909  0.93810838  0.94060121  0.94178475  0.94309404
  0.94427758  0.9455674   0.94558687  0.94677041  0.94684461  0.95584038
  0.95805103  0.96054386  0.9617274   0.9618342   0.96279884  0.96422023
  0.96478832  0.96552952  0.96728115  0.96808492  0.96920589  0.97051518
  0.9714154   0.97387021  0.9750004   0.97550084  0.97799367  0.98174425
  0.98297934  0.9847083   0.987965    0.99045783  1.01040048  1.01289331
  1.01538614  1.01619498  1.01909008  1.0203718   1.04152556  1.06854153
  1.08598462  1.09493281  1.18725434  1.64839536]

  UserWarning,

2022-10-31 11:02:02,421:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86821138 -0.41627215 -0.34521038 -0.30481588 -0.19782507 -0.13964246
 -0.1281651  -0.06704292 -0.03453797 -0.0339275  -0.02631181 -0.01070708
  0.00356016  0.01370993  0.02414879  0.0382608   0.03973397  0.04143897
  0.04415541  0.05337656  0.05403384  0.05446404  0.06648931  0.08377465
  0.0860325   0.09848429  0.10076863  0.10336003  0.10705908  0.10864966
  0.10882053  0.12199957  0.12859439  0.13028449  0.1324365   0.13552094
  0.13741546  0.14078979  0.14252533  0.14979687  0.15153475  0.15243743
  0.15849283  0.16198053  0.16202134  0.16607999  0.16684354  0.16920514
  0.1715289   0.17197011  0.17532646  0.18366279  0.1849292   0.18605475
  0.18627298  0.18755957  0.19269061  0.19326164  0.19358822  0.19396053
  0.19701206  0.19873624  0.20089365  0.2014707   0.20304726  0.20799749
  0.21074237  0.21249663  0.21582978  0.21711386  0.21947729  0.22068715
  0.22083743  0.22186356  0.22467495  0.22740963  0.2318154   0.23632626
  0.23679977  0.23681653  0.23724062  0.24577864  0.24649391  0.24717756
  0.24732385  0.24747054  0.24885615  0.25083531  0.25217726  0.25271118
  0.25361266  0.25569707  0.25645269  0.25654834  0.25723125  0.25724374
  0.25739408  0.26029156  0.26141662  0.26197689  0.26336421  0.26380849
  0.26530415  0.2666759   0.26745797  0.26854789  0.27112649  0.27184765
  0.27230229  0.27341336  0.27399045  0.27459095  0.2746045   0.2750008
  0.27596792  0.27646421  0.27748549  0.27755789  0.27819372  0.27905374
  0.28132266  0.28217493  0.28384679  0.28417371  0.28419253  0.28455635
  0.28596017  0.28626828  0.28641089  0.28727788  0.28772456  0.28867535
  0.28947631  0.29031261  0.29113033  0.29123184  0.29387024  0.29574084
  0.29600249  0.29600498  0.29650499  0.29661227  0.29812019  0.29856178
  0.29895597  0.2990051   0.29906444  0.2994074   0.30022008  0.30128197
  0.30130718  0.30158787  0.30229833  0.3029501   0.30470122  0.30524084
  0.30538603  0.30715464  0.30958633  0.3098442   0.30986773  0.31000137
  0.31044458  0.3122555   0.31252366  0.31296428  0.31455832  0.31545083
  0.31795735  0.31979716  0.32090439  0.32416614  0.32489346  0.32612783
  0.32668508  0.32679876  0.3275928   0.32875768  0.32917443  0.33018053
  0.33166212  0.33193306  0.33224848  0.3336653   0.33377245  0.33423312
  0.33435125  0.33638113  0.33667021  0.33697776  0.3374412   0.33787422
  0.33998777  0.34391286  0.34411963  0.34543857  0.34618201  0.34680002
  0.34741487  0.34809419  0.34839042  0.35063692  0.35266755  0.35277781
  0.35321635  0.35344775  0.35444916  0.35468736  0.35509563  0.35533942
  0.35572312  0.35694262  0.35761805  0.35884104  0.35932223  0.35982161
  0.36112101  0.36159656  0.36532921  0.36747725  0.36800901  0.36890293
  0.36897545  0.3690361   0.36985858  0.37158115  0.37179997  0.3721529
  0.37451858  0.375747    0.3759972   0.37672556  0.37714185  0.37745973
  0.37806958  0.37870943  0.37899194  0.38197021  0.38226954  0.3831097
  0.38386152  0.38425172  0.38511204  0.38569773  0.38605738  0.38646684
  0.38655209  0.38707277  0.38717177  0.387836    0.38858677  0.38882192
  0.38976855  0.39156562  0.3917005   0.39257416  0.39261172  0.39275338
  0.39300626  0.39353892  0.39405559  0.39410252  0.39479472  0.39543248
  0.39560659  0.39673122  0.39741888  0.39843915  0.39859087  0.39924543
  0.39989539  0.40105478  0.4029168   0.40384563  0.405067    0.40734106
  0.40785401  0.40846004  0.40873971  0.40879756  0.40887283  0.41273014
  0.41471781  0.41526513  0.41546092  0.41593278  0.41921446  0.41993248
  0.42157375  0.42199789  0.42232859  0.42233     0.42313974  0.42430646
  0.4243376   0.42452501  0.42557826  0.42787873  0.43033607  0.43228667
  0.43321595  0.43420015  0.43655775  0.4385803   0.44044565  0.44376745
  0.45065052  0.45257668  0.45507439  0.45571728  0.45617002  0.45708732
  0.45750051  0.45810202  0.45917392  0.45993357  0.46242753  0.46256278
  0.46291935  0.46352131  0.46441319  0.46449616  0.46517004  0.46590216
  0.46733772  0.46770572  0.46831257  0.46857797  0.46942458  0.47318592
  0.47340512  0.480319    0.48241784  0.48735243  0.49035016  0.49380098
  0.49487597  0.49967813  0.50362289  0.50378825  0.50541271  0.50559135
  0.50601319  0.50674206  0.50812916  0.5089786   0.50928168  0.51203831
  0.51230714  0.52911327  0.5293766   0.53083045  0.53130848  0.53924615
  0.54770049  0.54967982  0.54988034  0.54989401  0.55122978  0.55178767
  0.55375823  0.55454207  0.55790214  0.5605456   0.5609777   0.56141414
  0.56261324  0.56286128  0.56348123  0.56422932  0.56528826  0.56603693
  0.56689515  0.56760555  0.57557322  0.57620649  0.58030712  0.58430337
  0.58455884  0.58607904  0.58646267  0.5871394   0.58941093  0.59086212
  0.59516915  0.59723153  0.59788559  0.60060203  0.60859863  0.60875137
  0.60917329  0.6122039   0.6169007   0.61961714  0.62226893  0.62473765
  0.62631602  0.63174891  0.6325453   0.63458493  0.63555795  0.6359158
  0.63651715  0.6380751   0.63863225  0.64069463  0.64134869  0.64170768
  0.64350799  0.645191    0.64622444  0.64949802  0.65195829  0.65221447
  0.65493091  0.65888463  0.6603638   0.66223129  0.6625231   0.66308024
  0.66509628  0.66579669  0.66778595  0.67063096  0.67067243  0.67122958
  0.67338887  0.67394602  0.67451474  0.67596872  0.67774106  0.67937891
  0.68042538  0.6811061   0.68213955  0.68370014  0.68425465  0.68471274
  0.68697109  0.68727206  0.68882913  0.68968754  0.69141814  0.69240398
  0.69512043  0.69524547  0.69633484  0.69769361  0.69783687  0.70055331
  0.70317284  0.70338169  0.70359515  0.70581616  0.70584294  0.7059862
  0.70654335  0.70855938  0.70870265  0.71141909  0.7123285   0.71399227
  0.71426057  0.71469268  0.71670118  0.71670872  0.71685198  0.71696104
  0.7221416   0.72336284  0.73244581  0.73358195  0.73562886  0.74099026
  0.74453043  0.74514678  0.74662851  0.74684192  0.74930604  0.7494493
  0.74955836  0.75133624  0.75168596  0.75170072  0.752058    0.75321745
  0.75429084  0.75473893  0.75716653  0.76009233  0.7607693   0.76515855
  0.76585703  0.76842715  0.77400636  0.77486695  0.78215569  0.78238388
  0.78294051  0.78487213  0.79030502  0.79302146  0.79573791  0.79845435
  0.80073869  0.8011708   0.80388724  0.80569415  0.80932013  0.81091746
  0.8201859   0.82254156  0.82777809  0.82833524  0.83207417  0.83550078
  0.83605246  0.83742463  0.83753075  0.83920101  0.84121705  0.84191745
  0.8446339   0.84644081  0.85372329  0.85396073  0.85778402  0.86050046
  0.86140807  0.86325805  0.86364901  0.86392274  0.87109793  0.87359528
  0.88632062  0.88787597  0.88845624  0.88982966  0.89217896  0.90533572
  0.90550217  0.90779593  0.90896158  0.91819076  0.91821754  0.92163439
  0.93179976  0.9345162   0.9347845   0.93521661  0.94155891  0.94266553
  0.94336594  0.94608238  0.95125909  0.95151527  0.96238105  0.96329526
  0.96437031  0.96795099  0.97415623  0.97526286  0.97652885  0.98358143
  0.98612864  0.99156152  0.99427797  0.99971086  1.0024273   1.00514374
  1.00843476  1.01057663  1.01572614  1.0210103   1.02144241  1.02670884
  1.02872488  1.03108617  1.05008332  1.07207105  1.07750394  1.08931065
  1.42131732  1.66807926]

  UserWarning,

2022-10-31 11:02:02,421:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14440674e+00 -5.70398127e-01 -4.96868754e-01 -4.67186848e-01
 -3.51418341e-01 -1.10199882e-01 -1.09194418e-01 -9.71994367e-02
 -9.37784574e-02 -8.96426914e-02 -3.74703666e-02 -2.77463776e-02
 -1.65385417e-02  4.16379104e-04  3.95158615e-02  4.75904049e-02
  5.95440796e-02  6.03586598e-02  6.92239115e-02  7.63013497e-02
  9.22543302e-02  9.49175593e-02  1.01658979e-01  1.14106431e-01
  1.18460576e-01  1.29956757e-01  1.30384367e-01  1.37624291e-01
  1.40113571e-01  1.46908348e-01  1.50673654e-01  1.68784034e-01
  1.72833293e-01  1.75701547e-01  1.80723116e-01  1.86069152e-01
  1.89510201e-01  1.94340280e-01  1.94718865e-01  1.94770969e-01
  1.98018796e-01  1.98617572e-01  2.00882539e-01  2.05370441e-01
  2.05912942e-01  2.07240504e-01  2.09598904e-01  2.11753472e-01
  2.14402255e-01  2.16761577e-01  2.18779903e-01  2.19524130e-01
  2.19671061e-01  2.23274527e-01  2.25128643e-01  2.25918644e-01
  2.26830311e-01  2.27601481e-01  2.28580324e-01  2.30138633e-01
  2.30227445e-01  2.31139280e-01  2.33878505e-01  2.34534380e-01
  2.35203066e-01  2.35628663e-01  2.36309875e-01  2.37370949e-01
  2.38871504e-01  2.41887123e-01  2.41947574e-01  2.42348063e-01
  2.42594571e-01  2.44759945e-01  2.46573170e-01  2.47760526e-01
  2.48562046e-01  2.48601472e-01  2.49133680e-01  2.50091709e-01
  2.50619840e-01  2.50822148e-01  2.50875160e-01  2.51340152e-01
  2.51939287e-01  2.52611875e-01  2.52633668e-01  2.53316397e-01
  2.54545149e-01  2.54796552e-01  2.56333176e-01  2.57083610e-01
  2.57803970e-01  2.58504973e-01  2.59085323e-01  2.61398861e-01
  2.63105748e-01  2.63128029e-01  2.63588167e-01  2.65116643e-01
  2.65975615e-01  2.66023589e-01  2.66437617e-01  2.66584860e-01
  2.69471393e-01  2.71307162e-01  2.71894109e-01  2.74782856e-01
  2.75105201e-01  2.76250626e-01  2.77449273e-01  2.78090414e-01
  2.78405643e-01  2.79172294e-01  2.80450675e-01  2.80613880e-01
  2.81802722e-01  2.86358571e-01  2.86783517e-01  2.86878981e-01
  2.87019347e-01  2.87201887e-01  2.87404905e-01  2.89207973e-01
  2.91761715e-01  2.92967913e-01  2.93837665e-01  2.95684090e-01
  2.95884059e-01  2.96047770e-01  2.96843022e-01  2.98401895e-01
  3.00006711e-01  3.00390201e-01  3.00735643e-01  3.01097868e-01
  3.01175439e-01  3.01452982e-01  3.03065953e-01  3.03174182e-01
  3.05529035e-01  3.05581244e-01  3.05824350e-01  3.07833882e-01
  3.09554500e-01  3.10152056e-01  3.10476996e-01  3.10836346e-01
  3.13853047e-01  3.14144235e-01  3.15574590e-01  3.16389201e-01
  3.16916900e-01  3.18656272e-01  3.20457792e-01  3.22466426e-01
  3.22931855e-01  3.24137526e-01  3.24474158e-01  3.25359776e-01
  3.25943268e-01  3.26105442e-01  3.26462235e-01  3.26886941e-01
  3.27767355e-01  3.28842466e-01  3.30025416e-01  3.30119757e-01
  3.30430271e-01  3.30557615e-01  3.31581112e-01  3.32985373e-01
  3.33166521e-01  3.33197874e-01  3.34294642e-01  3.34317028e-01
  3.34761712e-01  3.35531985e-01  3.35730926e-01  3.36213046e-01
  3.36283840e-01  3.37017130e-01  3.37565671e-01  3.37846254e-01
  3.40484784e-01  3.41104113e-01  3.41200206e-01  3.41700917e-01
  3.41883249e-01  3.42759823e-01  3.43126643e-01  3.43928408e-01
  3.43989153e-01  3.44330106e-01  3.44339962e-01  3.44820318e-01
  3.45952323e-01  3.46830564e-01  3.47397672e-01  3.49893365e-01
  3.50765923e-01  3.50902311e-01  3.51525535e-01  3.53477338e-01
  3.54297714e-01  3.54768100e-01  3.54781361e-01  3.54848854e-01
  3.55150942e-01  3.55625873e-01  3.58614292e-01  3.58978202e-01
  3.59732537e-01  3.60385750e-01  3.60392536e-01  3.60618008e-01
  3.60921501e-01  3.60959108e-01  3.60997038e-01  3.61012305e-01
  3.61541533e-01  3.62292607e-01  3.65949449e-01  3.65988446e-01
  3.66679938e-01  3.67070827e-01  3.67429016e-01  3.68092602e-01
  3.68116287e-01  3.70808087e-01  3.72774716e-01  3.73715338e-01
  3.73766211e-01  3.74037627e-01  3.74322321e-01  3.75464561e-01
  3.76363386e-01  3.76724045e-01  3.77169882e-01  3.77544785e-01
  3.78522096e-01  3.79069464e-01  3.80373492e-01  3.80797574e-01
  3.83728207e-01  3.83748275e-01  3.84464772e-01  3.84702358e-01
  3.84795453e-01  3.85864632e-01  3.86229266e-01  3.87520715e-01
  3.87565711e-01  3.87598263e-01  3.88650915e-01  3.89698317e-01
  3.90505514e-01  3.90925836e-01  3.91513004e-01  3.91557585e-01
  3.91749231e-01  3.92762978e-01  3.95631772e-01  3.96140062e-01
  3.96249028e-01  3.96878255e-01  3.97174113e-01  3.98933920e-01
  3.99165409e-01  3.99201682e-01  3.99718661e-01  4.00099900e-01
  4.00465472e-01  4.06946380e-01  4.07572622e-01  4.08120356e-01
  4.08951884e-01  4.08953360e-01  4.09025190e-01  4.10114440e-01
  4.10480012e-01  4.10992929e-01  4.11352385e-01  4.12835326e-01
  4.15861402e-01  4.16954688e-01  4.17139795e-01  4.18934206e-01
  4.19511207e-01  4.20418020e-01  4.22449255e-01  4.26155014e-01
  4.26809009e-01  4.32875024e-01  4.32895576e-01  4.35749400e-01
  4.35995572e-01  4.36499002e-01  4.38897599e-01  4.42413045e-01
  4.43197084e-01  4.45334549e-01  4.45597448e-01  4.47052273e-01
  4.47742383e-01  4.47989188e-01  4.49425302e-01  4.50806728e-01
  4.53094268e-01  4.53336636e-01  4.54938173e-01  4.55681037e-01
  4.55807485e-01  4.57969950e-01  4.61247615e-01  4.62569586e-01
  4.63908922e-01  4.64054500e-01  4.64075398e-01  4.64796610e-01
  4.67063838e-01  4.69327805e-01  4.69778670e-01  4.73043982e-01
  4.73818090e-01  4.76829073e-01  4.78863073e-01  4.79643425e-01
  4.84141485e-01  4.92525204e-01  5.01335080e-01  5.05846171e-01
  5.08273642e-01  5.15021513e-01  5.17320015e-01  5.18944461e-01
  5.20980944e-01  5.22520612e-01  5.34272739e-01  5.36882508e-01
  5.39669663e-01  5.43568585e-01  5.46218219e-01  5.48189865e-01
  5.50983602e-01  5.56665500e-01  5.63001295e-01  5.64899683e-01
  5.66979994e-01  5.69863391e-01  5.74750341e-01  5.75169268e-01
  5.79547534e-01  5.81946130e-01  5.86234670e-01  5.86743324e-01
  5.88814267e-01  5.89710743e-01  5.93939113e-01  5.96337710e-01
  5.99589476e-01  6.02359365e-01  6.05872374e-01  6.13127886e-01
  6.15017829e-01  6.17398409e-01  6.20323676e-01  6.22822549e-01
  6.23044312e-01  6.25120869e-01  6.26109845e-01  6.27519466e-01
  6.29918062e-01  6.30488839e-01  6.30812036e-01  6.32316659e-01
  6.34302301e-01  6.34715255e-01  6.37113852e-01  6.37436141e-01
  6.39512448e-01  6.39834738e-01  6.43296289e-01  6.45152990e-01
  6.45301032e-01  6.46708238e-01  6.47654286e-01  6.49950183e-01
  6.51710226e-01  6.51731672e-01  6.53904028e-01  6.56302625e-01
  6.57205005e-01  6.58221723e-01  6.58467361e-01  6.58701221e-01
  6.58900359e-01  6.59121375e-01  6.59385514e-01  6.61099818e-01
  6.63018916e-01  6.63820704e-01  6.63918568e-01  6.65417513e-01
  6.65897011e-01  6.68295608e-01  6.69736211e-01  6.71339441e-01
  6.72136060e-01  6.72763352e-01  6.73092801e-01  6.75161949e-01
  6.75709394e-01  6.77889994e-01  6.80407310e-01  6.84996027e-01
  6.85085784e-01  6.87004882e-01  6.87026717e-01  6.87875237e-01
  6.87935542e-01  6.89508841e-01  6.91802075e-01  6.92105660e-01
  6.94680170e-01  6.95605791e-01  6.96599268e-01  6.96665813e-01
  6.97078767e-01  6.97401964e-01  6.98997865e-01  7.00416130e-01
  7.02296113e-01  7.03031008e-01  7.03795058e-01  7.05716412e-01
  7.08592251e-01  7.08742301e-01  7.09078949e-01  7.09522911e-01
  7.09614684e-01  7.09688348e-01  7.10990848e-01  7.13539494e-01
  7.14335492e-01  7.15788041e-01  7.15938090e-01  7.18186638e-01
  7.23133880e-01  7.27579294e-01  7.28232185e-01  7.29263520e-01
  7.30502818e-01  7.31110280e-01  7.33477869e-01  7.33508877e-01
  7.41695361e-01  7.43103263e-01  7.44571200e-01  7.44721250e-01
  7.49141905e-01  7.49368393e-01  7.50299053e-01  7.50854936e-01
  7.52001812e-01  7.56564183e-01  7.57494843e-01  7.59797122e-01
  7.59893439e-01  7.62516424e-01  7.65076568e-01  7.67411518e-01
  7.69487826e-01  7.71080810e-01  7.71886422e-01  7.72914394e-01
  7.74285019e-01  7.79983362e-01  7.81480809e-01  7.83879405e-01
  7.86278002e-01  7.88347150e-01  7.88676598e-01  7.89519947e-01
  7.91075195e-01  7.93473791e-01  7.93650079e-01  7.95872388e-01
  7.97941536e-01  7.98270985e-01  7.98826868e-01  8.03068178e-01
  8.05137326e-01  8.06022657e-01  8.06064992e-01  8.06068045e-01
  8.06172707e-01  8.07646660e-01  8.07865371e-01  8.09615154e-01
  8.10263968e-01  8.22580148e-01  8.28364263e-01  8.28494747e-01
  8.30031457e-01  8.36319081e-01  8.39047127e-01  8.41046551e-01
  8.44505566e-01  8.57733519e-01  8.80243422e-01  8.83680911e-01
  8.83993529e-01  8.86322220e-01  8.90505026e-01  8.97700816e-01
  9.00099412e-01  9.03099279e-01  9.04896605e-01  9.05218645e-01
  9.07295202e-01  9.08241249e-01  9.12092395e-01  9.16560140e-01
  9.16889588e-01  9.17158357e-01  9.18958736e-01  9.21178128e-01
  9.21357333e-01  9.26483975e-01  9.27054751e-01  9.28882571e-01
  9.31281168e-01  9.33679764e-01  9.34034608e-01  9.36078361e-01
  9.36347130e-01  9.38147509e-01  9.40202782e-01  9.40875554e-01
  9.49511947e-01  9.50793138e-01  9.51289266e-01  9.52539088e-01
  9.55830751e-01  9.57665730e-01  9.59734878e-01  9.62786120e-01
  9.64324643e-01  9.68415165e-01  9.69329264e-01  9.74876060e-01
  9.76016400e-01  9.76525054e-01  9.83570794e-01  9.83786264e-01
  9.88518037e-01  9.88538187e-01  9.91281379e-01  9.94064832e-01
  9.96868875e-01  1.00051102e+00  1.00332977e+00  1.00530821e+00
  1.00679476e+00  1.00770681e+00  1.01010541e+00  1.01672313e+00
  1.01782770e+00  1.02151578e+00  1.04414545e+00  1.05763568e+00]

  UserWarning,

2022-10-31 11:02:02,497:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24183509 -0.84358835 -0.60779896 -0.40205737 -0.38098777 -0.24130505
 -0.16903573 -0.10412578 -0.1026627  -0.09316583 -0.0852385  -0.0838159
 -0.05696668 -0.05604889 -0.03817931 -0.0346327  -0.01272463 -0.00509218
 -0.00398535  0.00833456  0.01736338  0.02076489  0.02525201  0.02581818
  0.0346664   0.05212424  0.05790097  0.06254634  0.06419381  0.09776756
  0.11011049  0.12700351  0.13444562  0.13555979  0.13749801  0.14333475
  0.14382151  0.1443721   0.149705    0.15056642  0.15205104  0.15551499
  0.15642883  0.15714386  0.15729845  0.15843314  0.15959143  0.16199522
  0.16336114  0.1644459   0.16940908  0.17129427  0.17449514  0.17768058
  0.17846609  0.18545851  0.19063058  0.19205521  0.19206591  0.19289549
  0.19480863  0.19681325  0.19747908  0.19942795  0.19968021  0.19991517
  0.20015902  0.20528002  0.20701622  0.20756361  0.20918455  0.20940038
  0.21270638  0.21542776  0.21612778  0.21756955  0.21789302  0.21796969
  0.22184351  0.22387663  0.22427741  0.22479225  0.22485697  0.22573355
  0.22640745  0.2282721   0.23073503  0.23271575  0.23924773  0.23931668
  0.24116574  0.24124408  0.24139332  0.24176657  0.24565927  0.24709226
  0.24779692  0.24873449  0.24895562  0.25109198  0.25131323  0.25173081
  0.25805143  0.25900158  0.26161667  0.26169258  0.26464396  0.26498152
  0.26695328  0.26788627  0.26792807  0.26815738  0.26888991  0.26974758
  0.27008491  0.27031724  0.27062009  0.27075694  0.27170273  0.27170824
  0.27189025  0.27323759  0.27423203  0.27477952  0.28178445  0.28195256
  0.28262638  0.28283717  0.28347751  0.28432881  0.28470898  0.28476411
  0.28718718  0.28807304  0.2883099   0.28925918  0.28955232  0.28977657
  0.29044622  0.29078763  0.29097713  0.29144472  0.29168391  0.29197198
  0.29246794  0.29261996  0.29461222  0.29534189  0.29583932  0.2964791
  0.29698032  0.29700417  0.29863452  0.29886471  0.29988796  0.30010004
  0.3038121   0.30439778  0.30453868  0.3064334   0.30649863  0.30687634
  0.30744376  0.30835508  0.30859675  0.3091419   0.31016547  0.31182656
  0.31347459  0.31441145  0.31450786  0.31451328  0.31468919  0.31470566
  0.3188211   0.31927063  0.32180234  0.32305458  0.32434957  0.32528303
  0.32636359  0.32647415  0.32693671  0.32733209  0.32770605  0.32796052
  0.32797346  0.32841973  0.33045964  0.33307105  0.3333686   0.33785393
  0.33850679  0.3387073   0.33959857  0.33974763  0.34053869  0.34056
  0.3406441   0.34066796  0.34122872  0.34248802  0.34433934  0.34650434
  0.34696803  0.3479533   0.34856833  0.34864106  0.34876319  0.34919704
  0.35285472  0.3530347   0.35404096  0.35608758  0.35652192  0.35905888
  0.35943734  0.36059817  0.36218923  0.36288163  0.36371488  0.36388996
  0.36390611  0.36402332  0.3642047   0.36426665  0.36444849  0.36489494
  0.36762207  0.36800665  0.36835271  0.36958802  0.37242447  0.37329012
  0.37411848  0.37659775  0.37741555  0.37787771  0.38042741  0.38286788
  0.38360399  0.3852381   0.38556744  0.38695424  0.38724646  0.38760832
  0.38767837  0.3876984   0.38793766  0.38806019  0.3887164   0.389461
  0.389586    0.38959509  0.38977785  0.38997854  0.3913961   0.39163422
  0.39221737  0.3924217   0.3936078   0.39435713  0.39471898  0.396469
  0.39708305  0.397337    0.39916568  0.39945942  0.40003729  0.40057472
  0.40220461  0.40287131  0.40306309  0.40411378  0.40451107  0.40509021
  0.40704773  0.40768785  0.40891308  0.4089403   0.41041282  0.41086472
  0.41187618  0.41269019  0.41379964  0.41570327  0.420109    0.4226369
  0.42366888  0.42416741  0.4249445   0.42773872  0.42835044  0.42849755
  0.43310219  0.4345458   0.43558763  0.43668711  0.43742601  0.43756828
  0.43797843  0.4399868   0.44034865  0.44167862  0.44183024  0.44266976
  0.4437785   0.4450891   0.44639289  0.4491468   0.44958773  0.45134258
  0.4514936   0.45202363  0.45219976  0.45326083  0.45581137  0.4562795
  0.45865823  0.46457549  0.46480838  0.46650995  0.47312418  0.47806018
  0.47853305  0.47882455  0.47976471  0.48090982  0.48386324  0.48673369
  0.49348351  0.49412711  0.49421153  0.49768457  0.49895198  0.50004364
  0.50404734  0.50822788  0.50929891  0.51076636  0.51449513  0.51561799
  0.51743338  0.51782419  0.52265418  0.52976484  0.53843123  0.53850867
  0.54145829  0.54271561  0.54373765  0.54635639  0.54729607  0.55715444
  0.55975102  0.56392227  0.56566582  0.5664396   0.56726363  0.56874055
  0.57072127  0.57645654  0.57657582  0.58005685  0.58184448  0.58411536
  0.58491294  0.58658492  0.5900587   0.59787523  0.59843602  0.59889971
  0.60489137  0.61027757  0.61312104  0.61561015  0.61683699  0.62002672
  0.6202317   0.62157743  0.62204409  0.62272082  0.63241214  0.63445302
  0.63601575  0.63682324  0.63998936  0.64156368  0.64200387  0.6439339
  0.64867434  0.65254886  0.65341479  0.65578501  0.65716294  0.65776258
  0.65815523  0.66052545  0.66261345  0.66265271  0.66289567  0.66334391
  0.66526589  0.66558713  0.66991728  0.67056542  0.67237655  0.67474677
  0.67513727  0.67711699  0.67948721  0.67960611  0.68008269  0.68185743
  0.68197633  0.68434655  0.68461816  0.68482314  0.68653265  0.68671677
  0.68692721  0.68719336  0.68868588  0.6900237   0.69145721  0.69370853
  0.69856787  0.69904446  0.70093809  0.70282756  0.70318942  0.7037849
  0.70519778  0.70567853  0.7079597   0.70999659  0.71001786  0.710214
  0.71024337  0.71030008  0.71089556  0.71161725  0.71333391  0.71735403
  0.71800622  0.71858525  0.72172218  0.72287475  0.7244229   0.72501106
  0.72874614  0.72920513  0.72938074  0.73157535  0.73163783  0.73618495
  0.736274    0.73631579  0.73744398  0.74578817  0.7461438   0.7481669
  0.75049532  0.75053712  0.75118931  0.75355953  0.75527756  0.760018
  0.76690316  0.76712866  0.76949888  0.7718691   0.77608551  0.77652049
  0.77660954  0.77701984  0.77897976  0.77916619  0.77941995  0.78323301
  0.78372021  0.78846065  0.78853775  0.79046901  0.79083087  0.79794153
  0.80031175  0.80268197  0.80505219  0.80870982  0.81074723  0.81216285
  0.81891166  0.82638418  0.82703652  0.82934988  0.83321262  0.8406055
  0.84297572  0.84534594  0.84704418  0.85024436  0.85380397  0.85588243
  0.86187226  0.86368169  0.87357287  0.87581501  0.87790302  0.88149473
  0.88351837  0.88386495  0.8873839   0.88797938  0.88975412  0.89212434
  0.89483836  0.90228223  0.90871589  0.91357518  0.91819677  0.92116247
  0.92293721  0.93241809  0.93282839  0.93292452  0.93502194  0.9360608
  0.93715853  0.94189897  0.94249446  0.94321614  0.94426919  0.94663941
  0.94703834  0.94960512  0.95219233  0.95434556  0.95474296  0.95671578
  0.95717591  0.959086    0.95980769  0.9709371   0.97271184  0.97330732
  0.97415781  0.97567754  0.9821275   0.98278821  0.98515843  0.98752865
  0.99226909  0.99307394  0.99664767  0.99850205  0.999701    1.00166092
  1.00174997  1.00300901  1.01491824  1.01502838  1.02669043  1.0269447
  1.02953191  1.03471952  1.03960828  1.04187577  1.04241869]

  UserWarning,

2022-10-31 11:02:02,544:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04576395 -0.84334916 -0.75821979 -0.72363098 -0.65553711 -0.57325674
 -0.51160024 -0.41039642 -0.39170596 -0.3849893  -0.35372235 -0.35334356
 -0.28544321 -0.2838158  -0.26010511 -0.24325735 -0.23801796 -0.22537232
 -0.18853982 -0.15661789 -0.13777648 -0.10153598 -0.08333086 -0.08122746
 -0.04275516 -0.03576546 -0.02895222 -0.02763272 -0.02334923 -0.00769664
 -0.00768045 -0.00313222  0.01634239  0.04581685  0.04626373  0.07018373
  0.07822252  0.07932295  0.08166283  0.09234299  0.099909    0.10104962
  0.10247218  0.1040501   0.11115111  0.11227514  0.11366854  0.1204501
  0.12806844  0.14249199  0.15411982  0.1549199   0.16183051  0.16425373
  0.16586597  0.17284873  0.17313285  0.17379239  0.17470806  0.17512655
  0.17630738  0.17825028  0.18235185  0.18839961  0.19243783  0.19323794
  0.19406019  0.19763888  0.19767733  0.20010856  0.2005508   0.20083531
  0.20155492  0.202045    0.20314781  0.21423743  0.21988116  0.22009586
  0.22056446  0.22127864  0.22366294  0.22373495  0.22422472  0.22528556
  0.22751419  0.2281136   0.23080115  0.23188298  0.23516173  0.23669142
  0.23727767  0.23905959  0.24043564  0.24202472  0.24212963  0.24294582
  0.24355527  0.24391793  0.24541152  0.24587494  0.25445347  0.25688798
  0.2575293   0.25761257  0.25933049  0.25961358  0.26062371  0.2610093
  0.26313188  0.26419606  0.26472293  0.26579137  0.26823625  0.26875927
  0.26900374  0.27043595  0.27048642  0.27174661  0.27182171  0.2729215
  0.27304888  0.27347465  0.27437458  0.27511096  0.27545311  0.27566489
  0.27580783  0.27632442  0.27714521  0.27916446  0.28044675  0.28049711
  0.281416    0.28264487  0.2831702   0.28462596  0.28523469  0.28708988
  0.28896627  0.28985677  0.29021082  0.29045875  0.29052883  0.29148324
  0.29165402  0.29533435  0.29587409  0.29677696  0.29704647  0.29768646
  0.29788322  0.29802647  0.29958323  0.30400942  0.30428549  0.30514162
  0.30740174  0.30862751  0.30959548  0.31146874  0.31167516  0.31210898
  0.31301033  0.31318383  0.31409041  0.31464295  0.31768655  0.3185926
  0.31927975  0.31935993  0.31963308  0.3213337   0.32163668  0.32236317
  0.32262487  0.32411217  0.32471125  0.32625244  0.3267074   0.32738779
  0.32876825  0.32885245  0.32938667  0.33013598  0.331448    0.33407701
  0.33603248  0.33603415  0.33687615  0.33704801  0.33794607  0.33797161
  0.33984041  0.34104186  0.34207887  0.34229228  0.34244317  0.34274906
  0.34349014  0.34435669  0.34603068  0.34668594  0.3467658   0.34680073
  0.34767272  0.34786616  0.34834763  0.34914067  0.34999894  0.35249762
  0.35293112  0.35349361  0.35359353  0.35431256  0.35487116  0.35624203
  0.35802725  0.35867054  0.3596784   0.35972539  0.35977385  0.3598778
  0.36020146  0.36100289  0.36122292  0.36164953  0.36169872  0.36322911
  0.36361873  0.36524541  0.3665012   0.36690826  0.36709665  0.36744629
  0.36928847  0.37046884  0.37066449  0.37100151  0.37145184  0.37209647
  0.37272494  0.37408093  0.37582791  0.37637502  0.37756162  0.37770151
  0.3796467   0.38005063  0.38021473  0.38090915  0.38158494  0.38264878
  0.38394328  0.38465365  0.38518007  0.38755401  0.38882622  0.38916873
  0.39595234  0.39688246  0.39716486  0.39784986  0.39872873  0.39948461
  0.39978284  0.39989713  0.40058687  0.40152548  0.40284823  0.40301531
  0.40419452  0.40533393  0.40543258  0.40654426  0.40672173  0.40771722
  0.40903549  0.41031222  0.41099717  0.41272056  0.41303781  0.41407531
  0.41493671  0.41556724  0.41565408  0.41644011  0.41650389  0.4166582
  0.41704871  0.41870166  0.42155445  0.421602    0.42303419  0.42359091
  0.42380063  0.42391187  0.42442221  0.42504876  0.42727686  0.42844808
  0.42846857  0.42859382  0.43118501  0.43133551  0.43186306  0.43223074
  0.43753167  0.43976291  0.44078693  0.4436121   0.4446136   0.44497116
  0.44703918  0.4474698   0.44770093  0.44784264  0.46148903  0.46464328
  0.46707501  0.47074276  0.47197012  0.47416727  0.47847295  0.48118343
  0.49435459  0.49927452  0.50010721  0.50759843  0.50767711  0.50916238
  0.51089362  0.51124302  0.51617641  0.51703198  0.52257033  0.52429273
  0.52563486  0.52904145  0.53322366  0.53418099  0.53643481  0.53675352
  0.53880292  0.54088509  0.54173161  0.54376846  0.54407104  0.54412128
  0.54485582  0.54538154  0.54746465  0.54816059  0.56291086  0.56348041
  0.56466786  0.57080124  0.57133835  0.57412539  0.57515466  0.57582653
  0.57789631  0.58535039  0.58632788  0.59686768  0.5987208   0.60244581
  0.60350378  0.60443003  0.60548992  0.60718458  0.61180696  0.61367205
  0.61457469  0.61532715  0.61619185  0.61964679  0.62011014  0.62050046
  0.62216806  0.62287786  0.62497373  0.62531571  0.62564559  0.62612261
  0.62841332  0.62916578  0.63368787  0.63394877  0.63471801  0.63521862
  0.63671649  0.63948422  0.64000161  0.64229775  0.64287971  0.64501967
  0.64577213  0.64758016  0.64889863  0.65332285  0.65491389  0.65609057
  0.66018748  0.66222657  0.66372189  0.66439375  0.66514622  0.66587646
  0.66660004  0.66678159  0.6699292   0.67021667  0.67026275  0.67048051
  0.67065559  0.67267085  0.67269693  0.67446291  0.67546465  0.67670652
  0.67679383  0.6779692   0.67803753  0.67823238  0.67905956  0.67975409
  0.68043056  0.68100011  0.68231598  0.68442776  0.6845203   0.68529414
  0.6872658   0.6892772   0.6894181   0.69139914  0.69207101  0.69256029
  0.6955912   0.6969346   0.69760646  0.69835892  0.69970232  0.70037419
  0.70074677  0.70247005  0.70314191  0.70389438  0.70425314  0.70523777
  0.70534009  0.70590964  0.7061971   0.7080055   0.70867736  0.71036726
  0.71178861  0.71219755  0.71249236  0.71421281  0.7144742   0.71450028
  0.71496528  0.71849882  0.71875306  0.7197573   0.71978354  0.72041669
  0.72051274  0.73140558  0.73291503  0.73555637  0.73648125  0.73939552
  0.73940981  0.74074851  0.74675366  0.74771299  0.748152    0.74817799
  0.74882544  0.75048071  0.75371344  0.75407221  0.75447925  0.75878389
  0.75926224  0.76064759  0.76155161  0.76288079  0.76985479  0.77262252
  0.77590764  0.77615949  0.77815797  0.77935432  0.78118298  0.7822054
  0.78526966  0.78646115  0.78719139  0.7919966   0.8030675   0.80510208
  0.80583523  0.80797911  0.81765859  0.82042632  0.82176972  0.82244158
  0.82520931  0.82596177  0.83127896  0.83462372  0.83739144  0.8398004
  0.84181566  0.84678156  0.84810358  0.85797678  0.85953325  0.87181541
  0.87689601  0.88288631  0.88842176  0.89949266  0.90024513  0.90205315
  0.90226039  0.90299064  0.90301285  0.90502811  0.9085483   0.91056357
  0.91571645  0.91576914  0.91886674  0.92681691  0.93270537  0.93322276
  0.9338166   0.93599049  0.9375122   0.93824082  0.94100855  0.94176101
  0.94320672  0.94729646  0.94931173  0.9515099   0.95207945  0.95283192
  0.95484718  0.95559964  0.95958438  0.96038263  0.96315035  0.96718794
  0.96943827  0.96945603  0.97422126  0.97497372  0.97975671  0.98252443
  0.98881235  0.99158008  0.99711553  1.00265098  1.00289098  1.01648961
  1.0210932   1.02479279  1.03143947  1.10844157  1.52238031]

  UserWarning,

2022-10-31 11:02:02,591:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.8121776  -0.51042801 -0.43857643 -0.37875454 -0.34646639 -0.33611543
 -0.25820646 -0.25427287 -0.1244459  -0.11857419 -0.098142   -0.08089684
 -0.07596489 -0.07183464 -0.03123232 -0.03036819 -0.02577378 -0.02057771
  0.01537521  0.02039074  0.02862751  0.03636212  0.03941127  0.0458492
  0.05259651  0.05286681  0.0632208   0.06580929  0.08687643  0.09416882
  0.10168896  0.1023761   0.10671195  0.1123313   0.11388465  0.11705447
  0.11773155  0.11830758  0.1276529   0.13778122  0.14045524  0.14274367
  0.15303885  0.15761774  0.16315835  0.16668586  0.17719605  0.17972168
  0.18259177  0.19469971  0.19521333  0.19793981  0.19847268  0.19847723
  0.19995416  0.20153432  0.20295622  0.20314009  0.20428385  0.20703571
  0.20953979  0.2105567   0.21471205  0.21680139  0.21766874  0.21849781
  0.21914725  0.22291434  0.22580711  0.22645418  0.23062485  0.23117544
  0.23188606  0.2359714   0.23800307  0.23851277  0.24059144  0.24097421
  0.24120797  0.2414421   0.24383681  0.24388561  0.24402964  0.2464873
  0.24823882  0.24905483  0.25129665  0.25153566  0.2521694   0.25296419
  0.25366842  0.25454554  0.2547018   0.25472008  0.25609699  0.2577383
  0.25776343  0.25937772  0.26052535  0.26096089  0.26160631  0.26214852
  0.26412075  0.26412995  0.26442714  0.2646031   0.26495839  0.26548357
  0.26823195  0.26862074  0.26901811  0.26907091  0.26927     0.26976173
  0.27106212  0.27113092  0.2713493   0.27275498  0.27390173  0.27515875
  0.27746312  0.27766168  0.27917161  0.28064345  0.28196273  0.28314144
  0.28419725  0.28446274  0.28474808  0.28506085  0.28540905  0.28548557
  0.28685665  0.28726362  0.28756844  0.28769986  0.28800445  0.28964018
  0.28991158  0.28993661  0.28999328  0.29041162  0.29100725  0.29112907
  0.29127704  0.29299459  0.29325762  0.29423683  0.29440634  0.29583168
  0.29818308  0.29850379  0.29875783  0.29990791  0.3004815   0.30141194
  0.30289606  0.30293872  0.30326306  0.30441956  0.30451323  0.30522213
  0.30744501  0.30762839  0.31001312  0.3116546   0.31303716  0.31495961
  0.31515252  0.31529337  0.31758235  0.31867993  0.31932256  0.31951928
  0.32008229  0.3201346   0.32156176  0.32294174  0.32394105  0.32517317
  0.3255033   0.32573454  0.32681277  0.32697229  0.32720409  0.33045667
  0.33097905  0.33221073  0.33363525  0.33465475  0.33776426  0.33914424
  0.33925025  0.3395397   0.33979445  0.34073986  0.34102976  0.34132211
  0.34164854  0.34210402  0.34219192  0.34357039  0.34413168  0.34540801
  0.34593051  0.34606002  0.3489704   0.3494161   0.35152699  0.35267078
  0.35371418  0.35619936  0.35734232  0.35822239  0.35871597  0.35988074
  0.36090509  0.36168184  0.36197028  0.36234561  0.36295015  0.36336892
  0.36343005  0.36458149  0.36641059  0.36670377  0.3668069   0.36734602
  0.36757265  0.36794999  0.37003593  0.37017176  0.37150485  0.37225982
  0.37280561  0.37304875  0.37306115  0.37449889  0.37462098  0.37840486
  0.37851097  0.37930317  0.37972552  0.38129514  0.38147262  0.38161399
  0.3818138   0.38229247  0.38331284  0.38393062  0.38477085  0.38842199
  0.38936149  0.38943659  0.38963706  0.38984426  0.39020393  0.39083258
  0.39116788  0.3916961   0.39197586  0.39382556  0.3939367   0.39442164
  0.39523851  0.39641522  0.39720208  0.39809454  0.39890685  0.39955985
  0.40058043  0.40069758  0.40221718  0.4035333   0.40573994  0.40591201
  0.4063951   0.40662493  0.40941745  0.40982967  0.41087279  0.41106728
  0.41278767  0.41420309  0.41486206  0.41532155  0.41907373  0.42022549
  0.42040342  0.42422132  0.42465807  0.42518721  0.42541398  0.42554594
  0.42792365  0.4292381   0.43075571  0.43091702  0.43161103  0.43324584
  0.43413353  0.43441523  0.43621888  0.43690445  0.43863715  0.43872346
  0.43949041  0.44802531  0.44827914  0.45079457  0.45340337  0.45373479
  0.45527003  0.45692684  0.45922468  0.46311241  0.46632588  0.46678154
  0.46679472  0.46849084  0.46977376  0.47615804  0.47735177  0.4815557
  0.48156063  0.48695214  0.48725186  0.48871353  0.48886921  0.49107612
  0.49130445  0.49218556  0.49343811  0.50238814  0.50247916  0.50431393
  0.50650931  0.50783775  0.51101607  0.51281625  0.5141289   0.52541149
  0.52770396  0.52883476  0.53113421  0.53414443  0.53884873  0.54300283
  0.55939992  0.56033532  0.56236683  0.56574059  0.56617066  0.56771074
  0.56884747  0.56952561  0.57069665  0.57310502  0.58083907  0.58105063
  0.58166905  0.58736063  0.58743351  0.59311848  0.59586112  0.59778749
  0.59781722  0.59853346  0.60121295  0.60175859  0.60434709  0.60693558
  0.60855709  0.6089949   0.61476358  0.61498897  0.61549196  0.61728957
  0.61886345  0.62246656  0.62351448  0.62524442  0.62578311  0.62879529
  0.62912599  0.63033187  0.63463141  0.6348027   0.63540903  0.63799753
  0.64070807  0.64131441  0.64317452  0.64512638  0.64576302  0.6469689
  0.6535285   0.6539279   0.65544736  0.656117    0.65630636  0.65837651
  0.658768    0.65894746  0.65943387  0.6608345   0.66129399  0.66388248
  0.6639842   0.66647098  0.66671472  0.66719936  0.66905947  0.67423647
  0.67662647  0.67682496  0.67803293  0.67941346  0.68014184  0.68024265
  0.68497546  0.68537516  0.68757834  0.69049582  0.6916863   0.69316874
  0.69513379  0.69664889  0.7008498   0.70103845  0.70270992  0.70402244
  0.70602679  0.7060893   0.7065043   0.70742742  0.70788691  0.70927292
  0.7104754   0.71066477  0.71305478  0.71493191  0.71532341  0.71565239
  0.71565968  0.71584176  0.71838776  0.72295839  0.72414554  0.72533674
  0.7263295   0.72792523  0.72878423  0.72953837  0.73100407  0.73167109
  0.73310223  0.7348027   0.73689954  0.73703624  0.73719655  0.73967723
  0.74086771  0.74120755  0.74168422  0.74327714  0.74345621  0.74571572
  0.7460447   0.75109214  0.75122169  0.75162121  0.75658805  0.75898718
  0.75980369  0.76212193  0.76239218  0.76308254  0.76435971  0.76901218
  0.77192966  0.77384196  0.77451815  0.77532673  0.77710665  0.77872816
  0.77936616  0.77969514  0.78228364  0.7855998   0.78746063  0.78818829
  0.78908214  0.79023849  0.79263762  0.79331515  0.79489713  0.79522612
  0.79940753  0.8029916   0.8055801   0.80567612  0.81852258  0.82573275
  0.82647743  0.82721435  0.82854757  0.83066586  0.83424291  0.83599444
  0.83692743  0.8384053   0.84135955  0.84314885  0.84407854  0.85226904
  0.85818484  0.8596999   0.86225688  0.86260706  0.86909     0.874578
  0.88058402  0.88212787  0.89035285  0.89041536  0.89294134  0.89319322
  0.89811833  0.90607318  0.90753972  0.91648968  0.91696546  0.92632166
  0.92659178  0.92918028  0.93143979  0.93202065  0.93229795  0.93694576
  0.93856727  0.93953426  0.93972362  0.94212275  0.94471125  0.94600378
  0.94633276  0.94741623  0.94916776  0.94988824  0.9526661   0.95765373
  0.95838211  0.9601026   0.96024222  0.96560858  0.96800771  0.97235946
  0.97783057  0.97836169  0.98631654  0.98971361  0.99149353  0.99667052
  0.99925902  1.00083291  1.00138803  1.00683514  1.00702451  1.00763141
  1.0162969   1.01996698  1.02514397  1.02701199  1.03420199  1.06515679
  1.0736049 ]

  UserWarning,

2022-10-31 11:02:02,622:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60508693 -0.84479933 -0.30974899 -0.23191767 -0.22954605 -0.18414641
 -0.17372977 -0.1594233  -0.05902957 -0.05426967 -0.04767941 -0.03812311
 -0.01198321 -0.00580603  0.00506848  0.01022864  0.0119005   0.02152776
  0.06077911  0.07138445  0.07486234  0.08407977  0.1021523   0.10388578
  0.11272059  0.11870033  0.12322437  0.12338536  0.14286832  0.14331972
  0.14478233  0.14684391  0.15411811  0.16334936  0.16335126  0.17634752
  0.18154092  0.18697855  0.19046337  0.19191789  0.19244467  0.19515
  0.19539995  0.19626254  0.19994919  0.2011712   0.20178419  0.20651002
  0.21003292  0.21016734  0.21095665  0.21240059  0.21416641  0.21456688
  0.21631924  0.21715158  0.2187957   0.22141918  0.22240285  0.22354364
  0.22410837  0.22509116  0.22626449  0.2273236   0.22801716  0.22868629
  0.22969882  0.23105392  0.23471397  0.23651003  0.23836106  0.24004096
  0.24072867  0.24098137  0.24637706  0.24685186  0.2472285   0.24814033
  0.24977021  0.25073328  0.25162175  0.252013    0.25322293  0.25449149
  0.25484169  0.25533009  0.25555024  0.25643243  0.25646815  0.25668463
  0.25848347  0.25879343  0.25882009  0.25889309  0.25997563  0.26268357
  0.26278858  0.26286388  0.26322638  0.26333313  0.26518808  0.26557543
  0.26605198  0.26648903  0.26650239  0.26663048  0.26818148  0.26884271
  0.26972873  0.26978282  0.27039891  0.27053538  0.27226794  0.27344077
  0.27400242  0.27516435  0.27539307  0.27561736  0.27662005  0.27819056
  0.28284964  0.28618616  0.28630062  0.28675516  0.28703452  0.28728947
  0.28758018  0.28835862  0.28921458  0.28963102  0.29065677  0.29080225
  0.2914089   0.29148103  0.29300564  0.29423071  0.29435771  0.29507579
  0.29567364  0.29600443  0.29835522  0.29941549  0.29973124  0.30029163
  0.30144328  0.30211322  0.3021623   0.30229547  0.30307949  0.30440711
  0.30597503  0.3079508   0.30887431  0.31193067  0.31218835  0.31313272
  0.31464783  0.31467222  0.31615055  0.31813219  0.31990763  0.32183073
  0.32247763  0.32284465  0.32316486  0.32395621  0.32463199  0.32463664
  0.32587527  0.32596479  0.32692297  0.3274513   0.32773723  0.32784967
  0.32868571  0.32931706  0.32986574  0.33000771  0.33076699  0.33110834
  0.33204486  0.33227974  0.33239781  0.33247205  0.33441892  0.33454049
  0.33481358  0.33520014  0.33783958  0.33868899  0.33870143  0.34037051
  0.34043295  0.34129425  0.34176111  0.34204223  0.34276011  0.34339421
  0.34372063  0.34454204  0.34622072  0.34699101  0.34747562  0.34829149
  0.34914073  0.34972732  0.35089694  0.35090224  0.35179359  0.3518882
  0.35226344  0.35426153  0.35493237  0.35504417  0.3565644   0.35697945
  0.35700511  0.35867228  0.35882496  0.36017101  0.36063059  0.36075482
  0.36126955  0.36275974  0.36352498  0.36630978  0.3676357   0.36823476
  0.37007405  0.37155024  0.37161903  0.37258447  0.37270003  0.37526396
  0.37610352  0.37663747  0.37756666  0.37859571  0.38098876  0.38105053
  0.38286133  0.3831841   0.38326542  0.38412461  0.3841775   0.38429577
  0.3855181   0.38565272  0.3859339   0.38603085  0.38829031  0.38891715
  0.38903632  0.3893212   0.38941051  0.38978762  0.39021244  0.39281462
  0.39462481  0.39466016  0.39485637  0.39520192  0.39729647  0.39749885
  0.39775314  0.39879486  0.39997652  0.40040324  0.40058213  0.40178594
  0.40227729  0.40306147  0.40320389  0.40326256  0.40475112  0.40478243
  0.40499983  0.40653126  0.40736884  0.40737349  0.40785503  0.41240544
  0.41307098  0.41366496  0.41520764  0.41696789  0.41822693  0.41842607
  0.41848168  0.42036732  0.4217445   0.42240965  0.42306912  0.42319929
  0.42483802  0.42494039  0.42508243  0.42508298  0.42796232  0.42829807
  0.43085685  0.43428454  0.43430798  0.43792004  0.43817081  0.43848772
  0.43919533  0.44181725  0.4424542   0.44304564  0.44569198  0.44670809
  0.44831427  0.4568416   0.45812738  0.46574518  0.46717768  0.47132268
  0.47489451  0.47537106  0.47595149  0.47740819  0.47809843  0.47909553
  0.4816589   0.48206984  0.48337562  0.48594075  0.48729182  0.48763497
  0.49461469  0.4976028   0.50203715  0.50274269  0.50317489  0.5094315
  0.51009365  0.5138764   0.51484181  0.51794792  0.51817967  0.51916769
  0.52101881  0.52924777  0.52962602  0.53169643  0.53774941  0.53789505
  0.54106795  0.54333738  0.54505207  0.54713377  0.55491586  0.55907028
  0.56424076  0.57296915  0.575604    0.57773855  0.57926292  0.58055598
  0.58255962  0.58463653  0.59249248  0.59895805  0.60129468  0.60204168
  0.60576014  0.60909908  0.61056439  0.61159088  0.61397818  0.62034302
  0.62049123  0.62458111  0.62517285  0.6256793   0.62591468  0.62696841
  0.62704838  0.62830198  0.62835563  0.63068928  0.63074293  0.63196711
  0.63353736  0.63481503  0.63572461  0.63964538  0.64307279  0.64352786
  0.64369908  0.64978768  0.65174486  0.65209067  0.65456228  0.66005706
  0.66006017  0.66172418  0.66346264  0.66649878  0.66782995  0.66813183
  0.66829298  0.66839237  0.67014442  0.67084326  0.67209406  0.67366069
  0.67522214  0.67604799  0.67843529  0.68147362  0.68261678  0.68320989
  0.68510347  0.68755436  0.68828746  0.68972294  0.68994166  0.69395958
  0.69465267  0.69942727  0.70230829  0.70420187  0.70708289  0.7087771
  0.71116697  0.71136377  0.71185749  0.71344932  0.71355427  0.71381466
  0.71396717  0.71460155  0.71584294  0.71594157  0.71613837  0.71652834
  0.71827756  0.71852567  0.71964299  0.72079757  0.72193949  0.72330027
  0.72443463  0.72653805  0.72787807  0.72807487  0.73026537  0.73052577
  0.73160693  0.7319052   0.73290312  0.73938188  0.73981457  0.74086185
  0.74215056  0.74399606  0.74505418  0.74744176  0.74759363  0.74936377
  0.75039029  0.75139205  0.751605    0.75301521  0.75546953  0.75731422
  0.75891297  0.75910978  0.75917337  0.76130027  0.76149708  0.76492195
  0.76496984  0.76561021  0.77041935  0.77562407  0.77607108  0.77736252
  0.78039867  0.78240949  0.78278597  0.78517327  0.78706685  0.78756057
  0.78860671  0.78876527  0.79266665  0.79434599  0.79472247  0.7959537
  0.79834988  0.79949707  0.80188437  0.80361297  0.80596589  0.80751112
  0.80988582  0.81267199  0.81816535  0.81859548  0.82055265  0.82098278
  0.82337008  0.83291928  0.8358256   0.85005595  0.85022213  0.85274877
  0.85864505  0.86875021  0.87439227  0.88037157  0.88340819  0.88394147
  0.89284182  0.8950825   0.89783515  0.90124432  0.90469743  0.90954307
  0.91020177  0.91497637  0.92346944  0.92930018  0.92974718  0.93168748
  0.93407478  0.93646208  0.93884938  0.94362398  0.94498804  0.94899032
  0.95214995  0.95513035  0.95556048  0.95794778  0.95931185  0.95990495
  0.96272238  0.97184145  0.97393333  0.97422875  0.97900336  0.98183766
  0.98377796  0.98557215  0.98579991  0.99099877  0.99332716  0.99855331
  0.99969359  1.00048906  1.00151093  1.00287636  1.00584109  1.00765096
  1.00974283  1.02371321  1.02734111  1.02834119  1.02909178  1.03128338
  1.05043562  1.05598871  1.06076331  1.0679845   1.11743892  1.26601968
  1.29653749  2.53563957]

  UserWarning,

2022-10-31 11:02:02,649:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77228358 -0.7487938  -0.50256376 -0.39033036 -0.2778407  -0.2574729
 -0.18776664 -0.16519221 -0.14686488 -0.08370009 -0.04002725 -0.00415394
  0.02138541  0.02365913  0.04334183  0.07131719  0.07554375  0.07687967
  0.10313715  0.10368789  0.11547808  0.11998871  0.12449625  0.12755704
  0.14071677  0.14113545  0.14522287  0.15426807  0.15856108  0.16000496
  0.16049131  0.16141065  0.16241606  0.16474468  0.16491056  0.16603467
  0.17330939  0.17965285  0.18376471  0.18516582  0.18625331  0.18690255
  0.18880126  0.19240744  0.19262447  0.19450087  0.19480334  0.19961942
  0.20016595  0.20175998  0.20273684  0.2071651   0.21592827  0.21710112
  0.21891808  0.2193353   0.21985619  0.22136587  0.22251318  0.22444037
  0.22629036  0.2276559   0.22876743  0.23257912  0.23288302  0.23539049
  0.23557115  0.23865175  0.23885095  0.24071243  0.24082565  0.24335323
  0.24347127  0.24355794  0.24475613  0.24789355  0.24794945  0.24824692
  0.2483612   0.25153525  0.2520258   0.25251184  0.25272532  0.25272827
  0.25463526  0.25504038  0.25592031  0.25620071  0.25726643  0.25966545
  0.26127994  0.26302073  0.26353568  0.2636932   0.26449401  0.26609917
  0.26663649  0.26797539  0.26954919  0.27021924  0.27037966  0.27049822
  0.27055446  0.27249222  0.27298647  0.27564286  0.27733318  0.27767768
  0.27883317  0.27966486  0.28032522  0.28075446  0.28132586  0.28280743
  0.28300734  0.28345951  0.28417459  0.28480351  0.28553871  0.28562874
  0.2866382   0.28793797  0.28807114  0.28928181  0.29194347  0.29441355
  0.29498794  0.29552182  0.29689317  0.29698174  0.29721087  0.29752227
  0.29767967  0.2997456   0.30160267  0.30171318  0.30261137  0.30327958
  0.30469337  0.30489201  0.30609312  0.30690713  0.3088409   0.30889029
  0.30895287  0.30976807  0.31168559  0.31202871  0.31224605  0.31440003
  0.31449985  0.31454123  0.31547866  0.31574061  0.31674312  0.31729724
  0.31746624  0.31841372  0.3204219   0.32149024  0.32254831  0.32414454
  0.32616836  0.32644901  0.32673651  0.32694476  0.32878139  0.3298106
  0.32991399  0.33003228  0.33090949  0.33255524  0.33345646  0.33431832
  0.33520734  0.33555863  0.33570679  0.33574038  0.336984    0.33747364
  0.33773197  0.3384887   0.33905841  0.33987707  0.33998249  0.34020637
  0.34063843  0.34139182  0.3423818   0.34333841  0.34452026  0.34463328
  0.3449466   0.34550193  0.3456298   0.34588499  0.34635217  0.3476643
  0.34906509  0.35121051  0.35125654  0.35126197  0.3522651   0.35248799
  0.35298315  0.3532297   0.3536275   0.35391267  0.35401819  0.35415778
  0.3542441   0.35572806  0.35585849  0.35614636  0.35648579  0.35658042
  0.35750298  0.35852609  0.35917159  0.36004971  0.36068355  0.36148367
  0.36163237  0.36274103  0.36287709  0.36415048  0.36477862  0.36595058
  0.36730576  0.36834582  0.36865637  0.36960187  0.37021391  0.37141674
  0.37196766  0.37277299  0.37411724  0.37427452  0.3746745   0.3779031
  0.3779087   0.37837364  0.38039049  0.38115308  0.38119648  0.38136179
  0.38208895  0.38317967  0.38351794  0.38388404  0.38527512  0.38700472
  0.38719199  0.38746798  0.38859812  0.39119906  0.39227417  0.3938859
  0.39437253  0.39551027  0.39640057  0.39701642  0.39844079  0.39902133
  0.40181756  0.40189952  0.40230419  0.40276427  0.40280068  0.40345767
  0.40355755  0.40663019  0.40710534  0.40759197  0.40764466  0.40951076
  0.4112803   0.41230668  0.41388075  0.4144456   0.41588354  0.41642048
  0.41880436  0.41991394  0.42206715  0.42511054  0.42891452  0.43074906
  0.43110833  0.43438199  0.43462554  0.43577332  0.43794657  0.43853528
  0.44212923  0.44635503  0.44950224  0.4525212   0.45400575  0.45656637
  0.46298873  0.463912    0.46407999  0.465199    0.46570252  0.46757365
  0.46778987  0.46800835  0.46926265  0.46940396  0.47030653  0.47071954
  0.47967311  0.48512067  0.48901919  0.48990838  0.49476564  0.50091578
  0.50363321  0.5077012   0.50853499  0.51009923  0.51526103  0.51808077
  0.52117471  0.52263698  0.52926361  0.53306274  0.53426794  0.53675412
  0.53983563  0.54084722  0.54506716  0.54982149  0.55073659  0.55074031
  0.55078738  0.55325806  0.55326186  0.558       0.55980074  0.56199834
  0.56706083  0.56794292  0.5681989   0.56993001  0.57150478  0.5725739
  0.58041501  0.58050556  0.58184578  0.58336373  0.59075454  0.59325792
  0.59372501  0.59506523  0.60026059  0.600353    0.60694446  0.60958835
  0.61211256  0.61223223  0.61460651  0.61752001  0.61880913  0.62280779
  0.62545168  0.62970475  0.63045091  0.63169682  0.63520761  0.63590232
  0.63602724  0.63942168  0.6411901   0.64125175  0.6466028   0.64761399
  0.64886604  0.64918342  0.64924668  0.65140394  0.65189057  0.6527518
  0.65371483  0.65453446  0.65635515  0.65982224  0.66246613  0.66511002
  0.66929369  0.67067547  0.67129591  0.67208917  0.67304169  0.67353835
  0.67426915  0.67433081  0.67482846  0.67486595  0.67568558  0.67832947
  0.67865792  0.68097336  0.68608324  0.68884176  0.68890502  0.69127922
  0.69441441  0.69481843  0.69488009  0.69538825  0.69647356  0.69671177
  0.69683669  0.69769791  0.69805315  0.69941732  0.69980904  0.70206121
  0.70464344  0.7047051   0.70483594  0.70734899  0.70741225  0.70753916
  0.70903109  0.70993122  0.71217449  0.71263677  0.71534392  0.71592191
  0.71620514  0.71699401  0.71792454  0.72054862  0.72203635  0.72272569
  0.72284919  0.72298704  0.72315066  0.72321232  0.72624793  0.7267807
  0.7285001   0.72985367  0.73064469  0.73114399  0.73153571  0.73221845
  0.73227739  0.73372622  0.73479907  0.73643177  0.73735625  0.73865524
  0.74000014  0.74528792  0.74534437  0.74694567  0.74695757  0.74754616
  0.74861037  0.74999774  0.75300984  0.75321959  0.75850737  0.7605733
  0.76115126  0.76232237  0.76394512  0.76502792  0.76545289  0.76706954
  0.76750917  0.76809678  0.76908292  0.77172681  0.77414542  0.7743707
  0.7748986   0.77776514  0.78759015  0.79023404  0.79287793  0.79752609
  0.8008096   0.80345348  0.80609737  0.81103495  0.81147918  0.81402904
  0.81458268  0.81492468  0.81931682  0.81974326  0.82301823  0.82411542
  0.82447968  0.82751126  0.82757694  0.83518016  0.83782405  0.84038142
  0.84791297  0.85055686  0.85320075  0.86066491  0.86413802  0.86420379
  0.86814899  0.87920598  0.89348377  0.8948855   0.89987566  0.90251955
  0.91058206  0.911991    0.91248355  0.915739    0.92090185  0.92354574
  0.92611037  0.92631455  0.92810132  0.93147741  0.93160233  0.93235288
  0.9345231   0.93689011  0.93940908  0.94469686  0.94482178  0.94914777
  0.95010956  0.95262852  0.9579163   0.96068511  0.963329    0.96849186
  0.97119741  0.97198082  0.97533325  0.97642353  0.97808823  0.98171131
  0.98337601  0.98393022  0.9843552   0.98620372  0.98699909  0.98745246
  0.98774963  0.98964297  0.99211395  0.99275831  0.99444412  0.99493075
  0.99638422  1.00393256  1.00550631  1.00766357  1.01343798  1.01762165
  1.02270727  1.02401354  1.0390815   1.04001109  1.04934832  1.31599104
  1.4412643 ]

  UserWarning,

2022-10-31 11:02:03,405:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.40549538 -0.40486112 -0.35109427 -0.23301428 -0.21260413 -0.1826596
 -0.13818525 -0.12710741 -0.09976673 -0.03948601 -0.02482887 -0.02018988
 -0.01562095 -0.00525573  0.02259929  0.03982186  0.04035425  0.0439559
  0.05126041  0.05171112  0.0746957   0.08231324  0.08628288  0.0893107
  0.10094365  0.10253246  0.10701834  0.11591937  0.12629156  0.12804584
  0.13259527  0.13354262  0.13412142  0.13605349  0.1475015   0.15158946
  0.15296912  0.15320132  0.15594778  0.16062221  0.16159806  0.16349381
  0.16555719  0.18681709  0.19156253  0.19260703  0.1947289   0.19500973
  0.19937274  0.20225709  0.20305167  0.20661084  0.20682444  0.20807625
  0.20857904  0.21032523  0.21344087  0.21369484  0.2144149   0.21596934
  0.21612537  0.21923755  0.21991459  0.22156907  0.22157172  0.22229169
  0.2224078   0.22304893  0.22441526  0.22877274  0.22897221  0.22897307
  0.23093195  0.2339594   0.23406357  0.23481269  0.23581429  0.2360135
  0.2364467   0.23664996  0.24173953  0.24178999  0.24228054  0.24339402
  0.2457538   0.24620689  0.24830351  0.24977404  0.25147951  0.25201866
  0.25501993  0.25724686  0.25818594  0.25932014  0.26366351  0.26674896
  0.26735689  0.2681555   0.26915166  0.26933057  0.26943039  0.27111536
  0.27135087  0.27219456  0.27332982  0.27356701  0.27371672  0.27514844
  0.27684964  0.2780035   0.27979011  0.28183447  0.28185512  0.28245855
  0.28309673  0.2847501   0.2863581   0.28666735  0.28706529  0.28729274
  0.2876582   0.2880358   0.2897362   0.28977261  0.28998619  0.29039683
  0.29069908  0.29094603  0.2917576   0.29176626  0.29217022  0.29349811
  0.29408378  0.29667645  0.29670615  0.29673978  0.29729256  0.29743053
  0.29770866  0.29824622  0.29839186  0.29916961  0.29919402  0.30007433
  0.30077823  0.3013935   0.30199138  0.30199307  0.30227201  0.30298883
  0.30360159  0.30381029  0.3040175   0.3042178   0.30463924  0.30481505
  0.30542346  0.30639812  0.3072116   0.3084898   0.30962824  0.31002676
  0.31092986  0.31253037  0.31335788  0.31383015  0.31402836  0.31427348
  0.31666439  0.31745101  0.31764655  0.32026634  0.32089651  0.32275618
  0.32408388  0.3248136   0.32496792  0.32541805  0.32572432  0.32606603
  0.32671229  0.32705492  0.32862183  0.32894523  0.32993876  0.33036351
  0.33162501  0.333895    0.3344303   0.33511034  0.33651258  0.33653619
  0.33702656  0.33722282  0.33764608  0.33799702  0.33942295  0.34136651
  0.3419137   0.34236502  0.34290092  0.34554771  0.34697034  0.34794464
  0.34800874  0.34806046  0.34895599  0.34907902  0.3495418   0.34972702
  0.34980816  0.35009415  0.3525333   0.35397111  0.35421744  0.35442319
  0.35497098  0.35524832  0.3556941   0.35628165  0.3570556   0.35739023
  0.35981593  0.36136834  0.36210702  0.36298106  0.36329216  0.3635125
  0.3643286   0.36459767  0.36506025  0.3653516   0.36536292  0.36732938
  0.36741177  0.36760742  0.37127411  0.37138453  0.37152194  0.37318129
  0.37400638  0.37429065  0.37456001  0.37629602  0.37645992  0.37813391
  0.37998074  0.38014434  0.38108887  0.38116182  0.38314981  0.38362656
  0.38494236  0.38616425  0.38679431  0.3875942   0.38945801  0.3901613
  0.39053302  0.39118423  0.39123964  0.39133664  0.39152403  0.39228319
  0.39420338  0.39421832  0.39552849  0.39631502  0.39736855  0.39752966
  0.3975393   0.39885272  0.39919224  0.40139041  0.40290101  0.40341654
  0.4038727   0.4039281   0.40431989  0.40444086  0.40598698  0.40646579
  0.40704558  0.40900349  0.40985254  0.41038841  0.41196689  0.41611249
  0.41881355  0.41948629  0.42178143  0.42219059  0.42247392  0.42404135
  0.42508959  0.42664066  0.43004421  0.43199162  0.43428458  0.43429415
  0.43678702  0.44112303  0.44426196  0.4465125   0.44735689  0.44989459
  0.44990092  0.45187503  0.45243228  0.45252793  0.45496997  0.45750767
  0.45968145  0.46202581  0.46208886  0.4643984   0.46496311  0.46710119
  0.47129032  0.4739087   0.47412221  0.47481174  0.48157236  0.48358719
  0.48441397  0.49087997  0.49233036  0.49301054  0.49849305  0.51069768
  0.51128015  0.51388769  0.51392191  0.51768221  0.51814306  0.52135498
  0.52156768  0.52253148  0.53148306  0.53402075  0.54411611  0.54417152
  0.54736799  0.55006863  0.55525995  0.56038059  0.56245684  0.57043961
  0.57474328  0.577175    0.58205943  0.58506262  0.58663726  0.5872553
  0.5872787   0.59240116  0.59438248  0.59731272  0.60051484  0.60161759
  0.60568191  0.60597735  0.60653373  0.6071714   0.60949328  0.61160911
  0.61209388  0.61524039  0.61777808  0.61867126  0.62031578  0.62285347
  0.62561218  0.62882203  0.63046655  0.63470871  0.63554193  0.64061732
  0.64315501  0.64452733  0.6456927   0.6482304   0.64838881  0.65076809
  0.65274941  0.65330578  0.65792138  0.66056068  0.66160018  0.66599424
  0.66714715  0.66826271  0.66853194  0.67045604  0.67106963  0.67135765
  0.67173995  0.67360732  0.67614501  0.67812634  0.67868271  0.67939538
  0.67994074  0.68104102  0.68135755  0.68465127  0.68477788  0.68573941
  0.68883348  0.69226435  0.69335249  0.69545492  0.69563031  0.69639115
  0.69644656  0.69670581  0.69711688  0.69733973  0.69842788  0.69898425
  0.70096557  0.70152194  0.70350326  0.70405963  0.71167271  0.71270851
  0.71554155  0.71619173  0.71770644  0.71957381  0.71990367  0.72756919
  0.72887699  0.72895126  0.73199304  0.73258917  0.73268144  0.7351359
  0.73885194  0.73915448  0.73922915  0.7418558   0.74202275  0.74279535
  0.74693118  0.74728015  0.74787073  0.74980329  0.75483342  0.75548381
  0.7576392   0.7580215   0.76036206  0.76435492  0.76723272  0.76817227
  0.76960679  0.77070996  0.77121057  0.77214448  0.77313981  0.77324766
  0.77578535  0.77832304  0.78086074  0.78331219  0.78389903  0.78593612
  0.78622414  0.78871546  0.78897442  0.7935492   0.79603149  0.79862458
  0.80035671  0.80116228  0.8014503   0.80398799  0.80618226  0.80726751
  0.80790479  0.81095487  0.81385074  0.81413876  0.8147506   0.81567718
  0.81676953  0.81887072  0.81996193  0.82064866  0.82175184  0.82503731
  0.83053973  0.83171084  0.83190261  0.8337941   0.83663457  0.8376125
  0.83922767  0.83951568  0.84712876  0.8477474   0.85163033  0.85282279
  0.85302669  0.85727953  0.86222577  0.86297356  0.86545584  0.86701314
  0.86892393  0.87185949  0.87414411  0.88315136  0.8832751   0.89439025
  0.89591111  0.89850125  0.90357664  0.90567514  0.90611433  0.90640235
  0.91118972  0.91147774  0.91372741  0.9162651   0.91880279  0.92134049
  0.92162851  0.9241662   0.92636047  0.92641587  0.93149126  0.93177928
  0.93402895  0.93642195  0.93910434  0.93939235  0.93989296  0.94417972
  0.94671741  0.94700543  0.94788042  0.94925511  0.9517928   0.95812354
  0.96194357  0.96223159  0.96448126  0.96476928  0.96687427  0.96955665
  0.96994538  0.97492005  0.97681155  0.97745774  0.97999544  0.98253313
  0.98303373  0.98507082  0.98760851  0.99212665  0.9926839   0.99522159
  0.99825989  1.00029698  1.00537236  1.01132274  1.01525391  1.02098421
  1.02300953  1.02794237  1.28351232]

  UserWarning,

2022-10-31 11:02:03,447:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94588565 -0.77727197 -0.47565439 -0.39285328 -0.33391399 -0.31797557
 -0.31537212 -0.30828983 -0.26836208 -0.25985396 -0.25899128 -0.25549365
 -0.1240723  -0.11841169 -0.10358507 -0.09830363 -0.05744355 -0.04693365
 -0.0292156  -0.02892426 -0.00201809  0.00648351  0.02309888  0.02581329
  0.03282859  0.03620779  0.05466422  0.06077021  0.06656182  0.06702878
  0.08121338  0.08493697  0.08583782  0.09244132  0.09706522  0.10289261
  0.11108173  0.12658984  0.1268368   0.12754332  0.13566329  0.14906557
  0.15196698  0.15895679  0.17079591  0.17242304  0.17452383  0.17649313
  0.17714351  0.17911148  0.18484923  0.19018789  0.19539106  0.20242528
  0.20558385  0.2056833   0.20621969  0.20918107  0.20939491  0.21018678
  0.21162605  0.21171809  0.21185075  0.21239972  0.21555219  0.21619719
  0.21646353  0.21711645  0.21751815  0.2186626   0.22033353  0.22437602
  0.22491418  0.23361704  0.23549686  0.23599469  0.23696735  0.23897691
  0.23942881  0.24154655  0.24163589  0.24164409  0.2446316   0.24656641
  0.24777079  0.24845523  0.24868366  0.24961201  0.25010709  0.25015193
  0.25145716  0.25468348  0.257334    0.2575669   0.25778608  0.26289283
  0.26297567  0.26303567  0.26415559  0.26435008  0.26510261  0.26567617
  0.26639543  0.26854239  0.26910837  0.27016245  0.27068233  0.2707573
  0.27181289  0.27218138  0.27251714  0.27330069  0.27361604  0.27397517
  0.27452643  0.275931    0.27670286  0.27671454  0.27679948  0.27860116
  0.27889296  0.28140762  0.28216092  0.28275527  0.28360062  0.28362816
  0.28379269  0.28408496  0.28460086  0.28616624  0.28628987  0.28731391
  0.2878076   0.29015921  0.29024346  0.29033971  0.29117493  0.2913321
  0.29161228  0.29205317  0.29521043  0.29690321  0.29732838  0.29759487
  0.29761159  0.29771755  0.29810946  0.30183584  0.30341189  0.30400443
  0.30402029  0.30642901  0.30767269  0.3112514   0.31192871  0.31205703
  0.31255091  0.31305583  0.31318724  0.31354832  0.31403606  0.31494962
  0.3174336   0.31808922  0.31815498  0.3182134   0.3192459   0.32005625
  0.32124619  0.32159704  0.32657281  0.32746898  0.32762349  0.3285217
  0.3287782   0.33129934  0.33336467  0.33337749  0.3341557   0.33536867
  0.33578004  0.33679284  0.33774462  0.33957982  0.34033558  0.34053864
  0.34149333  0.34243696  0.34256778  0.34525111  0.34625361  0.34635853
  0.34683705  0.34711638  0.34786127  0.34860901  0.34903809  0.34907297
  0.35094995  0.35134711  0.35268745  0.35309941  0.35488541  0.35929282
  0.35990749  0.36370362  0.36409746  0.36667944  0.36754425  0.37233168
  0.37269469  0.37270394  0.37450325  0.37607724  0.37674068  0.37709392
  0.3776036   0.37798113  0.37870633  0.37907762  0.3797822   0.37989007
  0.37992384  0.38007515  0.38039288  0.38144769  0.38161024  0.38209292
  0.38314179  0.38352182  0.3847417   0.38498081  0.38540777  0.38565985
  0.38657452  0.38703333  0.38735934  0.38763063  0.38833904  0.38902429
  0.39025303  0.39065622  0.39085856  0.39281366  0.39345572  0.39353629
  0.39419662  0.39432139  0.39484539  0.39536218  0.39617224  0.39764119
  0.40201495  0.40317422  0.40393078  0.40442424  0.40476198  0.40651889
  0.40652941  0.4072342   0.4083153   0.40908769  0.40912803  0.40927083
  0.41091954  0.41131232  0.41156656  0.41224125  0.4124991   0.41364638
  0.41420307  0.41559055  0.41644041  0.41677403  0.41710223  0.41913122
  0.42133585  0.4221319   0.42251581  0.42524987  0.42536601  0.42562301
  0.42904496  0.43130934  0.43385538  0.43536875  0.43642546  0.43902408
  0.44301149  0.44422133  0.44600723  0.4460192   0.44654676  0.44929776
  0.44956748  0.45351254  0.45456571  0.45536872  0.45563243  0.45572388
  0.4569548   0.45713388  0.45721445  0.45814755  0.45849114  0.4620268
  0.46431281  0.46698757  0.48250318  0.48345794  0.48639841  0.48848555
  0.48898747  0.49507422  0.49617324  0.49767172  0.49858343  0.50069355
  0.50106681  0.50310665  0.50340089  0.5045241   0.50589079  0.50917469
  0.51000635  0.51237509  0.51363934  0.51972864  0.52143823  0.53180516
  0.53256183  0.53340322  0.53707428  0.54155778  0.54947334  0.55077074
  0.5523727   0.55518398  0.55526465  0.55531159  0.55534038  0.56256838
  0.5684567   0.56923109  0.57509475  0.57514129  0.5753981   0.57675698
  0.58195423  0.58455285  0.59070459  0.59159806  0.59194109  0.59295657
  0.59427161  0.59494735  0.59550787  0.60699474  0.60723411  0.60794047
  0.61219199  0.61833497  0.62872946  0.63203187  0.63298098  0.63392671
  0.63652533  0.63912396  0.63972233  0.6478547   0.65101339  0.65156452
  0.65211708  0.65278595  0.6534025   0.65463513  0.6547157   0.65497019
  0.65731432  0.65750506  0.65991295  0.66122414  0.66352701  0.66370895
  0.66382276  0.66480626  0.6651102   0.66612564  0.66761805  0.66770882
  0.66872426  0.67024318  0.67030744  0.67223575  0.67290607  0.67322818
  0.67550469  0.67652013  0.67711851  0.67882073  0.67911876  0.6794145
  0.68070194  0.68431601  0.68461175  0.68521013  0.68761842  0.68951325
  0.69339113  0.6947105   0.69500625  0.69780873  0.69941952  0.69990775
  0.70020349  0.70068571  0.70267884  0.70280212  0.70408956  0.70428634
  0.705105    0.70521075  0.70660761  0.70668818  0.70770362  0.71030224
  0.71440349  0.71448405  0.71540008  0.71549949  0.71579524  0.7208114
  0.72238187  0.72410752  0.73368986  0.73725248  0.73848534  0.74108397
  0.74260208  0.74400379  0.74612123  0.74628122  0.74887984  0.75016689
  0.75147846  0.75188023  0.75397167  0.75399652  0.75989128  0.75997185
  0.76444744  0.76447158  0.7647937   0.76707021  0.76993881  0.77065176
  0.77226746  0.77348137  0.77463626  0.77931266  0.77931432  0.78038544
  0.78057084  0.78315659  0.78552971  0.78608764  0.7878592   0.79045782
  0.79106648  0.79635886  0.7982537   0.8007788   0.80345094  0.80604957
  0.80864819  0.81124682  0.81249979  0.81318357  0.81483674  0.81644406
  0.82423994  0.82646936  0.8285098   0.83439205  0.83949428  0.8395893
  0.83983168  0.84036033  0.84165927  0.84998379  0.85306921  0.85871974
  0.86366738  0.86539124  0.86557554  0.87175572  0.88143726  0.89514333
  0.90143986  0.90350984  0.90447433  0.9045549   0.90586608  0.90715352
  0.91494939  0.91509219  0.91596483  0.92006607  0.92014664  0.92097856
  0.92274526  0.92500777  0.93305919  0.93445094  0.93855402  0.94093563
  0.94345369  0.94484544  0.94572524  0.94613288  0.94627568  0.9473635
  0.94744406  0.9487315   0.95004269  0.95191966  0.95264131  0.95419873
  0.95783856  0.95971553  0.96043718  0.96172462  0.96303581  0.96555387
  0.96952049  0.97211912  0.97270865  0.97274234  0.97471774  0.97673272
  0.97862755  0.98122618  0.9838248   0.98642342  0.98821393  0.98902205
  0.99162067  0.99681792  0.99869489  1.00322204  1.00461379  1.00569072
  1.00721242  1.01163284  1.01588809  1.01623171  1.02599232  1.02987838
  1.03539775  1.07276755  1.07362844  1.08139384  1.22725658  1.25929725
  1.27239887  1.3342835   1.39575305]

  UserWarning,

2022-10-31 11:02:03,447:INFO:Calculating mean and std
2022-10-31 11:02:03,447:INFO:Creating metrics dataframe
2022-10-31 11:02:03,447:INFO:Uploading results into container
2022-10-31 11:02:03,447:INFO:Uploading model into container now
2022-10-31 11:02:03,447:INFO:master_model_container: 6
2022-10-31 11:02:03,447:INFO:display_container: 2
2022-10-31 11:02:03,447:INFO:Ridge(random_state=3360)
2022-10-31 11:02:03,447:INFO:create_model() successfully completed......................................
2022-10-31 11:02:03,560:ERROR:create_model() for Ridge(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:02:03,560:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:03,560:INFO:Initializing Elastic Net
2022-10-31 11:02:03,560:INFO:Total runtime is 0.5094940225283305 minutes
2022-10-31 11:02:03,560:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:03,560:INFO:Initializing create_model()
2022-10-31 11:02:03,560:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:03,560:INFO:Checking exceptions
2022-10-31 11:02:03,560:INFO:Importing libraries
2022-10-31 11:02:03,560:INFO:Copying training dataset
2022-10-31 11:02:03,560:INFO:Defining folds
2022-10-31 11:02:03,560:INFO:Declaring metric variables
2022-10-31 11:02:03,560:INFO:Importing untrained model
2022-10-31 11:02:03,560:INFO:Elastic Net Imported successfully
2022-10-31 11:02:03,560:INFO:Starting cross validation
2022-10-31 11:02:03,575:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:05,710:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.86154649 -0.81648859 -0.72173906 -0.47249487 -0.34332216 -0.22677426
 -0.11474304 -0.10780268 -0.09189211 -0.06601807  0.04094484  0.05064935
  0.07021153  0.08143175  0.09059926  0.10410877  0.11131704  0.11185059
  0.12073641  0.12638281  0.13459767  0.14717459  0.16144074  0.16490015
  0.17043741  0.18532754  0.18941445  0.19074497  0.1990364   0.205701
  0.22385668  0.2291259   0.23068891  0.23405091  0.24413522  0.25533312
  0.2577896   0.26009541  0.26051724  0.26199846  0.26222414  0.26541022
  0.26888396  0.27354587  0.27612277  0.30048842  0.30094751  0.3128021
  0.32389346  0.32576891  0.32632419  0.3275027   0.33031128  0.33859556
  0.34258402  0.3467513   0.35032135  0.35057343  0.35225072  0.35568553
  0.35579314  0.35903112  0.37036217  0.37401816  0.37405763  0.37541884
  0.37598539  0.38039625  0.38459315  0.38766983  0.38794091  0.39023799
  0.39675184  0.39770903  0.3988419   0.40023969  0.40211811  0.40511498
  0.40576043  0.40606206  0.40640811  0.41161012  0.41344381  0.41527596
  0.41650048  0.41675523  0.42050992  0.42066001  0.42403118  0.42735438
  0.42908898  0.43396074  0.4340026   0.43688191  0.43817032  0.44046281
  0.44093952  0.44096563  0.44155747  0.44197052  0.44215635  0.44425899
  0.44457487  0.44509687  0.4453443   0.44664821  0.44812846  0.44822485
  0.44875085  0.44913913  0.44999736  0.45352875  0.45403222  0.45496807
  0.4567798   0.45714232  0.45848827  0.45971665  0.46123578  0.4645095
  0.46511449  0.46627635  0.46795074  0.46915332  0.46957419  0.46970984
  0.47145311  0.47229677  0.47253844  0.47264535  0.47300778  0.47511139
  0.47545945  0.47695041  0.47708769  0.4808221   0.48102112  0.48219464
  0.48442296  0.48598267  0.48645795  0.48668076  0.48772822  0.48887037
  0.48902129  0.48938767  0.48980709  0.49062561  0.49179964  0.49184321
  0.49185621  0.49245471  0.49264638  0.49303007  0.49400998  0.49565719
  0.495744    0.49609521  0.49679544  0.49688475  0.49712729  0.49737581
  0.49771916  0.49792678  0.49862759  0.4987453   0.49887641  0.49919584
  0.49922424  0.49970365  0.5000003   0.50021446  0.50026354  0.50066625
  0.50095899  0.50213839  0.50268586  0.50282621  0.50296311  0.50322091
  0.50404485  0.50421786  0.50432223  0.5059541   0.50637653  0.50638745
  0.50765392  0.50802879  0.5097786   0.50991976  0.51036602  0.51117641
  0.51125778  0.5118833   0.51220116  0.51279149  0.51329548  0.51348796
  0.51394984  0.51424958  0.51476386  0.5153525   0.51631293  0.51661633
  0.51668561  0.51798154  0.51817186  0.51909588  0.51961423  0.52082921
  0.52125332  0.52140958  0.52222343  0.52262782  0.52320811  0.52357092
  0.52623563  0.526904    0.52818016  0.53017883  0.53055416  0.53071278
  0.53166399  0.53228636  0.53325218  0.53454405  0.53483917  0.53830775
  0.53836397  0.53846275  0.54022173  0.5422906   0.54229393  0.54235578
  0.54411315  0.54519891  0.54531903  0.54538662  0.54560882  0.54806117
  0.54867181  0.54979561  0.5498592   0.5500196   0.55062693  0.55070485
  0.55119942  0.55245119  0.55288451  0.55370785  0.55476535  0.5553279
  0.55624312  0.55675516  0.55728778  0.55798812  0.55806115  0.55854377
  0.55955661  0.56146137  0.56417349  0.5641969   0.56495572  0.56587078
  0.56689603  0.5699258   0.56994443  0.57009793  0.57101696  0.57108156
  0.57305337  0.57352449  0.57402255  0.5742125   0.57694373  0.57840265
  0.57842985  0.57842991  0.57847268  0.57946998  0.57954183  0.57968212
  0.58004835  0.58009859  0.58012344  0.58112255  0.58121013  0.58132061
  0.5819306   0.58211051  0.58286495  0.58291883  0.58316354  0.58383413
  0.58387782  0.58488416  0.58522458  0.58552915  0.58657639  0.58680315
  0.58802469  0.58977751  0.59042496  0.59199429  0.59239476  0.59248788
  0.59291551  0.5937531   0.59395985  0.59416051  0.59432612  0.5964418
  0.59712539  0.59828929  0.59884744  0.59910053  0.60043555  0.6006029
  0.60124148  0.60191399  0.60230453  0.60287229  0.60369638  0.60503485
  0.60687803  0.60918788  0.61010591  0.61011032  0.61243625  0.61386923
  0.61800085  0.61930723  0.62161318  0.62399494  0.6274493   0.62911777
  0.63324104  0.63421415  0.63861223  0.64266123  0.644313    0.65187265
  0.6651195   0.68065746  0.69787208  0.70413838  0.71568651  0.74043851
  0.74881766  0.75923124  0.82190412  0.91858694  1.27259698]

  UserWarning,

2022-10-31 11:02:05,757:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.30140808 -1.03863051 -0.74996748 -0.68950773 -0.24770218 -0.21903757
 -0.20091772 -0.17517732 -0.16876105 -0.14211709 -0.09523156 -0.0683454
 -0.05355187 -0.04750161 -0.04278179 -0.02782982 -0.00756864  0.00338059
  0.01511101  0.0289957   0.06981368  0.08078171  0.09333595  0.11596134
  0.12029497  0.12707647  0.13587231  0.13886724  0.14916107  0.16989646
  0.1739962   0.17953413  0.19440945  0.19649014  0.20064799  0.2111773
  0.21168322  0.21631011  0.21778111  0.21910272  0.22065967  0.22565038
  0.22779953  0.23529348  0.23665791  0.24367825  0.25085809  0.25617804
  0.25770458  0.25861546  0.27460313  0.27486612  0.27557473  0.27609778
  0.27758624  0.294372    0.29807131  0.2981713   0.3004416   0.30707264
  0.30723029  0.30975009  0.31269202  0.32956827  0.33494957  0.33507843
  0.33615303  0.3373036   0.33975319  0.36175027  0.36364975  0.36863382
  0.37066778  0.3714161   0.37143039  0.37341088  0.37476057  0.3753108
  0.38293997  0.38511978  0.38984528  0.39018226  0.39176755  0.39313737
  0.39470676  0.39648667  0.39839491  0.39867855  0.40045295  0.40113175
  0.40263328  0.40284549  0.40381029  0.40428891  0.41045279  0.41650547
  0.41720959  0.41842612  0.42259505  0.42797518  0.43122725  0.43151933
  0.43194401  0.43263676  0.43408751  0.43686837  0.43877784  0.44227989
  0.4428221   0.44340365  0.44362309  0.44486196  0.4450412   0.446699
  0.44758123  0.44796869  0.45040709  0.45236761  0.45345985  0.45459495
  0.45460575  0.45563533  0.45586291  0.45772037  0.4582495   0.45841307
  0.45857703  0.45987793  0.46173535  0.46177554  0.46261724  0.46263693
  0.46288067  0.46324207  0.46349291  0.46532213  0.46555067  0.46583062
  0.46658306  0.4675554   0.46815374  0.46838716  0.47109541  0.47155913
  0.47255133  0.47283319  0.47303906  0.47819125  0.47860359  0.48168856
  0.48251823  0.48380292  0.48592834  0.48892833  0.48963236  0.49232795
  0.49346783  0.49356487  0.49374101  0.49472912  0.49506652  0.49543642
  0.49618415  0.49630996  0.49733894  0.49843409  0.49856459  0.49880749
  0.49951797  0.49954079  0.49958777  0.50024802  0.5006521   0.5012425
  0.50161977  0.50184475  0.50246364  0.50267454  0.50294172  0.50313265
  0.50369528  0.50388161  0.50481274  0.50503291  0.50553918  0.50592757
  0.50609417  0.50779581  0.50782748  0.50867764  0.51038566  0.51065766
  0.51076047  0.51194759  0.51195517  0.51196924  0.51214932  0.51318368
  0.51391584  0.51514304  0.51606822  0.51672547  0.5173869   0.51753661
  0.51984936  0.52212949  0.52219128  0.52237194  0.5227335   0.52282201
  0.52285408  0.52286205  0.52327813  0.52339791  0.52491058  0.52519173
  0.5253032   0.52538702  0.52577702  0.52980614  0.53011623  0.53211525
  0.5335973   0.53363447  0.53466087  0.53574286  0.53616635  0.53619658
  0.53666714  0.53671147  0.53698531  0.53832764  0.53857863  0.53899022
  0.53904281  0.53917313  0.54076426  0.54346505  0.54397191  0.54529087
  0.54541069  0.54547572  0.54641155  0.5472699   0.54767142  0.54780014
  0.5482634   0.54831854  0.54903468  0.54950648  0.55051636  0.55085937
  0.55088477  0.55219815  0.552776    0.55480094  0.55532815  0.55550933
  0.55567134  0.55601811  0.5568607   0.56048533  0.56206328  0.56755245
  0.56856086  0.57210743  0.572889    0.57545614  0.57555764  0.57650387
  0.57798582  0.57856791  0.57979695  0.58123424  0.58200481  0.58351712
  0.5836697   0.58498875  0.58528476  0.58541664  0.5854623   0.58712519
  0.58773136  0.5879628   0.58830614  0.58852996  0.58875359  0.58877927
  0.5898301   0.59032996  0.59102643  0.59124376  0.59186138  0.59224491
  0.59422489  0.59474847  0.59509279  0.59552557  0.59587414  0.59626762
  0.59632152  0.59699252  0.60058468  0.60185944  0.6034177   0.60470579
  0.60535666  0.605625    0.60705167  0.60767789  0.60794533  0.60835623
  0.60970888  0.61139336  0.61658696  0.61764931  0.61809862  0.61920699
  0.62008297  0.62238563  0.62260737  0.62457117  0.62457575  0.62665105
  0.62856322  0.62913988  0.63077141  0.63103673  0.63440942  0.63785878
  0.64007903  0.64581542  0.64711905  0.65695056  0.66388958  0.66785101
  0.67176299  0.67261524  0.68322778  0.6944131   0.71033187  0.71981711
  0.72301416  0.73491313  0.74712802  0.76404283  0.81012389  0.81022089
  0.82357152  0.83787294  0.99596379  1.02660209  1.26722889  1.76376347]

  UserWarning,

2022-10-31 11:02:05,788:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.87096716 -0.61299648 -0.47819463 -0.24253601 -0.21631656 -0.14014632
 -0.08806319 -0.06695921 -0.00979282  0.01755776  0.05606274  0.05932075
  0.06216485  0.07193952  0.07227453  0.10760293  0.11677208  0.11773978
  0.12396667  0.15612548  0.20783189  0.21636065  0.22030458  0.23454011
  0.24239874  0.24366061  0.24473166  0.27004067  0.27578383  0.28198421
  0.28788968  0.29600179  0.29841753  0.30333652  0.30611403  0.30626391
  0.30920973  0.31531761  0.31646761  0.33035806  0.33155426  0.33380527
  0.33511024  0.34296805  0.34304803  0.34526578  0.34934641  0.35182794
  0.35277686  0.35429799  0.3565014   0.35685614  0.36955274  0.37066991
  0.37072109  0.37275467  0.37275525  0.37510689  0.38008454  0.38400378
  0.38722263  0.39386841  0.39476352  0.39962047  0.40211681  0.40312467
  0.40400757  0.40429929  0.40433438  0.40491552  0.40565273  0.40870636
  0.41195898  0.4136477   0.41374309  0.41423398  0.41436886  0.41529103
  0.41601267  0.4163802   0.41865589  0.41950843  0.42112061  0.42257828
  0.42472294  0.42521956  0.42662528  0.42875617  0.43096595  0.43233941
  0.43422333  0.43774981  0.43929709  0.43956068  0.44007665  0.44545507
  0.44582197  0.44699971  0.44749356  0.44753221  0.44803174  0.44884448
  0.4488869   0.44963731  0.44998752  0.45004844  0.45447305  0.45701798
  0.45761048  0.45846337  0.45928643  0.459609    0.46044468  0.46444254
  0.46504736  0.46519847  0.4660111   0.46726671  0.46735869  0.467435
  0.46780055  0.47141379  0.47156124  0.47176583  0.47188123  0.47344817
  0.4740106   0.47452117  0.47532698  0.4768625   0.47722134  0.47859604
  0.47981595  0.47999373  0.48010115  0.48067355  0.48098438  0.48130249
  0.48155824  0.48203726  0.48257169  0.48326285  0.48412878  0.48538629
  0.48777806  0.49016006  0.49080032  0.49081289  0.49086325  0.49362943
  0.49713589  0.49715097  0.49759811  0.49786815  0.49787827  0.49798909
  0.49839344  0.49879613  0.49924528  0.49989049  0.50099564  0.50303877
  0.50309664  0.5035935   0.50365133  0.50411453  0.5044479   0.50444935
  0.50481457  0.50526262  0.50549477  0.50561805  0.50619316  0.50652818
  0.50711136  0.50739099  0.50840578  0.50844127  0.5088104   0.50930553
  0.50993156  0.51000918  0.51007337  0.51047587  0.51059532  0.51158166
  0.51227293  0.51243708  0.51291112  0.51319724  0.51377612  0.51445665
  0.51456437  0.51465277  0.51493057  0.51550776  0.51608265  0.51740444
  0.51773508  0.51847315  0.51904798  0.51992797  0.52078399  0.52098706
  0.52435395  0.5245992   0.5249298   0.52573865  0.52946561  0.53067117
  0.53118997  0.53131592  0.53183077  0.53224585  0.53394373  0.53424461
  0.53458971  0.53552837  0.53666992  0.53697194  0.53705605  0.53707117
  0.53835199  0.53894158  0.53947839  0.53962344  0.53975012  0.54028712
  0.54040562  0.54092664  0.54098497  0.54141976  0.54228171  0.54335248
  0.54430142  0.54465181  0.54467765  0.5455849   0.54578701  0.54694414
  0.54730215  0.54764812  0.54843552  0.54947763  0.55008096  0.55135903
  0.55251507  0.55269726  0.55318435  0.55417988  0.55454922  0.55469106
  0.55484073  0.55520152  0.55699575  0.55771203  0.55933921  0.56135261
  0.56241833  0.56256155  0.56388584  0.56471127  0.56493371  0.56566302
  0.56586989  0.56754608  0.56923592  0.56988574  0.57239786  0.5740547
  0.57409343  0.57445554  0.57454923  0.57460388  0.57520773  0.57563052
  0.57591826  0.57638965  0.57796063  0.57868799  0.57979983  0.58118416
  0.58159781  0.58186045  0.58593891  0.58601301  0.58675249  0.58753174
  0.58799347  0.58805487  0.58983237  0.58997194  0.59065367  0.59079881
  0.59115066  0.59192885  0.59234309  0.59262244  0.59262544  0.59335627
  0.59367551  0.59402455  0.59415663  0.59476893  0.59499855  0.59542411
  0.59591506  0.59626199  0.59650031  0.59659594  0.59664718  0.59696272
  0.5971812   0.59725573  0.59743292  0.59875116  0.59911918  0.59933764
  0.59993177  0.60064246  0.60071937  0.60112484  0.60120056  0.60142355
  0.60204422  0.60232938  0.60271279  0.60316005  0.60377118  0.60477718
  0.60481114  0.60589446  0.6059931   0.60612686  0.60663396  0.60676533
  0.60712071  0.607202    0.60773734  0.60803585  0.6105187   0.61089898
  0.61276028  0.61302488  0.61414726  0.61497786  0.61896568  0.62012919
  0.62165775  0.62287594  0.6240454   0.62531719  0.62898481  0.63017621
  0.63643166  0.63900627  0.6401099   0.66635327  0.66876499  0.66912573
  0.67192201  0.6759558   0.67655122  0.68654697  0.69198094  0.69310986
  0.6979568   0.7094756   0.72121573  0.73208745  0.74451955  0.80281516
  0.81725955  0.85638751  0.87270516  0.87928934  0.89403536  0.99131815
  1.09360982  1.34878432  1.65875232]

  UserWarning,

2022-10-31 11:02:05,788:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.33441862 -1.15704122 -0.91759268 -0.54405835 -0.49366919 -0.23859717
 -0.19629451 -0.13832442 -0.12927736 -0.11795282 -0.11504883 -0.1064084
 -0.10130799 -0.04352279 -0.03725236 -0.02575491 -0.00281496  0.05062781
  0.06192732  0.07648884  0.07692548  0.0975952   0.10012893  0.10464642
  0.10952785  0.11119514  0.11153239  0.12037678  0.12956653  0.13859271
  0.14049523  0.14080755  0.14548941  0.15381507  0.15436184  0.15838483
  0.1952452   0.1954057   0.20076871  0.20581733  0.21032894  0.21902309
  0.22170095  0.22511505  0.22586269  0.24244737  0.24658435  0.25347127
  0.25967583  0.26337132  0.26982405  0.27006736  0.27009243  0.27045587
  0.27404163  0.28432554  0.2898743   0.29112521  0.30209073  0.31034013
  0.3147914   0.31519848  0.3166344   0.31806386  0.31813936  0.32183061
  0.33318999  0.33548546  0.33591682  0.33640834  0.33848501  0.33998253
  0.34739639  0.35199426  0.35393107  0.35817327  0.35961971  0.3696981
  0.37027046  0.3706287   0.37083329  0.3749643   0.37942653  0.3802958
  0.38073803  0.38494261  0.38555451  0.38627696  0.38985772  0.39008681
  0.39343478  0.39477339  0.39638046  0.39652616  0.39660016  0.3987869
  0.40385398  0.40916254  0.40918645  0.41042842  0.41130767  0.41251005
  0.41923889  0.41982174  0.42102719  0.42446505  0.4283035   0.42994007
  0.43127424  0.43157734  0.43224015  0.43393666  0.43467982  0.43756991
  0.43892452  0.44031914  0.4404326   0.44237795  0.44635998  0.44757695
  0.4477225   0.44801454  0.44916555  0.44939431  0.45142516  0.45349615
  0.45453047  0.4550576   0.45559651  0.45694598  0.45753602  0.4597462
  0.46003859  0.46034778  0.46061111  0.46111683  0.46127912  0.4648259
  0.46601294  0.46664804  0.46730359  0.46742299  0.46743224  0.46754354
  0.46883301  0.46914153  0.47041465  0.47185305  0.4780231   0.47848076
  0.48069722  0.48224153  0.482823    0.48287771  0.48316483  0.48378897
  0.48649581  0.48684602  0.48684932  0.48739482  0.48744873  0.48757303
  0.48859336  0.48897907  0.489255    0.48964717  0.49001652  0.49105953
  0.49152189  0.49230375  0.49247385  0.49336756  0.49410723  0.49445656
  0.49447491  0.49522287  0.4955366   0.49575155  0.49688856  0.49713862
  0.49715755  0.49763572  0.49817947  0.49835262  0.49878633  0.49903122
  0.49987716  0.50052958  0.50187757  0.50292477  0.50348269  0.50364906
  0.50382413  0.50423302  0.50434612  0.50453522  0.50528841  0.50529914
  0.50587385  0.50611664  0.50621436  0.50641183  0.50642184  0.50652623
  0.50668703  0.50722156  0.50822401  0.5082517   0.50854165  0.50886183
  0.50886908  0.50923248  0.50971041  0.51010899  0.51069466  0.51118716
  0.51196394  0.51422983  0.51562414  0.5178712   0.51836194  0.51844761
  0.52009824  0.52109832  0.52327286  0.523307    0.52345495  0.52427486
  0.52571271  0.52663991  0.52875005  0.52888602  0.53082332  0.53144877
  0.53156063  0.53321871  0.53399926  0.53408107  0.53473186  0.53514994
  0.53585079  0.53657457  0.5366088   0.53807573  0.53862193  0.53999027
  0.54031722  0.54117095  0.54167944  0.54176969  0.54210257  0.54241062
  0.54271452  0.54410853  0.54498739  0.54522183  0.54596761  0.54812813
  0.5481324   0.54960927  0.5498477   0.5514052   0.5531915   0.55575307
  0.55785138  0.55995829  0.56068824  0.56185582  0.56396906  0.56506127
  0.56546083  0.56744518  0.5676132   0.56888661  0.56897893  0.57021952
  0.57109361  0.57222962  0.5722424   0.57224627  0.57321039  0.5748922
  0.57580903  0.57652709  0.57653208  0.57667784  0.57980145  0.58044967
  0.58087585  0.58180127  0.58188502  0.58318885  0.58367222  0.58746324
  0.58878957  0.58901598  0.59020732  0.59041769  0.59190181  0.59192407
  0.59203652  0.59207572  0.59275703  0.59413859  0.59478197  0.5958498
  0.5963337   0.59633987  0.5982853   0.59859913  0.60149796  0.60230414
  0.60340527  0.60371903  0.60402756  0.60591418  0.60630956  0.60679455
  0.60878571  0.60988044  0.61247925  0.62123864  0.62234483  0.62467926
  0.62760388  0.63092361  0.63259295  0.65250969  0.67238643  0.69199364
  0.71376794  0.7320786   0.76331468  0.76636344  0.79277738  0.80965239
  0.81994568  0.85035874  0.87823802  0.98919653  1.07842122]

  UserWarning,

2022-10-31 11:02:05,895:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.72760046 -1.0583233  -0.51340546 -0.36788487 -0.35279819 -0.26955828
 -0.26731555 -0.19493308 -0.17722806 -0.17146413 -0.07949312 -0.04678985
 -0.03441213 -0.03406773  0.00727663  0.02004619  0.06054585  0.06942281
  0.09874534  0.12223442  0.131219    0.13695112  0.13851479  0.15108301
  0.15120542  0.17523191  0.20777597  0.21816513  0.21867828  0.22738784
  0.23902937  0.25602388  0.26268421  0.26635309  0.2772811   0.28867029
  0.29467726  0.30911953  0.31165049  0.31663487  0.32141975  0.32288787
  0.32591547  0.33768812  0.35022539  0.35214598  0.35591438  0.35877391
  0.35981507  0.36734491  0.36982124  0.37260007  0.37283398  0.37285923
  0.37352338  0.37777085  0.38104594  0.38351516  0.38546517  0.38674801
  0.38791302  0.38856382  0.3892196   0.39911667  0.39931607  0.39971608
  0.4018208   0.40411768  0.40416999  0.40500327  0.4080848   0.41112088
  0.41271911  0.41313447  0.41463576  0.42105348  0.42254512  0.42304969
  0.42326432  0.42429283  0.42477825  0.42835157  0.42840011  0.42937148
  0.42964923  0.4303832   0.4311725   0.43398188  0.43600245  0.43800518
  0.43859822  0.44105418  0.44302324  0.44350618  0.44354542  0.44389222
  0.44457743  0.44491074  0.4451919   0.44594441  0.44891705  0.44926583
  0.44930089  0.45081986  0.45548318  0.45716311  0.45891095  0.45954137
  0.46069334  0.46089033  0.46119184  0.46143156  0.46179287  0.46219396
  0.46241727  0.46368826  0.46591947  0.46672941  0.46790946  0.4683165
  0.46868165  0.46902657  0.46940868  0.47020104  0.47103351  0.47149413
  0.47204202  0.47240519  0.47350221  0.47451056  0.4745311   0.47637331
  0.4769307   0.47757822  0.47850013  0.47924438  0.48081525  0.48101529
  0.48134466  0.48174471  0.4833882   0.48348127  0.48638788  0.48709713
  0.4882351   0.48861383  0.48866397  0.48872045  0.4896067   0.49021097
  0.49072975  0.49223514  0.49441973  0.49514018  0.49551931  0.49617231
  0.49729925  0.49814357  0.49880389  0.49979893  0.5000737   0.50043391
  0.50047422  0.50152609  0.50172033  0.50220423  0.50258657  0.50271456
  0.50390884  0.50406677  0.50429948  0.50470848  0.50607868  0.50667571
  0.50681076  0.50727761  0.50735971  0.5080567   0.50841398  0.50957484
  0.50984906  0.5111275   0.51133312  0.51152028  0.51170601  0.5118975
  0.51267591  0.51417797  0.5143083   0.51455213  0.51505563  0.51576122
  0.51588646  0.51626896  0.51658562  0.51680597  0.51693994  0.51712072
  0.51810049  0.51840288  0.51905052  0.51983776  0.51999478  0.52321509
  0.52333914  0.52446165  0.52526892  0.52554746  0.52560918  0.52620635
  0.52829666  0.52903524  0.52954968  0.53030858  0.53160579  0.5324087
  0.53255694  0.53337856  0.53548344  0.53574414  0.53621783  0.53723233
  0.53817931  0.53837522  0.53892428  0.54057988  0.54255673  0.54365175
  0.54472839  0.54509844  0.54568436  0.54596587  0.5460298   0.54672871
  0.54836343  0.54856457  0.54914818  0.54969917  0.55033821  0.55130189
  0.5513661   0.55220932  0.55295153  0.55379241  0.55382296  0.55415129
  0.55479031  0.55535113  0.55691527  0.55853633  0.55992745  0.56000186
  0.56005193  0.56095902  0.56224184  0.56331035  0.56683445  0.56701088
  0.5679455   0.56814059  0.56821083  0.56869702  0.56872767  0.5691574
  0.57165198  0.5736976   0.57398589  0.57769821  0.57883562  0.57924996
  0.57946741  0.57968306  0.57994947  0.58050684  0.58082077  0.58190894
  0.58195907  0.58212149  0.58291896  0.58366802  0.58380921  0.58432575
  0.584453    0.58531561  0.5855826   0.58579411  0.58587338  0.585978
  0.58603786  0.58610737  0.58643969  0.58707844  0.58741559  0.58792664
  0.58804733  0.58815918  0.58843569  0.58863028  0.58895647  0.58903239
  0.58915043  0.58926669  0.58960938  0.59067533  0.59089076  0.59103988
  0.59190718  0.59242534  0.59266412  0.59291968  0.59308452  0.59324772
  0.59576222  0.59577463  0.59746326  0.59820853  0.5989875   0.60035295
  0.60036837  0.60061624  0.60062233  0.60100171  0.60146545  0.60305362
  0.60431672  0.60518704  0.60520409  0.60609354  0.60682545  0.60888282
  0.60936547  0.61008477  0.6103317   0.61180151  0.61429908  0.61519682
  0.61573198  0.61696016  0.61836504  0.61877773  0.62436334  0.62461348
  0.6312323   0.63637051  0.63790668  0.64335675  0.65442907  0.65482042
  0.66153079  0.67310504  0.67958908  0.68037719  0.68296699  0.6843881
  0.69237693  0.70790385  0.70948745  0.71309618  0.73336632  0.74080063
  0.74662566  0.7695282   0.79403056  0.80646006  0.86526844  0.8784359
  0.92052616  0.99559763  1.05268225  1.0586921   1.1797896   1.2752117 ]

  UserWarning,

2022-10-31 11:02:05,895:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.06415027 -0.94743646 -0.92605756 -0.88976996 -0.86471327 -0.81879129
 -0.55416125 -0.49288984 -0.49235448 -0.47497677 -0.45445063 -0.43279492
 -0.42013284 -0.40194082 -0.38409484 -0.34578871 -0.32243323 -0.31676064
 -0.29502316 -0.28924827 -0.1916609  -0.15995478 -0.15973152 -0.13174758
 -0.09476444 -0.00310019  0.00552903  0.023122    0.03283247  0.04497375
  0.06590818  0.07044796  0.08128118  0.08151031  0.11652568  0.13699685
  0.1410888   0.15050525  0.16261642  0.18200604  0.19246831  0.1952326
  0.19907386  0.20398854  0.21417787  0.22300358  0.22356073  0.22665391
  0.24525834  0.24881764  0.25162356  0.25293203  0.25324757  0.26052494
  0.26531681  0.26718194  0.27698091  0.27948847  0.28191218  0.29038493
  0.29584996  0.29976277  0.30640077  0.31695674  0.31796297  0.31864553
  0.32728021  0.33183006  0.33268358  0.34389263  0.34692176  0.34843866
  0.35148384  0.35300493  0.36040508  0.36278622  0.36284179  0.36468603
  0.36563021  0.37079699  0.37251767  0.37577062  0.38220572  0.38489225
  0.39177211  0.39248632  0.39329837  0.39339244  0.39389915  0.39566082
  0.39613426  0.39737621  0.3998156   0.400841    0.40233209  0.40471724
  0.40706422  0.40831263  0.40958113  0.41058588  0.41231459  0.41265052
  0.41509822  0.41671207  0.41900591  0.42229291  0.42276313  0.42548788
  0.4257012   0.42670702  0.42832131  0.4295916   0.42992997  0.43318091
  0.43774957  0.43902634  0.43978063  0.44065128  0.44247543  0.44253205
  0.44310945  0.44711493  0.44720217  0.44798913  0.44807066  0.4488605
  0.4489495   0.44907432  0.45031811  0.45159878  0.45481829  0.45693771
  0.45828839  0.45863133  0.45933331  0.46039096  0.46094373  0.46380546
  0.46395045  0.46578592  0.46615068  0.46698086  0.46707647  0.46854377
  0.46895883  0.46920259  0.46996256  0.470221    0.47031643  0.47035885
  0.47061738  0.47108931  0.47269912  0.47480065  0.47491776  0.47566779
  0.47818237  0.47932954  0.47933887  0.48006497  0.48357186  0.48477829
  0.48596799  0.4868175   0.48736084  0.48739424  0.48808919  0.48834626
  0.48854936  0.48859743  0.48864373  0.48967783  0.48988735  0.49138764
  0.49202972  0.49212782  0.49224793  0.49231376  0.49251248  0.49259247
  0.49273866  0.49328334  0.4933286   0.49341752  0.49380775  0.49445819
  0.49466099  0.49500787  0.49523037  0.49527112  0.49550095  0.49698281
  0.49840137  0.49951979  0.5001542   0.50057961  0.50083332  0.50105498
  0.50309658  0.50382303  0.5039463   0.50442789  0.50564664  0.50643495
  0.50674303  0.50681131  0.50753911  0.50766985  0.50835961  0.50928264
  0.50929416  0.51023761  0.51100279  0.51115172  0.51159313  0.51329807
  0.51370911  0.51438774  0.51473231  0.51573241  0.51737427  0.51752071
  0.51752535  0.51866813  0.52031244  0.520723    0.52087489  0.52366079
  0.52391192  0.52429837  0.52644718  0.52851427  0.52923888  0.52935857
  0.52965106  0.53198383  0.53240805  0.53505694  0.53770565  0.53849326
  0.53903855  0.53932415  0.53945526  0.53949839  0.54001958  0.54008622
  0.5401793   0.54022435  0.5427621   0.54663781  0.54672602  0.5468891
  0.54733576  0.54826832  0.55060391  0.55060413  0.5507264   0.55114647
  0.55165892  0.55189694  0.55196823  0.55239411  0.55516656  0.55580689
  0.55646343  0.55735507  0.55760149  0.55921695  0.56320458  0.56328235
  0.56367791  0.56615058  0.56747236  0.56799131  0.56932845  0.5696369
  0.57026615  0.57083341  0.57135192  0.5728945   0.57447488  0.57507512
  0.57509554  0.57631191  0.57721741  0.57728037  0.57736672  0.57911658
  0.57965883  0.57986494  0.58156377  0.58242711  0.58453079  0.5855316
  0.58578699  0.5861217   0.58673711  0.58822652  0.58891577  0.58955613
  0.58956171  0.5899917   0.59154385  0.592368    0.59297787  0.59328095
  0.59340284  0.59379277  0.59416085  0.59417202  0.59423301  0.59466137
  0.59473777  0.59480273  0.59488779  0.59551846  0.59580611  0.59591628
  0.59617731  0.59702564  0.59725769  0.59865343  0.59902088  0.59919196
  0.60020911  0.60169611  0.60182902  0.60204305  0.60209154  0.60318108
  0.60545209  0.60577878  0.60581609  0.60647351  0.60763596  0.60923199
  0.60970858  0.6102888   0.61081973  0.61114234  0.61283877  0.6130309
  0.61419497  0.61682333  0.61954487  0.61981879  0.62041088  0.6208977
  0.62268182  0.62711806  0.63117853  0.63128041  0.63179362  0.63230217
  0.63320463  0.63447768  0.63712189  0.64534525  0.64929244  0.64988509
  0.65214894  0.67319402  0.67585983  0.68038778  0.69590317  0.69895188
  0.69971452  0.70563546  0.7260924   0.7270484   0.74348496  0.78298838
  0.88874538  0.9010531   0.90696563]

  UserWarning,

2022-10-31 11:02:05,910:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.94364155 -0.63193962 -0.51804704 -0.51605051 -0.47643197 -0.47604934
 -0.47315726 -0.28555538 -0.19872631 -0.18459889 -0.13965434 -0.13014298
 -0.11484936 -0.07299953 -0.0619606  -0.03431962 -0.01685652 -0.00627428
  0.02303906  0.03761404  0.03796543  0.04000949  0.0629487   0.06659916
  0.11074085  0.11737354  0.12474909  0.1276645   0.12831014  0.15201848
  0.16868871  0.19395116  0.20281909  0.2111219   0.21614961  0.2356776
  0.23840881  0.24273359  0.24940953  0.25254242  0.25910441  0.2598348
  0.26077537  0.26134086  0.26363506  0.26533886  0.2681949   0.26972834
  0.27206799  0.28080576  0.28826403  0.28924996  0.29446585  0.29840146
  0.29987781  0.30746093  0.30910618  0.3154185   0.32191756  0.3226231
  0.32807486  0.3348927   0.33820582  0.35083288  0.35097239  0.35242809
  0.35541631  0.35544853  0.35730288  0.3578396   0.36553671  0.36778481
  0.37182646  0.37456812  0.37733097  0.3809118   0.38336913  0.3835866
  0.38386166  0.38664498  0.38771941  0.38893883  0.3891456   0.3894009
  0.38965463  0.39397635  0.39830792  0.39931254  0.401367    0.40517558
  0.40547691  0.40665769  0.40738527  0.40867282  0.41145595  0.41253868
  0.41423906  0.41467965  0.41699578  0.41786086  0.41787731  0.41827181
  0.42629742  0.42891379  0.42894042  0.43094024  0.43107234  0.436606
  0.43698577  0.4387934   0.43979593  0.44244323  0.44323084  0.44370243
  0.44422978  0.4466054   0.44771264  0.44907273  0.45026409  0.45136638
  0.4539384   0.45520851  0.4552858   0.45680157  0.45706618  0.45726191
  0.45753586  0.45989446  0.46219384  0.46485261  0.46511671  0.46568173
  0.46634863  0.46639774  0.46775097  0.47010072  0.47028715  0.47037309
  0.47146517  0.47224463  0.4731543   0.47399255  0.47441275  0.47559917
  0.47664636  0.47723565  0.47792677  0.47818082  0.47882482  0.48088755
  0.48095811  0.48131872  0.48419239  0.48591229  0.48610001  0.48732253
  0.4874935   0.48783505  0.48818602  0.4891727   0.48983635  0.49047
  0.49054162  0.49090025  0.49134798  0.49150812  0.49205004  0.49223064
  0.49252656  0.49256316  0.49266675  0.49369874  0.49398559  0.49422499
  0.49444793  0.49468553  0.49592835  0.49681936  0.49700083  0.49760583
  0.49767753  0.49900843  0.49901773  0.49920035  0.49944681  0.50031693
  0.50130309  0.50195481  0.50218651  0.50233344  0.50252117  0.5041378
  0.50511542  0.50514423  0.50531609  0.50553639  0.50564774  0.50569415
  0.50615211  0.50679974  0.50715814  0.5073049   0.50924147  0.50939041
  0.50974701  0.50977116  0.51000226  0.5103659   0.5105661   0.51081052
  0.51090503  0.51171977  0.51296461  0.51474071  0.51476627  0.51540232
  0.51546235  0.51602677  0.51605857  0.5167361   0.51686442  0.51719023
  0.51744124  0.51752722  0.51953776  0.52087645  0.52147115  0.52183024
  0.52206888  0.5233017   0.52496437  0.52534784  0.5258641   0.52595892
  0.52657091  0.52836048  0.529502    0.53126961  0.53175698  0.53433015
  0.53519093  0.53524531  0.53541757  0.53686207  0.53728105  0.53750004
  0.54304454  0.54310801  0.543203    0.54324458  0.5435271   0.54449326
  0.54708294  0.54767797  0.54837367  0.54899994  0.55192115  0.55297426
  0.55331475  0.55688574  0.56090436  0.56105742  0.56232284  0.56254868
  0.563145    0.56489381  0.56516567  0.56597314  0.56683842  0.56707597
  0.57072468  0.57155569  0.57413496  0.57497383  0.57533517  0.57594505
  0.57769814  0.57791281  0.57965385  0.58076549  0.58267094  0.58333412
  0.58359013  0.58359614  0.58414044  0.58592164  0.58609814  0.58614228
  0.58673637  0.58683742  0.58718956  0.58719471  0.58721138  0.58776973
  0.58794798  0.5882527   0.58831997  0.58856868  0.5889409   0.58906608
  0.58911517  0.58997483  0.59065856  0.59114506  0.59115177  0.59146723
  0.59154507  0.59178699  0.59243559  0.59255656  0.59302298  0.59333011
  0.59346513  0.59349414  0.59454045  0.59881442  0.59979497  0.60025499
  0.60258755  0.60289712  0.6037481   0.60380658  0.60385593  0.60500227
  0.6064139   0.60671029  0.6068989   0.60729924  0.60740697  0.60835374
  0.61238768  0.61488211  0.61596411  0.61697228  0.61710527  0.61726917
  0.61804252  0.6202217   0.63073557  0.63274018  0.63666542  0.6386181
  0.64144374  0.64175761  0.64245634  0.64459056  0.64639255  0.64947774
  0.65000501  0.65374907  0.66023669  0.6652143   0.67335785  0.6753152
  0.6784793   0.67879455  0.68311944  0.69840283  0.70120418  0.70684278
  0.73196319  0.75439611  0.75617496  0.75934746  0.83541662  0.84530561
  0.9143831   1.08244343]

  UserWarning,

2022-10-31 11:02:05,926:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.88856143 -0.83142376 -0.82236622 -0.31442294 -0.26261078 -0.25207462
 -0.22714211 -0.19919461 -0.15736817 -0.13922429 -0.11202346 -0.07929376
 -0.06375389 -0.05481096  0.01945674  0.02423353  0.04461794  0.0468159
  0.05444938  0.05846466  0.06234651  0.06253592  0.07013791  0.0939489
  0.10308853  0.10931375  0.12897557  0.13817381  0.16904994  0.18594118
  0.19020061  0.19355149  0.19560554  0.20706634  0.2220258   0.22295179
  0.23037838  0.2566969   0.26567816  0.26715539  0.27024305  0.27550676
  0.28123633  0.28414192  0.28999014  0.30740282  0.31414789  0.32132855
  0.32402099  0.32547596  0.326535    0.33399763  0.33846761  0.35232562
  0.3530261   0.3573328   0.35766612  0.36119996  0.36269583  0.37895857
  0.38088342  0.38124195  0.38199359  0.3834097   0.38812001  0.39846618
  0.39939669  0.39993059  0.40373774  0.40591941  0.40839944  0.40910902
  0.41014219  0.41101161  0.41615077  0.41660386  0.42052135  0.42138667
  0.42198797  0.42343225  0.42399214  0.42440613  0.42662418  0.43023238
  0.43372241  0.43410034  0.43561707  0.4363247   0.4367879   0.43696321
  0.43860489  0.43935676  0.44023481  0.4418729   0.44206251  0.44298588
  0.44336809  0.44724796  0.44729118  0.44733352  0.44742194  0.44769181
  0.44797601  0.4488956   0.44891614  0.44916582  0.45026756  0.45039477
  0.45106624  0.45335898  0.45369776  0.45493729  0.4558494   0.46047922
  0.46085638  0.46091894  0.46115075  0.46118432  0.46192657  0.46477341
  0.46499515  0.46660306  0.46998556  0.47490901  0.47606357  0.4785984
  0.48284697  0.48338305  0.48406873  0.48458764  0.48591349  0.48692618
  0.48696126  0.48708127  0.48782949  0.488291    0.48969338  0.48992605
  0.4912623   0.49133471  0.49377425  0.49382602  0.49430137  0.49432549
  0.49496106  0.49524008  0.49544919  0.49622205  0.49791461  0.49842178
  0.49863608  0.4989181   0.49905603  0.49918614  0.49933355  0.49980237
  0.50017497  0.50087848  0.50262633  0.50280335  0.50282116  0.50305818
  0.50332954  0.50337626  0.50359702  0.50361789  0.5040072   0.50413627
  0.50425471  0.50473851  0.50480364  0.50501009  0.50550299  0.50554605
  0.50814687  0.50888036  0.50974969  0.50978797  0.51026993  0.5118772
  0.51218585  0.51355268  0.51525334  0.51609443  0.51716342  0.51944633
  0.52021975  0.5202757   0.52071688  0.52211936  0.52215391  0.52225685
  0.52322907  0.52323495  0.52386858  0.52420276  0.52440177  0.52467141
  0.52521587  0.5258982   0.52603461  0.52804277  0.52804606  0.52875207
  0.53023138  0.53060499  0.53097797  0.53279668  0.53330897  0.53392739
  0.53527025  0.53618901  0.53631647  0.53659271  0.53688719  0.53774358
  0.53866137  0.53949566  0.54134855  0.54159013  0.54170428  0.54284996
  0.54378276  0.54385573  0.54412483  0.54484365  0.54494313  0.54578295
  0.54644633  0.54679459  0.54730147  0.54732921  0.54801303  0.54862516
  0.5487562   0.54938154  0.5510053   0.55255928  0.55314638  0.55555217
  0.55601089  0.55775551  0.55806152  0.55820798  0.55890482  0.55894407
  0.55946362  0.560342    0.56042319  0.5605751   0.56060743  0.56100509
  0.56197395  0.56258084  0.56348967  0.56531417  0.56633945  0.56732692
  0.56888603  0.57107679  0.57117144  0.57136671  0.57184328  0.57187833
  0.57267016  0.57350752  0.5743595   0.57500371  0.57592011  0.57614484
  0.57685597  0.57704071  0.57846039  0.5785921   0.57890575  0.5804478
  0.58187624  0.58202966  0.58308079  0.58382269  0.58451618  0.58560917
  0.58576911  0.58589834  0.58642743  0.58720108  0.58720894  0.58731649
  0.58857914  0.58899846  0.59017942  0.5906756   0.59077231  0.59131607
  0.59252623  0.59373868  0.59403613  0.59413602  0.59424554  0.59488846
  0.59514454  0.59588419  0.5964277   0.59662651  0.59670801  0.597052
  0.59709539  0.59738755  0.59745096  0.59748473  0.59761262  0.5979115
  0.59904894  0.59909783  0.60059855  0.60065536  0.60127184  0.60208531
  0.60243116  0.60260551  0.60539834  0.60548449  0.60787908  0.60811282
  0.60819686  0.60987878  0.61175989  0.61246074  0.61258101  0.6139837
  0.6161777   0.61620221  0.61621553  0.6162511   0.61714438  0.6179935
  0.62328569  0.6282726   0.62945452  0.6321051   0.63321954  0.64286644
  0.65037257  0.65331767  0.66085202  0.66745398  0.67020468  0.67040511
  0.68267477  0.68571681  0.69852411  0.70062434  0.70978869  0.71949697
  0.72480884  0.75385692  0.7628412   0.90998016  0.97056526  1.30164519
  1.50429955]

  UserWarning,

2022-10-31 11:02:06,726:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.45609442 -1.2307927  -0.93497037 -0.43354278 -0.4252861  -0.42399523
 -0.41933935 -0.36476724 -0.36157305 -0.33547293 -0.33126972 -0.25067909
 -0.13805964 -0.12087232 -0.106176   -0.06587921 -0.05230342 -0.04666053
 -0.01673473  0.04182439  0.06634848  0.10003128  0.10923891  0.11919519
  0.12113729  0.12526394  0.1557757   0.15646276  0.17101677  0.1802208
  0.18793279  0.19155514  0.20015945  0.20213345  0.20246997  0.22687402
  0.22860383  0.23016658  0.23082711  0.24087417  0.2471835   0.2483081
  0.25778406  0.26667276  0.26811226  0.27899344  0.27927673  0.28538999
  0.29261905  0.29862293  0.30371609  0.3049619   0.31615096  0.32172346
  0.32334584  0.3244124   0.32467685  0.32639488  0.33486572  0.34175005
  0.34484129  0.3502524   0.35462279  0.35545723  0.35884832  0.36156526
  0.36424715  0.36687859  0.37719577  0.38174542  0.38182548  0.38484058
  0.38661778  0.38700753  0.39207801  0.39468045  0.39675568  0.3968631
  0.39991811  0.40160542  0.40413345  0.40480001  0.40679878  0.4098805
  0.41166636  0.41350958  0.41406149  0.4221292   0.42408898  0.4257903
  0.42643551  0.42795095  0.42797855  0.42847019  0.42921935  0.43048822
  0.43096364  0.43140651  0.43168697  0.43760837  0.43946316  0.43961195
  0.44216547  0.44235418  0.44239635  0.44610526  0.44711921  0.44891194
  0.44993961  0.45009494  0.45033492  0.45320632  0.45332543  0.45571848
  0.45668959  0.45706158  0.45767286  0.45908388  0.46058025  0.46066922
  0.46076892  0.46131974  0.46336248  0.46374155  0.46521873  0.46578043
  0.46618529  0.46689814  0.46727357  0.46874551  0.46875089  0.46889775
  0.47031998  0.47041029  0.47302831  0.4739516   0.47435736  0.47510363
  0.47519971  0.47649653  0.4782517   0.48042308  0.48353344  0.48436182
  0.48455273  0.48485565  0.48555316  0.48642327  0.48643847  0.48845514
  0.48994121  0.49112035  0.49136873  0.49182403  0.49316945  0.49320663
  0.49337929  0.49469956  0.49488068  0.49488771  0.49508617  0.496136
  0.49692573  0.49716231  0.49760987  0.49905619  0.49964836  0.50018644
  0.50047895  0.50201759  0.50220753  0.50222622  0.50288459  0.50334946
  0.50383118  0.50388732  0.50427521  0.50516272  0.50521274  0.50565702
  0.50652728  0.50780396  0.50867241  0.50922257  0.5093431   0.5105671
  0.51067548  0.5108296   0.51105402  0.51109111  0.51112417  0.51465062
  0.51570389  0.51726466  0.51743793  0.51746171  0.51762117  0.51843459
  0.51953972  0.51956987  0.52079443  0.52160983  0.52281295  0.52375695
  0.52552253  0.52714406  0.52871218  0.52877678  0.52901521  0.52918872
  0.5306732   0.53236661  0.53309795  0.53315576  0.53317474  0.53346295
  0.53426406  0.53522249  0.53706953  0.53949616  0.54120031  0.54191259
  0.54215693  0.54220491  0.54280925  0.54281552  0.54290352  0.54332546
  0.5434449   0.54359905  0.54370643  0.54384458  0.54408226  0.5452533
  0.54540928  0.54570543  0.5461906   0.54621649  0.54669965  0.54757894
  0.54799585  0.55115585  0.55177715  0.55324815  0.55339513  0.55649404
  0.55901058  0.55932155  0.55958479  0.56180932  0.56245849  0.5653341
  0.56533726  0.56562553  0.56604453  0.56802091  0.5720212   0.57249761
  0.57257623  0.57288471  0.57376445  0.57447552  0.57483915  0.575907
  0.5759738   0.5762773   0.57646167  0.5766457   0.57692582  0.57726474
  0.57757343  0.58057703  0.58072097  0.58220532  0.58246674  0.58292987
  0.58345337  0.58360827  0.58417036  0.58428898  0.58483039  0.58521994
  0.58618404  0.58704596  0.58731745  0.58751256  0.58793009  0.58809193
  0.58937248  0.58969512  0.59058776  0.59173664  0.59179556  0.59188379
  0.59201793  0.59393418  0.59452988  0.59469139  0.59473162  0.59473987
  0.59507117  0.59728487  0.59887089  0.59922374  0.60025043  0.60110101
  0.60285271  0.60333365  0.6037253   0.60478618  0.60646787  0.60654354
  0.60677578  0.6070712   0.60711503  0.60830349  0.61099552  0.61132321
  0.61330017  0.61541424  0.6177932   0.62137502  0.62169656  0.62529764
  0.63061037  0.63068675  0.63083761  0.64690934  0.65078186  0.65278282
  0.65581162  0.67222849  0.67906421  0.67972754  0.69963138  0.71041208
  0.71152804  0.7193006   0.75985989  0.76119739  0.78574401  0.82454782
  0.82782464  0.90301403  1.12955981  1.13152542  1.1430839   1.32646567]

  UserWarning,

2022-10-31 11:02:06,741:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.82014406 -0.56245224 -0.38483663 -0.37877349 -0.33680464 -0.31121951
 -0.22747141 -0.22588933 -0.2197741  -0.21882697 -0.14783181 -0.09661873
 -0.06887791 -0.01977398 -0.00171632  0.0072468   0.02854366  0.05313097
  0.07083683  0.07369299  0.09116366  0.10121088  0.10672199  0.10965769
  0.11219042  0.11463538  0.132531    0.13485049  0.14885194  0.1559586
  0.15609593  0.1679049   0.19623803  0.1996295   0.21092087  0.22397746
  0.23188346  0.24021518  0.25043327  0.27381851  0.27535214  0.28313936
  0.28465432  0.28776623  0.30517136  0.31729345  0.32812302  0.32996928
  0.33048606  0.33226321  0.3385668   0.34028471  0.34983951  0.35033486
  0.35118874  0.35139304  0.35555556  0.35577523  0.35776416  0.35785982
  0.35847161  0.35998382  0.36185392  0.36253031  0.36448994  0.36460658
  0.36652175  0.36742689  0.36844513  0.36944506  0.37051707  0.37263899
  0.37404121  0.37651074  0.38076718  0.38717248  0.38807221  0.38872765
  0.39132059  0.39180977  0.39203639  0.39376396  0.39523582  0.39544889
  0.39606857  0.40074995  0.4021395   0.40402623  0.40614683  0.4064846
  0.40794801  0.40892552  0.40898036  0.41036761  0.41072593  0.41095756
  0.41680541  0.42177986  0.42288986  0.42353633  0.42695492  0.42955828
  0.43170342  0.43204246  0.43377531  0.43408599  0.43528746  0.43790454
  0.44064502  0.44216008  0.44299219  0.44335101  0.44336447  0.44345084
  0.4447781   0.44488428  0.44641837  0.44648366  0.44852654  0.44855498
  0.4487385   0.45371357  0.45407174  0.45540637  0.45567073  0.45674403
  0.45689806  0.45757028  0.46499451  0.46507485  0.46523048  0.46629042
  0.46755439  0.46946528  0.47108044  0.47142     0.47510361  0.47546115
  0.47636313  0.47810622  0.47974725  0.48158597  0.48264913  0.48299262
  0.48390369  0.48401609  0.48413093  0.48468354  0.48515789  0.48594176
  0.48915691  0.48921973  0.49022109  0.4903285   0.49042128  0.49150987
  0.49159003  0.49250447  0.49358249  0.49568617  0.49642946  0.49686928
  0.49693868  0.49825774  0.49841537  0.49843913  0.49892304  0.49955763
  0.49962121  0.4996528   0.49993875  0.49998221  0.50026976  0.50071824
  0.50074207  0.50093757  0.5018661   0.5019481   0.50267915  0.50279287
  0.50362059  0.50375495  0.50401373  0.50464386  0.50530763  0.50561372
  0.50633136  0.50839433  0.50901383  0.50939858  0.51032761  0.5106843
  0.51131148  0.51195393  0.51229011  0.51230758  0.5123237   0.51334847
  0.5134974   0.51389682  0.5149685   0.5167289   0.5170809   0.51744271
  0.51898008  0.52052284  0.52238922  0.52266767  0.52456688  0.52498835
  0.52586531  0.52692156  0.5280663   0.52933089  0.52950885  0.53165597
  0.53183035  0.5323951   0.53507445  0.53604499  0.53608578  0.53671951
  0.53898923  0.54065466  0.54138249  0.54189429  0.54223195  0.54363491
  0.54470297  0.54639209  0.54849451  0.54875984  0.54932451  0.55065028
  0.55109993  0.55260036  0.55299615  0.5541409   0.55414104  0.55416579
  0.55458235  0.55484527  0.55596089  0.55635342  0.55657997  0.55876469
  0.55916002  0.55984428  0.55991469  0.56191881  0.56247155  0.56335355
  0.56395516  0.56400633  0.56431779  0.56467501  0.5656663   0.56699098
  0.56834419  0.56835071  0.56884061  0.56903602  0.57163554  0.57336043
  0.57458792  0.57459959  0.57482614  0.57535499  0.57552494  0.57559738
  0.57952786  0.58210993  0.58265322  0.58491667  0.58552192  0.58570999
  0.58637817  0.58690937  0.58720461  0.58765827  0.58785674  0.58799366
  0.58829605  0.588625    0.58905129  0.5891149   0.58927895  0.59000371
  0.59036395  0.59077704  0.59077825  0.591622    0.59179198  0.59424705
  0.59523358  0.5962695   0.59639413  0.59725798  0.59732356  0.60028454
  0.60111408  0.601302    0.60245325  0.60325619  0.60345457  0.6036497
  0.60606651  0.60805956  0.6092263   0.60949581  0.61041571  0.61281409
  0.61378311  0.61766118  0.62424587  0.62501746  0.62566297  0.62804246
  0.62842459  0.63502632  0.63516969  0.64068175  0.64286383  0.64802378
  0.656259    0.66049461  0.66111061  0.66225885  0.66666499  0.66818015
  0.67189704  0.67803583  0.68244744  0.69271619  0.6979106   0.72109873
  0.72580216  0.7326244   0.73449568  0.76707372  0.79517804  0.81841187
  0.95887836  1.25592804]

  UserWarning,

2022-10-31 11:02:06,741:INFO:Calculating mean and std
2022-10-31 11:02:06,741:INFO:Creating metrics dataframe
2022-10-31 11:02:06,741:INFO:Uploading results into container
2022-10-31 11:02:06,741:INFO:Uploading model into container now
2022-10-31 11:02:06,741:INFO:master_model_container: 7
2022-10-31 11:02:06,741:INFO:display_container: 2
2022-10-31 11:02:06,741:INFO:ElasticNet(random_state=3360)
2022-10-31 11:02:06,741:INFO:create_model() successfully completed......................................
2022-10-31 11:02:06,862:WARNING:create_model() for ElasticNet(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:06,862:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:06,863:INFO:Initializing create_model()
2022-10-31 11:02:06,863:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:06,863:INFO:Checking exceptions
2022-10-31 11:02:06,866:INFO:Importing libraries
2022-10-31 11:02:06,866:INFO:Copying training dataset
2022-10-31 11:02:06,871:INFO:Defining folds
2022-10-31 11:02:06,871:INFO:Declaring metric variables
2022-10-31 11:02:06,872:INFO:Importing untrained model
2022-10-31 11:02:06,872:INFO:Elastic Net Imported successfully
2022-10-31 11:02:06,873:INFO:Starting cross validation
2022-10-31 11:02:06,874:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:09,110:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.87096716 -0.61299648 -0.47819463 -0.24253601 -0.21631656 -0.14014632
 -0.08806319 -0.06695921 -0.00979282  0.01755776  0.05606274  0.05932075
  0.06216485  0.07193952  0.07227453  0.10760293  0.11677208  0.11773978
  0.12396667  0.15612548  0.20783189  0.21636065  0.22030458  0.23454011
  0.24239874  0.24366061  0.24473166  0.27004067  0.27578383  0.28198421
  0.28788968  0.29600179  0.29841753  0.30333652  0.30611403  0.30626391
  0.30920973  0.31531761  0.31646761  0.33035806  0.33155426  0.33380527
  0.33511024  0.34296805  0.34304803  0.34526578  0.34934641  0.35182794
  0.35277686  0.35429799  0.3565014   0.35685614  0.36955274  0.37066991
  0.37072109  0.37275467  0.37275525  0.37510689  0.38008454  0.38400378
  0.38722263  0.39386841  0.39476352  0.39962047  0.40211681  0.40312467
  0.40400757  0.40429929  0.40433438  0.40491552  0.40565273  0.40870636
  0.41195898  0.4136477   0.41374309  0.41423398  0.41436886  0.41529103
  0.41601267  0.4163802   0.41865589  0.41950843  0.42112061  0.42257828
  0.42472294  0.42521956  0.42662528  0.42875617  0.43096595  0.43233941
  0.43422333  0.43774981  0.43929709  0.43956068  0.44007665  0.44545507
  0.44582197  0.44699971  0.44749356  0.44753221  0.44803174  0.44884448
  0.4488869   0.44963731  0.44998752  0.45004844  0.45447305  0.45701798
  0.45761048  0.45846337  0.45928643  0.459609    0.46044468  0.46444254
  0.46504736  0.46519847  0.4660111   0.46726671  0.46735869  0.467435
  0.46780055  0.47141379  0.47156124  0.47176583  0.47188123  0.47344817
  0.4740106   0.47452117  0.47532698  0.4768625   0.47722134  0.47859604
  0.47981595  0.47999373  0.48010115  0.48067355  0.48098438  0.48130249
  0.48155824  0.48203726  0.48257169  0.48326285  0.48412878  0.48538629
  0.48777806  0.49016006  0.49080032  0.49081289  0.49086325  0.49362943
  0.49713589  0.49715097  0.49759811  0.49786815  0.49787827  0.49798909
  0.49839344  0.49879613  0.49924528  0.49989049  0.50099564  0.50303877
  0.50309664  0.5035935   0.50365133  0.50411453  0.5044479   0.50444935
  0.50481457  0.50526262  0.50549477  0.50561805  0.50619316  0.50652818
  0.50711136  0.50739099  0.50840578  0.50844127  0.5088104   0.50930553
  0.50993156  0.51000918  0.51007337  0.51047587  0.51059532  0.51158166
  0.51227293  0.51243708  0.51291112  0.51319724  0.51377612  0.51445665
  0.51456437  0.51465277  0.51493057  0.51550776  0.51608265  0.51740444
  0.51773508  0.51847315  0.51904798  0.51992797  0.52078399  0.52098706
  0.52435395  0.5245992   0.5249298   0.52573865  0.52946561  0.53067117
  0.53118997  0.53131592  0.53183077  0.53224585  0.53394373  0.53424461
  0.53458971  0.53552837  0.53666992  0.53697194  0.53705605  0.53707117
  0.53835199  0.53894158  0.53947839  0.53962344  0.53975012  0.54028712
  0.54040562  0.54092664  0.54098497  0.54141976  0.54228171  0.54335248
  0.54430142  0.54465181  0.54467765  0.5455849   0.54578701  0.54694414
  0.54730215  0.54764812  0.54843552  0.54947763  0.55008096  0.55135903
  0.55251507  0.55269726  0.55318435  0.55417988  0.55454922  0.55469106
  0.55484073  0.55520152  0.55699575  0.55771203  0.55933921  0.56135261
  0.56241833  0.56256155  0.56388584  0.56471127  0.56493371  0.56566302
  0.56586989  0.56754608  0.56923592  0.56988574  0.57239786  0.5740547
  0.57409343  0.57445554  0.57454923  0.57460388  0.57520773  0.57563052
  0.57591826  0.57638965  0.57796063  0.57868799  0.57979983  0.58118416
  0.58159781  0.58186045  0.58593891  0.58601301  0.58675249  0.58753174
  0.58799347  0.58805487  0.58983237  0.58997194  0.59065367  0.59079881
  0.59115066  0.59192885  0.59234309  0.59262244  0.59262544  0.59335627
  0.59367551  0.59402455  0.59415663  0.59476893  0.59499855  0.59542411
  0.59591506  0.59626199  0.59650031  0.59659594  0.59664718  0.59696272
  0.5971812   0.59725573  0.59743292  0.59875116  0.59911918  0.59933764
  0.59993177  0.60064246  0.60071937  0.60112484  0.60120056  0.60142355
  0.60204422  0.60232938  0.60271279  0.60316005  0.60377118  0.60477718
  0.60481114  0.60589446  0.6059931   0.60612686  0.60663396  0.60676533
  0.60712071  0.607202    0.60773734  0.60803585  0.6105187   0.61089898
  0.61276028  0.61302488  0.61414726  0.61497786  0.61896568  0.62012919
  0.62165775  0.62287594  0.6240454   0.62531719  0.62898481  0.63017621
  0.63643166  0.63900627  0.6401099   0.66635327  0.66876499  0.66912573
  0.67192201  0.6759558   0.67655122  0.68654697  0.69198094  0.69310986
  0.6979568   0.7094756   0.72121573  0.73208745  0.74451955  0.80281516
  0.81725955  0.85638751  0.87270516  0.87928934  0.89403536  0.99131815
  1.09360982  1.34878432  1.65875232]

  UserWarning,

2022-10-31 11:02:09,141:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.86154649 -0.81648859 -0.72173906 -0.47249487 -0.34332216 -0.22677426
 -0.11474304 -0.10780268 -0.09189211 -0.06601807  0.04094484  0.05064935
  0.07021153  0.08143175  0.09059926  0.10410877  0.11131704  0.11185059
  0.12073641  0.12638281  0.13459767  0.14717459  0.16144074  0.16490015
  0.17043741  0.18532754  0.18941445  0.19074497  0.1990364   0.205701
  0.22385668  0.2291259   0.23068891  0.23405091  0.24413522  0.25533312
  0.2577896   0.26009541  0.26051724  0.26199846  0.26222414  0.26541022
  0.26888396  0.27354587  0.27612277  0.30048842  0.30094751  0.3128021
  0.32389346  0.32576891  0.32632419  0.3275027   0.33031128  0.33859556
  0.34258402  0.3467513   0.35032135  0.35057343  0.35225072  0.35568553
  0.35579314  0.35903112  0.37036217  0.37401816  0.37405763  0.37541884
  0.37598539  0.38039625  0.38459315  0.38766983  0.38794091  0.39023799
  0.39675184  0.39770903  0.3988419   0.40023969  0.40211811  0.40511498
  0.40576043  0.40606206  0.40640811  0.41161012  0.41344381  0.41527596
  0.41650048  0.41675523  0.42050992  0.42066001  0.42403118  0.42735438
  0.42908898  0.43396074  0.4340026   0.43688191  0.43817032  0.44046281
  0.44093952  0.44096563  0.44155747  0.44197052  0.44215635  0.44425899
  0.44457487  0.44509687  0.4453443   0.44664821  0.44812846  0.44822485
  0.44875085  0.44913913  0.44999736  0.45352875  0.45403222  0.45496807
  0.4567798   0.45714232  0.45848827  0.45971665  0.46123578  0.4645095
  0.46511449  0.46627635  0.46795074  0.46915332  0.46957419  0.46970984
  0.47145311  0.47229677  0.47253844  0.47264535  0.47300778  0.47511139
  0.47545945  0.47695041  0.47708769  0.4808221   0.48102112  0.48219464
  0.48442296  0.48598267  0.48645795  0.48668076  0.48772822  0.48887037
  0.48902129  0.48938767  0.48980709  0.49062561  0.49179964  0.49184321
  0.49185621  0.49245471  0.49264638  0.49303007  0.49400998  0.49565719
  0.495744    0.49609521  0.49679544  0.49688475  0.49712729  0.49737581
  0.49771916  0.49792678  0.49862759  0.4987453   0.49887641  0.49919584
  0.49922424  0.49970365  0.5000003   0.50021446  0.50026354  0.50066625
  0.50095899  0.50213839  0.50268586  0.50282621  0.50296311  0.50322091
  0.50404485  0.50421786  0.50432223  0.5059541   0.50637653  0.50638745
  0.50765392  0.50802879  0.5097786   0.50991976  0.51036602  0.51117641
  0.51125778  0.5118833   0.51220116  0.51279149  0.51329548  0.51348796
  0.51394984  0.51424958  0.51476386  0.5153525   0.51631293  0.51661633
  0.51668561  0.51798154  0.51817186  0.51909588  0.51961423  0.52082921
  0.52125332  0.52140958  0.52222343  0.52262782  0.52320811  0.52357092
  0.52623563  0.526904    0.52818016  0.53017883  0.53055416  0.53071278
  0.53166399  0.53228636  0.53325218  0.53454405  0.53483917  0.53830775
  0.53836397  0.53846275  0.54022173  0.5422906   0.54229393  0.54235578
  0.54411315  0.54519891  0.54531903  0.54538662  0.54560882  0.54806117
  0.54867181  0.54979561  0.5498592   0.5500196   0.55062693  0.55070485
  0.55119942  0.55245119  0.55288451  0.55370785  0.55476535  0.5553279
  0.55624312  0.55675516  0.55728778  0.55798812  0.55806115  0.55854377
  0.55955661  0.56146137  0.56417349  0.5641969   0.56495572  0.56587078
  0.56689603  0.5699258   0.56994443  0.57009793  0.57101696  0.57108156
  0.57305337  0.57352449  0.57402255  0.5742125   0.57694373  0.57840265
  0.57842985  0.57842991  0.57847268  0.57946998  0.57954183  0.57968212
  0.58004835  0.58009859  0.58012344  0.58112255  0.58121013  0.58132061
  0.5819306   0.58211051  0.58286495  0.58291883  0.58316354  0.58383413
  0.58387782  0.58488416  0.58522458  0.58552915  0.58657639  0.58680315
  0.58802469  0.58977751  0.59042496  0.59199429  0.59239476  0.59248788
  0.59291551  0.5937531   0.59395985  0.59416051  0.59432612  0.5964418
  0.59712539  0.59828929  0.59884744  0.59910053  0.60043555  0.6006029
  0.60124148  0.60191399  0.60230453  0.60287229  0.60369638  0.60503485
  0.60687803  0.60918788  0.61010591  0.61011032  0.61243625  0.61386923
  0.61800085  0.61930723  0.62161318  0.62399494  0.6274493   0.62911777
  0.63324104  0.63421415  0.63861223  0.64266123  0.644313    0.65187265
  0.6651195   0.68065746  0.69787208  0.70413838  0.71568651  0.74043851
  0.74881766  0.75923124  0.82190412  0.91858694  1.27259698]

  UserWarning,

2022-10-31 11:02:09,172:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.33441862 -1.15704122 -0.91759268 -0.54405835 -0.49366919 -0.23859717
 -0.19629451 -0.13832442 -0.12927736 -0.11795282 -0.11504883 -0.1064084
 -0.10130799 -0.04352279 -0.03725236 -0.02575491 -0.00281496  0.05062781
  0.06192732  0.07648884  0.07692548  0.0975952   0.10012893  0.10464642
  0.10952785  0.11119514  0.11153239  0.12037678  0.12956653  0.13859271
  0.14049523  0.14080755  0.14548941  0.15381507  0.15436184  0.15838483
  0.1952452   0.1954057   0.20076871  0.20581733  0.21032894  0.21902309
  0.22170095  0.22511505  0.22586269  0.24244737  0.24658435  0.25347127
  0.25967583  0.26337132  0.26982405  0.27006736  0.27009243  0.27045587
  0.27404163  0.28432554  0.2898743   0.29112521  0.30209073  0.31034013
  0.3147914   0.31519848  0.3166344   0.31806386  0.31813936  0.32183061
  0.33318999  0.33548546  0.33591682  0.33640834  0.33848501  0.33998253
  0.34739639  0.35199426  0.35393107  0.35817327  0.35961971  0.3696981
  0.37027046  0.3706287   0.37083329  0.3749643   0.37942653  0.3802958
  0.38073803  0.38494261  0.38555451  0.38627696  0.38985772  0.39008681
  0.39343478  0.39477339  0.39638046  0.39652616  0.39660016  0.3987869
  0.40385398  0.40916254  0.40918645  0.41042842  0.41130767  0.41251005
  0.41923889  0.41982174  0.42102719  0.42446505  0.4283035   0.42994007
  0.43127424  0.43157734  0.43224015  0.43393666  0.43467982  0.43756991
  0.43892452  0.44031914  0.4404326   0.44237795  0.44635998  0.44757695
  0.4477225   0.44801454  0.44916555  0.44939431  0.45142516  0.45349615
  0.45453047  0.4550576   0.45559651  0.45694598  0.45753602  0.4597462
  0.46003859  0.46034778  0.46061111  0.46111683  0.46127912  0.4648259
  0.46601294  0.46664804  0.46730359  0.46742299  0.46743224  0.46754354
  0.46883301  0.46914153  0.47041465  0.47185305  0.4780231   0.47848076
  0.48069722  0.48224153  0.482823    0.48287771  0.48316483  0.48378897
  0.48649581  0.48684602  0.48684932  0.48739482  0.48744873  0.48757303
  0.48859336  0.48897907  0.489255    0.48964717  0.49001652  0.49105953
  0.49152189  0.49230375  0.49247385  0.49336756  0.49410723  0.49445656
  0.49447491  0.49522287  0.4955366   0.49575155  0.49688856  0.49713862
  0.49715755  0.49763572  0.49817947  0.49835262  0.49878633  0.49903122
  0.49987716  0.50052958  0.50187757  0.50292477  0.50348269  0.50364906
  0.50382413  0.50423302  0.50434612  0.50453522  0.50528841  0.50529914
  0.50587385  0.50611664  0.50621436  0.50641183  0.50642184  0.50652623
  0.50668703  0.50722156  0.50822401  0.5082517   0.50854165  0.50886183
  0.50886908  0.50923248  0.50971041  0.51010899  0.51069466  0.51118716
  0.51196394  0.51422983  0.51562414  0.5178712   0.51836194  0.51844761
  0.52009824  0.52109832  0.52327286  0.523307    0.52345495  0.52427486
  0.52571271  0.52663991  0.52875005  0.52888602  0.53082332  0.53144877
  0.53156063  0.53321871  0.53399926  0.53408107  0.53473186  0.53514994
  0.53585079  0.53657457  0.5366088   0.53807573  0.53862193  0.53999027
  0.54031722  0.54117095  0.54167944  0.54176969  0.54210257  0.54241062
  0.54271452  0.54410853  0.54498739  0.54522183  0.54596761  0.54812813
  0.5481324   0.54960927  0.5498477   0.5514052   0.5531915   0.55575307
  0.55785138  0.55995829  0.56068824  0.56185582  0.56396906  0.56506127
  0.56546083  0.56744518  0.5676132   0.56888661  0.56897893  0.57021952
  0.57109361  0.57222962  0.5722424   0.57224627  0.57321039  0.5748922
  0.57580903  0.57652709  0.57653208  0.57667784  0.57980145  0.58044967
  0.58087585  0.58180127  0.58188502  0.58318885  0.58367222  0.58746324
  0.58878957  0.58901598  0.59020732  0.59041769  0.59190181  0.59192407
  0.59203652  0.59207572  0.59275703  0.59413859  0.59478197  0.5958498
  0.5963337   0.59633987  0.5982853   0.59859913  0.60149796  0.60230414
  0.60340527  0.60371903  0.60402756  0.60591418  0.60630956  0.60679455
  0.60878571  0.60988044  0.61247925  0.62123864  0.62234483  0.62467926
  0.62760388  0.63092361  0.63259295  0.65250969  0.67238643  0.69199364
  0.71376794  0.7320786   0.76331468  0.76636344  0.79277738  0.80965239
  0.81994568  0.85035874  0.87823802  0.98919653  1.07842122]

  UserWarning,

2022-10-31 11:02:09,172:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.30140808 -1.03863051 -0.74996748 -0.68950773 -0.24770218 -0.21903757
 -0.20091772 -0.17517732 -0.16876105 -0.14211709 -0.09523156 -0.0683454
 -0.05355187 -0.04750161 -0.04278179 -0.02782982 -0.00756864  0.00338059
  0.01511101  0.0289957   0.06981368  0.08078171  0.09333595  0.11596134
  0.12029497  0.12707647  0.13587231  0.13886724  0.14916107  0.16989646
  0.1739962   0.17953413  0.19440945  0.19649014  0.20064799  0.2111773
  0.21168322  0.21631011  0.21778111  0.21910272  0.22065967  0.22565038
  0.22779953  0.23529348  0.23665791  0.24367825  0.25085809  0.25617804
  0.25770458  0.25861546  0.27460313  0.27486612  0.27557473  0.27609778
  0.27758624  0.294372    0.29807131  0.2981713   0.3004416   0.30707264
  0.30723029  0.30975009  0.31269202  0.32956827  0.33494957  0.33507843
  0.33615303  0.3373036   0.33975319  0.36175027  0.36364975  0.36863382
  0.37066778  0.3714161   0.37143039  0.37341088  0.37476057  0.3753108
  0.38293997  0.38511978  0.38984528  0.39018226  0.39176755  0.39313737
  0.39470676  0.39648667  0.39839491  0.39867855  0.40045295  0.40113175
  0.40263328  0.40284549  0.40381029  0.40428891  0.41045279  0.41650547
  0.41720959  0.41842612  0.42259505  0.42797518  0.43122725  0.43151933
  0.43194401  0.43263676  0.43408751  0.43686837  0.43877784  0.44227989
  0.4428221   0.44340365  0.44362309  0.44486196  0.4450412   0.446699
  0.44758123  0.44796869  0.45040709  0.45236761  0.45345985  0.45459495
  0.45460575  0.45563533  0.45586291  0.45772037  0.4582495   0.45841307
  0.45857703  0.45987793  0.46173535  0.46177554  0.46261724  0.46263693
  0.46288067  0.46324207  0.46349291  0.46532213  0.46555067  0.46583062
  0.46658306  0.4675554   0.46815374  0.46838716  0.47109541  0.47155913
  0.47255133  0.47283319  0.47303906  0.47819125  0.47860359  0.48168856
  0.48251823  0.48380292  0.48592834  0.48892833  0.48963236  0.49232795
  0.49346783  0.49356487  0.49374101  0.49472912  0.49506652  0.49543642
  0.49618415  0.49630996  0.49733894  0.49843409  0.49856459  0.49880749
  0.49951797  0.49954079  0.49958777  0.50024802  0.5006521   0.5012425
  0.50161977  0.50184475  0.50246364  0.50267454  0.50294172  0.50313265
  0.50369528  0.50388161  0.50481274  0.50503291  0.50553918  0.50592757
  0.50609417  0.50779581  0.50782748  0.50867764  0.51038566  0.51065766
  0.51076047  0.51194759  0.51195517  0.51196924  0.51214932  0.51318368
  0.51391584  0.51514304  0.51606822  0.51672547  0.5173869   0.51753661
  0.51984936  0.52212949  0.52219128  0.52237194  0.5227335   0.52282201
  0.52285408  0.52286205  0.52327813  0.52339791  0.52491058  0.52519173
  0.5253032   0.52538702  0.52577702  0.52980614  0.53011623  0.53211525
  0.5335973   0.53363447  0.53466087  0.53574286  0.53616635  0.53619658
  0.53666714  0.53671147  0.53698531  0.53832764  0.53857863  0.53899022
  0.53904281  0.53917313  0.54076426  0.54346505  0.54397191  0.54529087
  0.54541069  0.54547572  0.54641155  0.5472699   0.54767142  0.54780014
  0.5482634   0.54831854  0.54903468  0.54950648  0.55051636  0.55085937
  0.55088477  0.55219815  0.552776    0.55480094  0.55532815  0.55550933
  0.55567134  0.55601811  0.5568607   0.56048533  0.56206328  0.56755245
  0.56856086  0.57210743  0.572889    0.57545614  0.57555764  0.57650387
  0.57798582  0.57856791  0.57979695  0.58123424  0.58200481  0.58351712
  0.5836697   0.58498875  0.58528476  0.58541664  0.5854623   0.58712519
  0.58773136  0.5879628   0.58830614  0.58852996  0.58875359  0.58877927
  0.5898301   0.59032996  0.59102643  0.59124376  0.59186138  0.59224491
  0.59422489  0.59474847  0.59509279  0.59552557  0.59587414  0.59626762
  0.59632152  0.59699252  0.60058468  0.60185944  0.6034177   0.60470579
  0.60535666  0.605625    0.60705167  0.60767789  0.60794533  0.60835623
  0.60970888  0.61139336  0.61658696  0.61764931  0.61809862  0.61920699
  0.62008297  0.62238563  0.62260737  0.62457117  0.62457575  0.62665105
  0.62856322  0.62913988  0.63077141  0.63103673  0.63440942  0.63785878
  0.64007903  0.64581542  0.64711905  0.65695056  0.66388958  0.66785101
  0.67176299  0.67261524  0.68322778  0.6944131   0.71033187  0.71981711
  0.72301416  0.73491313  0.74712802  0.76404283  0.81012389  0.81022089
  0.82357152  0.83787294  0.99596379  1.02660209  1.26722889  1.76376347]

  UserWarning,

2022-10-31 11:02:09,188:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.94364155 -0.63193962 -0.51804704 -0.51605051 -0.47643197 -0.47604934
 -0.47315726 -0.28555538 -0.19872631 -0.18459889 -0.13965434 -0.13014298
 -0.11484936 -0.07299953 -0.0619606  -0.03431962 -0.01685652 -0.00627428
  0.02303906  0.03761404  0.03796543  0.04000949  0.0629487   0.06659916
  0.11074085  0.11737354  0.12474909  0.1276645   0.12831014  0.15201848
  0.16868871  0.19395116  0.20281909  0.2111219   0.21614961  0.2356776
  0.23840881  0.24273359  0.24940953  0.25254242  0.25910441  0.2598348
  0.26077537  0.26134086  0.26363506  0.26533886  0.2681949   0.26972834
  0.27206799  0.28080576  0.28826403  0.28924996  0.29446585  0.29840146
  0.29987781  0.30746093  0.30910618  0.3154185   0.32191756  0.3226231
  0.32807486  0.3348927   0.33820582  0.35083288  0.35097239  0.35242809
  0.35541631  0.35544853  0.35730288  0.3578396   0.36553671  0.36778481
  0.37182646  0.37456812  0.37733097  0.3809118   0.38336913  0.3835866
  0.38386166  0.38664498  0.38771941  0.38893883  0.3891456   0.3894009
  0.38965463  0.39397635  0.39830792  0.39931254  0.401367    0.40517558
  0.40547691  0.40665769  0.40738527  0.40867282  0.41145595  0.41253868
  0.41423906  0.41467965  0.41699578  0.41786086  0.41787731  0.41827181
  0.42629742  0.42891379  0.42894042  0.43094024  0.43107234  0.436606
  0.43698577  0.4387934   0.43979593  0.44244323  0.44323084  0.44370243
  0.44422978  0.4466054   0.44771264  0.44907273  0.45026409  0.45136638
  0.4539384   0.45520851  0.4552858   0.45680157  0.45706618  0.45726191
  0.45753586  0.45989446  0.46219384  0.46485261  0.46511671  0.46568173
  0.46634863  0.46639774  0.46775097  0.47010072  0.47028715  0.47037309
  0.47146517  0.47224463  0.4731543   0.47399255  0.47441275  0.47559917
  0.47664636  0.47723565  0.47792677  0.47818082  0.47882482  0.48088755
  0.48095811  0.48131872  0.48419239  0.48591229  0.48610001  0.48732253
  0.4874935   0.48783505  0.48818602  0.4891727   0.48983635  0.49047
  0.49054162  0.49090025  0.49134798  0.49150812  0.49205004  0.49223064
  0.49252656  0.49256316  0.49266675  0.49369874  0.49398559  0.49422499
  0.49444793  0.49468553  0.49592835  0.49681936  0.49700083  0.49760583
  0.49767753  0.49900843  0.49901773  0.49920035  0.49944681  0.50031693
  0.50130309  0.50195481  0.50218651  0.50233344  0.50252117  0.5041378
  0.50511542  0.50514423  0.50531609  0.50553639  0.50564774  0.50569415
  0.50615211  0.50679974  0.50715814  0.5073049   0.50924147  0.50939041
  0.50974701  0.50977116  0.51000226  0.5103659   0.5105661   0.51081052
  0.51090503  0.51171977  0.51296461  0.51474071  0.51476627  0.51540232
  0.51546235  0.51602677  0.51605857  0.5167361   0.51686442  0.51719023
  0.51744124  0.51752722  0.51953776  0.52087645  0.52147115  0.52183024
  0.52206888  0.5233017   0.52496437  0.52534784  0.5258641   0.52595892
  0.52657091  0.52836048  0.529502    0.53126961  0.53175698  0.53433015
  0.53519093  0.53524531  0.53541757  0.53686207  0.53728105  0.53750004
  0.54304454  0.54310801  0.543203    0.54324458  0.5435271   0.54449326
  0.54708294  0.54767797  0.54837367  0.54899994  0.55192115  0.55297426
  0.55331475  0.55688574  0.56090436  0.56105742  0.56232284  0.56254868
  0.563145    0.56489381  0.56516567  0.56597314  0.56683842  0.56707597
  0.57072468  0.57155569  0.57413496  0.57497383  0.57533517  0.57594505
  0.57769814  0.57791281  0.57965385  0.58076549  0.58267094  0.58333412
  0.58359013  0.58359614  0.58414044  0.58592164  0.58609814  0.58614228
  0.58673637  0.58683742  0.58718956  0.58719471  0.58721138  0.58776973
  0.58794798  0.5882527   0.58831997  0.58856868  0.5889409   0.58906608
  0.58911517  0.58997483  0.59065856  0.59114506  0.59115177  0.59146723
  0.59154507  0.59178699  0.59243559  0.59255656  0.59302298  0.59333011
  0.59346513  0.59349414  0.59454045  0.59881442  0.59979497  0.60025499
  0.60258755  0.60289712  0.6037481   0.60380658  0.60385593  0.60500227
  0.6064139   0.60671029  0.6068989   0.60729924  0.60740697  0.60835374
  0.61238768  0.61488211  0.61596411  0.61697228  0.61710527  0.61726917
  0.61804252  0.6202217   0.63073557  0.63274018  0.63666542  0.6386181
  0.64144374  0.64175761  0.64245634  0.64459056  0.64639255  0.64947774
  0.65000501  0.65374907  0.66023669  0.6652143   0.67335785  0.6753152
  0.6784793   0.67879455  0.68311944  0.69840283  0.70120418  0.70684278
  0.73196319  0.75439611  0.75617496  0.75934746  0.83541662  0.84530561
  0.9143831   1.08244343]

  UserWarning,

2022-10-31 11:02:09,203:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.06415027 -0.94743646 -0.92605756 -0.88976996 -0.86471327 -0.81879129
 -0.55416125 -0.49288984 -0.49235448 -0.47497677 -0.45445063 -0.43279492
 -0.42013284 -0.40194082 -0.38409484 -0.34578871 -0.32243323 -0.31676064
 -0.29502316 -0.28924827 -0.1916609  -0.15995478 -0.15973152 -0.13174758
 -0.09476444 -0.00310019  0.00552903  0.023122    0.03283247  0.04497375
  0.06590818  0.07044796  0.08128118  0.08151031  0.11652568  0.13699685
  0.1410888   0.15050525  0.16261642  0.18200604  0.19246831  0.1952326
  0.19907386  0.20398854  0.21417787  0.22300358  0.22356073  0.22665391
  0.24525834  0.24881764  0.25162356  0.25293203  0.25324757  0.26052494
  0.26531681  0.26718194  0.27698091  0.27948847  0.28191218  0.29038493
  0.29584996  0.29976277  0.30640077  0.31695674  0.31796297  0.31864553
  0.32728021  0.33183006  0.33268358  0.34389263  0.34692176  0.34843866
  0.35148384  0.35300493  0.36040508  0.36278622  0.36284179  0.36468603
  0.36563021  0.37079699  0.37251767  0.37577062  0.38220572  0.38489225
  0.39177211  0.39248632  0.39329837  0.39339244  0.39389915  0.39566082
  0.39613426  0.39737621  0.3998156   0.400841    0.40233209  0.40471724
  0.40706422  0.40831263  0.40958113  0.41058588  0.41231459  0.41265052
  0.41509822  0.41671207  0.41900591  0.42229291  0.42276313  0.42548788
  0.4257012   0.42670702  0.42832131  0.4295916   0.42992997  0.43318091
  0.43774957  0.43902634  0.43978063  0.44065128  0.44247543  0.44253205
  0.44310945  0.44711493  0.44720217  0.44798913  0.44807066  0.4488605
  0.4489495   0.44907432  0.45031811  0.45159878  0.45481829  0.45693771
  0.45828839  0.45863133  0.45933331  0.46039096  0.46094373  0.46380546
  0.46395045  0.46578592  0.46615068  0.46698086  0.46707647  0.46854377
  0.46895883  0.46920259  0.46996256  0.470221    0.47031643  0.47035885
  0.47061738  0.47108931  0.47269912  0.47480065  0.47491776  0.47566779
  0.47818237  0.47932954  0.47933887  0.48006497  0.48357186  0.48477829
  0.48596799  0.4868175   0.48736084  0.48739424  0.48808919  0.48834626
  0.48854936  0.48859743  0.48864373  0.48967783  0.48988735  0.49138764
  0.49202972  0.49212782  0.49224793  0.49231376  0.49251248  0.49259247
  0.49273866  0.49328334  0.4933286   0.49341752  0.49380775  0.49445819
  0.49466099  0.49500787  0.49523037  0.49527112  0.49550095  0.49698281
  0.49840137  0.49951979  0.5001542   0.50057961  0.50083332  0.50105498
  0.50309658  0.50382303  0.5039463   0.50442789  0.50564664  0.50643495
  0.50674303  0.50681131  0.50753911  0.50766985  0.50835961  0.50928264
  0.50929416  0.51023761  0.51100279  0.51115172  0.51159313  0.51329807
  0.51370911  0.51438774  0.51473231  0.51573241  0.51737427  0.51752071
  0.51752535  0.51866813  0.52031244  0.520723    0.52087489  0.52366079
  0.52391192  0.52429837  0.52644718  0.52851427  0.52923888  0.52935857
  0.52965106  0.53198383  0.53240805  0.53505694  0.53770565  0.53849326
  0.53903855  0.53932415  0.53945526  0.53949839  0.54001958  0.54008622
  0.5401793   0.54022435  0.5427621   0.54663781  0.54672602  0.5468891
  0.54733576  0.54826832  0.55060391  0.55060413  0.5507264   0.55114647
  0.55165892  0.55189694  0.55196823  0.55239411  0.55516656  0.55580689
  0.55646343  0.55735507  0.55760149  0.55921695  0.56320458  0.56328235
  0.56367791  0.56615058  0.56747236  0.56799131  0.56932845  0.5696369
  0.57026615  0.57083341  0.57135192  0.5728945   0.57447488  0.57507512
  0.57509554  0.57631191  0.57721741  0.57728037  0.57736672  0.57911658
  0.57965883  0.57986494  0.58156377  0.58242711  0.58453079  0.5855316
  0.58578699  0.5861217   0.58673711  0.58822652  0.58891577  0.58955613
  0.58956171  0.5899917   0.59154385  0.592368    0.59297787  0.59328095
  0.59340284  0.59379277  0.59416085  0.59417202  0.59423301  0.59466137
  0.59473777  0.59480273  0.59488779  0.59551846  0.59580611  0.59591628
  0.59617731  0.59702564  0.59725769  0.59865343  0.59902088  0.59919196
  0.60020911  0.60169611  0.60182902  0.60204305  0.60209154  0.60318108
  0.60545209  0.60577878  0.60581609  0.60647351  0.60763596  0.60923199
  0.60970858  0.6102888   0.61081973  0.61114234  0.61283877  0.6130309
  0.61419497  0.61682333  0.61954487  0.61981879  0.62041088  0.6208977
  0.62268182  0.62711806  0.63117853  0.63128041  0.63179362  0.63230217
  0.63320463  0.63447768  0.63712189  0.64534525  0.64929244  0.64988509
  0.65214894  0.67319402  0.67585983  0.68038778  0.69590317  0.69895188
  0.69971452  0.70563546  0.7260924   0.7270484   0.74348496  0.78298838
  0.88874538  0.9010531   0.90696563]

  UserWarning,

2022-10-31 11:02:09,203:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.88856143 -0.83142376 -0.82236622 -0.31442294 -0.26261078 -0.25207462
 -0.22714211 -0.19919461 -0.15736817 -0.13922429 -0.11202346 -0.07929376
 -0.06375389 -0.05481096  0.01945674  0.02423353  0.04461794  0.0468159
  0.05444938  0.05846466  0.06234651  0.06253592  0.07013791  0.0939489
  0.10308853  0.10931375  0.12897557  0.13817381  0.16904994  0.18594118
  0.19020061  0.19355149  0.19560554  0.20706634  0.2220258   0.22295179
  0.23037838  0.2566969   0.26567816  0.26715539  0.27024305  0.27550676
  0.28123633  0.28414192  0.28999014  0.30740282  0.31414789  0.32132855
  0.32402099  0.32547596  0.326535    0.33399763  0.33846761  0.35232562
  0.3530261   0.3573328   0.35766612  0.36119996  0.36269583  0.37895857
  0.38088342  0.38124195  0.38199359  0.3834097   0.38812001  0.39846618
  0.39939669  0.39993059  0.40373774  0.40591941  0.40839944  0.40910902
  0.41014219  0.41101161  0.41615077  0.41660386  0.42052135  0.42138667
  0.42198797  0.42343225  0.42399214  0.42440613  0.42662418  0.43023238
  0.43372241  0.43410034  0.43561707  0.4363247   0.4367879   0.43696321
  0.43860489  0.43935676  0.44023481  0.4418729   0.44206251  0.44298588
  0.44336809  0.44724796  0.44729118  0.44733352  0.44742194  0.44769181
  0.44797601  0.4488956   0.44891614  0.44916582  0.45026756  0.45039477
  0.45106624  0.45335898  0.45369776  0.45493729  0.4558494   0.46047922
  0.46085638  0.46091894  0.46115075  0.46118432  0.46192657  0.46477341
  0.46499515  0.46660306  0.46998556  0.47490901  0.47606357  0.4785984
  0.48284697  0.48338305  0.48406873  0.48458764  0.48591349  0.48692618
  0.48696126  0.48708127  0.48782949  0.488291    0.48969338  0.48992605
  0.4912623   0.49133471  0.49377425  0.49382602  0.49430137  0.49432549
  0.49496106  0.49524008  0.49544919  0.49622205  0.49791461  0.49842178
  0.49863608  0.4989181   0.49905603  0.49918614  0.49933355  0.49980237
  0.50017497  0.50087848  0.50262633  0.50280335  0.50282116  0.50305818
  0.50332954  0.50337626  0.50359702  0.50361789  0.5040072   0.50413627
  0.50425471  0.50473851  0.50480364  0.50501009  0.50550299  0.50554605
  0.50814687  0.50888036  0.50974969  0.50978797  0.51026993  0.5118772
  0.51218585  0.51355268  0.51525334  0.51609443  0.51716342  0.51944633
  0.52021975  0.5202757   0.52071688  0.52211936  0.52215391  0.52225685
  0.52322907  0.52323495  0.52386858  0.52420276  0.52440177  0.52467141
  0.52521587  0.5258982   0.52603461  0.52804277  0.52804606  0.52875207
  0.53023138  0.53060499  0.53097797  0.53279668  0.53330897  0.53392739
  0.53527025  0.53618901  0.53631647  0.53659271  0.53688719  0.53774358
  0.53866137  0.53949566  0.54134855  0.54159013  0.54170428  0.54284996
  0.54378276  0.54385573  0.54412483  0.54484365  0.54494313  0.54578295
  0.54644633  0.54679459  0.54730147  0.54732921  0.54801303  0.54862516
  0.5487562   0.54938154  0.5510053   0.55255928  0.55314638  0.55555217
  0.55601089  0.55775551  0.55806152  0.55820798  0.55890482  0.55894407
  0.55946362  0.560342    0.56042319  0.5605751   0.56060743  0.56100509
  0.56197395  0.56258084  0.56348967  0.56531417  0.56633945  0.56732692
  0.56888603  0.57107679  0.57117144  0.57136671  0.57184328  0.57187833
  0.57267016  0.57350752  0.5743595   0.57500371  0.57592011  0.57614484
  0.57685597  0.57704071  0.57846039  0.5785921   0.57890575  0.5804478
  0.58187624  0.58202966  0.58308079  0.58382269  0.58451618  0.58560917
  0.58576911  0.58589834  0.58642743  0.58720108  0.58720894  0.58731649
  0.58857914  0.58899846  0.59017942  0.5906756   0.59077231  0.59131607
  0.59252623  0.59373868  0.59403613  0.59413602  0.59424554  0.59488846
  0.59514454  0.59588419  0.5964277   0.59662651  0.59670801  0.597052
  0.59709539  0.59738755  0.59745096  0.59748473  0.59761262  0.5979115
  0.59904894  0.59909783  0.60059855  0.60065536  0.60127184  0.60208531
  0.60243116  0.60260551  0.60539834  0.60548449  0.60787908  0.60811282
  0.60819686  0.60987878  0.61175989  0.61246074  0.61258101  0.6139837
  0.6161777   0.61620221  0.61621553  0.6162511   0.61714438  0.6179935
  0.62328569  0.6282726   0.62945452  0.6321051   0.63321954  0.64286644
  0.65037257  0.65331767  0.66085202  0.66745398  0.67020468  0.67040511
  0.68267477  0.68571681  0.69852411  0.70062434  0.70978869  0.71949697
  0.72480884  0.75385692  0.7628412   0.90998016  0.97056526  1.30164519
  1.50429955]

  UserWarning,

2022-10-31 11:02:09,219:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.72760046 -1.0583233  -0.51340546 -0.36788487 -0.35279819 -0.26955828
 -0.26731555 -0.19493308 -0.17722806 -0.17146413 -0.07949312 -0.04678985
 -0.03441213 -0.03406773  0.00727663  0.02004619  0.06054585  0.06942281
  0.09874534  0.12223442  0.131219    0.13695112  0.13851479  0.15108301
  0.15120542  0.17523191  0.20777597  0.21816513  0.21867828  0.22738784
  0.23902937  0.25602388  0.26268421  0.26635309  0.2772811   0.28867029
  0.29467726  0.30911953  0.31165049  0.31663487  0.32141975  0.32288787
  0.32591547  0.33768812  0.35022539  0.35214598  0.35591438  0.35877391
  0.35981507  0.36734491  0.36982124  0.37260007  0.37283398  0.37285923
  0.37352338  0.37777085  0.38104594  0.38351516  0.38546517  0.38674801
  0.38791302  0.38856382  0.3892196   0.39911667  0.39931607  0.39971608
  0.4018208   0.40411768  0.40416999  0.40500327  0.4080848   0.41112088
  0.41271911  0.41313447  0.41463576  0.42105348  0.42254512  0.42304969
  0.42326432  0.42429283  0.42477825  0.42835157  0.42840011  0.42937148
  0.42964923  0.4303832   0.4311725   0.43398188  0.43600245  0.43800518
  0.43859822  0.44105418  0.44302324  0.44350618  0.44354542  0.44389222
  0.44457743  0.44491074  0.4451919   0.44594441  0.44891705  0.44926583
  0.44930089  0.45081986  0.45548318  0.45716311  0.45891095  0.45954137
  0.46069334  0.46089033  0.46119184  0.46143156  0.46179287  0.46219396
  0.46241727  0.46368826  0.46591947  0.46672941  0.46790946  0.4683165
  0.46868165  0.46902657  0.46940868  0.47020104  0.47103351  0.47149413
  0.47204202  0.47240519  0.47350221  0.47451056  0.4745311   0.47637331
  0.4769307   0.47757822  0.47850013  0.47924438  0.48081525  0.48101529
  0.48134466  0.48174471  0.4833882   0.48348127  0.48638788  0.48709713
  0.4882351   0.48861383  0.48866397  0.48872045  0.4896067   0.49021097
  0.49072975  0.49223514  0.49441973  0.49514018  0.49551931  0.49617231
  0.49729925  0.49814357  0.49880389  0.49979893  0.5000737   0.50043391
  0.50047422  0.50152609  0.50172033  0.50220423  0.50258657  0.50271456
  0.50390884  0.50406677  0.50429948  0.50470848  0.50607868  0.50667571
  0.50681076  0.50727761  0.50735971  0.5080567   0.50841398  0.50957484
  0.50984906  0.5111275   0.51133312  0.51152028  0.51170601  0.5118975
  0.51267591  0.51417797  0.5143083   0.51455213  0.51505563  0.51576122
  0.51588646  0.51626896  0.51658562  0.51680597  0.51693994  0.51712072
  0.51810049  0.51840288  0.51905052  0.51983776  0.51999478  0.52321509
  0.52333914  0.52446165  0.52526892  0.52554746  0.52560918  0.52620635
  0.52829666  0.52903524  0.52954968  0.53030858  0.53160579  0.5324087
  0.53255694  0.53337856  0.53548344  0.53574414  0.53621783  0.53723233
  0.53817931  0.53837522  0.53892428  0.54057988  0.54255673  0.54365175
  0.54472839  0.54509844  0.54568436  0.54596587  0.5460298   0.54672871
  0.54836343  0.54856457  0.54914818  0.54969917  0.55033821  0.55130189
  0.5513661   0.55220932  0.55295153  0.55379241  0.55382296  0.55415129
  0.55479031  0.55535113  0.55691527  0.55853633  0.55992745  0.56000186
  0.56005193  0.56095902  0.56224184  0.56331035  0.56683445  0.56701088
  0.5679455   0.56814059  0.56821083  0.56869702  0.56872767  0.5691574
  0.57165198  0.5736976   0.57398589  0.57769821  0.57883562  0.57924996
  0.57946741  0.57968306  0.57994947  0.58050684  0.58082077  0.58190894
  0.58195907  0.58212149  0.58291896  0.58366802  0.58380921  0.58432575
  0.584453    0.58531561  0.5855826   0.58579411  0.58587338  0.585978
  0.58603786  0.58610737  0.58643969  0.58707844  0.58741559  0.58792664
  0.58804733  0.58815918  0.58843569  0.58863028  0.58895647  0.58903239
  0.58915043  0.58926669  0.58960938  0.59067533  0.59089076  0.59103988
  0.59190718  0.59242534  0.59266412  0.59291968  0.59308452  0.59324772
  0.59576222  0.59577463  0.59746326  0.59820853  0.5989875   0.60035295
  0.60036837  0.60061624  0.60062233  0.60100171  0.60146545  0.60305362
  0.60431672  0.60518704  0.60520409  0.60609354  0.60682545  0.60888282
  0.60936547  0.61008477  0.6103317   0.61180151  0.61429908  0.61519682
  0.61573198  0.61696016  0.61836504  0.61877773  0.62436334  0.62461348
  0.6312323   0.63637051  0.63790668  0.64335675  0.65442907  0.65482042
  0.66153079  0.67310504  0.67958908  0.68037719  0.68296699  0.6843881
  0.69237693  0.70790385  0.70948745  0.71309618  0.73336632  0.74080063
  0.74662566  0.7695282   0.79403056  0.80646006  0.86526844  0.8784359
  0.92052616  0.99559763  1.05268225  1.0586921   1.1797896   1.2752117 ]

  UserWarning,

2022-10-31 11:02:10,071:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.82014406 -0.56245224 -0.38483663 -0.37877349 -0.33680464 -0.31121951
 -0.22747141 -0.22588933 -0.2197741  -0.21882697 -0.14783181 -0.09661873
 -0.06887791 -0.01977398 -0.00171632  0.0072468   0.02854366  0.05313097
  0.07083683  0.07369299  0.09116366  0.10121088  0.10672199  0.10965769
  0.11219042  0.11463538  0.132531    0.13485049  0.14885194  0.1559586
  0.15609593  0.1679049   0.19623803  0.1996295   0.21092087  0.22397746
  0.23188346  0.24021518  0.25043327  0.27381851  0.27535214  0.28313936
  0.28465432  0.28776623  0.30517136  0.31729345  0.32812302  0.32996928
  0.33048606  0.33226321  0.3385668   0.34028471  0.34983951  0.35033486
  0.35118874  0.35139304  0.35555556  0.35577523  0.35776416  0.35785982
  0.35847161  0.35998382  0.36185392  0.36253031  0.36448994  0.36460658
  0.36652175  0.36742689  0.36844513  0.36944506  0.37051707  0.37263899
  0.37404121  0.37651074  0.38076718  0.38717248  0.38807221  0.38872765
  0.39132059  0.39180977  0.39203639  0.39376396  0.39523582  0.39544889
  0.39606857  0.40074995  0.4021395   0.40402623  0.40614683  0.4064846
  0.40794801  0.40892552  0.40898036  0.41036761  0.41072593  0.41095756
  0.41680541  0.42177986  0.42288986  0.42353633  0.42695492  0.42955828
  0.43170342  0.43204246  0.43377531  0.43408599  0.43528746  0.43790454
  0.44064502  0.44216008  0.44299219  0.44335101  0.44336447  0.44345084
  0.4447781   0.44488428  0.44641837  0.44648366  0.44852654  0.44855498
  0.4487385   0.45371357  0.45407174  0.45540637  0.45567073  0.45674403
  0.45689806  0.45757028  0.46499451  0.46507485  0.46523048  0.46629042
  0.46755439  0.46946528  0.47108044  0.47142     0.47510361  0.47546115
  0.47636313  0.47810622  0.47974725  0.48158597  0.48264913  0.48299262
  0.48390369  0.48401609  0.48413093  0.48468354  0.48515789  0.48594176
  0.48915691  0.48921973  0.49022109  0.4903285   0.49042128  0.49150987
  0.49159003  0.49250447  0.49358249  0.49568617  0.49642946  0.49686928
  0.49693868  0.49825774  0.49841537  0.49843913  0.49892304  0.49955763
  0.49962121  0.4996528   0.49993875  0.49998221  0.50026976  0.50071824
  0.50074207  0.50093757  0.5018661   0.5019481   0.50267915  0.50279287
  0.50362059  0.50375495  0.50401373  0.50464386  0.50530763  0.50561372
  0.50633136  0.50839433  0.50901383  0.50939858  0.51032761  0.5106843
  0.51131148  0.51195393  0.51229011  0.51230758  0.5123237   0.51334847
  0.5134974   0.51389682  0.5149685   0.5167289   0.5170809   0.51744271
  0.51898008  0.52052284  0.52238922  0.52266767  0.52456688  0.52498835
  0.52586531  0.52692156  0.5280663   0.52933089  0.52950885  0.53165597
  0.53183035  0.5323951   0.53507445  0.53604499  0.53608578  0.53671951
  0.53898923  0.54065466  0.54138249  0.54189429  0.54223195  0.54363491
  0.54470297  0.54639209  0.54849451  0.54875984  0.54932451  0.55065028
  0.55109993  0.55260036  0.55299615  0.5541409   0.55414104  0.55416579
  0.55458235  0.55484527  0.55596089  0.55635342  0.55657997  0.55876469
  0.55916002  0.55984428  0.55991469  0.56191881  0.56247155  0.56335355
  0.56395516  0.56400633  0.56431779  0.56467501  0.5656663   0.56699098
  0.56834419  0.56835071  0.56884061  0.56903602  0.57163554  0.57336043
  0.57458792  0.57459959  0.57482614  0.57535499  0.57552494  0.57559738
  0.57952786  0.58210993  0.58265322  0.58491667  0.58552192  0.58570999
  0.58637817  0.58690937  0.58720461  0.58765827  0.58785674  0.58799366
  0.58829605  0.588625    0.58905129  0.5891149   0.58927895  0.59000371
  0.59036395  0.59077704  0.59077825  0.591622    0.59179198  0.59424705
  0.59523358  0.5962695   0.59639413  0.59725798  0.59732356  0.60028454
  0.60111408  0.601302    0.60245325  0.60325619  0.60345457  0.6036497
  0.60606651  0.60805956  0.6092263   0.60949581  0.61041571  0.61281409
  0.61378311  0.61766118  0.62424587  0.62501746  0.62566297  0.62804246
  0.62842459  0.63502632  0.63516969  0.64068175  0.64286383  0.64802378
  0.656259    0.66049461  0.66111061  0.66225885  0.66666499  0.66818015
  0.67189704  0.67803583  0.68244744  0.69271619  0.6979106   0.72109873
  0.72580216  0.7326244   0.73449568  0.76707372  0.79517804  0.81841187
  0.95887836  1.25592804]

  UserWarning,

2022-10-31 11:02:10,094:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.45609442 -1.2307927  -0.93497037 -0.43354278 -0.4252861  -0.42399523
 -0.41933935 -0.36476724 -0.36157305 -0.33547293 -0.33126972 -0.25067909
 -0.13805964 -0.12087232 -0.106176   -0.06587921 -0.05230342 -0.04666053
 -0.01673473  0.04182439  0.06634848  0.10003128  0.10923891  0.11919519
  0.12113729  0.12526394  0.1557757   0.15646276  0.17101677  0.1802208
  0.18793279  0.19155514  0.20015945  0.20213345  0.20246997  0.22687402
  0.22860383  0.23016658  0.23082711  0.24087417  0.2471835   0.2483081
  0.25778406  0.26667276  0.26811226  0.27899344  0.27927673  0.28538999
  0.29261905  0.29862293  0.30371609  0.3049619   0.31615096  0.32172346
  0.32334584  0.3244124   0.32467685  0.32639488  0.33486572  0.34175005
  0.34484129  0.3502524   0.35462279  0.35545723  0.35884832  0.36156526
  0.36424715  0.36687859  0.37719577  0.38174542  0.38182548  0.38484058
  0.38661778  0.38700753  0.39207801  0.39468045  0.39675568  0.3968631
  0.39991811  0.40160542  0.40413345  0.40480001  0.40679878  0.4098805
  0.41166636  0.41350958  0.41406149  0.4221292   0.42408898  0.4257903
  0.42643551  0.42795095  0.42797855  0.42847019  0.42921935  0.43048822
  0.43096364  0.43140651  0.43168697  0.43760837  0.43946316  0.43961195
  0.44216547  0.44235418  0.44239635  0.44610526  0.44711921  0.44891194
  0.44993961  0.45009494  0.45033492  0.45320632  0.45332543  0.45571848
  0.45668959  0.45706158  0.45767286  0.45908388  0.46058025  0.46066922
  0.46076892  0.46131974  0.46336248  0.46374155  0.46521873  0.46578043
  0.46618529  0.46689814  0.46727357  0.46874551  0.46875089  0.46889775
  0.47031998  0.47041029  0.47302831  0.4739516   0.47435736  0.47510363
  0.47519971  0.47649653  0.4782517   0.48042308  0.48353344  0.48436182
  0.48455273  0.48485565  0.48555316  0.48642327  0.48643847  0.48845514
  0.48994121  0.49112035  0.49136873  0.49182403  0.49316945  0.49320663
  0.49337929  0.49469956  0.49488068  0.49488771  0.49508617  0.496136
  0.49692573  0.49716231  0.49760987  0.49905619  0.49964836  0.50018644
  0.50047895  0.50201759  0.50220753  0.50222622  0.50288459  0.50334946
  0.50383118  0.50388732  0.50427521  0.50516272  0.50521274  0.50565702
  0.50652728  0.50780396  0.50867241  0.50922257  0.5093431   0.5105671
  0.51067548  0.5108296   0.51105402  0.51109111  0.51112417  0.51465062
  0.51570389  0.51726466  0.51743793  0.51746171  0.51762117  0.51843459
  0.51953972  0.51956987  0.52079443  0.52160983  0.52281295  0.52375695
  0.52552253  0.52714406  0.52871218  0.52877678  0.52901521  0.52918872
  0.5306732   0.53236661  0.53309795  0.53315576  0.53317474  0.53346295
  0.53426406  0.53522249  0.53706953  0.53949616  0.54120031  0.54191259
  0.54215693  0.54220491  0.54280925  0.54281552  0.54290352  0.54332546
  0.5434449   0.54359905  0.54370643  0.54384458  0.54408226  0.5452533
  0.54540928  0.54570543  0.5461906   0.54621649  0.54669965  0.54757894
  0.54799585  0.55115585  0.55177715  0.55324815  0.55339513  0.55649404
  0.55901058  0.55932155  0.55958479  0.56180932  0.56245849  0.5653341
  0.56533726  0.56562553  0.56604453  0.56802091  0.5720212   0.57249761
  0.57257623  0.57288471  0.57376445  0.57447552  0.57483915  0.575907
  0.5759738   0.5762773   0.57646167  0.5766457   0.57692582  0.57726474
  0.57757343  0.58057703  0.58072097  0.58220532  0.58246674  0.58292987
  0.58345337  0.58360827  0.58417036  0.58428898  0.58483039  0.58521994
  0.58618404  0.58704596  0.58731745  0.58751256  0.58793009  0.58809193
  0.58937248  0.58969512  0.59058776  0.59173664  0.59179556  0.59188379
  0.59201793  0.59393418  0.59452988  0.59469139  0.59473162  0.59473987
  0.59507117  0.59728487  0.59887089  0.59922374  0.60025043  0.60110101
  0.60285271  0.60333365  0.6037253   0.60478618  0.60646787  0.60654354
  0.60677578  0.6070712   0.60711503  0.60830349  0.61099552  0.61132321
  0.61330017  0.61541424  0.6177932   0.62137502  0.62169656  0.62529764
  0.63061037  0.63068675  0.63083761  0.64690934  0.65078186  0.65278282
  0.65581162  0.67222849  0.67906421  0.67972754  0.69963138  0.71041208
  0.71152804  0.7193006   0.75985989  0.76119739  0.78574401  0.82454782
  0.82782464  0.90301403  1.12955981  1.13152542  1.1430839   1.32646567]

  UserWarning,

2022-10-31 11:02:10,094:INFO:Calculating mean and std
2022-10-31 11:02:10,094:INFO:Creating metrics dataframe
2022-10-31 11:02:10,109:INFO:Uploading results into container
2022-10-31 11:02:10,109:INFO:Uploading model into container now
2022-10-31 11:02:10,109:INFO:master_model_container: 8
2022-10-31 11:02:10,109:INFO:display_container: 2
2022-10-31 11:02:10,109:INFO:ElasticNet(random_state=3360)
2022-10-31 11:02:10,109:INFO:create_model() successfully completed......................................
2022-10-31 11:02:10,219:ERROR:create_model() for ElasticNet(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:02:10,219:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:10,219:INFO:Initializing Least Angle Regression
2022-10-31 11:02:10,219:INFO:Total runtime is 0.6204774578412374 minutes
2022-10-31 11:02:10,219:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:10,219:INFO:Initializing create_model()
2022-10-31 11:02:10,219:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:10,219:INFO:Checking exceptions
2022-10-31 11:02:10,219:INFO:Importing libraries
2022-10-31 11:02:10,219:INFO:Copying training dataset
2022-10-31 11:02:10,219:INFO:Defining folds
2022-10-31 11:02:10,219:INFO:Declaring metric variables
2022-10-31 11:02:10,219:INFO:Importing untrained model
2022-10-31 11:02:10,219:INFO:Least Angle Regression Imported successfully
2022-10-31 11:02:10,219:INFO:Starting cross validation
2022-10-31 11:02:10,234:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:11,571:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,586:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,586:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,618:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,694:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,695:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,703:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:11,742:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:12,482:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390677 -0.9404312  -0.590322   -0.54798138 -0.29263285 -0.20438721
 -0.1866832  -0.13798057 -0.12398716 -0.12304452 -0.1198953  -0.08278575
 -0.07046144 -0.05659303 -0.03509535 -0.02298702 -0.00978239 -0.00867918
 -0.00382677 -0.0026685   0.01637773  0.02021382  0.02536482  0.0286603
  0.02940902  0.03994095  0.0466716   0.0482134   0.04841486  0.05029733
  0.05073295  0.0719822   0.07856243  0.07874466  0.07996084  0.08056576
  0.08314081  0.08389327  0.09049631  0.09099067  0.09817093  0.101219
  0.10143571  0.10146786  0.10982714  0.11249419  0.14611001  0.15545589
  0.15665583  0.15985543  0.16139877  0.16279983  0.16686824  0.17524696
  0.18183328  0.18679973  0.18830515  0.19158659  0.19190296  0.19252427
  0.1926502   0.1951644   0.20279927  0.20425062  0.20733099  0.2074614
  0.21343426  0.21642321  0.22152616  0.22450732  0.2254441   0.2264667
  0.22749343  0.22963944  0.23249826  0.23352633  0.23594711  0.2364368
  0.23653825  0.23672313  0.24006843  0.24218397  0.24388555  0.24753795
  0.24923427  0.2493424   0.24944902  0.24982936  0.25171546  0.25178211
  0.25209328  0.25342516  0.25419332  0.25434203  0.25640774  0.25774491
  0.25815876  0.25831133  0.25931453  0.26218984  0.26220736  0.26361477
  0.26420538  0.2652288   0.26639281  0.26674639  0.26805315  0.2686495
  0.27090326  0.2751332   0.27573181  0.27579385  0.27583188  0.27714872
  0.27715522  0.27716429  0.27779335  0.27802854  0.27837289  0.27848577
  0.27910157  0.27930592  0.28126373  0.28155147  0.28181826  0.28260855
  0.28266623  0.28311093  0.28351711  0.28357219  0.28387128  0.2844636
  0.28725363  0.28759254  0.28846417  0.28895381  0.2890973   0.28915778
  0.28940858  0.28968461  0.29104614  0.29139901  0.29168013  0.29205209
  0.29239633  0.29287621  0.29320078  0.29656362  0.29709445  0.29730662
  0.29770865  0.29779807  0.2980921   0.30065335  0.30320717  0.30372513
  0.3060367   0.3065846   0.30782986  0.30783688  0.31016565  0.31217423
  0.31231064  0.31380226  0.31474381  0.31477784  0.31552171  0.31572159
  0.31644943  0.31673138  0.31727018  0.3180071   0.31925789  0.3198505
  0.32070667  0.32086699  0.32321097  0.32406432  0.32537297  0.32637709
  0.32709331  0.32934075  0.32943596  0.33042909  0.33074056  0.33403231
  0.33505735  0.33842389  0.33963374  0.34179339  0.34190047  0.342241
  0.34360276  0.34361855  0.34398     0.34737001  0.34746404  0.34843494
  0.34997945  0.35064859  0.35092599  0.35175     0.35408262  0.35469468
  0.35636466  0.3570634   0.35712433  0.35920264  0.35942988  0.36116294
  0.36166034  0.36407137  0.36737264  0.36947401  0.37051185  0.37078933
  0.37172624  0.37224861  0.37288026  0.37308115  0.37317483  0.37404887
  0.37503896  0.37512675  0.37597968  0.37657759  0.37766158  0.3779937
  0.37940359  0.38082543  0.38317591  0.3837317   0.38419512  0.38430621
  0.38567953  0.38619893  0.38629087  0.38655359  0.38674364  0.38970153
  0.38992702  0.39066709  0.39086599  0.39245987  0.39273984  0.39284276
  0.39371492  0.39417023  0.3946333   0.3948346   0.39591428  0.39797407
  0.39868131  0.39994992  0.40313597  0.4038835   0.40410309  0.40442945
  0.40470724  0.40652781  0.40849959  0.40871711  0.40880784  0.41013149
  0.4102992   0.4106173   0.41147821  0.41210082  0.41233539  0.41516063
  0.41838704  0.41960785  0.41970929  0.42105177  0.42106439  0.42210204
  0.42241033  0.42271587  0.42339005  0.42386382  0.42585448  0.42815084
  0.43081642  0.43255117  0.43260956  0.43425387  0.43530128  0.43775747
  0.44180064  0.44226667  0.44407468  0.44410794  0.44557073  0.44797072
  0.4479862   0.44862344  0.44888002  0.45467508  0.4551799   0.4567633
  0.45716664  0.45868611  0.45900235  0.46025031  0.46066686  0.46270305
  0.46366913  0.46405048  0.46699396  0.46817765  0.46830359  0.46920927
  0.47093077  0.47318923  0.47409716  0.47547221  0.47787205  0.48313319
  0.48640268  0.48737567  0.48822826  0.48912915  0.49198735  0.49322307
  0.49410273  0.49538602  0.49756572  0.5007252   0.50188517  0.50568469
  0.5103218   0.5135527   0.52288708  0.53062247  0.53311624  0.53575509
  0.53654311  0.53677499  0.53876682  0.54807891  0.55128789  0.55524213
  0.55556024  0.55649443  0.55711916  0.56127471  0.56709427  0.56985869
  0.5713711   0.57491537  0.57559213  0.57777783  0.58034736  0.58455071
  0.58704449  0.59173113  0.59475049  0.59701959  0.59951337  0.60073038
  0.60418282  0.60420954  0.60450093  0.6069947   0.6075276   0.6091971
  0.61198226  0.61590775  0.61732749  0.61788447  0.61946359  0.61950477
  0.62168996  0.62195737  0.62249575  0.62569813  0.62997159  0.63003628
  0.63193247  0.63766504  0.63809056  0.63912242  0.6394138   0.6416162
  0.64190758  0.64439034  0.64440136  0.64938891  0.65095373  0.65188269
  0.65458024  0.65464711  0.65687024  0.65711176  0.65804077  0.65936402
  0.66053455  0.66062203  0.6618578   0.66228401  0.66435158  0.66462222
  0.66488447  0.66655397  0.66684535  0.6684748   0.66933913  0.67361279
  0.67432668  0.67510811  0.67801215  0.67931424  0.68070742  0.68151663
  0.68211188  0.68213905  0.68375307  0.68928935  0.68981122  0.69178312
  0.69398552  0.69493568  0.69532028  0.69607114  0.69897307  0.70146685
  0.70175823  0.70301624  0.70455587  0.7064544   0.70674579  0.70704965
  0.70756749  0.70782812  0.70828617  0.70894818  0.70923956  0.71049758
  0.71144196  0.71292503  0.71464777  0.71797891  0.72237504  0.72598916
  0.72708082  0.72949364  0.73107407  0.73326735  0.73674274  0.7378628
  0.73949593  0.7404229   0.74196253  0.7428963   0.7441317   0.74541045
  0.74631679  0.74695008  0.74944386  0.75007991  0.75310816  0.75538556
  0.75730837  0.75866171  0.75941897  0.75949588  0.75986868  0.76280559
  0.76440652  0.7669003   0.76692747  0.76818549  0.76882112  0.76939407
  0.77017255  0.77064256  0.77090432  0.77188785  0.7751601   0.7766112
  0.77809241  0.77999391  0.78435674  0.78685051  0.78983437  0.79138078
  0.79682562  0.7993194   0.80057741  0.80250871  0.80650935  0.80680073
  0.80929451  0.80973072  0.81423893  0.82035195  0.8272934   0.82743799
  0.82969836  0.83228096  0.83798407  0.8451003   0.84665444  0.84670116
  0.84846272  0.85168872  0.85253248  0.85721873  0.85775861  0.8597125
  0.86719384  0.88041837  0.89267148  0.89462538  0.89465574  0.89975753
  0.90046464  0.90460049  0.9058585   0.90632902  0.90803418  0.90958805
  0.91446061  0.91583361  0.92205693  0.92704449  0.92801183  0.93203204
  0.93256493  0.93701959  0.93827761  0.94077138  0.94200715  0.94326516
  0.94450092  0.94574792  0.94575894  0.9469947   0.94724201  0.95600469
  0.95822782  0.9607216   0.96189213  0.96195736  0.96282009  0.96445114
  0.96513819  0.96570915  0.96763196  0.96837628  0.9694387   0.97069671
  0.9716418   0.97413487  0.97527446  0.97568426  0.97817804  0.98215489
  0.98316559  0.98524512  0.98815315  0.99064692  1.01059714  1.01309092
  1.0155847   1.0164329   1.01936097  1.02057225  1.04180496  1.06903635
  1.085963    1.09490337  1.18722084  1.64802087]

  UserWarning,

2022-10-31 11:02:12,493:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14373561e+00 -5.70012693e-01 -4.97240939e-01 -4.66859112e-01
 -3.51236114e-01 -1.10337661e-01 -1.09337646e-01 -9.72805052e-02
 -9.38809151e-02 -9.00388052e-02 -3.74152838e-02 -2.78680818e-02
 -1.68677049e-02  5.80980708e-04  3.97522753e-02  4.75240221e-02
  5.95697462e-02  6.00827058e-02  6.91305057e-02  7.62875938e-02
  9.19447980e-02  9.47890882e-02  1.01724313e-01  1.14038118e-01
  1.17769758e-01  1.29634607e-01  1.30390927e-01  1.37518191e-01
  1.40044133e-01  1.46840056e-01  1.50567796e-01  1.68955894e-01
  1.73091335e-01  1.75903598e-01  1.80631183e-01  1.85791350e-01
  1.89410975e-01  1.94198407e-01  1.94411578e-01  1.94663462e-01
  1.97955313e-01  1.98489324e-01  2.00742921e-01  2.05333219e-01
  2.05768764e-01  2.07154371e-01  2.09546518e-01  2.11620963e-01
  2.14412587e-01  2.16677509e-01  2.18654773e-01  2.19387307e-01
  2.19793876e-01  2.23214708e-01  2.25017147e-01  2.25780540e-01
  2.26410209e-01  2.27467991e-01  2.28574611e-01  2.30096944e-01
  2.30103192e-01  2.31049853e-01  2.33516468e-01  2.34412203e-01
  2.35118093e-01  2.35607656e-01  2.36201810e-01  2.37132159e-01
  2.38792758e-01  2.41769797e-01  2.41809652e-01  2.42253222e-01
  2.42564558e-01  2.44404383e-01  2.46463182e-01  2.47708173e-01
  2.48437349e-01  2.48469760e-01  2.49036108e-01  2.50007834e-01
  2.50486106e-01  2.50731258e-01  2.50790169e-01  2.51295706e-01
  2.51808917e-01  2.52514824e-01  2.52625007e-01  2.53239417e-01
  2.54453227e-01  2.54668436e-01  2.56430967e-01  2.56950434e-01
  2.57737196e-01  2.58427144e-01  2.58957890e-01  2.61262106e-01
  2.62929950e-01  2.62978031e-01  2.63466226e-01  2.65023467e-01
  2.65863430e-01  2.66027181e-01  2.66309582e-01  2.66450900e-01
  2.69356799e-01  2.71180945e-01  2.71709311e-01  2.74668161e-01
  2.75125605e-01  2.75892040e-01  2.77342410e-01  2.77965037e-01
  2.78303355e-01  2.79175077e-01  2.80325555e-01  2.80489069e-01
  2.81678691e-01  2.86308603e-01  2.86657415e-01  2.86753294e-01
  2.86896241e-01  2.87110300e-01  2.87282589e-01  2.89092692e-01
  2.91639286e-01  2.92844701e-01  2.93443186e-01  2.95562423e-01
  2.95701493e-01  2.96048772e-01  2.96719617e-01  2.98334748e-01
  3.00229862e-01  3.00369692e-01  3.00601679e-01  3.00974439e-01
  3.01088933e-01  3.01343613e-01  3.02937986e-01  3.03121506e-01
  3.05403691e-01  3.05414632e-01  3.05529991e-01  3.07689524e-01
  3.09544947e-01  3.10047745e-01  3.10363938e-01  3.10714214e-01
  3.13859107e-01  3.14024667e-01  3.15433581e-01  3.16210961e-01
  3.16785211e-01  3.18493464e-01  3.20398478e-01  3.22392993e-01
  3.22761073e-01  3.23963109e-01  3.24331503e-01  3.25592528e-01
  3.25825392e-01  3.25994370e-01  3.26321545e-01  3.26754065e-01
  3.27707429e-01  3.28520923e-01  3.29851171e-01  3.29976653e-01
  3.30325008e-01  3.30511310e-01  3.31729442e-01  3.32929170e-01
  3.33064942e-01  3.33296512e-01  3.34168210e-01  3.34218646e-01
  3.34635259e-01  3.35396290e-01  3.35643629e-01  3.36039765e-01
  3.36150160e-01  3.36891802e-01  3.37446884e-01  3.37882255e-01
  3.40408382e-01  3.41053935e-01  3.41082011e-01  3.41500931e-01
  3.41591697e-01  3.42528522e-01  3.42957555e-01  3.43754993e-01
  3.43915455e-01  3.44215465e-01  3.44272408e-01  3.44695459e-01
  3.45839873e-01  3.46658420e-01  3.47224615e-01  3.49957858e-01
  3.50816507e-01  3.50835830e-01  3.51353920e-01  3.53415829e-01
  3.54159389e-01  3.54671340e-01  3.54679234e-01  3.54730354e-01
  3.55028756e-01  3.55532046e-01  3.58903691e-01  3.58905253e-01
  3.59624043e-01  3.60258021e-01  3.60352543e-01  3.60508744e-01
  3.60839382e-01  3.60847096e-01  3.60890585e-01  3.60914819e-01
  3.61429526e-01  3.62122273e-01  3.65822213e-01  3.65879817e-01
  3.66574140e-01  3.67239735e-01  3.67305454e-01  3.67940663e-01
  3.67994460e-01  3.70652492e-01  3.72791133e-01  3.73594172e-01
  3.73769162e-01  3.73923530e-01  3.74251939e-01  3.75704487e-01
  3.76202457e-01  3.76668272e-01  3.77109405e-01  3.77461180e-01
  3.78448876e-01  3.78930912e-01  3.80190244e-01  3.80678646e-01
  3.83414975e-01  3.83622464e-01  3.84381580e-01  3.84592096e-01
  3.84636379e-01  3.85756355e-01  3.86159832e-01  3.87446522e-01
  3.87456680e-01  3.87462581e-01  3.88575281e-01  3.89720628e-01
  3.90388201e-01  3.90841828e-01  3.91399822e-01  3.91456461e-01
  3.91822474e-01  3.92674801e-01  3.95630522e-01  3.95728211e-01
  3.96089785e-01  3.96790038e-01  3.97232307e-01  3.99027577e-01
  3.99128385e-01  3.99351317e-01  3.99556535e-01  3.99984790e-01
  4.00363881e-01  4.06851515e-01  4.07451217e-01  4.08408973e-01
  4.08741084e-01  4.08792408e-01  4.08953202e-01  4.10106404e-01
  4.10388387e-01  4.10920551e-01  4.11272302e-01  4.12834909e-01
  4.15703785e-01  4.16872160e-01  4.17061353e-01  4.18505024e-01
  4.19389753e-01  4.20349354e-01  4.22435931e-01  4.25986124e-01
  4.26746878e-01  4.32780897e-01  4.32853956e-01  4.35582712e-01
  4.35918877e-01  4.36497022e-01  4.38896169e-01  4.42518521e-01
  4.43056993e-01  4.45314622e-01  4.45524588e-01  4.46884830e-01
  4.47578448e-01  4.47821143e-01  4.49353248e-01  4.50731881e-01
  4.52770461e-01  4.53372767e-01  4.54775889e-01  4.55612776e-01
  4.55659783e-01  4.57954765e-01  4.61524195e-01  4.62602499e-01
  4.63655507e-01  4.63828229e-01  4.63990786e-01  4.64908871e-01
  4.67342055e-01  4.69162274e-01  4.70031689e-01  4.73085721e-01
  4.73707842e-01  4.76755361e-01  4.79120954e-01  4.79583729e-01
  4.84090680e-01  4.92393654e-01  5.01303424e-01  5.05861966e-01
  5.08304252e-01  5.14990457e-01  5.17280650e-01  5.18935241e-01
  5.21014564e-01  5.22477892e-01  5.34237524e-01  5.36880137e-01
  5.39576851e-01  5.43621707e-01  5.46215414e-01  5.48224140e-01
  5.50916534e-01  5.56629317e-01  5.63039494e-01  5.64953191e-01
  5.67022123e-01  5.69885459e-01  5.74810619e-01  5.75315849e-01
  5.79608913e-01  5.82008060e-01  5.86263823e-01  5.86806355e-01
  5.88793534e-01  5.89569656e-01  5.94003796e-01  5.96402943e-01
  5.99267816e-01  6.02364511e-01  6.05992184e-01  6.13196973e-01
  6.15053588e-01  6.17374179e-01  6.20394414e-01  6.22792822e-01
  6.23132104e-01  6.25192708e-01  6.26081888e-01  6.27591855e-01
  6.29991002e-01  6.30561910e-01  6.30886054e-01  6.32390149e-01
  6.34338287e-01  6.34789297e-01  6.37188444e-01  6.37517802e-01
  6.39587591e-01  6.39916949e-01  6.43290078e-01  6.45226187e-01
  6.45297858e-01  6.46785032e-01  6.47727374e-01  6.50024481e-01
  6.51706589e-01  6.51825737e-01  6.53982473e-01  6.56381620e-01
  6.57239997e-01  6.58335702e-01  6.58520252e-01  6.58780767e-01
  6.58887538e-01  6.59305793e-01  6.59468244e-01  6.61179915e-01
  6.63133996e-01  6.63908420e-01  6.64104087e-01  6.65533143e-01
  6.65978209e-01  6.68377356e-01  6.70015716e-01  6.71439190e-01
  6.71909959e-01  6.72786366e-01  6.73175650e-01  6.75185513e-01
  6.75622720e-01  6.77973944e-01  6.80533607e-01  6.85044520e-01
  6.85171386e-01  6.86655505e-01  6.87125467e-01  6.87511978e-01
  6.88012063e-01  6.88884315e-01  6.91923761e-01  6.92172965e-01
  6.94767974e-01  6.95637074e-01  6.96716111e-01  6.96722056e-01
  6.97167121e-01  6.97491265e-01  6.99121203e-01  7.00458557e-01
  7.02490441e-01  7.03156235e-01  7.03919497e-01  7.05498018e-01
  7.08717791e-01  7.08773572e-01  7.09236872e-01  7.09604387e-01
  7.09649764e-01  7.09715914e-01  7.11116938e-01  7.13571867e-01
  7.14458427e-01  7.15915232e-01  7.15971014e-01  7.18314379e-01
  7.23168455e-01  7.27584221e-01  7.28352498e-01  7.29292577e-01
  7.30634259e-01  7.31196711e-01  7.33595858e-01  7.33679353e-01
  7.41485224e-01  7.43192446e-01  7.44704997e-01  7.44760779e-01
  7.49190949e-01  7.49503292e-01  7.50389887e-01  7.51332828e-01
  7.51980531e-01  7.56700733e-01  7.57587329e-01  7.59843138e-01
  7.59986476e-01  7.62689847e-01  7.65118402e-01  7.67513276e-01
  7.69583064e-01  7.71594533e-01  7.71982211e-01  7.73319476e-01
  7.74381358e-01  7.80031416e-01  7.81578800e-01  7.83977947e-01
  7.86377094e-01  7.88386957e-01  7.88776241e-01  7.89616543e-01
  7.91175388e-01  7.93574535e-01  7.93952331e-01  7.95973682e-01
  7.97983545e-01  7.98372829e-01  7.99315770e-01  8.03171123e-01
  8.05180986e-01  8.06175851e-01  8.06298017e-01  8.06513211e-01
  8.06568992e-01  8.07649386e-01  8.07969418e-01  8.09663658e-01
  8.10368565e-01  8.22688444e-01  8.28629476e-01  8.28800955e-01
  8.30213908e-01  8.36369899e-01  8.39158330e-01  8.40922639e-01
  8.44259265e-01  8.57450647e-01  8.80468856e-01  8.83962261e-01
  8.84225152e-01  8.86595776e-01  8.90735117e-01  8.97932559e-01
  9.00331706e-01  9.03458599e-01  9.05130000e-01  9.05468543e-01
  9.07529147e-01  9.08471489e-01  9.12327441e-01  9.16736451e-01
  9.17125735e-01  9.17342032e-01  9.19135598e-01  9.21381497e-01
  9.21534745e-01  9.26722324e-01  9.27293231e-01  9.29121471e-01
  9.31520618e-01  9.33919765e-01  9.34217957e-01  9.36318912e-01
  9.36535209e-01  9.38328775e-01  9.40479787e-01  9.41117206e-01
  9.49953008e-01  9.51037939e-01  9.51873658e-01  9.52723658e-01
  9.56023071e-01  9.57911236e-01  9.59921099e-01  9.63033674e-01
  9.64680439e-01  9.68698645e-01  9.69517687e-01  9.75230291e-01
  9.76172596e-01  9.76715129e-01  9.83856788e-01  9.84295011e-01
  9.88710864e-01  9.88801459e-01  9.91478647e-01  9.94423468e-01
  9.97099126e-01  1.00070660e+00  1.00363077e+00  1.00550489e+00
  1.00733443e+00  1.00790404e+00  1.01030319e+00  1.01698564e+00
  1.01831995e+00  1.02211860e+00  1.04396582e+00  1.05775952e+00]

  UserWarning,

2022-10-31 11:02:12,493:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86838184 -0.4162217  -0.34501471 -0.30478228 -0.1980063  -0.13967709
 -0.1284054  -0.06729697 -0.03472098 -0.0340365  -0.0263516  -0.01094653
  0.00368368  0.01329788  0.02390228  0.03834598  0.0394689   0.0411893
  0.04390586  0.05374874  0.05378996  0.05437416  0.06622262  0.08364501
  0.08592252  0.09823717  0.10052055  0.10327234  0.10686868  0.10845667
  0.10871827  0.12183695  0.12852057  0.13017266  0.13246469  0.13543075
  0.13742164  0.14068731  0.14243097  0.14966285  0.1514042   0.15235997
  0.15835264  0.16182235  0.16206588  0.16607312  0.16693317  0.16907892
  0.17143992  0.17191538  0.17529989  0.18353606  0.1848029   0.18592183
  0.18612489  0.18742978  0.19267727  0.19314548  0.19346119  0.19391005
  0.19686382  0.19858248  0.20074874  0.20136356  0.20293017  0.20784257
  0.21074845  0.21236285  0.2157182   0.21710681  0.21932425  0.22058348
  0.22071201  0.22171023  0.22453284  0.22726721  0.23147221  0.23619296
  0.23666677  0.2367486   0.23713181  0.24564271  0.24637143  0.24705837
  0.24716517  0.24733782  0.24876559  0.25070208  0.2523058   0.25257604
  0.25349147  0.25552383  0.25631956  0.25642644  0.25708058  0.25713867
  0.25725298  0.25973376  0.2612723   0.26197751  0.26299295  0.26367214
  0.26529006  0.26661749  0.26735088  0.26841313  0.27099379  0.27200248
  0.27216762  0.27328023  0.27386172  0.27444797  0.27477828  0.27491115
  0.27590316  0.2764938   0.27735432  0.27742492  0.27804886  0.28098029
  0.2812084   0.28203981  0.28384907  0.28405936  0.28418944  0.28442383
  0.28582102  0.28626392  0.28627727  0.28712702  0.28758147  0.28853118
  0.28963685  0.29017489  0.2910957   0.29113461  0.29373926  0.29559755
  0.29586983  0.29646019  0.29653764  0.29792669  0.29798855  0.29841489
  0.29881307  0.2989323   0.29897531  0.29927547  0.30023952  0.30118086
  0.30119702  0.3014272   0.30217187  0.30292266  0.30457278  0.30509954
  0.30524606  0.30716968  0.30943471  0.30975939  0.30978325  0.30998422
  0.31061164  0.31212364  0.31267834  0.31279543  0.31442427  0.31531979
  0.31794945  0.31966574  0.32078111  0.32403136  0.32475517  0.32597118
  0.32665424  0.32674068  0.32746541  0.32869246  0.32914194  0.33016377
  0.33173065  0.33179249  0.33210058  0.3334353   0.33348665  0.33420218
  0.33424985  0.33623247  0.33653521  0.3369212   0.33728725  0.3377365
  0.33990904  0.3436232   0.34405918  0.34530494  0.3459732   0.34665775
  0.34760875  0.34811931  0.34838183  0.35049004  0.35263057  0.35281272
  0.35291618  0.35331679  0.35430184  0.3544011   0.35482738  0.35519122
  0.35590937  0.35681416  0.35770588  0.35881161  0.35951474  0.3596768
  0.3609671   0.36155881  0.36518572  0.36734833  0.3678763   0.36886878
  0.36982598  0.37081889  0.37117558  0.37134577  0.37182285  0.37235353
  0.37439159  0.37558801  0.37589703  0.37659766  0.37701099  0.37733022
  0.37798038  0.37855727  0.37885702  0.38197758  0.38222373  0.38302288
  0.38373069  0.38410367  0.38507687  0.38564481  0.3858884   0.38615843
  0.38643171  0.38704934  0.38732772  0.38770219  0.3882477   0.38876961
  0.38963771  0.39159672  0.39167262  0.39243984  0.39248252  0.39260276
  0.39287303  0.39320678  0.3939828   0.39401     0.39483049  0.39530889
  0.39547806  0.39677376  0.39830616  0.39844022  0.3994639   0.39959623
  0.39975912  0.40074745  0.4027579   0.4037171   0.40497733  0.40729427
  0.40772722  0.40822889  0.40870319  0.40872175  0.40873924  0.41263459
  0.41459038  0.41519151  0.41550086  0.4158436   0.41916654  0.42148332
  0.42186905  0.4219193   0.42235729  0.42247928  0.42300731  0.42428966
  0.42447559  0.42456296  0.42540791  0.42790243  0.43025818  0.43231818
  0.43322566  0.43414956  0.43643378  0.43874422  0.44047568  0.4437993
  0.4502761   0.45261014  0.45498038  0.45566046  0.45641913  0.45667953
  0.45742384  0.45805621  0.45905775  0.45993453  0.4623745   0.46251939
  0.46259475  0.46341151  0.46437306  0.46439001  0.46511071  0.46583597
  0.46726244  0.46769118  0.46827309  0.46845944  0.46918477  0.47308764
  0.47333401  0.48026067  0.48233978  0.48731566  0.49028506  0.49375778
  0.49492652  0.4997721   0.50330516  0.50340724  0.50546938  0.50554842
  0.50608354  0.50657388  0.50818595  0.50891486  0.50923587  0.51197998
  0.51225881  0.5291019   0.52932203  0.53050889  0.5312162   0.53918324
  0.54764492  0.54961631  0.54982444  0.54989238  0.55133082  0.55202315
  0.55375432  0.55438344  0.55784819  0.56048257  0.56091576  0.56152305
  0.56253724  0.56287018  0.56345738  0.56432106  0.56526344  0.56598217
  0.56689217  0.56758693  0.5754692   0.57612056  0.58024547  0.5842794
  0.58482412  0.58602699  0.586598    0.58698223  0.58935932  0.5908411
  0.59514566  0.59719917  0.59786223  0.60057879  0.60870813  0.60872849
  0.60916717  0.61208692  0.61687818  0.61959475  0.62221127  0.62469955
  0.62629174  0.63172487  0.63251452  0.6344277   0.63552957  0.63589414
  0.63649552  0.63821275  0.63861071  0.64066422  0.64132727  0.64175131
  0.64364588  0.64517241  0.64636244  0.64947697  0.65194661  0.65219354
  0.6549101   0.66034323  0.66076072  0.66216618  0.66266184  0.6630598
  0.6651296   0.66577636  0.66775724  0.67054497  0.67081153  0.67120949
  0.6735281   0.67392606  0.67465667  0.675984    0.6778435   0.67935919
  0.68045047  0.68124461  0.68227917  0.68371399  0.68439436  0.68474693
  0.68711093  0.68726196  0.68858676  0.68982749  0.69115237  0.69254406
  0.6952254   0.69526062  0.69627074  0.69772839  0.69797719  0.70069375
  0.70314522  0.70334475  0.7033488   0.70578916  0.70587808  0.70612688
  0.70652484  0.70859465  0.70884345  0.71156001  0.71247052  0.71402778
  0.71424136  0.71467454  0.71674434  0.7167482   0.71699315  0.71700678
  0.72217747  0.72327345  0.7324171   0.73386266  0.73783947  0.74129947
  0.74452215  0.7451871   0.74653595  0.746889    0.74934313  0.74959193
  0.74960556  0.75123346  0.75169304  0.7517104   0.75177556  0.75329774
  0.75439193  0.75477626  0.75730844  0.76036525  0.76082627  0.76494712
  0.76590496  0.76859643  0.77405465  0.77483316  0.78220435  0.78227098
  0.78268451  0.78492091  0.79035405  0.79307061  0.79578718  0.79850374
  0.80078712  0.80122031  0.80393687  0.80544566  0.80937     0.81095021
  0.82023627  0.82246376  0.827988    0.82838596  0.831912    0.83568303
  0.83610247  0.83750372  0.83771686  0.83925222  0.84132203  0.84196879
  0.84468535  0.84619414  0.85380311  0.85410471  0.857835    0.86055156
  0.86175828  0.8630246   0.86370131  0.8637268   0.87120425  0.87353185
  0.8864019   0.88765782  0.88845796  0.89001433  0.89231683  0.90531182
  0.90541785  0.90760288  0.90904492  0.918265    0.91835392  0.92171725
  0.93193674  0.93465331  0.93486689  0.93530007  0.94164371  0.94280301
  0.94344977  0.94616634  0.95135254  0.95159947  0.96246573  0.96305529
  0.96444661  0.96807964  0.97424249  0.97540179  0.97666565  0.98371968
  0.98626805  0.99170119  0.99441775  0.99985088  1.00256745  1.00528401
  1.0088195   1.01071714  1.01594441  1.02115022  1.02158341  1.02712257
  1.02919237  1.0334274   1.05036606  1.07234552  1.07777865  1.08903399
  1.42095824  1.66752361]

  UserWarning,

2022-10-31 11:02:12,524:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24194797 -0.84419775 -0.60693611 -0.40250413 -0.38078738 -0.24163741
 -0.16921183 -0.1044453  -0.1032711  -0.09339279 -0.08550376 -0.08414727
 -0.05753256 -0.05487387 -0.03848074 -0.03501986 -0.01256979 -0.00582977
 -0.00403834  0.00838846  0.01732571  0.02044145  0.02498321  0.0259042
  0.03474579  0.05187124  0.05781138  0.06219944  0.06392184  0.09782585
  0.11172976  0.12673803  0.13431239  0.13534562  0.13742668  0.14325894
  0.14374922  0.14416599  0.14956382  0.15030022  0.15192921  0.15541783
  0.15647455  0.15699477  0.15699883  0.1580581   0.15945985  0.16203864
  0.16318305  0.16421437  0.16917848  0.17124075  0.17433662  0.17763702
  0.17825802  0.18686835  0.19049503  0.19169473  0.19211793  0.19284836
  0.19477492  0.19665897  0.19725865  0.19927061  0.19964648  0.19991583
  0.20005353  0.20507564  0.20683075  0.20734666  0.20902668  0.20949185
  0.21259572  0.21538975  0.21580861  0.21742839  0.21774558  0.21774654
  0.22214797  0.22366603  0.22428223  0.22458386  0.22471285  0.22554627
  0.22627782  0.22815116  0.23082553  0.23301337  0.23904117  0.23910414
  0.24100187  0.2410416   0.24143609  0.2417633   0.24548442  0.24693015
  0.24759788  0.24879089  0.25024618  0.2509244   0.2512435   0.25156084
  0.25787422  0.25880685  0.26157628  0.26167447  0.26452477  0.26487773
  0.26696599  0.26769566  0.2678095   0.26800434  0.26868639  0.26977468
  0.2699369   0.27031504  0.27045193  0.27069761  0.27156818  0.27173318
  0.27175027  0.27317142  0.27405799  0.27461939  0.28164931  0.28184533
  0.28241683  0.28267359  0.28356957  0.2842039   0.28455937  0.28456142
  0.28679553  0.28791818  0.28812227  0.28918756  0.289397    0.28962978
  0.29032217  0.29060298  0.29075738  0.29133833  0.29169289  0.29180384
  0.29259643  0.29392365  0.29445183  0.29516252  0.29582985  0.29674996
  0.29682996  0.29685291  0.29847564  0.29884191  0.29977104  0.29993794
  0.30363438  0.30419349  0.30445826  0.30622599  0.30645135  0.30672153
  0.30729403  0.30817941  0.30841695  0.30906594  0.30998953  0.31176733
  0.31329342  0.31423203  0.31433617  0.31437619  0.31456746  0.31465037
  0.31864183  0.31910114  0.32169752  0.32300928  0.32418067  0.32513643
  0.32619141  0.32640176  0.3267397   0.32720084  0.32761983  0.32781635
  0.32843027  0.32944887  0.33042621  0.33293227  0.33324225  0.33770183
  0.33845096  0.3386033   0.33957879  0.33958711  0.34037982  0.34046935
  0.34048039  0.3406224   0.34107704  0.34264339  0.34423549  0.34637294
  0.34686134  0.3476103   0.34810343  0.34858122  0.34864333  0.34920478
  0.35278689  0.35297722  0.35387322  0.35603449  0.35648441  0.35860548
  0.35930156  0.36042819  0.36199685  0.36265625  0.36372923  0.36374398
  0.36375204  0.36388844  0.36403059  0.36411462  0.36442531  0.36486431
  0.36739865  0.36797795  0.36817473  0.36950421  0.3731721   0.37374609
  0.37403703  0.37650864  0.37738133  0.37785964  0.38027268  0.38272166
  0.38507513  0.38509286  0.38536131  0.38685032  0.38710101  0.38746406
  0.38753897  0.38765696  0.38773251  0.38794028  0.38865106  0.38931515
  0.38941719  0.38949344  0.38983526  0.38989653  0.39127654  0.39153241
  0.39204988  0.39239655  0.39421461  0.39457767  0.39511083  0.39631528
  0.3970948   0.3973278   0.39914046  0.39932007  0.39989873  0.40042602
  0.40224528  0.40292443  0.4030223   0.40412152  0.40440031  0.40494198
  0.40694782  0.40765774  0.40880488  0.40906699  0.41031519  0.41079913
  0.41185754  0.41258532  0.41378836  0.41555966  0.41999468  0.42258202
  0.42349569  0.42405704  0.42484868  0.42771504  0.42825626  0.42840844
  0.43300722  0.4345335   0.43531996  0.43633591  0.43733194  0.43783528
  0.43789325  0.4399014   0.44026445  0.44154558  0.44172092  0.44263135
  0.44357427  0.44500686  0.44628604  0.44948108  0.44967225  0.4512558
  0.45138596  0.45198011  0.45212047  0.4531742   0.45570418  0.45634256
  0.45855144  0.46449103  0.46469634  0.46649949  0.47263431  0.47796143
  0.47843994  0.4788777   0.47990696  0.48089973  0.48389914  0.486715
  0.49340992  0.49399987  0.49411947  0.49764769  0.49886188  0.50025322
  0.5039352   0.50816465  0.50922259  0.51067708  0.51441435  0.5153858
  0.5173044   0.51767853  0.5225739   0.52968751  0.53834701  0.5384
  0.54137426  0.54255046  0.54389154  0.54628592  0.5472298   0.55714737
  0.55969032  0.56384887  0.56556066  0.56661469  0.56715291  0.56867475
  0.57054863  0.57650182  0.57810332  0.58018243  0.58196759  0.5840787
  0.58649233  0.58670999  0.58964322  0.59781519  0.598566    0.59891098
  0.60451127  0.61021644  0.61313819  0.61575047  0.61678481  0.62006977
  0.6202518   0.62152721  0.62197367  0.62286407  0.63237626  0.63447901
  0.63590205  0.63685022  0.64015242  0.64159262  0.64203299  0.64396382
  0.64870623  0.65246682  0.65344863  0.65581984  0.65768456  0.65819104
  0.65896527  0.66056224  0.66281146  0.66289945  0.66293344  0.66344296
  0.66530464  0.66560456  0.66995597  0.67072849  0.67241825  0.67478945
  0.67524136  0.67716066  0.67953186  0.67977293  0.6801658   0.68190306
  0.68214413  0.68451533  0.68472617  0.6849082   0.6867591   0.68688654
  0.68691391  0.68727941  0.68898268  0.69005801  0.69162894  0.69375907
  0.69874255  0.69913542  0.70111375  0.70288082  0.70324388  0.70387782
  0.70525203  0.70585615  0.70813628  0.70972316  0.71030879  0.7103235
  0.71035749  0.71041653  0.71099143  0.71173413  0.7131761   0.7174224
  0.71810504  0.71875351  0.72175165  0.72320445  0.72413241  0.72491099
  0.72927841  0.72956818  0.73049421  0.73151444  0.73164961  0.73589734
  0.73598842  0.73639202  0.7388017   0.74603915  0.74637312  0.74824803
  0.75021563  0.75061923  0.75130187  0.75367307  0.75536164  0.76010404
  0.76721765  0.76723235  0.76958885  0.77196005  0.77621338  0.77661138
  0.77670246  0.77709345  0.77900866  0.77907366  0.77951403  0.78360004
  0.78381606  0.78839594  0.78855847  0.79056662  0.79092967  0.79804328
  0.80041448  0.80278568  0.80515688  0.8085526   0.81077907  0.81227049
  0.81902104  0.82649771  0.82748337  0.82950285  0.83357732  0.84072492
  0.84309612  0.84546732  0.846902    0.85061611  0.85350472  0.85599348
  0.86217937  0.86379976  0.87367556  0.87568976  0.87802697  0.8816693
  0.88342796  0.8840405   0.88751178  0.88814572  0.88988298  0.89225419
  0.89522858  0.90242892  0.9088526   0.91375289  0.91833741  0.92134255
  0.92307982  0.93256462  0.93295562  0.93310749  0.93520671  0.9362597
  0.93730703  0.94204943  0.94268338  0.94342608  0.94442064  0.94679184
  0.94691776  0.94979698  0.95231581  0.95425928  0.95453939  0.95691059
  0.95731799  0.95928179  0.96002449  0.9711378   0.97287506  0.973509
  0.97434037  0.97588021  0.9824735   0.98299381  0.98536502  0.98773622
  0.99247862  0.99301909  0.99685797  0.9989243   0.99989215  1.00187236
  1.00196343  1.00486779  1.01519609  1.01563634  1.02702113  1.02737872
  1.02989755  1.03503555  1.0400163   1.04141377  1.04282787]

  UserWarning,

2022-10-31 11:02:12,556:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81219705 -0.51089605 -0.43866517 -0.37893118 -0.34634484 -0.3359245
 -0.25773317 -0.25435879 -0.12485832 -0.11859412 -0.09815983 -0.08087424
 -0.07552297 -0.07194131 -0.03130715 -0.03060841 -0.02587335 -0.02055669
  0.01539194  0.02053805  0.02863662  0.03610948  0.03945943  0.04566762
  0.05250905  0.05257065  0.06292916  0.06551879  0.08744528  0.09413208
  0.10170225  0.10210926  0.10658488  0.1123544   0.11434204  0.11683864
  0.11766464  0.11808358  0.12737825  0.13843173  0.14029367  0.14259209
  0.15297841  0.15754003  0.16303453  0.1665088   0.17717489  0.17959636
  0.18250296  0.19469571  0.19505191  0.19783217  0.19817061  0.19834019
  0.19963234  0.20149143  0.20283392  0.20305354  0.20421738  0.20671652
  0.20948633  0.21053412  0.2146518   0.21662185  0.21752118  0.21839719
  0.2190275   0.2228486   0.22568004  0.2263259   0.23048969  0.2311156
  0.23172223  0.23583047  0.23787254  0.23839018  0.24051224  0.24082491
  0.24107988  0.24131903  0.24369319  0.24387164  0.24398317  0.24635369
  0.24810895  0.24893144  0.25117699  0.25138521  0.25203679  0.25284204
  0.25356743  0.25457878  0.25458084  0.25519708  0.25599929  0.25760307
  0.25769993  0.25925479  0.26037581  0.26088318  0.2620578   0.26217824
  0.26395018  0.26401469  0.26429811  0.26443887  0.26493539  0.26538749
  0.26848426  0.26871262  0.26889502  0.26897605  0.26911809  0.26966634
  0.27097376  0.27104305  0.27122971  0.27265608  0.27377168  0.27486936
  0.2773301   0.27754153  0.27908151  0.28053696  0.28185057  0.28310103
  0.28409925  0.28438621  0.28465877  0.28498257  0.28537244  0.28558351
  0.28672081  0.28717727  0.28745869  0.28758295  0.28789283  0.28952915
  0.2898024   0.28985368  0.28992554  0.2903031   0.2908729   0.29100535
  0.29114302  0.29295482  0.29354277  0.29412604  0.29431757  0.29596697
  0.29807516  0.29834155  0.29864787  0.29981589  0.30036115  0.3012905
  0.30278503  0.30280127  0.30315045  0.30436978  0.30440015  0.30504682
  0.30735261  0.30750092  0.31005774  0.31157789  0.31291965  0.3149045
  0.31500822  0.31521267  0.31748521  0.31854167  0.31926676  0.31940544
  0.31992948  0.32007662  0.32142702  0.32281632  0.32388934  0.32505618
  0.325394    0.32564456  0.32675348  0.32684679  0.32709985  0.33040602
  0.33096728  0.33214233  0.33329148  0.334534    0.33795799  0.3391392
  0.3392043   0.33940811  0.33979728  0.34061624  0.34077884  0.34121875
  0.34179887  0.3421378   0.34224776  0.34346589  0.34407164  0.34534393
  0.34580396  0.3459266   0.34883301  0.34929817  0.35137195  0.35256222
  0.3536592   0.35607378  0.35731553  0.35814705  0.35829731  0.35976451
  0.36077129  0.36185894  0.36188584  0.36237111  0.36284178  0.3632492
  0.36335432  0.36444259  0.36631566  0.36656523  0.36669753  0.36719791
  0.3677919   0.36789852  0.36991664  0.3700443   0.37142498  0.37181744
  0.37274885  0.37292875  0.37293194  0.37442781  0.3745111   0.37827511
  0.37839438  0.37952488  0.37972929  0.3812321   0.38133165  0.3815468
  0.38210745  0.38217755  0.38315143  0.38382605  0.38453131  0.38831188
  0.38933519  0.38960002  0.38969699  0.38971902  0.39007773  0.39070082
  0.3913012   0.39159494  0.39188358  0.39376085  0.39383279  0.39458935
  0.39524504  0.39629964  0.39710408  0.39799278  0.39914648  0.39939704
  0.40048156  0.40060111  0.40209658  0.40343072  0.40567489  0.40594098
  0.40628968  0.4063406   0.40936457  0.40979436  0.41049896  0.41097181
  0.41272646  0.41413823  0.41480637  0.41523148  0.41898439  0.42008924
  0.42027267  0.42443198  0.42456188  0.4251781   0.4253635   0.42545647
  0.42779654  0.42909323  0.43061683  0.43080229  0.43147292  0.43310381
  0.43403205  0.43403361  0.43609901  0.43683657  0.43859755  0.43860328
  0.43937632  0.44767248  0.4481551   0.45082585  0.45300545  0.4536478
  0.45515548  0.45681604  0.45923871  0.46310204  0.46624999  0.46664597
  0.46673787  0.46837651  0.46969947  0.47617474  0.47736303  0.48154006
  0.48155882  0.48750264  0.48763206  0.48864913  0.48875542  0.49102516
  0.49120188  0.49211861  0.49333622  0.50230752  0.50237791  0.504216
  0.50647707  0.50773491  0.51100701  0.51267951  0.51398465  0.5253638
  0.52770028  0.52868426  0.53104157  0.53413811  0.53880994  0.54291528
  0.55943054  0.56025454  0.56228717  0.56575812  0.56633673  0.56764064
  0.56885492  0.56956945  0.57072201  0.57285103  0.58099625  0.58108053
  0.58165692  0.58739783  0.58742916  0.59301327  0.59600274  0.59769819
  0.59778768  0.59858569  0.60111204  0.60179756  0.60438719  0.60697682
  0.60858853  0.60898387  0.6147015   0.61545565  0.61547371  0.61733533
  0.61890164  0.62251459  0.6236705   0.62525472  0.62573124  0.6294158
  0.62990378  0.63107315  0.63463532  0.63486333  0.63546273  0.63805236
  0.6408148   0.6414142   0.64323162  0.64519901  0.64582125  0.64699062
  0.65359013  0.65403246  0.65545656  0.65617976  0.65633026  0.65843951
  0.65872519  0.65883068  0.6595416   0.66093391  0.66135902  0.66394865
  0.66396147  0.66653828  0.66666706  0.66731049  0.6691279   0.67430716
  0.67666213  0.67689679  0.67814805  0.67948642  0.68024411  0.68025863
  0.68496166  0.68542878  0.68769763  0.69061715  0.69171136  0.69392129
  0.69517469  0.6966334   0.70097566  0.70108101  0.70279308  0.7040601
  0.70611071  0.70615492  0.70655208  0.70754722  0.70797234  0.70960233
  0.71056196  0.71071247  0.71306744  0.71512566  0.71541134  0.71564714
  0.71574122  0.71589172  0.71816226  0.723085    0.72423716  0.72537653
  0.72603833  0.72796616  0.72883987  0.72953001  0.73097651  0.73165708
  0.73314542  0.73491105  0.73691397  0.73710304  0.73714872  0.73982009
  0.7409143   0.74092577  0.74146892  0.74304417  0.74350393  0.74576368
  0.74609356  0.75127282  0.75150878  0.75172141  0.75660258  0.7590417
  0.75959632  0.76209537  0.76218595  0.76317892  0.76475674  0.76907034
  0.77198985  0.77383099  0.77457948  0.77538841  0.77716911  0.77878082
  0.77942885  0.77975873  0.78234836  0.78566505  0.78752762  0.78825468
  0.78913934  0.79026775  0.79270688  0.79358673  0.79496662  0.79529651
  0.79946138  0.80306539  0.80559407  0.80565502  0.81860316  0.82548394
  0.82652255  0.82735413  0.8286318   0.83106406  0.83429144  0.83614633
  0.83682012  0.83844738  0.84148471  0.8435871   0.84416957  0.85220739
  0.85907329  0.85983152  0.86235224  0.86276967  0.8694361   0.87450994
  0.88067654  0.88233454  0.89048433  0.89052853  0.89311816  0.89322446
  0.89829742  0.90621681  0.9075952   0.91653112  0.91715188  0.92650873
  0.92678333  0.92937296  0.93163271  0.93206889  0.93245499  0.93714185
  0.93875357  0.93973148  0.93988198  0.94232111  0.94491074  0.94619257
  0.94652245  0.94761954  0.94947443  0.95008999  0.95283012  0.95785888
  0.95863109  0.96026912  0.96044851  0.96577827  0.96821739  0.97252384
  0.97811085  0.97857591  0.9864953   0.98989385  0.99167455  0.99685381
  0.99944344  1.00100975  1.00160796  1.00706182  1.00721233  1.00780073
  1.01655162  1.02016047  1.02533973  1.02731379  1.03439045  1.06508329
  1.07392711]

  UserWarning,

2022-10-31 11:02:12,571:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04602623 -0.84332998 -0.75815329 -0.72378269 -0.65507834 -0.57382694
 -0.51062582 -0.41031553 -0.39184235 -0.38498398 -0.35385629 -0.35319306
 -0.28530344 -0.28401411 -0.26001254 -0.24322189 -0.2379901  -0.22543094
 -0.18854615 -0.15641655 -0.13776864 -0.10165344 -0.08344072 -0.08151435
 -0.04285185 -0.03609068 -0.02911575 -0.02763678 -0.02334932 -0.00797357
 -0.00779856 -0.00316579  0.01604145  0.04585153  0.0459524   0.07021959
  0.07819721  0.07943861  0.08137006  0.09220573  0.09998941  0.10073869
  0.10184522  0.10397276  0.11101953  0.1121763   0.11357561  0.12020607
  0.12799487  0.14236152  0.15393916  0.15488832  0.16166636  0.16417037
  0.16570563  0.17273235  0.17380542  0.1738621   0.17461713  0.17496176
  0.17618227  0.17814193  0.18168691  0.18827384  0.19310441  0.19312773
  0.19396588  0.19761632  0.19763355  0.20016568  0.20069067  0.20082482
  0.20154775  0.20187227  0.20309592  0.21412437  0.21971231  0.21993504
  0.22045778  0.22116935  0.22349152  0.22352586  0.22408235  0.22513635
  0.22743463  0.22800574  0.23074463  0.23198206  0.23486439  0.23657057
  0.23713044  0.23871501  0.24032984  0.24190612  0.24206001  0.24286801
  0.24353644  0.24383856  0.24532357  0.2456746   0.25431716  0.25680667
  0.25738799  0.2574842   0.25918734  0.25951401  0.26046965  0.26069937
  0.2629975   0.26402185  0.26462454  0.26565811  0.2681493   0.26862298
  0.26890069  0.27031524  0.27033501  0.27162932  0.27198062  0.27280418
  0.27293207  0.27341383  0.27424996  0.27531101  0.27555241  0.27565565
  0.2757062   0.27621206  0.27701716  0.27896984  0.28035345  0.28041623
  0.28131452  0.28252967  0.2830827   0.28453233  0.28511573  0.28694259
  0.28884817  0.28974463  0.29007473  0.29034247  0.29069461  0.29136547
  0.29151674  0.29522217  0.29602975  0.29665803  0.2968958   0.29776048
  0.29794264  0.29795022  0.29942936  0.30393445  0.30417144  0.30506949
  0.30730329  0.30850285  0.3094855   0.31146253  0.31155781  0.3119878
  0.3129221   0.31311156  0.313976    0.31451404  0.31755496  0.31862324
  0.31917805  0.31933174  0.31949389  0.32128953  0.32159751  0.3222298
  0.32247224  0.32399357  0.32456226  0.32611124  0.32653783  0.32729271
  0.32864592  0.32870635  0.3292738   0.33001889  0.33133016  0.33403159
  0.33589042  0.33593041  0.33674533  0.33697648  0.33779593  0.33784999
  0.33973899  0.34087472  0.34201787  0.342062    0.34231484  0.34268263
  0.34375535  0.34424387  0.34570838  0.34655725  0.3466895   0.34741895
  0.34763344  0.34776778  0.34837625  0.34902319  0.34988584  0.35239226
  0.35281542  0.35349465  0.35375421  0.35431006  0.35459479  0.35609631
  0.35788084  0.35856433  0.35956163  0.35959866  0.3596282   0.35975679
  0.36009964  0.36083063  0.36117063  0.36159746  0.36164423  0.36309858
  0.36350201  0.36514151  0.36642129  0.36689233  0.36698951  0.36731718
  0.36921405  0.37033919  0.37053969  0.37108029  0.37131573  0.37204272
  0.37264345  0.37393736  0.37577995  0.37595509  0.37744343  0.37758908
  0.37956937  0.38020713  0.38072185  0.38079394  0.38145717  0.38252642
  0.38380803  0.38491735  0.38506762  0.38743638  0.38872018  0.38906008
  0.39590066  0.39673153  0.39702288  0.39770805  0.39857193  0.39938113
  0.39979639  0.39994833  0.40045592  0.40217352  0.40282843  0.40295157
  0.40409503  0.40533449  0.4053786   0.40642468  0.40661771  0.4080029
  0.40897683  0.41021143  0.41096418  0.41261215  0.41292924  0.41397726
  0.41483073  0.41543606  0.41555885  0.41633547  0.4165261   0.41669907
  0.41688605  0.41860875  0.42147668  0.42148263  0.42273286  0.42351973
  0.42380797  0.42405456  0.42432846  0.42495421  0.42723382  0.42809443
  0.42837999  0.42853252  0.43104441  0.43136426  0.43182125  0.43214816
  0.4376696   0.43966571  0.44060406  0.44358868  0.44490992  0.44491427
  0.44692314  0.44735294  0.44758772  0.44777822  0.46157271  0.46456497
  0.46697108  0.47076902  0.47192374  0.47410238  0.47856112  0.48115575
  0.49428084  0.49928423  0.50010041  0.50754113  0.5076128   0.50907679
  0.51080837  0.5116005   0.51618256  0.51697131  0.52241067  0.52429882
  0.52520582  0.52925899  0.5332023   0.53421672  0.53648455  0.53683084
  0.53879419  0.54054697  0.54174168  0.5434914   0.54418402  0.54422307
  0.54476223  0.5453905   0.54741263  0.54809167  0.56284935  0.56342035
  0.56442506  0.57078339  0.57132369  0.5741095   0.57521532  0.57585007
  0.57790672  0.58530687  0.58635527  0.59680694  0.59871994  0.60242816
  0.60354058  0.6044011   0.60554937  0.60722203  0.61184774  0.61437448
  0.61461679  0.61533424  0.61606412  0.61955251  0.62015489  0.62043123
  0.62197977  0.62292395  0.62505825  0.62538546  0.625693    0.62617908
  0.62846205  0.6291795   0.63367032  0.63400015  0.63476976  0.6351333
  0.6367692   0.63953826  0.63995395  0.64231686  0.64299622  0.64507636
  0.64579381  0.64760331  0.64905677  0.65338351  0.65491107  0.65615256
  0.66024273  0.66296948  0.66382497  0.66445972  0.66517717  0.66589662
  0.6665729   0.66762251  0.66999782  0.67020573  0.67024775  0.67044295
  0.67063762  0.67268922  0.67276687  0.67459364  0.67553593  0.67670991
  0.67685704  0.67805624  0.67815632  0.67830498  0.6791018   0.67979204
  0.68050303  0.68107403  0.68229146  0.68447562  0.68456053  0.68540639
  0.68733158  0.68930353  0.68985554  0.69151549  0.69215024  0.69261895
  0.69563674  0.69705359  0.69768834  0.69840579  0.69982264  0.70045739
  0.70156859  0.7025917   0.70322644  0.7039439   0.7044378   0.70536075
  0.70542449  0.70599549  0.70624542  0.7081298   0.70876454  0.71121494
  0.71187499  0.71225105  0.71254532  0.71430265  0.71447492  0.71455257
  0.7150201   0.71821801  0.71857978  0.71967296  0.72020012  0.72040303
  0.72057676  0.73116337  0.73305126  0.73601702  0.73646421  0.73944326
  0.73947404  0.74083466  0.74689652  0.74778119  0.74810669  0.74824872
  0.74929128  0.75055024  0.75378682  0.75421568  0.75428073  0.7588574
  0.75903633  0.76078621  0.76162645  0.76294756  0.7699336   0.77270265
  0.7758874   0.77624132  0.77824076  0.78031339  0.78118381  0.78243853
  0.78545775  0.78654791  0.78726736  0.79208602  0.80316222  0.80510234
  0.80593127  0.80774005  0.81772493  0.82049398  0.82191084  0.82254558
  0.82531463  0.82603209  0.83130863  0.83483315  0.8376022   0.83987735
  0.84192894  0.84689605  0.8481845   0.85813944  0.85975461  0.87198469
  0.87710898  0.8830609   0.888599    0.89967521  0.90039266  0.90220216
  0.90244426  0.90316171  0.90316371  0.90521331  0.90869982  0.91075142
  0.91564644  0.91598198  0.91905857  0.9273315   0.93290383  0.93331952
  0.93411519  0.93608857  0.93757564  0.93844193  0.94121098  0.94192844
  0.94340903  0.94746654  0.94951814  0.95171619  0.95228719  0.95300464
  0.95505624  0.95577369  0.9601563   0.96059434  0.96336339  0.96726559
  0.96954287  0.96961895  0.9744396   0.97515705  0.9799777   0.98274676
  0.98900231  0.99177136  0.99730947  1.00284757  1.00318234  1.01669283
  1.02133103  1.02499998  1.03174944  1.10853919  1.52274401]

  UserWarning,

2022-10-31 11:02:12,587:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60470879 -0.84478708 -0.31001272 -0.23181711 -0.22925958 -0.18388184
 -0.17375493 -0.15939072 -0.0589028  -0.05433673 -0.04762999 -0.03805126
 -0.01194197 -0.00576226  0.00481885  0.01000654  0.01190334  0.02147636
  0.06059361  0.07117711  0.07460485  0.08399168  0.1019461   0.10397485
  0.1127978   0.1184939   0.1231407   0.1234011   0.14262182  0.14327143
  0.14461337  0.14687813  0.15400446  0.16326069  0.16334379  0.17621338
  0.18122406  0.18711622  0.19040683  0.19182796  0.19249545  0.1950319
  0.19532825  0.19607945  0.20023442  0.2011216   0.20186613  0.20679127
  0.20997048  0.21001827  0.21073364  0.21229152  0.21408936  0.21443799
  0.21612068  0.21706476  0.21864666  0.22136675  0.22235533  0.22350263
  0.2240505   0.22494087  0.22612033  0.22727753  0.22801278  0.22816124
  0.22965419  0.23071314  0.23470041  0.23643053  0.2382808   0.23994042
  0.24060604  0.2409866   0.24617892  0.24673872  0.24711723  0.24804804
  0.24970946  0.25070528  0.25156132  0.25192229  0.2531271   0.25439267
  0.2547662   0.25533802  0.25540359  0.25625564  0.25641315  0.25662577
  0.25845769  0.25870054  0.25872585  0.25895     0.25989356  0.26261927
  0.26270334  0.26281105  0.26314879  0.26322826  0.26515705  0.26555267
  0.26597203  0.26640316  0.26642117  0.26652057  0.26808957  0.26874787
  0.26951459  0.26972458  0.27030322  0.27047059  0.27222882  0.27341672
  0.27375312  0.27512217  0.27535875  0.27552695  0.27649965  0.27814485
  0.28280997  0.28582948  0.28622529  0.28673768  0.28696301  0.28722409
  0.28755421  0.28823392  0.28915519  0.28942761  0.29053647  0.29074591
  0.29134011  0.29170649  0.29295831  0.29417098  0.29430165  0.29505041
  0.29546911  0.29600105  0.29833401  0.29963867  0.29971372  0.30023446
  0.30137032  0.30205436  0.3020922   0.30221455  0.30302158  0.30433206
  0.30586645  0.30784663  0.30878058  0.31213021  0.31241801  0.31308032
  0.31457202  0.3146049   0.31605988  0.31804905  0.31974926  0.3217056
  0.32225099  0.32245936  0.32307597  0.3238702   0.32449181  0.32457576
  0.32580563  0.32592995  0.32709836  0.32740353  0.32766754  0.3281436
  0.32869647  0.32925215  0.32984152  0.32997867  0.33089801  0.33093212
  0.33208825  0.33237908  0.33242087  0.33249107  0.33431356  0.33434605
  0.33445376  0.33515712  0.33752799  0.33856788  0.33861026  0.34026276
  0.34041652  0.34118112  0.34166332  0.34214162  0.34276107  0.34332645
  0.34369824  0.34446564  0.34618057  0.34693592  0.34742643  0.34822122
  0.3491309   0.34965011  0.35078526  0.35079828  0.35209405  0.3521503
  0.35255261  0.35416277  0.35486102  0.35492884  0.35651903  0.35693773
  0.35694874  0.35860531  0.3586325   0.3601311   0.36077102  0.3608579
  0.36135351  0.36265732  0.36353222  0.36622361  0.36754006  0.36816123
  0.37001079  0.37153022  0.3715447   0.37260543  0.37279561  0.37519972
  0.37606024  0.37659015  0.37771838  0.37853774  0.38060026  0.38100688
  0.38279326  0.3831032   0.38323119  0.38412235  0.38412839  0.38435305
  0.38537346  0.3856215   0.38586005  0.38595411  0.38825662  0.38884622
  0.38926689  0.38932228  0.38940774  0.38964807  0.39019835  0.39279244
  0.39457328  0.39461366  0.39472921  0.39518276  0.39712245  0.3974392
  0.39769251  0.39872363  0.39996338  0.40033605  0.40054594  0.4017234
  0.40219994  0.40305786  0.40312481  0.40364     0.40472246  0.40474401
  0.4049287   0.40646747  0.40694992  0.40725682  0.40785314  0.41230995
  0.41301391  0.41365165  0.41516554  0.41696125  0.41814221  0.41842242
  0.41844063  0.42032923  0.42170758  0.42233232  0.42300205  0.42321538
  0.42482828  0.42484058  0.4250305   0.42530722  0.42820911  0.4282697
  0.43094281  0.43424308  0.43428753  0.43776209  0.43807822  0.43838693
  0.43914666  0.44176836  0.44250557  0.44299156  0.44517127  0.44664696
  0.44840203  0.45684142  0.45835847  0.46567722  0.46668409  0.47157969
  0.47493696  0.47539092  0.47594528  0.47736355  0.47806417  0.47893596
  0.48153439  0.48201992  0.48331561  0.48626294  0.48725237  0.48833257
  0.49411924  0.49762476  0.50196857  0.50270286  0.50305404  0.50930593
  0.51009982  0.51383287  0.51480715  0.5178661   0.51786661  0.51906084
  0.52100075  0.52898606  0.52954023  0.5316103   0.53773601  0.53787236
  0.5411771   0.5435626   0.54499424  0.5470858   0.55488101  0.55903737
  0.56424387  0.57283897  0.57590675  0.57767038  0.57955207  0.58055019
  0.58226181  0.58456431  0.59250176  0.59890056  0.60131368  0.60206302
  0.6058214   0.60878054  0.61081671  0.61162427  0.61401459  0.6203174
  0.62052206  0.62459431  0.62522668  0.62562317  0.62596616  0.62698462
  0.62701508  0.62835647  0.62844833  0.63074679  0.63083864  0.63191233
  0.63353162  0.63486394  0.63556034  0.63971033  0.64314593  0.64350297
  0.64368205  0.6498693   0.65177012  0.65203531  0.65464992  0.65999437
  0.6603507   0.66182087  0.66354771  0.66660149  0.66796956  0.66797619
  0.6683941   0.66850834  0.67020157  0.67089263  0.67222251  0.67377244
  0.67473027  0.67616275  0.67855306  0.68177963  0.68273598  0.68333369
  0.68524054  0.68762482  0.68837496  0.68984116  0.69001514  0.69448111
  0.6948018   0.69958243  0.7024562   0.70436305  0.70723683  0.70930646
  0.71115188  0.71153399  0.71201746  0.71354219  0.71362445  0.71388342
  0.71391828  0.71419969  0.7159325   0.71601861  0.71631462  0.71710599
  0.71844893  0.71870494  0.71971737  0.7207439   0.72209298  0.72348556
  0.72447151  0.72615126  0.72788407  0.72826619  0.73027439  0.73065048
  0.73197623  0.73208734  0.73313868  0.73983564  0.73989104  0.74049314
  0.74235206  0.74401856  0.74518366  0.74711686  0.74717459  0.7493969
  0.75039463  0.75156181  0.75170061  0.75292081  0.75554559  0.75758233
  0.75895815  0.75933424  0.75934027  0.76134847  0.76173059  0.76481068
  0.76523674  0.76525008  0.77042023  0.77569035  0.77613793  0.7774172
  0.78047098  0.78246366  0.78286129  0.78525161  0.78715846  0.78764192
  0.78880784  0.78904775  0.79264191  0.79441522  0.79481286  0.79594645
  0.79838526  0.79959349  0.80198381  0.80375951  0.80644374  0.80777289
  0.80978215  0.81258498  0.81822651  0.818716    0.82061682  0.82110632
  0.82349663  0.83305789  0.83581436  0.85017606  0.85042245  0.85288212
  0.85856677  0.86916085  0.87450022  0.88044677  0.88379472  0.88406148
  0.89295926  0.89522973  0.89791387  0.90142605  0.90460867  0.90974032
  0.91035493  0.91513556  0.92367457  0.92947744  0.92992502  0.93186776
  0.93425807  0.93664839  0.9390387   0.94381933  0.94512243  0.94923233
  0.95229338  0.9552814   0.9557709   0.95816121  0.95946432  0.96006203
  0.96294184  0.9720136   0.9741079   0.97440391  0.97918454  0.98202243
  0.98396517  0.98575777  0.98606165  0.99087434  0.99352643  0.9990448
  0.99991404  1.00069737  1.00181923  1.00308768  1.00567291  1.00786831
  1.00996261  1.02393704  1.02762319  1.02859781  1.02905495  1.0315544
  1.05055272  1.05630696  1.06108759  1.06846241  1.11726747  1.26569606
  1.29631179  2.53663387]

  UserWarning,

2022-10-31 11:02:12,602:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77224717 -0.74862747 -0.50257446 -0.39064432 -0.27797477 -0.2575772
 -0.18786206 -0.16517645 -0.14688948 -0.08380591 -0.04005814 -0.00448239
  0.02125225  0.02372309  0.04330348  0.0709786   0.07553126  0.07662885
  0.10338273  0.10356763  0.11604264  0.11999216  0.1241977   0.12719506
  0.14068104  0.14102355  0.14505905  0.1540924   0.15874226  0.15999593
  0.16031228  0.16135147  0.16232583  0.16458831  0.16473908  0.16571599
  0.17340688  0.17952491  0.18359781  0.18503832  0.18623298  0.18672808
  0.18881302  0.19249899  0.19267569  0.1946815   0.19476261  0.19949249
  0.20000918  0.20175381  0.20280571  0.20710966  0.2157884   0.21708049
  0.2191776   0.21973637  0.21982091  0.22119713  0.22243484  0.22443534
  0.22643384  0.22749138  0.22860573  0.2325713   0.23277061  0.23541846
  0.23549136  0.23853315  0.23869463  0.2405921   0.24065688  0.24320639
  0.24322132  0.24342744  0.24459623  0.24774478  0.24779208  0.2481148
  0.24824068  0.25136634  0.25256218  0.25261197  0.25281521  0.25324136
  0.25449153  0.25488607  0.25580673  0.25608386  0.25712793  0.2595164
  0.26112881  0.26286558  0.26325697  0.26353501  0.26436498  0.26608131
  0.26655176  0.26783682  0.2696009   0.27032145  0.27034555  0.27044203
  0.27095939  0.27238098  0.27284223  0.27558105  0.27728976  0.27753638
  0.27869551  0.27955982  0.28018593  0.28060053  0.28119514  0.28268721
  0.282859    0.28332022  0.28402988  0.28480613  0.28546211  0.28549196
  0.28650053  0.28781687  0.28793453  0.28914477  0.29180416  0.29450022
  0.2948403   0.29524978  0.29676067  0.29682725  0.29705566  0.29735555
  0.29755529  0.29974094  0.30143192  0.30245402  0.30247274  0.30313988
  0.30453661  0.30474875  0.30593981  0.30677767  0.30870298  0.30881064
  0.3088274   0.30961326  0.31163279  0.31224999  0.31278855  0.31424541
  0.31443234  0.31530146  0.31543769  0.31546162  0.31661261  0.3171257
  0.31732583  0.3182638   0.32040045  0.32133473  0.32242321  0.32425895
  0.32610929  0.3262021   0.32643825  0.32680861  0.32870924  0.32965889
  0.32976931  0.32987434  0.33088922  0.33332625  0.3333389   0.33416345
  0.33516516  0.33540837  0.33542195  0.33636557  0.33685834  0.33715668
  0.33760065  0.33833001  0.33890728  0.33973435  0.33993323  0.34005312
  0.34047984  0.34125822  0.34225294  0.34321037  0.34445026  0.34460426
  0.34547874  0.34549347  0.3455912   0.34573359  0.3462022   0.34751411
  0.34891674  0.35108395  0.3511297   0.35117584  0.35198412  0.35243644
  0.35295734  0.35307571  0.35350711  0.35372173  0.35376534  0.35400469
  0.35420472  0.35561131  0.35590274  0.35621407  0.35644596  0.35651604
  0.3574543   0.35839882  0.35902469  0.36001987  0.36053098  0.3613415
  0.36164064  0.362688    0.36273803  0.36409052  0.3646359   0.36580315
  0.36728314  0.36804987  0.3685228   0.36947764  0.37009459  0.37127581
  0.37181819  0.37272167  0.37399051  0.37430092  0.37465202  0.37778656
  0.37788004  0.37831055  0.38035442  0.38101848  0.38108453  0.38124549
  0.38182749  0.38284276  0.38337605  0.38377627  0.38513473  0.38687263
  0.38705679  0.38734051  0.38846438  0.39104952  0.39216233  0.39375365
  0.39424167  0.39546889  0.39625894  0.39688631  0.39830052  0.39891115
  0.40168756  0.40179182  0.40217558  0.40264239  0.40268493  0.40331508
  0.4034817   0.40650615  0.40697684  0.40746486  0.40764088  0.40940135
  0.41124649  0.41204357  0.41384848  0.41444119  0.41551791  0.41633993
  0.41865442  0.41998981  0.42188996  0.4250019   0.42876614  0.43069886
  0.43105449  0.43428012  0.43452141  0.43569456  0.43789149  0.43838439
  0.44209567  0.44625054  0.44960676  0.45267189  0.45403136  0.45666809
  0.46285443  0.46388885  0.46411607  0.46528541  0.46562173  0.46750222
  0.46765569  0.46796348  0.46917211  0.4694428   0.47015967  0.47064228
  0.4795525   0.48524537  0.48897399  0.48974921  0.49469714  0.50089745
  0.5035524   0.50769368  0.5085044   0.51006409  0.51524094  0.51803263
  0.52112283  0.52265813  0.52969549  0.53388749  0.53430328  0.53671731
  0.53974391  0.54086825  0.54504344  0.54978995  0.55066786  0.55071968
  0.55073264  0.55322927  0.55323834  0.55796515  0.55990175  0.56202536
  0.56787261  0.56797407  0.56901327  0.56995927  0.57145529  0.57260391
  0.58047903  0.58053782  0.58191991  0.58340747  0.59073691  0.59323709
  0.59376101  0.5951431   0.6003109   0.60043237  0.6069842   0.60962883
  0.61211366  0.61227347  0.61457461  0.61756275  0.61894593  0.62285202
  0.62549666  0.62978476  0.63145962  0.63160972  0.63524872  0.63592739
  0.63607521  0.63947061  0.64121667  0.64137873  0.64665376  0.64744449
  0.6488991   0.6492984   0.64931265  0.65145502  0.65194303  0.65282362
  0.65376118  0.65458767  0.65645231  0.65987695  0.66252159  0.66516622
  0.66948388  0.6703667   0.6713107   0.67207012  0.67310014  0.67350865
  0.67432126  0.67448332  0.67491828  0.6749666   0.67574477  0.67838941
  0.67884767  0.68103405  0.68607711  0.68896796  0.68898221  0.69125273
  0.69470873  0.69486018  0.69502225  0.69533584  0.69651058  0.69675406
  0.69690187  0.69778246  0.69801218  0.69956076  0.70000477  0.7022054
  0.70468797  0.70484562  0.70485004  0.70748042  0.70749467  0.70750285
  0.70918061  0.70997725  0.71213592  0.71278395  0.71541434  0.71612865
  0.71629492  0.71708853  0.71807322  0.7205313   0.7219786   0.72287448
  0.7229712   0.72320043  0.7233625   0.72402194  0.72645115  0.72687348
  0.72865177  0.7299465   0.73054213  0.73129641  0.73174042  0.73207289
  0.73239604  0.73377898  0.73473178  0.73658569  0.73745203  0.73870451
  0.74009666  0.74534532  0.74538594  0.74692941  0.74700217  0.7475569
  0.74882301  0.75040819  0.75311826  0.75331985  0.75860913  0.76098674
  0.76125376  0.76238198  0.76380854  0.76518868  0.76551464  0.76704728
  0.76765691  0.76815927  0.76918768  0.77183231  0.77447695  0.77513639
  0.77518346  0.77787235  0.78770014  0.79034478  0.79298942  0.7978496
  0.80092333  0.80356797  0.8062126   0.81152099  0.8120135   0.81414652
  0.81479113  0.81482592  0.81943579  0.81989291  0.82311941  0.82423185
  0.82457725  0.82760599  0.82782796  0.83530362  0.83794826  0.84063267
  0.84803879  0.85068343  0.85332806  0.86068852  0.86417283  0.86424681
  0.86831799  0.87938482  0.89379274  0.89515866  0.90005364  0.90269828
  0.91062777  0.91230521  0.91264099  0.91592146  0.92106292  0.92370756
  0.92650001  0.92695141  0.92836647  0.93164147  0.93178929  0.93254005
  0.93472036  0.93707856  0.93957539  0.94486466  0.94501248  0.94934305
  0.95030175  0.95279857  0.95808785  0.9608803   0.96352494  0.9686664
  0.9714731   0.97230538  0.97552719  0.97660031  0.97842115  0.98188959
  0.98371042  0.98420827  0.98453423  0.98644905  0.98717886  0.98778494
  0.98792962  0.9898235   0.99230304  0.99321527  0.99462476  0.99511278
  0.99672567  1.00416056  1.00569133  1.00784795  1.01362524  1.0179429
  1.02288928  1.02420379  1.0393418   1.03995274  1.04967855  1.3159016
  1.44084446]

  UserWarning,

2022-10-31 11:02:13,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:13,123:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:13,449:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94547024 -0.7769212  -0.475268   -0.39286633 -0.3342177  -0.31828377
 -0.31582689 -0.30841293 -0.2684237  -0.26011261 -0.25918253 -0.255448
 -0.12385182 -0.11839672 -0.10368724 -0.09856177 -0.05762328 -0.04689406
 -0.02913981 -0.02853047 -0.00235619  0.00617414  0.0230898   0.02551127
  0.0327106   0.03591053  0.05462452  0.06077473  0.06645141  0.066876
  0.08110093  0.08489302  0.08581859  0.09230081  0.09677853  0.1026558
  0.1111432   0.12642578  0.126697    0.12749689  0.13560586  0.14893953
  0.15158257  0.1586888   0.17082699  0.17288944  0.17438129  0.17638239
  0.1770858   0.17894021  0.18470933  0.19003952  0.19523459  0.2023678
  0.20553985  0.20557671  0.20615     0.20907266  0.2092559   0.21011132
  0.21157819  0.21158038  0.21170576  0.21228063  0.21544919  0.2160408
  0.21659591  0.21695481  0.21735582  0.21854989  0.22021886  0.22436046
  0.22471874  0.2335439   0.23539492  0.23585576  0.23690381  0.23883767
  0.2393378   0.24141753  0.24151799  0.24162932  0.24448089  0.24656035
  0.24764854  0.24807353  0.24853669  0.24948812  0.24999802  0.25010911
  0.25142998  0.25452029  0.25720127  0.25745161  0.25764395  0.26281678
  0.26283238  0.26291113  0.26425001  0.26439542  0.26501076  0.26550845
  0.26629931  0.26841303  0.26895505  0.27008826  0.27043693  0.2705421
  0.27167103  0.27207276  0.27239086  0.27315774  0.27346394  0.27384822
  0.27441927  0.27587736  0.27656424  0.27657219  0.27673234  0.27849738
  0.2787583   0.2812762   0.28195562  0.28268919  0.28348847  0.28350872
  0.28397096  0.2840047   0.28447047  0.28608999  0.28614168  0.28721282
  0.2877382   0.29007026  0.29010392  0.29021936  0.2910541   0.291314
  0.291468    0.29192382  0.29487624  0.2969387   0.29724117  0.29749065
  0.29749077  0.29787786  0.29799934  0.30171722  0.30328991  0.30389433
  0.3044576   0.30630384  0.30755444  0.31122846  0.31198961  0.31234061
  0.31247107  0.31295219  0.31307725  0.31343401  0.3139147   0.31453246
  0.31753968  0.31798459  0.31805659  0.31811404  0.31914702  0.31992056
  0.3211042   0.32153237  0.32643793  0.32736012  0.32806381  0.3283722
  0.32868585  0.3312465   0.33325703  0.33335466  0.33400093  0.33525222
  0.33567382  0.33668267  0.33761054  0.33946344  0.34021865  0.3404125
  0.34171032  0.34232112  0.3424479   0.34524721  0.34615581  0.34624389
  0.34706415  0.34707618  0.34777239  0.34831392  0.34888026  0.34894465
  0.35084192  0.35127049  0.35252923  0.35296862  0.35480602  0.35947403
  0.35978647  0.36358045  0.36434788  0.36655751  0.36743964  0.37213312
  0.37255604  0.37257191  0.37446411  0.3761311   0.37662853  0.37698291
  0.37725052  0.37791427  0.37832209  0.37895605  0.37973217  0.3798117
  0.37984411  0.38028643  0.38056454  0.38132042  0.3815238   0.38225617
  0.38303407  0.38342287  0.38458412  0.38465436  0.38510293  0.38555306
  0.38646736  0.38716559  0.38728324  0.38752793  0.3882337   0.38891002
  0.39014733  0.3905364   0.39075877  0.39272507  0.39335251  0.39343333
  0.39409365  0.39424075  0.39472553  0.39530645  0.39612324  0.39754191
  0.40189592  0.40314096  0.4038326   0.40464543  0.40485059  0.40638542
  0.40643241  0.40708489  0.40821777  0.40899031  0.40903223  0.40923198
  0.41084814  0.4112559   0.41158041  0.41240366  0.41246552  0.41354546
  0.41408385  0.41549468  0.41644835  0.41702303  0.41709515  0.41913417
  0.42123369  0.42229338  0.42242483  0.42517515  0.42533601  0.42555114
  0.42894292  0.43126092  0.43381942  0.43537043  0.43631764  0.43891745
  0.44265308  0.44411709  0.44590245  0.44593936  0.44618902  0.4492295
  0.44954016  0.45346589  0.45456103  0.45523738  0.45559959  0.45566281
  0.45687339  0.45703535  0.45711617  0.45806308  0.45830616  0.46195324
  0.46421528  0.46721648  0.48241399  0.48325441  0.48661998  0.48830335
  0.48907486  0.49500913  0.4957891   0.49802634  0.49852835  0.5006127
  0.50101285  0.50301471  0.50303699  0.50451729  0.50581233  0.50919945
  0.50998364  0.5124732   0.51345032  0.51968394  0.52140135  0.5317766
  0.53255673  0.53334799  0.53701013  0.54121616  0.54942401  0.55077029
  0.55232651  0.55515458  0.55520884  0.55533708  0.55533766  0.5625152
  0.56845103  0.56921866  0.57515943  0.57519063  0.57537321  0.57676845
  0.58196808  0.5845679   0.59104095  0.59144481  0.59194911  0.59289351
  0.59424395  0.59496717  0.59533187  0.60699338  0.60722231  0.60796625
  0.61219301  0.61836551  0.62876477  0.63206841  0.63299154  0.63396441
  0.63656422  0.63916404  0.63976269  0.64792872  0.65100065  0.65152654
  0.65216312  0.65307165  0.65345882  0.65468211  0.65476294  0.65510297
  0.65736275  0.65754316  0.65996257  0.66124982  0.66360844  0.66384945
  0.66384963  0.6648008   0.6651622   0.66620825  0.66762873  0.66776202
  0.66880807  0.67036183  0.67043018  0.67274122  0.67296165  0.67380863
  0.67556146  0.67660752  0.67720617  0.6788006   0.67920733  0.67944853
  0.6807611   0.68440696  0.68464816  0.68524681  0.6877106   0.6896066
  0.69339878  0.69480623  0.69504743  0.69746916  0.69947454  0.70000586
  0.70024706  0.70066737  0.70284687  0.70291918  0.70415944  0.70432419
  0.70495086  0.70520549  0.70667844  0.70675926  0.70780531  0.71040513
  0.71447788  0.71455871  0.71541855  0.71560476  0.71584596  0.72077949
  0.7224045   0.72445291  0.73380347  0.73724121  0.73854396  0.74114378
  0.74277462  0.74412191  0.74634341  0.74646651  0.74894323  0.75040446
  0.75154304  0.75200218  0.75406204  0.75407008  0.75996201  0.76004283
  0.76454212  0.76471611  0.7653891   0.76714194  0.76999141  0.77073255
  0.77234157  0.77384942  0.77461026  0.77921848  0.77973913  0.78034553
  0.780988    0.78315373  0.78562563  0.78605641  0.78794047  0.79054028
  0.79103937  0.79644373  0.79833973  0.80113483  0.80353936  0.80613918
  0.808739    0.81133881  0.81233972  0.81314583  0.81515867  0.81653845
  0.82433789  0.82663825  0.82866353  0.83452538  0.83943682  0.83972501
  0.83993679  0.84023007  0.84203154  0.85012427  0.85347686  0.85868322
  0.86354759  0.86520024  0.86572317  0.8719136   0.88157172  0.89531194
  0.90131309  0.90361963  0.90463959  0.90472041  0.90600766  0.90732023
  0.91511968  0.91531943  0.91616573  0.92023849  0.92031931  0.92129199
  0.92291912  0.92512601  0.93323757  0.93460564  0.93851246  0.94111784
  0.94363683  0.9450049   0.94589923  0.94631747  0.94651722  0.9475239
  0.94760472  0.94891728  0.95020453  0.952278    0.95280435  0.95436657
  0.95800398  0.96007745  0.9606038   0.96191637  0.96320361  0.96572261
  0.96971581  0.97231563  0.97293863  0.97307653  0.97491545  0.97690651
  0.97880251  0.98140233  0.98400214  0.98660196  0.98860325  0.98920178
  0.99180159  0.99700122  0.99907469  1.0034326   1.00480067  1.00597288
  1.00740049  1.01217185  1.01609426  1.01677283  1.0263601   1.03027248
  1.03631909  1.07315679  1.07389955  1.08187185  1.2273629   1.25953641
  1.27264062  1.33467606  1.39584365]

  UserWarning,

2022-10-31 11:02:13,473:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4051563  -0.40513932 -0.35103044 -0.2330514  -0.21247933 -0.18264918
 -0.13817349 -0.12681862 -0.09967875 -0.03978759 -0.02461572 -0.02023458
 -0.01592225 -0.00524282  0.0222814   0.03979957  0.04002484  0.04368484
  0.05127418  0.05165241  0.07441814  0.08253408  0.0862313   0.089347
  0.10093851  0.10241407  0.10688991  0.11597369  0.12625719  0.12801135
  0.13247995  0.13357617  0.13383359  0.13594837  0.14739853  0.15127366
  0.15286655  0.15316628  0.15611028  0.16050176  0.16194962  0.16348284
  0.16542811  0.18669191  0.19146363  0.19262117  0.19475407  0.1948737
  0.19910781  0.20209062  0.20292288  0.20647704  0.20704483  0.20799744
  0.20845553  0.21020249  0.21329357  0.21357014  0.21434883  0.21581551
  0.21586655  0.21914479  0.21979914  0.22149076  0.22152114  0.22227012
  0.22232146  0.22291616  0.22428778  0.22864595  0.22884517  0.22894572
  0.23085042  0.23382831  0.23397248  0.23468876  0.23572877  0.23590029
  0.23632385  0.23652307  0.24145602  0.24183377  0.24215233  0.24332384
  0.24568205  0.24617115  0.24823232  0.24964627  0.25136275  0.251919
  0.25493376  0.25713651  0.25828862  0.25918749  0.2635571   0.26665266
  0.26726465  0.26802519  0.26902843  0.26923876  0.26929472  0.27100927
  0.27122636  0.27210955  0.27326415  0.2734712   0.2736799   0.2749884
  0.27675862  0.27788084  0.27969626  0.2819594   0.28215278  0.28233622
  0.28297377  0.28463041  0.28623232  0.28654467  0.28694381  0.28717355
  0.28759229  0.28791311  0.28961843  0.28964992  0.2898756   0.29030085
  0.29059693  0.29082643  0.29163415  0.29163953  0.29202848  0.29338239
  0.2939855   0.29655731  0.29656512  0.29658344  0.29716614  0.29737117
  0.29761043  0.29816707  0.29827158  0.29904856  0.29907447  0.29995422
  0.30065878  0.30130793  0.30192443  0.30215546  0.3022679   0.30314669
  0.30335369  0.30368624  0.30397468  0.30411493  0.30469389  0.30476432
  0.30530817  0.30626912  0.30705926  0.3083699   0.30951013  0.3100403
  0.31081157  0.31245291  0.31320944  0.31370859  0.31395819  0.31407625
  0.31654586  0.31742393  0.31751827  0.32014082  0.32077153  0.32269314
  0.32425021  0.32469806  0.32490107  0.32511623  0.32586379  0.32623024
  0.32658796  0.32692621  0.32845691  0.32877224  0.32987487  0.33019465
  0.33151717  0.33377394  0.33414388  0.33498874  0.33629318  0.3363925
  0.33691417  0.33753388  0.33757744  0.33787672  0.33935184  0.34125655
  0.34192837  0.34227128  0.34283387  0.34547919  0.34717693  0.34779053
  0.34782649  0.34807262  0.34879075  0.3489736   0.3497017   0.34976122
  0.34990619  0.34993565  0.35243988  0.35391699  0.35410907  0.35435556
  0.35485313  0.35512392  0.3556231   0.35615433  0.35694377  0.35727452
  0.35970756  0.36125935  0.36201449  0.36285703  0.36318316  0.36340267
  0.36416031  0.36447428  0.36489656  0.36510507  0.36525511  0.36720818
  0.36732218  0.36749937  0.37103736  0.3711649   0.37198539  0.37308854
  0.37389575  0.37412984  0.374578    0.37611352  0.37613771  0.37806883
  0.37985751  0.38017507  0.38086249  0.38097032  0.38309973  0.3835084
  0.38482274  0.38604648  0.38667511  0.38743639  0.38934425  0.39004224
  0.39026359  0.39106707  0.39112265  0.39121828  0.39140443  0.3922089
  0.39410376  0.39413604  0.3952324   0.39619881  0.39732136  0.39740915
  0.39743531  0.39873689  0.39893662  0.40127497  0.40280145  0.40331907
  0.40375748  0.40381306  0.40420302  0.40434282  0.40586565  0.40635114
  0.40705802  0.40888922  0.40975244  0.41026948  0.41199583  0.41578098
  0.41874656  0.41940299  0.42168507  0.42221054  0.42237937  0.42385883
  0.42501109  0.42658712  0.43001543  0.43168059  0.43402432  0.43425835
  0.43662006  0.44101641  0.44432751  0.44641932  0.4471961   0.44973419
  0.4501304   0.45194175  0.45227227  0.45275831  0.45481035  0.45734843
  0.4596998   0.46202019  0.46209408  0.46435636  0.46482729  0.46717024
  0.47115615  0.47393034  0.47405777  0.47474419  0.48148677  0.48350206
  0.48442394  0.49077086  0.49198973  0.49289988  0.4983851   0.51046369
  0.51099674  0.51366113  0.513837    0.51769008  0.51816754  0.52127698
  0.5215249   0.52258466  0.53138017  0.53391825  0.544015    0.54407058
  0.54733618  0.55003868  0.5551812   0.56030459  0.5623855   0.5704556
  0.57464328  0.57724513  0.58201411  0.58491554  0.5865934   0.58703272
  0.58724828  0.59247363  0.59447058  0.59729193  0.60056809  0.60164453
  0.60593644  0.60607118  0.60661231  0.607133    0.60943807  0.61168848
  0.61218967  0.61531636  0.61785444  0.61868269  0.62039253  0.62293061
  0.62565036  0.62883502  0.63054485  0.63456417  0.63562102  0.64069718
  0.64323526  0.64454142  0.64577334  0.64829514  0.64831143  0.65084951
  0.65284646  0.65338759  0.65794527  0.66096883  0.66161449  0.666078
  0.66694823  0.66852122  0.66861608  0.67047418  0.67115416  0.67138982
  0.67181718  0.67369224  0.67623033  0.67822728  0.67876841  0.67973995
  0.68003028  0.68107264  0.68133683  0.68467282  0.68487742  0.68584152
  0.68892074  0.69228707  0.69345577  0.69549443  0.69575048  0.69647941
  0.69653498  0.69663329  0.697198    0.69736323  0.69853193  0.69907306
  0.70107001  0.70161115  0.7036081   0.70414923  0.71176347  0.71305562
  0.71555834  0.7162985   0.71773831  0.71961337  0.71999326  0.72765498
  0.72886885  0.72892349  0.73220893  0.73266384  0.73267557  0.7353396
  0.73887939  0.73906866  0.73964937  0.74205146  0.7421256   0.74288348
  0.74720176  0.74732337  0.74795964  0.75008837  0.75475001  0.75557388
  0.7576846   0.75811197  0.76045652  0.76445     0.76750642  0.76826429
  0.76952564  0.77080238  0.7712826   0.77206372  0.7732374   0.77334046
  0.77587854  0.77841662  0.7809547   0.78335425  0.78397301  0.78603087
  0.78626652  0.78887498  0.78904917  0.79364511  0.79612762  0.79872128
  0.80040521  0.80125936  0.80149501  0.8040331   0.80627995  0.80736741
  0.80777695  0.81137868  0.81394977  0.81418542  0.81517863  0.81565678
  0.81679873  0.81897036  0.82031808  0.82052293  0.82179967  0.82539424
  0.83052782  0.831952    0.83225061  0.83422142  0.83673693  0.83775777
  0.83933059  0.83956624  0.84718049  0.84795257  0.85179688  0.85302874
  0.85325592  0.85733281  0.86212457  0.86318107  0.86566357  0.86696508
  0.86930398  0.87229265  0.87436624  0.88327522  0.88348572  0.89461532
  0.89612865  0.89871421  0.90379037  0.90594278  0.90632846  0.90656411
  0.91140462  0.91164028  0.9139427   0.91648078  0.91901887  0.92155695
  0.9217926   0.92433069  0.92657754  0.92663311  0.93170927  0.93194493
  0.93424736  0.93663939  0.93932352  0.93955918  0.9400394   0.94439968
  0.94693777  0.94717342  0.94822018  0.94947585  0.95201393  0.95831111
  0.96216626  0.96240191  0.96470434  0.96494     0.96709637  0.9697805
  0.97020935  0.97509232  0.97736174  0.9776304   0.98016849  0.98270657
  0.98318679  0.98524465  0.98778273  0.99252838  0.9928589   0.99539698
  0.99841528  1.00047314  1.0055493   1.01167222  1.01560678  1.02098168
  1.02324721  1.02829718  1.28353226]

  UserWarning,

2022-10-31 11:02:13,475:INFO:Calculating mean and std
2022-10-31 11:02:13,476:INFO:Creating metrics dataframe
2022-10-31 11:02:13,480:INFO:Uploading results into container
2022-10-31 11:02:13,480:INFO:Uploading model into container now
2022-10-31 11:02:13,481:INFO:master_model_container: 9
2022-10-31 11:02:13,481:INFO:display_container: 2
2022-10-31 11:02:13,481:INFO:Lars(random_state=3360)
2022-10-31 11:02:13,481:INFO:create_model() successfully completed......................................
2022-10-31 11:02:13,585:WARNING:create_model() for Lars(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:13,585:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:13,585:INFO:Initializing create_model()
2022-10-31 11:02:13,585:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:13,585:INFO:Checking exceptions
2022-10-31 11:02:13,585:INFO:Importing libraries
2022-10-31 11:02:13,585:INFO:Copying training dataset
2022-10-31 11:02:13,585:INFO:Defining folds
2022-10-31 11:02:13,585:INFO:Declaring metric variables
2022-10-31 11:02:13,585:INFO:Importing untrained model
2022-10-31 11:02:13,601:INFO:Least Angle Regression Imported successfully
2022-10-31 11:02:13,601:INFO:Starting cross validation
2022-10-31 11:02:13,601:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:14,864:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:14,866:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:14,887:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,001:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,091:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,106:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,139:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:15,754:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.14373561e+00 -5.70012693e-01 -4.97240939e-01 -4.66859112e-01
 -3.51236114e-01 -1.10337661e-01 -1.09337646e-01 -9.72805052e-02
 -9.38809151e-02 -9.00388052e-02 -3.74152838e-02 -2.78680818e-02
 -1.68677049e-02  5.80980708e-04  3.97522753e-02  4.75240221e-02
  5.95697462e-02  6.00827058e-02  6.91305057e-02  7.62875938e-02
  9.19447980e-02  9.47890882e-02  1.01724313e-01  1.14038118e-01
  1.17769758e-01  1.29634607e-01  1.30390927e-01  1.37518191e-01
  1.40044133e-01  1.46840056e-01  1.50567796e-01  1.68955894e-01
  1.73091335e-01  1.75903598e-01  1.80631183e-01  1.85791350e-01
  1.89410975e-01  1.94198407e-01  1.94411578e-01  1.94663462e-01
  1.97955313e-01  1.98489324e-01  2.00742921e-01  2.05333219e-01
  2.05768764e-01  2.07154371e-01  2.09546518e-01  2.11620963e-01
  2.14412587e-01  2.16677509e-01  2.18654773e-01  2.19387307e-01
  2.19793876e-01  2.23214708e-01  2.25017147e-01  2.25780540e-01
  2.26410209e-01  2.27467991e-01  2.28574611e-01  2.30096944e-01
  2.30103192e-01  2.31049853e-01  2.33516468e-01  2.34412203e-01
  2.35118093e-01  2.35607656e-01  2.36201810e-01  2.37132159e-01
  2.38792758e-01  2.41769797e-01  2.41809652e-01  2.42253222e-01
  2.42564558e-01  2.44404383e-01  2.46463182e-01  2.47708173e-01
  2.48437349e-01  2.48469760e-01  2.49036108e-01  2.50007834e-01
  2.50486106e-01  2.50731258e-01  2.50790169e-01  2.51295706e-01
  2.51808917e-01  2.52514824e-01  2.52625007e-01  2.53239417e-01
  2.54453227e-01  2.54668436e-01  2.56430967e-01  2.56950434e-01
  2.57737196e-01  2.58427144e-01  2.58957890e-01  2.61262106e-01
  2.62929950e-01  2.62978031e-01  2.63466226e-01  2.65023467e-01
  2.65863430e-01  2.66027181e-01  2.66309582e-01  2.66450900e-01
  2.69356799e-01  2.71180945e-01  2.71709311e-01  2.74668161e-01
  2.75125605e-01  2.75892040e-01  2.77342410e-01  2.77965037e-01
  2.78303355e-01  2.79175077e-01  2.80325555e-01  2.80489069e-01
  2.81678691e-01  2.86308603e-01  2.86657415e-01  2.86753294e-01
  2.86896241e-01  2.87110300e-01  2.87282589e-01  2.89092692e-01
  2.91639286e-01  2.92844701e-01  2.93443186e-01  2.95562423e-01
  2.95701493e-01  2.96048772e-01  2.96719617e-01  2.98334748e-01
  3.00229862e-01  3.00369692e-01  3.00601679e-01  3.00974439e-01
  3.01088933e-01  3.01343613e-01  3.02937986e-01  3.03121506e-01
  3.05403691e-01  3.05414632e-01  3.05529991e-01  3.07689524e-01
  3.09544947e-01  3.10047745e-01  3.10363938e-01  3.10714214e-01
  3.13859107e-01  3.14024667e-01  3.15433581e-01  3.16210961e-01
  3.16785211e-01  3.18493464e-01  3.20398478e-01  3.22392993e-01
  3.22761073e-01  3.23963109e-01  3.24331503e-01  3.25592528e-01
  3.25825392e-01  3.25994370e-01  3.26321545e-01  3.26754065e-01
  3.27707429e-01  3.28520923e-01  3.29851171e-01  3.29976653e-01
  3.30325008e-01  3.30511310e-01  3.31729442e-01  3.32929170e-01
  3.33064942e-01  3.33296512e-01  3.34168210e-01  3.34218646e-01
  3.34635259e-01  3.35396290e-01  3.35643629e-01  3.36039765e-01
  3.36150160e-01  3.36891802e-01  3.37446884e-01  3.37882255e-01
  3.40408382e-01  3.41053935e-01  3.41082011e-01  3.41500931e-01
  3.41591697e-01  3.42528522e-01  3.42957555e-01  3.43754993e-01
  3.43915455e-01  3.44215465e-01  3.44272408e-01  3.44695459e-01
  3.45839873e-01  3.46658420e-01  3.47224615e-01  3.49957858e-01
  3.50816507e-01  3.50835830e-01  3.51353920e-01  3.53415829e-01
  3.54159389e-01  3.54671340e-01  3.54679234e-01  3.54730354e-01
  3.55028756e-01  3.55532046e-01  3.58903691e-01  3.58905253e-01
  3.59624043e-01  3.60258021e-01  3.60352543e-01  3.60508744e-01
  3.60839382e-01  3.60847096e-01  3.60890585e-01  3.60914819e-01
  3.61429526e-01  3.62122273e-01  3.65822213e-01  3.65879817e-01
  3.66574140e-01  3.67239735e-01  3.67305454e-01  3.67940663e-01
  3.67994460e-01  3.70652492e-01  3.72791133e-01  3.73594172e-01
  3.73769162e-01  3.73923530e-01  3.74251939e-01  3.75704487e-01
  3.76202457e-01  3.76668272e-01  3.77109405e-01  3.77461180e-01
  3.78448876e-01  3.78930912e-01  3.80190244e-01  3.80678646e-01
  3.83414975e-01  3.83622464e-01  3.84381580e-01  3.84592096e-01
  3.84636379e-01  3.85756355e-01  3.86159832e-01  3.87446522e-01
  3.87456680e-01  3.87462581e-01  3.88575281e-01  3.89720628e-01
  3.90388201e-01  3.90841828e-01  3.91399822e-01  3.91456461e-01
  3.91822474e-01  3.92674801e-01  3.95630522e-01  3.95728211e-01
  3.96089785e-01  3.96790038e-01  3.97232307e-01  3.99027577e-01
  3.99128385e-01  3.99351317e-01  3.99556535e-01  3.99984790e-01
  4.00363881e-01  4.06851515e-01  4.07451217e-01  4.08408973e-01
  4.08741084e-01  4.08792408e-01  4.08953202e-01  4.10106404e-01
  4.10388387e-01  4.10920551e-01  4.11272302e-01  4.12834909e-01
  4.15703785e-01  4.16872160e-01  4.17061353e-01  4.18505024e-01
  4.19389753e-01  4.20349354e-01  4.22435931e-01  4.25986124e-01
  4.26746878e-01  4.32780897e-01  4.32853956e-01  4.35582712e-01
  4.35918877e-01  4.36497022e-01  4.38896169e-01  4.42518521e-01
  4.43056993e-01  4.45314622e-01  4.45524588e-01  4.46884830e-01
  4.47578448e-01  4.47821143e-01  4.49353248e-01  4.50731881e-01
  4.52770461e-01  4.53372767e-01  4.54775889e-01  4.55612776e-01
  4.55659783e-01  4.57954765e-01  4.61524195e-01  4.62602499e-01
  4.63655507e-01  4.63828229e-01  4.63990786e-01  4.64908871e-01
  4.67342055e-01  4.69162274e-01  4.70031689e-01  4.73085721e-01
  4.73707842e-01  4.76755361e-01  4.79120954e-01  4.79583729e-01
  4.84090680e-01  4.92393654e-01  5.01303424e-01  5.05861966e-01
  5.08304252e-01  5.14990457e-01  5.17280650e-01  5.18935241e-01
  5.21014564e-01  5.22477892e-01  5.34237524e-01  5.36880137e-01
  5.39576851e-01  5.43621707e-01  5.46215414e-01  5.48224140e-01
  5.50916534e-01  5.56629317e-01  5.63039494e-01  5.64953191e-01
  5.67022123e-01  5.69885459e-01  5.74810619e-01  5.75315849e-01
  5.79608913e-01  5.82008060e-01  5.86263823e-01  5.86806355e-01
  5.88793534e-01  5.89569656e-01  5.94003796e-01  5.96402943e-01
  5.99267816e-01  6.02364511e-01  6.05992184e-01  6.13196973e-01
  6.15053588e-01  6.17374179e-01  6.20394414e-01  6.22792822e-01
  6.23132104e-01  6.25192708e-01  6.26081888e-01  6.27591855e-01
  6.29991002e-01  6.30561910e-01  6.30886054e-01  6.32390149e-01
  6.34338287e-01  6.34789297e-01  6.37188444e-01  6.37517802e-01
  6.39587591e-01  6.39916949e-01  6.43290078e-01  6.45226187e-01
  6.45297858e-01  6.46785032e-01  6.47727374e-01  6.50024481e-01
  6.51706589e-01  6.51825737e-01  6.53982473e-01  6.56381620e-01
  6.57239997e-01  6.58335702e-01  6.58520252e-01  6.58780767e-01
  6.58887538e-01  6.59305793e-01  6.59468244e-01  6.61179915e-01
  6.63133996e-01  6.63908420e-01  6.64104087e-01  6.65533143e-01
  6.65978209e-01  6.68377356e-01  6.70015716e-01  6.71439190e-01
  6.71909959e-01  6.72786366e-01  6.73175650e-01  6.75185513e-01
  6.75622720e-01  6.77973944e-01  6.80533607e-01  6.85044520e-01
  6.85171386e-01  6.86655505e-01  6.87125467e-01  6.87511978e-01
  6.88012063e-01  6.88884315e-01  6.91923761e-01  6.92172965e-01
  6.94767974e-01  6.95637074e-01  6.96716111e-01  6.96722056e-01
  6.97167121e-01  6.97491265e-01  6.99121203e-01  7.00458557e-01
  7.02490441e-01  7.03156235e-01  7.03919497e-01  7.05498018e-01
  7.08717791e-01  7.08773572e-01  7.09236872e-01  7.09604387e-01
  7.09649764e-01  7.09715914e-01  7.11116938e-01  7.13571867e-01
  7.14458427e-01  7.15915232e-01  7.15971014e-01  7.18314379e-01
  7.23168455e-01  7.27584221e-01  7.28352498e-01  7.29292577e-01
  7.30634259e-01  7.31196711e-01  7.33595858e-01  7.33679353e-01
  7.41485224e-01  7.43192446e-01  7.44704997e-01  7.44760779e-01
  7.49190949e-01  7.49503292e-01  7.50389887e-01  7.51332828e-01
  7.51980531e-01  7.56700733e-01  7.57587329e-01  7.59843138e-01
  7.59986476e-01  7.62689847e-01  7.65118402e-01  7.67513276e-01
  7.69583064e-01  7.71594533e-01  7.71982211e-01  7.73319476e-01
  7.74381358e-01  7.80031416e-01  7.81578800e-01  7.83977947e-01
  7.86377094e-01  7.88386957e-01  7.88776241e-01  7.89616543e-01
  7.91175388e-01  7.93574535e-01  7.93952331e-01  7.95973682e-01
  7.97983545e-01  7.98372829e-01  7.99315770e-01  8.03171123e-01
  8.05180986e-01  8.06175851e-01  8.06298017e-01  8.06513211e-01
  8.06568992e-01  8.07649386e-01  8.07969418e-01  8.09663658e-01
  8.10368565e-01  8.22688444e-01  8.28629476e-01  8.28800955e-01
  8.30213908e-01  8.36369899e-01  8.39158330e-01  8.40922639e-01
  8.44259265e-01  8.57450647e-01  8.80468856e-01  8.83962261e-01
  8.84225152e-01  8.86595776e-01  8.90735117e-01  8.97932559e-01
  9.00331706e-01  9.03458599e-01  9.05130000e-01  9.05468543e-01
  9.07529147e-01  9.08471489e-01  9.12327441e-01  9.16736451e-01
  9.17125735e-01  9.17342032e-01  9.19135598e-01  9.21381497e-01
  9.21534745e-01  9.26722324e-01  9.27293231e-01  9.29121471e-01
  9.31520618e-01  9.33919765e-01  9.34217957e-01  9.36318912e-01
  9.36535209e-01  9.38328775e-01  9.40479787e-01  9.41117206e-01
  9.49953008e-01  9.51037939e-01  9.51873658e-01  9.52723658e-01
  9.56023071e-01  9.57911236e-01  9.59921099e-01  9.63033674e-01
  9.64680439e-01  9.68698645e-01  9.69517687e-01  9.75230291e-01
  9.76172596e-01  9.76715129e-01  9.83856788e-01  9.84295011e-01
  9.88710864e-01  9.88801459e-01  9.91478647e-01  9.94423468e-01
  9.97099126e-01  1.00070660e+00  1.00363077e+00  1.00550489e+00
  1.00733443e+00  1.00790404e+00  1.01030319e+00  1.01698564e+00
  1.01831995e+00  1.02211860e+00  1.04396582e+00  1.05775952e+00]

  UserWarning,

2022-10-31 11:02:15,754:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95390677 -0.9404312  -0.590322   -0.54798138 -0.29263285 -0.20438721
 -0.1866832  -0.13798057 -0.12398716 -0.12304452 -0.1198953  -0.08278575
 -0.07046144 -0.05659303 -0.03509535 -0.02298702 -0.00978239 -0.00867918
 -0.00382677 -0.0026685   0.01637773  0.02021382  0.02536482  0.0286603
  0.02940902  0.03994095  0.0466716   0.0482134   0.04841486  0.05029733
  0.05073295  0.0719822   0.07856243  0.07874466  0.07996084  0.08056576
  0.08314081  0.08389327  0.09049631  0.09099067  0.09817093  0.101219
  0.10143571  0.10146786  0.10982714  0.11249419  0.14611001  0.15545589
  0.15665583  0.15985543  0.16139877  0.16279983  0.16686824  0.17524696
  0.18183328  0.18679973  0.18830515  0.19158659  0.19190296  0.19252427
  0.1926502   0.1951644   0.20279927  0.20425062  0.20733099  0.2074614
  0.21343426  0.21642321  0.22152616  0.22450732  0.2254441   0.2264667
  0.22749343  0.22963944  0.23249826  0.23352633  0.23594711  0.2364368
  0.23653825  0.23672313  0.24006843  0.24218397  0.24388555  0.24753795
  0.24923427  0.2493424   0.24944902  0.24982936  0.25171546  0.25178211
  0.25209328  0.25342516  0.25419332  0.25434203  0.25640774  0.25774491
  0.25815876  0.25831133  0.25931453  0.26218984  0.26220736  0.26361477
  0.26420538  0.2652288   0.26639281  0.26674639  0.26805315  0.2686495
  0.27090326  0.2751332   0.27573181  0.27579385  0.27583188  0.27714872
  0.27715522  0.27716429  0.27779335  0.27802854  0.27837289  0.27848577
  0.27910157  0.27930592  0.28126373  0.28155147  0.28181826  0.28260855
  0.28266623  0.28311093  0.28351711  0.28357219  0.28387128  0.2844636
  0.28725363  0.28759254  0.28846417  0.28895381  0.2890973   0.28915778
  0.28940858  0.28968461  0.29104614  0.29139901  0.29168013  0.29205209
  0.29239633  0.29287621  0.29320078  0.29656362  0.29709445  0.29730662
  0.29770865  0.29779807  0.2980921   0.30065335  0.30320717  0.30372513
  0.3060367   0.3065846   0.30782986  0.30783688  0.31016565  0.31217423
  0.31231064  0.31380226  0.31474381  0.31477784  0.31552171  0.31572159
  0.31644943  0.31673138  0.31727018  0.3180071   0.31925789  0.3198505
  0.32070667  0.32086699  0.32321097  0.32406432  0.32537297  0.32637709
  0.32709331  0.32934075  0.32943596  0.33042909  0.33074056  0.33403231
  0.33505735  0.33842389  0.33963374  0.34179339  0.34190047  0.342241
  0.34360276  0.34361855  0.34398     0.34737001  0.34746404  0.34843494
  0.34997945  0.35064859  0.35092599  0.35175     0.35408262  0.35469468
  0.35636466  0.3570634   0.35712433  0.35920264  0.35942988  0.36116294
  0.36166034  0.36407137  0.36737264  0.36947401  0.37051185  0.37078933
  0.37172624  0.37224861  0.37288026  0.37308115  0.37317483  0.37404887
  0.37503896  0.37512675  0.37597968  0.37657759  0.37766158  0.3779937
  0.37940359  0.38082543  0.38317591  0.3837317   0.38419512  0.38430621
  0.38567953  0.38619893  0.38629087  0.38655359  0.38674364  0.38970153
  0.38992702  0.39066709  0.39086599  0.39245987  0.39273984  0.39284276
  0.39371492  0.39417023  0.3946333   0.3948346   0.39591428  0.39797407
  0.39868131  0.39994992  0.40313597  0.4038835   0.40410309  0.40442945
  0.40470724  0.40652781  0.40849959  0.40871711  0.40880784  0.41013149
  0.4102992   0.4106173   0.41147821  0.41210082  0.41233539  0.41516063
  0.41838704  0.41960785  0.41970929  0.42105177  0.42106439  0.42210204
  0.42241033  0.42271587  0.42339005  0.42386382  0.42585448  0.42815084
  0.43081642  0.43255117  0.43260956  0.43425387  0.43530128  0.43775747
  0.44180064  0.44226667  0.44407468  0.44410794  0.44557073  0.44797072
  0.4479862   0.44862344  0.44888002  0.45467508  0.4551799   0.4567633
  0.45716664  0.45868611  0.45900235  0.46025031  0.46066686  0.46270305
  0.46366913  0.46405048  0.46699396  0.46817765  0.46830359  0.46920927
  0.47093077  0.47318923  0.47409716  0.47547221  0.47787205  0.48313319
  0.48640268  0.48737567  0.48822826  0.48912915  0.49198735  0.49322307
  0.49410273  0.49538602  0.49756572  0.5007252   0.50188517  0.50568469
  0.5103218   0.5135527   0.52288708  0.53062247  0.53311624  0.53575509
  0.53654311  0.53677499  0.53876682  0.54807891  0.55128789  0.55524213
  0.55556024  0.55649443  0.55711916  0.56127471  0.56709427  0.56985869
  0.5713711   0.57491537  0.57559213  0.57777783  0.58034736  0.58455071
  0.58704449  0.59173113  0.59475049  0.59701959  0.59951337  0.60073038
  0.60418282  0.60420954  0.60450093  0.6069947   0.6075276   0.6091971
  0.61198226  0.61590775  0.61732749  0.61788447  0.61946359  0.61950477
  0.62168996  0.62195737  0.62249575  0.62569813  0.62997159  0.63003628
  0.63193247  0.63766504  0.63809056  0.63912242  0.6394138   0.6416162
  0.64190758  0.64439034  0.64440136  0.64938891  0.65095373  0.65188269
  0.65458024  0.65464711  0.65687024  0.65711176  0.65804077  0.65936402
  0.66053455  0.66062203  0.6618578   0.66228401  0.66435158  0.66462222
  0.66488447  0.66655397  0.66684535  0.6684748   0.66933913  0.67361279
  0.67432668  0.67510811  0.67801215  0.67931424  0.68070742  0.68151663
  0.68211188  0.68213905  0.68375307  0.68928935  0.68981122  0.69178312
  0.69398552  0.69493568  0.69532028  0.69607114  0.69897307  0.70146685
  0.70175823  0.70301624  0.70455587  0.7064544   0.70674579  0.70704965
  0.70756749  0.70782812  0.70828617  0.70894818  0.70923956  0.71049758
  0.71144196  0.71292503  0.71464777  0.71797891  0.72237504  0.72598916
  0.72708082  0.72949364  0.73107407  0.73326735  0.73674274  0.7378628
  0.73949593  0.7404229   0.74196253  0.7428963   0.7441317   0.74541045
  0.74631679  0.74695008  0.74944386  0.75007991  0.75310816  0.75538556
  0.75730837  0.75866171  0.75941897  0.75949588  0.75986868  0.76280559
  0.76440652  0.7669003   0.76692747  0.76818549  0.76882112  0.76939407
  0.77017255  0.77064256  0.77090432  0.77188785  0.7751601   0.7766112
  0.77809241  0.77999391  0.78435674  0.78685051  0.78983437  0.79138078
  0.79682562  0.7993194   0.80057741  0.80250871  0.80650935  0.80680073
  0.80929451  0.80973072  0.81423893  0.82035195  0.8272934   0.82743799
  0.82969836  0.83228096  0.83798407  0.8451003   0.84665444  0.84670116
  0.84846272  0.85168872  0.85253248  0.85721873  0.85775861  0.8597125
  0.86719384  0.88041837  0.89267148  0.89462538  0.89465574  0.89975753
  0.90046464  0.90460049  0.9058585   0.90632902  0.90803418  0.90958805
  0.91446061  0.91583361  0.92205693  0.92704449  0.92801183  0.93203204
  0.93256493  0.93701959  0.93827761  0.94077138  0.94200715  0.94326516
  0.94450092  0.94574792  0.94575894  0.9469947   0.94724201  0.95600469
  0.95822782  0.9607216   0.96189213  0.96195736  0.96282009  0.96445114
  0.96513819  0.96570915  0.96763196  0.96837628  0.9694387   0.97069671
  0.9716418   0.97413487  0.97527446  0.97568426  0.97817804  0.98215489
  0.98316559  0.98524512  0.98815315  0.99064692  1.01059714  1.01309092
  1.0155847   1.0164329   1.01936097  1.02057225  1.04180496  1.06903635
  1.085963    1.09490337  1.18722084  1.64802087]

  UserWarning,

2022-10-31 11:02:15,785:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.86838184 -0.4162217  -0.34501471 -0.30478228 -0.1980063  -0.13967709
 -0.1284054  -0.06729697 -0.03472098 -0.0340365  -0.0263516  -0.01094653
  0.00368368  0.01329788  0.02390228  0.03834598  0.0394689   0.0411893
  0.04390586  0.05374874  0.05378996  0.05437416  0.06622262  0.08364501
  0.08592252  0.09823717  0.10052055  0.10327234  0.10686868  0.10845667
  0.10871827  0.12183695  0.12852057  0.13017266  0.13246469  0.13543075
  0.13742164  0.14068731  0.14243097  0.14966285  0.1514042   0.15235997
  0.15835264  0.16182235  0.16206588  0.16607312  0.16693317  0.16907892
  0.17143992  0.17191538  0.17529989  0.18353606  0.1848029   0.18592183
  0.18612489  0.18742978  0.19267727  0.19314548  0.19346119  0.19391005
  0.19686382  0.19858248  0.20074874  0.20136356  0.20293017  0.20784257
  0.21074845  0.21236285  0.2157182   0.21710681  0.21932425  0.22058348
  0.22071201  0.22171023  0.22453284  0.22726721  0.23147221  0.23619296
  0.23666677  0.2367486   0.23713181  0.24564271  0.24637143  0.24705837
  0.24716517  0.24733782  0.24876559  0.25070208  0.2523058   0.25257604
  0.25349147  0.25552383  0.25631956  0.25642644  0.25708058  0.25713867
  0.25725298  0.25973376  0.2612723   0.26197751  0.26299295  0.26367214
  0.26529006  0.26661749  0.26735088  0.26841313  0.27099379  0.27200248
  0.27216762  0.27328023  0.27386172  0.27444797  0.27477828  0.27491115
  0.27590316  0.2764938   0.27735432  0.27742492  0.27804886  0.28098029
  0.2812084   0.28203981  0.28384907  0.28405936  0.28418944  0.28442383
  0.28582102  0.28626392  0.28627727  0.28712702  0.28758147  0.28853118
  0.28963685  0.29017489  0.2910957   0.29113461  0.29373926  0.29559755
  0.29586983  0.29646019  0.29653764  0.29792669  0.29798855  0.29841489
  0.29881307  0.2989323   0.29897531  0.29927547  0.30023952  0.30118086
  0.30119702  0.3014272   0.30217187  0.30292266  0.30457278  0.30509954
  0.30524606  0.30716968  0.30943471  0.30975939  0.30978325  0.30998422
  0.31061164  0.31212364  0.31267834  0.31279543  0.31442427  0.31531979
  0.31794945  0.31966574  0.32078111  0.32403136  0.32475517  0.32597118
  0.32665424  0.32674068  0.32746541  0.32869246  0.32914194  0.33016377
  0.33173065  0.33179249  0.33210058  0.3334353   0.33348665  0.33420218
  0.33424985  0.33623247  0.33653521  0.3369212   0.33728725  0.3377365
  0.33990904  0.3436232   0.34405918  0.34530494  0.3459732   0.34665775
  0.34760875  0.34811931  0.34838183  0.35049004  0.35263057  0.35281272
  0.35291618  0.35331679  0.35430184  0.3544011   0.35482738  0.35519122
  0.35590937  0.35681416  0.35770588  0.35881161  0.35951474  0.3596768
  0.3609671   0.36155881  0.36518572  0.36734833  0.3678763   0.36886878
  0.36982598  0.37081889  0.37117558  0.37134577  0.37182285  0.37235353
  0.37439159  0.37558801  0.37589703  0.37659766  0.37701099  0.37733022
  0.37798038  0.37855727  0.37885702  0.38197758  0.38222373  0.38302288
  0.38373069  0.38410367  0.38507687  0.38564481  0.3858884   0.38615843
  0.38643171  0.38704934  0.38732772  0.38770219  0.3882477   0.38876961
  0.38963771  0.39159672  0.39167262  0.39243984  0.39248252  0.39260276
  0.39287303  0.39320678  0.3939828   0.39401     0.39483049  0.39530889
  0.39547806  0.39677376  0.39830616  0.39844022  0.3994639   0.39959623
  0.39975912  0.40074745  0.4027579   0.4037171   0.40497733  0.40729427
  0.40772722  0.40822889  0.40870319  0.40872175  0.40873924  0.41263459
  0.41459038  0.41519151  0.41550086  0.4158436   0.41916654  0.42148332
  0.42186905  0.4219193   0.42235729  0.42247928  0.42300731  0.42428966
  0.42447559  0.42456296  0.42540791  0.42790243  0.43025818  0.43231818
  0.43322566  0.43414956  0.43643378  0.43874422  0.44047568  0.4437993
  0.4502761   0.45261014  0.45498038  0.45566046  0.45641913  0.45667953
  0.45742384  0.45805621  0.45905775  0.45993453  0.4623745   0.46251939
  0.46259475  0.46341151  0.46437306  0.46439001  0.46511071  0.46583597
  0.46726244  0.46769118  0.46827309  0.46845944  0.46918477  0.47308764
  0.47333401  0.48026067  0.48233978  0.48731566  0.49028506  0.49375778
  0.49492652  0.4997721   0.50330516  0.50340724  0.50546938  0.50554842
  0.50608354  0.50657388  0.50818595  0.50891486  0.50923587  0.51197998
  0.51225881  0.5291019   0.52932203  0.53050889  0.5312162   0.53918324
  0.54764492  0.54961631  0.54982444  0.54989238  0.55133082  0.55202315
  0.55375432  0.55438344  0.55784819  0.56048257  0.56091576  0.56152305
  0.56253724  0.56287018  0.56345738  0.56432106  0.56526344  0.56598217
  0.56689217  0.56758693  0.5754692   0.57612056  0.58024547  0.5842794
  0.58482412  0.58602699  0.586598    0.58698223  0.58935932  0.5908411
  0.59514566  0.59719917  0.59786223  0.60057879  0.60870813  0.60872849
  0.60916717  0.61208692  0.61687818  0.61959475  0.62221127  0.62469955
  0.62629174  0.63172487  0.63251452  0.6344277   0.63552957  0.63589414
  0.63649552  0.63821275  0.63861071  0.64066422  0.64132727  0.64175131
  0.64364588  0.64517241  0.64636244  0.64947697  0.65194661  0.65219354
  0.6549101   0.66034323  0.66076072  0.66216618  0.66266184  0.6630598
  0.6651296   0.66577636  0.66775724  0.67054497  0.67081153  0.67120949
  0.6735281   0.67392606  0.67465667  0.675984    0.6778435   0.67935919
  0.68045047  0.68124461  0.68227917  0.68371399  0.68439436  0.68474693
  0.68711093  0.68726196  0.68858676  0.68982749  0.69115237  0.69254406
  0.6952254   0.69526062  0.69627074  0.69772839  0.69797719  0.70069375
  0.70314522  0.70334475  0.7033488   0.70578916  0.70587808  0.70612688
  0.70652484  0.70859465  0.70884345  0.71156001  0.71247052  0.71402778
  0.71424136  0.71467454  0.71674434  0.7167482   0.71699315  0.71700678
  0.72217747  0.72327345  0.7324171   0.73386266  0.73783947  0.74129947
  0.74452215  0.7451871   0.74653595  0.746889    0.74934313  0.74959193
  0.74960556  0.75123346  0.75169304  0.7517104   0.75177556  0.75329774
  0.75439193  0.75477626  0.75730844  0.76036525  0.76082627  0.76494712
  0.76590496  0.76859643  0.77405465  0.77483316  0.78220435  0.78227098
  0.78268451  0.78492091  0.79035405  0.79307061  0.79578718  0.79850374
  0.80078712  0.80122031  0.80393687  0.80544566  0.80937     0.81095021
  0.82023627  0.82246376  0.827988    0.82838596  0.831912    0.83568303
  0.83610247  0.83750372  0.83771686  0.83925222  0.84132203  0.84196879
  0.84468535  0.84619414  0.85380311  0.85410471  0.857835    0.86055156
  0.86175828  0.8630246   0.86370131  0.8637268   0.87120425  0.87353185
  0.8864019   0.88765782  0.88845796  0.89001433  0.89231683  0.90531182
  0.90541785  0.90760288  0.90904492  0.918265    0.91835392  0.92171725
  0.93193674  0.93465331  0.93486689  0.93530007  0.94164371  0.94280301
  0.94344977  0.94616634  0.95135254  0.95159947  0.96246573  0.96305529
  0.96444661  0.96807964  0.97424249  0.97540179  0.97666565  0.98371968
  0.98626805  0.99170119  0.99441775  0.99985088  1.00256745  1.00528401
  1.0088195   1.01071714  1.01594441  1.02115022  1.02158341  1.02712257
  1.02919237  1.0334274   1.05036606  1.07234552  1.07777865  1.08903399
  1.42095824  1.66752361]

  UserWarning,

2022-10-31 11:02:15,891:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.24194797 -0.84419775 -0.60693611 -0.40250413 -0.38078738 -0.24163741
 -0.16921183 -0.1044453  -0.1032711  -0.09339279 -0.08550376 -0.08414727
 -0.05753256 -0.05487387 -0.03848074 -0.03501986 -0.01256979 -0.00582977
 -0.00403834  0.00838846  0.01732571  0.02044145  0.02498321  0.0259042
  0.03474579  0.05187124  0.05781138  0.06219944  0.06392184  0.09782585
  0.11172976  0.12673803  0.13431239  0.13534562  0.13742668  0.14325894
  0.14374922  0.14416599  0.14956382  0.15030022  0.15192921  0.15541783
  0.15647455  0.15699477  0.15699883  0.1580581   0.15945985  0.16203864
  0.16318305  0.16421437  0.16917848  0.17124075  0.17433662  0.17763702
  0.17825802  0.18686835  0.19049503  0.19169473  0.19211793  0.19284836
  0.19477492  0.19665897  0.19725865  0.19927061  0.19964648  0.19991583
  0.20005353  0.20507564  0.20683075  0.20734666  0.20902668  0.20949185
  0.21259572  0.21538975  0.21580861  0.21742839  0.21774558  0.21774654
  0.22214797  0.22366603  0.22428223  0.22458386  0.22471285  0.22554627
  0.22627782  0.22815116  0.23082553  0.23301337  0.23904117  0.23910414
  0.24100187  0.2410416   0.24143609  0.2417633   0.24548442  0.24693015
  0.24759788  0.24879089  0.25024618  0.2509244   0.2512435   0.25156084
  0.25787422  0.25880685  0.26157628  0.26167447  0.26452477  0.26487773
  0.26696599  0.26769566  0.2678095   0.26800434  0.26868639  0.26977468
  0.2699369   0.27031504  0.27045193  0.27069761  0.27156818  0.27173318
  0.27175027  0.27317142  0.27405799  0.27461939  0.28164931  0.28184533
  0.28241683  0.28267359  0.28356957  0.2842039   0.28455937  0.28456142
  0.28679553  0.28791818  0.28812227  0.28918756  0.289397    0.28962978
  0.29032217  0.29060298  0.29075738  0.29133833  0.29169289  0.29180384
  0.29259643  0.29392365  0.29445183  0.29516252  0.29582985  0.29674996
  0.29682996  0.29685291  0.29847564  0.29884191  0.29977104  0.29993794
  0.30363438  0.30419349  0.30445826  0.30622599  0.30645135  0.30672153
  0.30729403  0.30817941  0.30841695  0.30906594  0.30998953  0.31176733
  0.31329342  0.31423203  0.31433617  0.31437619  0.31456746  0.31465037
  0.31864183  0.31910114  0.32169752  0.32300928  0.32418067  0.32513643
  0.32619141  0.32640176  0.3267397   0.32720084  0.32761983  0.32781635
  0.32843027  0.32944887  0.33042621  0.33293227  0.33324225  0.33770183
  0.33845096  0.3386033   0.33957879  0.33958711  0.34037982  0.34046935
  0.34048039  0.3406224   0.34107704  0.34264339  0.34423549  0.34637294
  0.34686134  0.3476103   0.34810343  0.34858122  0.34864333  0.34920478
  0.35278689  0.35297722  0.35387322  0.35603449  0.35648441  0.35860548
  0.35930156  0.36042819  0.36199685  0.36265625  0.36372923  0.36374398
  0.36375204  0.36388844  0.36403059  0.36411462  0.36442531  0.36486431
  0.36739865  0.36797795  0.36817473  0.36950421  0.3731721   0.37374609
  0.37403703  0.37650864  0.37738133  0.37785964  0.38027268  0.38272166
  0.38507513  0.38509286  0.38536131  0.38685032  0.38710101  0.38746406
  0.38753897  0.38765696  0.38773251  0.38794028  0.38865106  0.38931515
  0.38941719  0.38949344  0.38983526  0.38989653  0.39127654  0.39153241
  0.39204988  0.39239655  0.39421461  0.39457767  0.39511083  0.39631528
  0.3970948   0.3973278   0.39914046  0.39932007  0.39989873  0.40042602
  0.40224528  0.40292443  0.4030223   0.40412152  0.40440031  0.40494198
  0.40694782  0.40765774  0.40880488  0.40906699  0.41031519  0.41079913
  0.41185754  0.41258532  0.41378836  0.41555966  0.41999468  0.42258202
  0.42349569  0.42405704  0.42484868  0.42771504  0.42825626  0.42840844
  0.43300722  0.4345335   0.43531996  0.43633591  0.43733194  0.43783528
  0.43789325  0.4399014   0.44026445  0.44154558  0.44172092  0.44263135
  0.44357427  0.44500686  0.44628604  0.44948108  0.44967225  0.4512558
  0.45138596  0.45198011  0.45212047  0.4531742   0.45570418  0.45634256
  0.45855144  0.46449103  0.46469634  0.46649949  0.47263431  0.47796143
  0.47843994  0.4788777   0.47990696  0.48089973  0.48389914  0.486715
  0.49340992  0.49399987  0.49411947  0.49764769  0.49886188  0.50025322
  0.5039352   0.50816465  0.50922259  0.51067708  0.51441435  0.5153858
  0.5173044   0.51767853  0.5225739   0.52968751  0.53834701  0.5384
  0.54137426  0.54255046  0.54389154  0.54628592  0.5472298   0.55714737
  0.55969032  0.56384887  0.56556066  0.56661469  0.56715291  0.56867475
  0.57054863  0.57650182  0.57810332  0.58018243  0.58196759  0.5840787
  0.58649233  0.58670999  0.58964322  0.59781519  0.598566    0.59891098
  0.60451127  0.61021644  0.61313819  0.61575047  0.61678481  0.62006977
  0.6202518   0.62152721  0.62197367  0.62286407  0.63237626  0.63447901
  0.63590205  0.63685022  0.64015242  0.64159262  0.64203299  0.64396382
  0.64870623  0.65246682  0.65344863  0.65581984  0.65768456  0.65819104
  0.65896527  0.66056224  0.66281146  0.66289945  0.66293344  0.66344296
  0.66530464  0.66560456  0.66995597  0.67072849  0.67241825  0.67478945
  0.67524136  0.67716066  0.67953186  0.67977293  0.6801658   0.68190306
  0.68214413  0.68451533  0.68472617  0.6849082   0.6867591   0.68688654
  0.68691391  0.68727941  0.68898268  0.69005801  0.69162894  0.69375907
  0.69874255  0.69913542  0.70111375  0.70288082  0.70324388  0.70387782
  0.70525203  0.70585615  0.70813628  0.70972316  0.71030879  0.7103235
  0.71035749  0.71041653  0.71099143  0.71173413  0.7131761   0.7174224
  0.71810504  0.71875351  0.72175165  0.72320445  0.72413241  0.72491099
  0.72927841  0.72956818  0.73049421  0.73151444  0.73164961  0.73589734
  0.73598842  0.73639202  0.7388017   0.74603915  0.74637312  0.74824803
  0.75021563  0.75061923  0.75130187  0.75367307  0.75536164  0.76010404
  0.76721765  0.76723235  0.76958885  0.77196005  0.77621338  0.77661138
  0.77670246  0.77709345  0.77900866  0.77907366  0.77951403  0.78360004
  0.78381606  0.78839594  0.78855847  0.79056662  0.79092967  0.79804328
  0.80041448  0.80278568  0.80515688  0.8085526   0.81077907  0.81227049
  0.81902104  0.82649771  0.82748337  0.82950285  0.83357732  0.84072492
  0.84309612  0.84546732  0.846902    0.85061611  0.85350472  0.85599348
  0.86217937  0.86379976  0.87367556  0.87568976  0.87802697  0.8816693
  0.88342796  0.8840405   0.88751178  0.88814572  0.88988298  0.89225419
  0.89522858  0.90242892  0.9088526   0.91375289  0.91833741  0.92134255
  0.92307982  0.93256462  0.93295562  0.93310749  0.93520671  0.9362597
  0.93730703  0.94204943  0.94268338  0.94342608  0.94442064  0.94679184
  0.94691776  0.94979698  0.95231581  0.95425928  0.95453939  0.95691059
  0.95731799  0.95928179  0.96002449  0.9711378   0.97287506  0.973509
  0.97434037  0.97588021  0.9824735   0.98299381  0.98536502  0.98773622
  0.99247862  0.99301909  0.99685797  0.9989243   0.99989215  1.00187236
  1.00196343  1.00486779  1.01519609  1.01563634  1.02702113  1.02737872
  1.02989755  1.03503555  1.0400163   1.04141377  1.04282787]

  UserWarning,

2022-10-31 11:02:15,970:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60470879 -0.84478708 -0.31001272 -0.23181711 -0.22925958 -0.18388184
 -0.17375493 -0.15939072 -0.0589028  -0.05433673 -0.04762999 -0.03805126
 -0.01194197 -0.00576226  0.00481885  0.01000654  0.01190334  0.02147636
  0.06059361  0.07117711  0.07460485  0.08399168  0.1019461   0.10397485
  0.1127978   0.1184939   0.1231407   0.1234011   0.14262182  0.14327143
  0.14461337  0.14687813  0.15400446  0.16326069  0.16334379  0.17621338
  0.18122406  0.18711622  0.19040683  0.19182796  0.19249545  0.1950319
  0.19532825  0.19607945  0.20023442  0.2011216   0.20186613  0.20679127
  0.20997048  0.21001827  0.21073364  0.21229152  0.21408936  0.21443799
  0.21612068  0.21706476  0.21864666  0.22136675  0.22235533  0.22350263
  0.2240505   0.22494087  0.22612033  0.22727753  0.22801278  0.22816124
  0.22965419  0.23071314  0.23470041  0.23643053  0.2382808   0.23994042
  0.24060604  0.2409866   0.24617892  0.24673872  0.24711723  0.24804804
  0.24970946  0.25070528  0.25156132  0.25192229  0.2531271   0.25439267
  0.2547662   0.25533802  0.25540359  0.25625564  0.25641315  0.25662577
  0.25845769  0.25870054  0.25872585  0.25895     0.25989356  0.26261927
  0.26270334  0.26281105  0.26314879  0.26322826  0.26515705  0.26555267
  0.26597203  0.26640316  0.26642117  0.26652057  0.26808957  0.26874787
  0.26951459  0.26972458  0.27030322  0.27047059  0.27222882  0.27341672
  0.27375312  0.27512217  0.27535875  0.27552695  0.27649965  0.27814485
  0.28280997  0.28582948  0.28622529  0.28673768  0.28696301  0.28722409
  0.28755421  0.28823392  0.28915519  0.28942761  0.29053647  0.29074591
  0.29134011  0.29170649  0.29295831  0.29417098  0.29430165  0.29505041
  0.29546911  0.29600105  0.29833401  0.29963867  0.29971372  0.30023446
  0.30137032  0.30205436  0.3020922   0.30221455  0.30302158  0.30433206
  0.30586645  0.30784663  0.30878058  0.31213021  0.31241801  0.31308032
  0.31457202  0.3146049   0.31605988  0.31804905  0.31974926  0.3217056
  0.32225099  0.32245936  0.32307597  0.3238702   0.32449181  0.32457576
  0.32580563  0.32592995  0.32709836  0.32740353  0.32766754  0.3281436
  0.32869647  0.32925215  0.32984152  0.32997867  0.33089801  0.33093212
  0.33208825  0.33237908  0.33242087  0.33249107  0.33431356  0.33434605
  0.33445376  0.33515712  0.33752799  0.33856788  0.33861026  0.34026276
  0.34041652  0.34118112  0.34166332  0.34214162  0.34276107  0.34332645
  0.34369824  0.34446564  0.34618057  0.34693592  0.34742643  0.34822122
  0.3491309   0.34965011  0.35078526  0.35079828  0.35209405  0.3521503
  0.35255261  0.35416277  0.35486102  0.35492884  0.35651903  0.35693773
  0.35694874  0.35860531  0.3586325   0.3601311   0.36077102  0.3608579
  0.36135351  0.36265732  0.36353222  0.36622361  0.36754006  0.36816123
  0.37001079  0.37153022  0.3715447   0.37260543  0.37279561  0.37519972
  0.37606024  0.37659015  0.37771838  0.37853774  0.38060026  0.38100688
  0.38279326  0.3831032   0.38323119  0.38412235  0.38412839  0.38435305
  0.38537346  0.3856215   0.38586005  0.38595411  0.38825662  0.38884622
  0.38926689  0.38932228  0.38940774  0.38964807  0.39019835  0.39279244
  0.39457328  0.39461366  0.39472921  0.39518276  0.39712245  0.3974392
  0.39769251  0.39872363  0.39996338  0.40033605  0.40054594  0.4017234
  0.40219994  0.40305786  0.40312481  0.40364     0.40472246  0.40474401
  0.4049287   0.40646747  0.40694992  0.40725682  0.40785314  0.41230995
  0.41301391  0.41365165  0.41516554  0.41696125  0.41814221  0.41842242
  0.41844063  0.42032923  0.42170758  0.42233232  0.42300205  0.42321538
  0.42482828  0.42484058  0.4250305   0.42530722  0.42820911  0.4282697
  0.43094281  0.43424308  0.43428753  0.43776209  0.43807822  0.43838693
  0.43914666  0.44176836  0.44250557  0.44299156  0.44517127  0.44664696
  0.44840203  0.45684142  0.45835847  0.46567722  0.46668409  0.47157969
  0.47493696  0.47539092  0.47594528  0.47736355  0.47806417  0.47893596
  0.48153439  0.48201992  0.48331561  0.48626294  0.48725237  0.48833257
  0.49411924  0.49762476  0.50196857  0.50270286  0.50305404  0.50930593
  0.51009982  0.51383287  0.51480715  0.5178661   0.51786661  0.51906084
  0.52100075  0.52898606  0.52954023  0.5316103   0.53773601  0.53787236
  0.5411771   0.5435626   0.54499424  0.5470858   0.55488101  0.55903737
  0.56424387  0.57283897  0.57590675  0.57767038  0.57955207  0.58055019
  0.58226181  0.58456431  0.59250176  0.59890056  0.60131368  0.60206302
  0.6058214   0.60878054  0.61081671  0.61162427  0.61401459  0.6203174
  0.62052206  0.62459431  0.62522668  0.62562317  0.62596616  0.62698462
  0.62701508  0.62835647  0.62844833  0.63074679  0.63083864  0.63191233
  0.63353162  0.63486394  0.63556034  0.63971033  0.64314593  0.64350297
  0.64368205  0.6498693   0.65177012  0.65203531  0.65464992  0.65999437
  0.6603507   0.66182087  0.66354771  0.66660149  0.66796956  0.66797619
  0.6683941   0.66850834  0.67020157  0.67089263  0.67222251  0.67377244
  0.67473027  0.67616275  0.67855306  0.68177963  0.68273598  0.68333369
  0.68524054  0.68762482  0.68837496  0.68984116  0.69001514  0.69448111
  0.6948018   0.69958243  0.7024562   0.70436305  0.70723683  0.70930646
  0.71115188  0.71153399  0.71201746  0.71354219  0.71362445  0.71388342
  0.71391828  0.71419969  0.7159325   0.71601861  0.71631462  0.71710599
  0.71844893  0.71870494  0.71971737  0.7207439   0.72209298  0.72348556
  0.72447151  0.72615126  0.72788407  0.72826619  0.73027439  0.73065048
  0.73197623  0.73208734  0.73313868  0.73983564  0.73989104  0.74049314
  0.74235206  0.74401856  0.74518366  0.74711686  0.74717459  0.7493969
  0.75039463  0.75156181  0.75170061  0.75292081  0.75554559  0.75758233
  0.75895815  0.75933424  0.75934027  0.76134847  0.76173059  0.76481068
  0.76523674  0.76525008  0.77042023  0.77569035  0.77613793  0.7774172
  0.78047098  0.78246366  0.78286129  0.78525161  0.78715846  0.78764192
  0.78880784  0.78904775  0.79264191  0.79441522  0.79481286  0.79594645
  0.79838526  0.79959349  0.80198381  0.80375951  0.80644374  0.80777289
  0.80978215  0.81258498  0.81822651  0.818716    0.82061682  0.82110632
  0.82349663  0.83305789  0.83581436  0.85017606  0.85042245  0.85288212
  0.85856677  0.86916085  0.87450022  0.88044677  0.88379472  0.88406148
  0.89295926  0.89522973  0.89791387  0.90142605  0.90460867  0.90974032
  0.91035493  0.91513556  0.92367457  0.92947744  0.92992502  0.93186776
  0.93425807  0.93664839  0.9390387   0.94381933  0.94512243  0.94923233
  0.95229338  0.9552814   0.9557709   0.95816121  0.95946432  0.96006203
  0.96294184  0.9720136   0.9741079   0.97440391  0.97918454  0.98202243
  0.98396517  0.98575777  0.98606165  0.99087434  0.99352643  0.9990448
  0.99991404  1.00069737  1.00181923  1.00308768  1.00567291  1.00786831
  1.00996261  1.02393704  1.02762319  1.02859781  1.02905495  1.0315544
  1.05055272  1.05630696  1.06108759  1.06846241  1.11726747  1.26569606
  1.29631179  2.53663387]

  UserWarning,

2022-10-31 11:02:15,970:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04602623 -0.84332998 -0.75815329 -0.72378269 -0.65507834 -0.57382694
 -0.51062582 -0.41031553 -0.39184235 -0.38498398 -0.35385629 -0.35319306
 -0.28530344 -0.28401411 -0.26001254 -0.24322189 -0.2379901  -0.22543094
 -0.18854615 -0.15641655 -0.13776864 -0.10165344 -0.08344072 -0.08151435
 -0.04285185 -0.03609068 -0.02911575 -0.02763678 -0.02334932 -0.00797357
 -0.00779856 -0.00316579  0.01604145  0.04585153  0.0459524   0.07021959
  0.07819721  0.07943861  0.08137006  0.09220573  0.09998941  0.10073869
  0.10184522  0.10397276  0.11101953  0.1121763   0.11357561  0.12020607
  0.12799487  0.14236152  0.15393916  0.15488832  0.16166636  0.16417037
  0.16570563  0.17273235  0.17380542  0.1738621   0.17461713  0.17496176
  0.17618227  0.17814193  0.18168691  0.18827384  0.19310441  0.19312773
  0.19396588  0.19761632  0.19763355  0.20016568  0.20069067  0.20082482
  0.20154775  0.20187227  0.20309592  0.21412437  0.21971231  0.21993504
  0.22045778  0.22116935  0.22349152  0.22352586  0.22408235  0.22513635
  0.22743463  0.22800574  0.23074463  0.23198206  0.23486439  0.23657057
  0.23713044  0.23871501  0.24032984  0.24190612  0.24206001  0.24286801
  0.24353644  0.24383856  0.24532357  0.2456746   0.25431716  0.25680667
  0.25738799  0.2574842   0.25918734  0.25951401  0.26046965  0.26069937
  0.2629975   0.26402185  0.26462454  0.26565811  0.2681493   0.26862298
  0.26890069  0.27031524  0.27033501  0.27162932  0.27198062  0.27280418
  0.27293207  0.27341383  0.27424996  0.27531101  0.27555241  0.27565565
  0.2757062   0.27621206  0.27701716  0.27896984  0.28035345  0.28041623
  0.28131452  0.28252967  0.2830827   0.28453233  0.28511573  0.28694259
  0.28884817  0.28974463  0.29007473  0.29034247  0.29069461  0.29136547
  0.29151674  0.29522217  0.29602975  0.29665803  0.2968958   0.29776048
  0.29794264  0.29795022  0.29942936  0.30393445  0.30417144  0.30506949
  0.30730329  0.30850285  0.3094855   0.31146253  0.31155781  0.3119878
  0.3129221   0.31311156  0.313976    0.31451404  0.31755496  0.31862324
  0.31917805  0.31933174  0.31949389  0.32128953  0.32159751  0.3222298
  0.32247224  0.32399357  0.32456226  0.32611124  0.32653783  0.32729271
  0.32864592  0.32870635  0.3292738   0.33001889  0.33133016  0.33403159
  0.33589042  0.33593041  0.33674533  0.33697648  0.33779593  0.33784999
  0.33973899  0.34087472  0.34201787  0.342062    0.34231484  0.34268263
  0.34375535  0.34424387  0.34570838  0.34655725  0.3466895   0.34741895
  0.34763344  0.34776778  0.34837625  0.34902319  0.34988584  0.35239226
  0.35281542  0.35349465  0.35375421  0.35431006  0.35459479  0.35609631
  0.35788084  0.35856433  0.35956163  0.35959866  0.3596282   0.35975679
  0.36009964  0.36083063  0.36117063  0.36159746  0.36164423  0.36309858
  0.36350201  0.36514151  0.36642129  0.36689233  0.36698951  0.36731718
  0.36921405  0.37033919  0.37053969  0.37108029  0.37131573  0.37204272
  0.37264345  0.37393736  0.37577995  0.37595509  0.37744343  0.37758908
  0.37956937  0.38020713  0.38072185  0.38079394  0.38145717  0.38252642
  0.38380803  0.38491735  0.38506762  0.38743638  0.38872018  0.38906008
  0.39590066  0.39673153  0.39702288  0.39770805  0.39857193  0.39938113
  0.39979639  0.39994833  0.40045592  0.40217352  0.40282843  0.40295157
  0.40409503  0.40533449  0.4053786   0.40642468  0.40661771  0.4080029
  0.40897683  0.41021143  0.41096418  0.41261215  0.41292924  0.41397726
  0.41483073  0.41543606  0.41555885  0.41633547  0.4165261   0.41669907
  0.41688605  0.41860875  0.42147668  0.42148263  0.42273286  0.42351973
  0.42380797  0.42405456  0.42432846  0.42495421  0.42723382  0.42809443
  0.42837999  0.42853252  0.43104441  0.43136426  0.43182125  0.43214816
  0.4376696   0.43966571  0.44060406  0.44358868  0.44490992  0.44491427
  0.44692314  0.44735294  0.44758772  0.44777822  0.46157271  0.46456497
  0.46697108  0.47076902  0.47192374  0.47410238  0.47856112  0.48115575
  0.49428084  0.49928423  0.50010041  0.50754113  0.5076128   0.50907679
  0.51080837  0.5116005   0.51618256  0.51697131  0.52241067  0.52429882
  0.52520582  0.52925899  0.5332023   0.53421672  0.53648455  0.53683084
  0.53879419  0.54054697  0.54174168  0.5434914   0.54418402  0.54422307
  0.54476223  0.5453905   0.54741263  0.54809167  0.56284935  0.56342035
  0.56442506  0.57078339  0.57132369  0.5741095   0.57521532  0.57585007
  0.57790672  0.58530687  0.58635527  0.59680694  0.59871994  0.60242816
  0.60354058  0.6044011   0.60554937  0.60722203  0.61184774  0.61437448
  0.61461679  0.61533424  0.61606412  0.61955251  0.62015489  0.62043123
  0.62197977  0.62292395  0.62505825  0.62538546  0.625693    0.62617908
  0.62846205  0.6291795   0.63367032  0.63400015  0.63476976  0.6351333
  0.6367692   0.63953826  0.63995395  0.64231686  0.64299622  0.64507636
  0.64579381  0.64760331  0.64905677  0.65338351  0.65491107  0.65615256
  0.66024273  0.66296948  0.66382497  0.66445972  0.66517717  0.66589662
  0.6665729   0.66762251  0.66999782  0.67020573  0.67024775  0.67044295
  0.67063762  0.67268922  0.67276687  0.67459364  0.67553593  0.67670991
  0.67685704  0.67805624  0.67815632  0.67830498  0.6791018   0.67979204
  0.68050303  0.68107403  0.68229146  0.68447562  0.68456053  0.68540639
  0.68733158  0.68930353  0.68985554  0.69151549  0.69215024  0.69261895
  0.69563674  0.69705359  0.69768834  0.69840579  0.69982264  0.70045739
  0.70156859  0.7025917   0.70322644  0.7039439   0.7044378   0.70536075
  0.70542449  0.70599549  0.70624542  0.7081298   0.70876454  0.71121494
  0.71187499  0.71225105  0.71254532  0.71430265  0.71447492  0.71455257
  0.7150201   0.71821801  0.71857978  0.71967296  0.72020012  0.72040303
  0.72057676  0.73116337  0.73305126  0.73601702  0.73646421  0.73944326
  0.73947404  0.74083466  0.74689652  0.74778119  0.74810669  0.74824872
  0.74929128  0.75055024  0.75378682  0.75421568  0.75428073  0.7588574
  0.75903633  0.76078621  0.76162645  0.76294756  0.7699336   0.77270265
  0.7758874   0.77624132  0.77824076  0.78031339  0.78118381  0.78243853
  0.78545775  0.78654791  0.78726736  0.79208602  0.80316222  0.80510234
  0.80593127  0.80774005  0.81772493  0.82049398  0.82191084  0.82254558
  0.82531463  0.82603209  0.83130863  0.83483315  0.8376022   0.83987735
  0.84192894  0.84689605  0.8481845   0.85813944  0.85975461  0.87198469
  0.87710898  0.8830609   0.888599    0.89967521  0.90039266  0.90220216
  0.90244426  0.90316171  0.90316371  0.90521331  0.90869982  0.91075142
  0.91564644  0.91598198  0.91905857  0.9273315   0.93290383  0.93331952
  0.93411519  0.93608857  0.93757564  0.93844193  0.94121098  0.94192844
  0.94340903  0.94746654  0.94951814  0.95171619  0.95228719  0.95300464
  0.95505624  0.95577369  0.9601563   0.96059434  0.96336339  0.96726559
  0.96954287  0.96961895  0.9744396   0.97515705  0.9799777   0.98274676
  0.98900231  0.99177136  0.99730947  1.00284757  1.00318234  1.01669283
  1.02133103  1.02499998  1.03174944  1.10853919  1.52274401]

  UserWarning,

2022-10-31 11:02:15,985:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81219705 -0.51089605 -0.43866517 -0.37893118 -0.34634484 -0.3359245
 -0.25773317 -0.25435879 -0.12485832 -0.11859412 -0.09815983 -0.08087424
 -0.07552297 -0.07194131 -0.03130715 -0.03060841 -0.02587335 -0.02055669
  0.01539194  0.02053805  0.02863662  0.03610948  0.03945943  0.04566762
  0.05250905  0.05257065  0.06292916  0.06551879  0.08744528  0.09413208
  0.10170225  0.10210926  0.10658488  0.1123544   0.11434204  0.11683864
  0.11766464  0.11808358  0.12737825  0.13843173  0.14029367  0.14259209
  0.15297841  0.15754003  0.16303453  0.1665088   0.17717489  0.17959636
  0.18250296  0.19469571  0.19505191  0.19783217  0.19817061  0.19834019
  0.19963234  0.20149143  0.20283392  0.20305354  0.20421738  0.20671652
  0.20948633  0.21053412  0.2146518   0.21662185  0.21752118  0.21839719
  0.2190275   0.2228486   0.22568004  0.2263259   0.23048969  0.2311156
  0.23172223  0.23583047  0.23787254  0.23839018  0.24051224  0.24082491
  0.24107988  0.24131903  0.24369319  0.24387164  0.24398317  0.24635369
  0.24810895  0.24893144  0.25117699  0.25138521  0.25203679  0.25284204
  0.25356743  0.25457878  0.25458084  0.25519708  0.25599929  0.25760307
  0.25769993  0.25925479  0.26037581  0.26088318  0.2620578   0.26217824
  0.26395018  0.26401469  0.26429811  0.26443887  0.26493539  0.26538749
  0.26848426  0.26871262  0.26889502  0.26897605  0.26911809  0.26966634
  0.27097376  0.27104305  0.27122971  0.27265608  0.27377168  0.27486936
  0.2773301   0.27754153  0.27908151  0.28053696  0.28185057  0.28310103
  0.28409925  0.28438621  0.28465877  0.28498257  0.28537244  0.28558351
  0.28672081  0.28717727  0.28745869  0.28758295  0.28789283  0.28952915
  0.2898024   0.28985368  0.28992554  0.2903031   0.2908729   0.29100535
  0.29114302  0.29295482  0.29354277  0.29412604  0.29431757  0.29596697
  0.29807516  0.29834155  0.29864787  0.29981589  0.30036115  0.3012905
  0.30278503  0.30280127  0.30315045  0.30436978  0.30440015  0.30504682
  0.30735261  0.30750092  0.31005774  0.31157789  0.31291965  0.3149045
  0.31500822  0.31521267  0.31748521  0.31854167  0.31926676  0.31940544
  0.31992948  0.32007662  0.32142702  0.32281632  0.32388934  0.32505618
  0.325394    0.32564456  0.32675348  0.32684679  0.32709985  0.33040602
  0.33096728  0.33214233  0.33329148  0.334534    0.33795799  0.3391392
  0.3392043   0.33940811  0.33979728  0.34061624  0.34077884  0.34121875
  0.34179887  0.3421378   0.34224776  0.34346589  0.34407164  0.34534393
  0.34580396  0.3459266   0.34883301  0.34929817  0.35137195  0.35256222
  0.3536592   0.35607378  0.35731553  0.35814705  0.35829731  0.35976451
  0.36077129  0.36185894  0.36188584  0.36237111  0.36284178  0.3632492
  0.36335432  0.36444259  0.36631566  0.36656523  0.36669753  0.36719791
  0.3677919   0.36789852  0.36991664  0.3700443   0.37142498  0.37181744
  0.37274885  0.37292875  0.37293194  0.37442781  0.3745111   0.37827511
  0.37839438  0.37952488  0.37972929  0.3812321   0.38133165  0.3815468
  0.38210745  0.38217755  0.38315143  0.38382605  0.38453131  0.38831188
  0.38933519  0.38960002  0.38969699  0.38971902  0.39007773  0.39070082
  0.3913012   0.39159494  0.39188358  0.39376085  0.39383279  0.39458935
  0.39524504  0.39629964  0.39710408  0.39799278  0.39914648  0.39939704
  0.40048156  0.40060111  0.40209658  0.40343072  0.40567489  0.40594098
  0.40628968  0.4063406   0.40936457  0.40979436  0.41049896  0.41097181
  0.41272646  0.41413823  0.41480637  0.41523148  0.41898439  0.42008924
  0.42027267  0.42443198  0.42456188  0.4251781   0.4253635   0.42545647
  0.42779654  0.42909323  0.43061683  0.43080229  0.43147292  0.43310381
  0.43403205  0.43403361  0.43609901  0.43683657  0.43859755  0.43860328
  0.43937632  0.44767248  0.4481551   0.45082585  0.45300545  0.4536478
  0.45515548  0.45681604  0.45923871  0.46310204  0.46624999  0.46664597
  0.46673787  0.46837651  0.46969947  0.47617474  0.47736303  0.48154006
  0.48155882  0.48750264  0.48763206  0.48864913  0.48875542  0.49102516
  0.49120188  0.49211861  0.49333622  0.50230752  0.50237791  0.504216
  0.50647707  0.50773491  0.51100701  0.51267951  0.51398465  0.5253638
  0.52770028  0.52868426  0.53104157  0.53413811  0.53880994  0.54291528
  0.55943054  0.56025454  0.56228717  0.56575812  0.56633673  0.56764064
  0.56885492  0.56956945  0.57072201  0.57285103  0.58099625  0.58108053
  0.58165692  0.58739783  0.58742916  0.59301327  0.59600274  0.59769819
  0.59778768  0.59858569  0.60111204  0.60179756  0.60438719  0.60697682
  0.60858853  0.60898387  0.6147015   0.61545565  0.61547371  0.61733533
  0.61890164  0.62251459  0.6236705   0.62525472  0.62573124  0.6294158
  0.62990378  0.63107315  0.63463532  0.63486333  0.63546273  0.63805236
  0.6408148   0.6414142   0.64323162  0.64519901  0.64582125  0.64699062
  0.65359013  0.65403246  0.65545656  0.65617976  0.65633026  0.65843951
  0.65872519  0.65883068  0.6595416   0.66093391  0.66135902  0.66394865
  0.66396147  0.66653828  0.66666706  0.66731049  0.6691279   0.67430716
  0.67666213  0.67689679  0.67814805  0.67948642  0.68024411  0.68025863
  0.68496166  0.68542878  0.68769763  0.69061715  0.69171136  0.69392129
  0.69517469  0.6966334   0.70097566  0.70108101  0.70279308  0.7040601
  0.70611071  0.70615492  0.70655208  0.70754722  0.70797234  0.70960233
  0.71056196  0.71071247  0.71306744  0.71512566  0.71541134  0.71564714
  0.71574122  0.71589172  0.71816226  0.723085    0.72423716  0.72537653
  0.72603833  0.72796616  0.72883987  0.72953001  0.73097651  0.73165708
  0.73314542  0.73491105  0.73691397  0.73710304  0.73714872  0.73982009
  0.7409143   0.74092577  0.74146892  0.74304417  0.74350393  0.74576368
  0.74609356  0.75127282  0.75150878  0.75172141  0.75660258  0.7590417
  0.75959632  0.76209537  0.76218595  0.76317892  0.76475674  0.76907034
  0.77198985  0.77383099  0.77457948  0.77538841  0.77716911  0.77878082
  0.77942885  0.77975873  0.78234836  0.78566505  0.78752762  0.78825468
  0.78913934  0.79026775  0.79270688  0.79358673  0.79496662  0.79529651
  0.79946138  0.80306539  0.80559407  0.80565502  0.81860316  0.82548394
  0.82652255  0.82735413  0.8286318   0.83106406  0.83429144  0.83614633
  0.83682012  0.83844738  0.84148471  0.8435871   0.84416957  0.85220739
  0.85907329  0.85983152  0.86235224  0.86276967  0.8694361   0.87450994
  0.88067654  0.88233454  0.89048433  0.89052853  0.89311816  0.89322446
  0.89829742  0.90621681  0.9075952   0.91653112  0.91715188  0.92650873
  0.92678333  0.92937296  0.93163271  0.93206889  0.93245499  0.93714185
  0.93875357  0.93973148  0.93988198  0.94232111  0.94491074  0.94619257
  0.94652245  0.94761954  0.94947443  0.95008999  0.95283012  0.95785888
  0.95863109  0.96026912  0.96044851  0.96577827  0.96821739  0.97252384
  0.97811085  0.97857591  0.9864953   0.98989385  0.99167455  0.99685381
  0.99944344  1.00100975  1.00160796  1.00706182  1.00721233  1.00780073
  1.01655162  1.02016047  1.02533973  1.02731379  1.03439045  1.06508329
  1.07392711]

  UserWarning,

2022-10-31 11:02:15,985:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.77224717 -0.74862747 -0.50257446 -0.39064432 -0.27797477 -0.2575772
 -0.18786206 -0.16517645 -0.14688948 -0.08380591 -0.04005814 -0.00448239
  0.02125225  0.02372309  0.04330348  0.0709786   0.07553126  0.07662885
  0.10338273  0.10356763  0.11604264  0.11999216  0.1241977   0.12719506
  0.14068104  0.14102355  0.14505905  0.1540924   0.15874226  0.15999593
  0.16031228  0.16135147  0.16232583  0.16458831  0.16473908  0.16571599
  0.17340688  0.17952491  0.18359781  0.18503832  0.18623298  0.18672808
  0.18881302  0.19249899  0.19267569  0.1946815   0.19476261  0.19949249
  0.20000918  0.20175381  0.20280571  0.20710966  0.2157884   0.21708049
  0.2191776   0.21973637  0.21982091  0.22119713  0.22243484  0.22443534
  0.22643384  0.22749138  0.22860573  0.2325713   0.23277061  0.23541846
  0.23549136  0.23853315  0.23869463  0.2405921   0.24065688  0.24320639
  0.24322132  0.24342744  0.24459623  0.24774478  0.24779208  0.2481148
  0.24824068  0.25136634  0.25256218  0.25261197  0.25281521  0.25324136
  0.25449153  0.25488607  0.25580673  0.25608386  0.25712793  0.2595164
  0.26112881  0.26286558  0.26325697  0.26353501  0.26436498  0.26608131
  0.26655176  0.26783682  0.2696009   0.27032145  0.27034555  0.27044203
  0.27095939  0.27238098  0.27284223  0.27558105  0.27728976  0.27753638
  0.27869551  0.27955982  0.28018593  0.28060053  0.28119514  0.28268721
  0.282859    0.28332022  0.28402988  0.28480613  0.28546211  0.28549196
  0.28650053  0.28781687  0.28793453  0.28914477  0.29180416  0.29450022
  0.2948403   0.29524978  0.29676067  0.29682725  0.29705566  0.29735555
  0.29755529  0.29974094  0.30143192  0.30245402  0.30247274  0.30313988
  0.30453661  0.30474875  0.30593981  0.30677767  0.30870298  0.30881064
  0.3088274   0.30961326  0.31163279  0.31224999  0.31278855  0.31424541
  0.31443234  0.31530146  0.31543769  0.31546162  0.31661261  0.3171257
  0.31732583  0.3182638   0.32040045  0.32133473  0.32242321  0.32425895
  0.32610929  0.3262021   0.32643825  0.32680861  0.32870924  0.32965889
  0.32976931  0.32987434  0.33088922  0.33332625  0.3333389   0.33416345
  0.33516516  0.33540837  0.33542195  0.33636557  0.33685834  0.33715668
  0.33760065  0.33833001  0.33890728  0.33973435  0.33993323  0.34005312
  0.34047984  0.34125822  0.34225294  0.34321037  0.34445026  0.34460426
  0.34547874  0.34549347  0.3455912   0.34573359  0.3462022   0.34751411
  0.34891674  0.35108395  0.3511297   0.35117584  0.35198412  0.35243644
  0.35295734  0.35307571  0.35350711  0.35372173  0.35376534  0.35400469
  0.35420472  0.35561131  0.35590274  0.35621407  0.35644596  0.35651604
  0.3574543   0.35839882  0.35902469  0.36001987  0.36053098  0.3613415
  0.36164064  0.362688    0.36273803  0.36409052  0.3646359   0.36580315
  0.36728314  0.36804987  0.3685228   0.36947764  0.37009459  0.37127581
  0.37181819  0.37272167  0.37399051  0.37430092  0.37465202  0.37778656
  0.37788004  0.37831055  0.38035442  0.38101848  0.38108453  0.38124549
  0.38182749  0.38284276  0.38337605  0.38377627  0.38513473  0.38687263
  0.38705679  0.38734051  0.38846438  0.39104952  0.39216233  0.39375365
  0.39424167  0.39546889  0.39625894  0.39688631  0.39830052  0.39891115
  0.40168756  0.40179182  0.40217558  0.40264239  0.40268493  0.40331508
  0.4034817   0.40650615  0.40697684  0.40746486  0.40764088  0.40940135
  0.41124649  0.41204357  0.41384848  0.41444119  0.41551791  0.41633993
  0.41865442  0.41998981  0.42188996  0.4250019   0.42876614  0.43069886
  0.43105449  0.43428012  0.43452141  0.43569456  0.43789149  0.43838439
  0.44209567  0.44625054  0.44960676  0.45267189  0.45403136  0.45666809
  0.46285443  0.46388885  0.46411607  0.46528541  0.46562173  0.46750222
  0.46765569  0.46796348  0.46917211  0.4694428   0.47015967  0.47064228
  0.4795525   0.48524537  0.48897399  0.48974921  0.49469714  0.50089745
  0.5035524   0.50769368  0.5085044   0.51006409  0.51524094  0.51803263
  0.52112283  0.52265813  0.52969549  0.53388749  0.53430328  0.53671731
  0.53974391  0.54086825  0.54504344  0.54978995  0.55066786  0.55071968
  0.55073264  0.55322927  0.55323834  0.55796515  0.55990175  0.56202536
  0.56787261  0.56797407  0.56901327  0.56995927  0.57145529  0.57260391
  0.58047903  0.58053782  0.58191991  0.58340747  0.59073691  0.59323709
  0.59376101  0.5951431   0.6003109   0.60043237  0.6069842   0.60962883
  0.61211366  0.61227347  0.61457461  0.61756275  0.61894593  0.62285202
  0.62549666  0.62978476  0.63145962  0.63160972  0.63524872  0.63592739
  0.63607521  0.63947061  0.64121667  0.64137873  0.64665376  0.64744449
  0.6488991   0.6492984   0.64931265  0.65145502  0.65194303  0.65282362
  0.65376118  0.65458767  0.65645231  0.65987695  0.66252159  0.66516622
  0.66948388  0.6703667   0.6713107   0.67207012  0.67310014  0.67350865
  0.67432126  0.67448332  0.67491828  0.6749666   0.67574477  0.67838941
  0.67884767  0.68103405  0.68607711  0.68896796  0.68898221  0.69125273
  0.69470873  0.69486018  0.69502225  0.69533584  0.69651058  0.69675406
  0.69690187  0.69778246  0.69801218  0.69956076  0.70000477  0.7022054
  0.70468797  0.70484562  0.70485004  0.70748042  0.70749467  0.70750285
  0.70918061  0.70997725  0.71213592  0.71278395  0.71541434  0.71612865
  0.71629492  0.71708853  0.71807322  0.7205313   0.7219786   0.72287448
  0.7229712   0.72320043  0.7233625   0.72402194  0.72645115  0.72687348
  0.72865177  0.7299465   0.73054213  0.73129641  0.73174042  0.73207289
  0.73239604  0.73377898  0.73473178  0.73658569  0.73745203  0.73870451
  0.74009666  0.74534532  0.74538594  0.74692941  0.74700217  0.7475569
  0.74882301  0.75040819  0.75311826  0.75331985  0.75860913  0.76098674
  0.76125376  0.76238198  0.76380854  0.76518868  0.76551464  0.76704728
  0.76765691  0.76815927  0.76918768  0.77183231  0.77447695  0.77513639
  0.77518346  0.77787235  0.78770014  0.79034478  0.79298942  0.7978496
  0.80092333  0.80356797  0.8062126   0.81152099  0.8120135   0.81414652
  0.81479113  0.81482592  0.81943579  0.81989291  0.82311941  0.82423185
  0.82457725  0.82760599  0.82782796  0.83530362  0.83794826  0.84063267
  0.84803879  0.85068343  0.85332806  0.86068852  0.86417283  0.86424681
  0.86831799  0.87938482  0.89379274  0.89515866  0.90005364  0.90269828
  0.91062777  0.91230521  0.91264099  0.91592146  0.92106292  0.92370756
  0.92650001  0.92695141  0.92836647  0.93164147  0.93178929  0.93254005
  0.93472036  0.93707856  0.93957539  0.94486466  0.94501248  0.94934305
  0.95030175  0.95279857  0.95808785  0.9608803   0.96352494  0.9686664
  0.9714731   0.97230538  0.97552719  0.97660031  0.97842115  0.98188959
  0.98371042  0.98420827  0.98453423  0.98644905  0.98717886  0.98778494
  0.98792962  0.9898235   0.99230304  0.99321527  0.99462476  0.99511278
  0.99672567  1.00416056  1.00569133  1.00784795  1.01362524  1.0179429
  1.02288928  1.02420379  1.0393418   1.03995274  1.04967855  1.3159016
  1.44084446]

  UserWarning,

2022-10-31 11:02:16,416:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:16,461:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:16,769:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.94547024 -0.7769212  -0.475268   -0.39286633 -0.3342177  -0.31828377
 -0.31582689 -0.30841293 -0.2684237  -0.26011261 -0.25918253 -0.255448
 -0.12385182 -0.11839672 -0.10368724 -0.09856177 -0.05762328 -0.04689406
 -0.02913981 -0.02853047 -0.00235619  0.00617414  0.0230898   0.02551127
  0.0327106   0.03591053  0.05462452  0.06077473  0.06645141  0.066876
  0.08110093  0.08489302  0.08581859  0.09230081  0.09677853  0.1026558
  0.1111432   0.12642578  0.126697    0.12749689  0.13560586  0.14893953
  0.15158257  0.1586888   0.17082699  0.17288944  0.17438129  0.17638239
  0.1770858   0.17894021  0.18470933  0.19003952  0.19523459  0.2023678
  0.20553985  0.20557671  0.20615     0.20907266  0.2092559   0.21011132
  0.21157819  0.21158038  0.21170576  0.21228063  0.21544919  0.2160408
  0.21659591  0.21695481  0.21735582  0.21854989  0.22021886  0.22436046
  0.22471874  0.2335439   0.23539492  0.23585576  0.23690381  0.23883767
  0.2393378   0.24141753  0.24151799  0.24162932  0.24448089  0.24656035
  0.24764854  0.24807353  0.24853669  0.24948812  0.24999802  0.25010911
  0.25142998  0.25452029  0.25720127  0.25745161  0.25764395  0.26281678
  0.26283238  0.26291113  0.26425001  0.26439542  0.26501076  0.26550845
  0.26629931  0.26841303  0.26895505  0.27008826  0.27043693  0.2705421
  0.27167103  0.27207276  0.27239086  0.27315774  0.27346394  0.27384822
  0.27441927  0.27587736  0.27656424  0.27657219  0.27673234  0.27849738
  0.2787583   0.2812762   0.28195562  0.28268919  0.28348847  0.28350872
  0.28397096  0.2840047   0.28447047  0.28608999  0.28614168  0.28721282
  0.2877382   0.29007026  0.29010392  0.29021936  0.2910541   0.291314
  0.291468    0.29192382  0.29487624  0.2969387   0.29724117  0.29749065
  0.29749077  0.29787786  0.29799934  0.30171722  0.30328991  0.30389433
  0.3044576   0.30630384  0.30755444  0.31122846  0.31198961  0.31234061
  0.31247107  0.31295219  0.31307725  0.31343401  0.3139147   0.31453246
  0.31753968  0.31798459  0.31805659  0.31811404  0.31914702  0.31992056
  0.3211042   0.32153237  0.32643793  0.32736012  0.32806381  0.3283722
  0.32868585  0.3312465   0.33325703  0.33335466  0.33400093  0.33525222
  0.33567382  0.33668267  0.33761054  0.33946344  0.34021865  0.3404125
  0.34171032  0.34232112  0.3424479   0.34524721  0.34615581  0.34624389
  0.34706415  0.34707618  0.34777239  0.34831392  0.34888026  0.34894465
  0.35084192  0.35127049  0.35252923  0.35296862  0.35480602  0.35947403
  0.35978647  0.36358045  0.36434788  0.36655751  0.36743964  0.37213312
  0.37255604  0.37257191  0.37446411  0.3761311   0.37662853  0.37698291
  0.37725052  0.37791427  0.37832209  0.37895605  0.37973217  0.3798117
  0.37984411  0.38028643  0.38056454  0.38132042  0.3815238   0.38225617
  0.38303407  0.38342287  0.38458412  0.38465436  0.38510293  0.38555306
  0.38646736  0.38716559  0.38728324  0.38752793  0.3882337   0.38891002
  0.39014733  0.3905364   0.39075877  0.39272507  0.39335251  0.39343333
  0.39409365  0.39424075  0.39472553  0.39530645  0.39612324  0.39754191
  0.40189592  0.40314096  0.4038326   0.40464543  0.40485059  0.40638542
  0.40643241  0.40708489  0.40821777  0.40899031  0.40903223  0.40923198
  0.41084814  0.4112559   0.41158041  0.41240366  0.41246552  0.41354546
  0.41408385  0.41549468  0.41644835  0.41702303  0.41709515  0.41913417
  0.42123369  0.42229338  0.42242483  0.42517515  0.42533601  0.42555114
  0.42894292  0.43126092  0.43381942  0.43537043  0.43631764  0.43891745
  0.44265308  0.44411709  0.44590245  0.44593936  0.44618902  0.4492295
  0.44954016  0.45346589  0.45456103  0.45523738  0.45559959  0.45566281
  0.45687339  0.45703535  0.45711617  0.45806308  0.45830616  0.46195324
  0.46421528  0.46721648  0.48241399  0.48325441  0.48661998  0.48830335
  0.48907486  0.49500913  0.4957891   0.49802634  0.49852835  0.5006127
  0.50101285  0.50301471  0.50303699  0.50451729  0.50581233  0.50919945
  0.50998364  0.5124732   0.51345032  0.51968394  0.52140135  0.5317766
  0.53255673  0.53334799  0.53701013  0.54121616  0.54942401  0.55077029
  0.55232651  0.55515458  0.55520884  0.55533708  0.55533766  0.5625152
  0.56845103  0.56921866  0.57515943  0.57519063  0.57537321  0.57676845
  0.58196808  0.5845679   0.59104095  0.59144481  0.59194911  0.59289351
  0.59424395  0.59496717  0.59533187  0.60699338  0.60722231  0.60796625
  0.61219301  0.61836551  0.62876477  0.63206841  0.63299154  0.63396441
  0.63656422  0.63916404  0.63976269  0.64792872  0.65100065  0.65152654
  0.65216312  0.65307165  0.65345882  0.65468211  0.65476294  0.65510297
  0.65736275  0.65754316  0.65996257  0.66124982  0.66360844  0.66384945
  0.66384963  0.6648008   0.6651622   0.66620825  0.66762873  0.66776202
  0.66880807  0.67036183  0.67043018  0.67274122  0.67296165  0.67380863
  0.67556146  0.67660752  0.67720617  0.6788006   0.67920733  0.67944853
  0.6807611   0.68440696  0.68464816  0.68524681  0.6877106   0.6896066
  0.69339878  0.69480623  0.69504743  0.69746916  0.69947454  0.70000586
  0.70024706  0.70066737  0.70284687  0.70291918  0.70415944  0.70432419
  0.70495086  0.70520549  0.70667844  0.70675926  0.70780531  0.71040513
  0.71447788  0.71455871  0.71541855  0.71560476  0.71584596  0.72077949
  0.7224045   0.72445291  0.73380347  0.73724121  0.73854396  0.74114378
  0.74277462  0.74412191  0.74634341  0.74646651  0.74894323  0.75040446
  0.75154304  0.75200218  0.75406204  0.75407008  0.75996201  0.76004283
  0.76454212  0.76471611  0.7653891   0.76714194  0.76999141  0.77073255
  0.77234157  0.77384942  0.77461026  0.77921848  0.77973913  0.78034553
  0.780988    0.78315373  0.78562563  0.78605641  0.78794047  0.79054028
  0.79103937  0.79644373  0.79833973  0.80113483  0.80353936  0.80613918
  0.808739    0.81133881  0.81233972  0.81314583  0.81515867  0.81653845
  0.82433789  0.82663825  0.82866353  0.83452538  0.83943682  0.83972501
  0.83993679  0.84023007  0.84203154  0.85012427  0.85347686  0.85868322
  0.86354759  0.86520024  0.86572317  0.8719136   0.88157172  0.89531194
  0.90131309  0.90361963  0.90463959  0.90472041  0.90600766  0.90732023
  0.91511968  0.91531943  0.91616573  0.92023849  0.92031931  0.92129199
  0.92291912  0.92512601  0.93323757  0.93460564  0.93851246  0.94111784
  0.94363683  0.9450049   0.94589923  0.94631747  0.94651722  0.9475239
  0.94760472  0.94891728  0.95020453  0.952278    0.95280435  0.95436657
  0.95800398  0.96007745  0.9606038   0.96191637  0.96320361  0.96572261
  0.96971581  0.97231563  0.97293863  0.97307653  0.97491545  0.97690651
  0.97880251  0.98140233  0.98400214  0.98660196  0.98860325  0.98920178
  0.99180159  0.99700122  0.99907469  1.0034326   1.00480067  1.00597288
  1.00740049  1.01217185  1.01609426  1.01677283  1.0263601   1.03027248
  1.03631909  1.07315679  1.07389955  1.08187185  1.2273629   1.25953641
  1.27264062  1.33467606  1.39584365]

  UserWarning,

2022-10-31 11:02:16,801:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4051563  -0.40513932 -0.35103044 -0.2330514  -0.21247933 -0.18264918
 -0.13817349 -0.12681862 -0.09967875 -0.03978759 -0.02461572 -0.02023458
 -0.01592225 -0.00524282  0.0222814   0.03979957  0.04002484  0.04368484
  0.05127418  0.05165241  0.07441814  0.08253408  0.0862313   0.089347
  0.10093851  0.10241407  0.10688991  0.11597369  0.12625719  0.12801135
  0.13247995  0.13357617  0.13383359  0.13594837  0.14739853  0.15127366
  0.15286655  0.15316628  0.15611028  0.16050176  0.16194962  0.16348284
  0.16542811  0.18669191  0.19146363  0.19262117  0.19475407  0.1948737
  0.19910781  0.20209062  0.20292288  0.20647704  0.20704483  0.20799744
  0.20845553  0.21020249  0.21329357  0.21357014  0.21434883  0.21581551
  0.21586655  0.21914479  0.21979914  0.22149076  0.22152114  0.22227012
  0.22232146  0.22291616  0.22428778  0.22864595  0.22884517  0.22894572
  0.23085042  0.23382831  0.23397248  0.23468876  0.23572877  0.23590029
  0.23632385  0.23652307  0.24145602  0.24183377  0.24215233  0.24332384
  0.24568205  0.24617115  0.24823232  0.24964627  0.25136275  0.251919
  0.25493376  0.25713651  0.25828862  0.25918749  0.2635571   0.26665266
  0.26726465  0.26802519  0.26902843  0.26923876  0.26929472  0.27100927
  0.27122636  0.27210955  0.27326415  0.2734712   0.2736799   0.2749884
  0.27675862  0.27788084  0.27969626  0.2819594   0.28215278  0.28233622
  0.28297377  0.28463041  0.28623232  0.28654467  0.28694381  0.28717355
  0.28759229  0.28791311  0.28961843  0.28964992  0.2898756   0.29030085
  0.29059693  0.29082643  0.29163415  0.29163953  0.29202848  0.29338239
  0.2939855   0.29655731  0.29656512  0.29658344  0.29716614  0.29737117
  0.29761043  0.29816707  0.29827158  0.29904856  0.29907447  0.29995422
  0.30065878  0.30130793  0.30192443  0.30215546  0.3022679   0.30314669
  0.30335369  0.30368624  0.30397468  0.30411493  0.30469389  0.30476432
  0.30530817  0.30626912  0.30705926  0.3083699   0.30951013  0.3100403
  0.31081157  0.31245291  0.31320944  0.31370859  0.31395819  0.31407625
  0.31654586  0.31742393  0.31751827  0.32014082  0.32077153  0.32269314
  0.32425021  0.32469806  0.32490107  0.32511623  0.32586379  0.32623024
  0.32658796  0.32692621  0.32845691  0.32877224  0.32987487  0.33019465
  0.33151717  0.33377394  0.33414388  0.33498874  0.33629318  0.3363925
  0.33691417  0.33753388  0.33757744  0.33787672  0.33935184  0.34125655
  0.34192837  0.34227128  0.34283387  0.34547919  0.34717693  0.34779053
  0.34782649  0.34807262  0.34879075  0.3489736   0.3497017   0.34976122
  0.34990619  0.34993565  0.35243988  0.35391699  0.35410907  0.35435556
  0.35485313  0.35512392  0.3556231   0.35615433  0.35694377  0.35727452
  0.35970756  0.36125935  0.36201449  0.36285703  0.36318316  0.36340267
  0.36416031  0.36447428  0.36489656  0.36510507  0.36525511  0.36720818
  0.36732218  0.36749937  0.37103736  0.3711649   0.37198539  0.37308854
  0.37389575  0.37412984  0.374578    0.37611352  0.37613771  0.37806883
  0.37985751  0.38017507  0.38086249  0.38097032  0.38309973  0.3835084
  0.38482274  0.38604648  0.38667511  0.38743639  0.38934425  0.39004224
  0.39026359  0.39106707  0.39112265  0.39121828  0.39140443  0.3922089
  0.39410376  0.39413604  0.3952324   0.39619881  0.39732136  0.39740915
  0.39743531  0.39873689  0.39893662  0.40127497  0.40280145  0.40331907
  0.40375748  0.40381306  0.40420302  0.40434282  0.40586565  0.40635114
  0.40705802  0.40888922  0.40975244  0.41026948  0.41199583  0.41578098
  0.41874656  0.41940299  0.42168507  0.42221054  0.42237937  0.42385883
  0.42501109  0.42658712  0.43001543  0.43168059  0.43402432  0.43425835
  0.43662006  0.44101641  0.44432751  0.44641932  0.4471961   0.44973419
  0.4501304   0.45194175  0.45227227  0.45275831  0.45481035  0.45734843
  0.4596998   0.46202019  0.46209408  0.46435636  0.46482729  0.46717024
  0.47115615  0.47393034  0.47405777  0.47474419  0.48148677  0.48350206
  0.48442394  0.49077086  0.49198973  0.49289988  0.4983851   0.51046369
  0.51099674  0.51366113  0.513837    0.51769008  0.51816754  0.52127698
  0.5215249   0.52258466  0.53138017  0.53391825  0.544015    0.54407058
  0.54733618  0.55003868  0.5551812   0.56030459  0.5623855   0.5704556
  0.57464328  0.57724513  0.58201411  0.58491554  0.5865934   0.58703272
  0.58724828  0.59247363  0.59447058  0.59729193  0.60056809  0.60164453
  0.60593644  0.60607118  0.60661231  0.607133    0.60943807  0.61168848
  0.61218967  0.61531636  0.61785444  0.61868269  0.62039253  0.62293061
  0.62565036  0.62883502  0.63054485  0.63456417  0.63562102  0.64069718
  0.64323526  0.64454142  0.64577334  0.64829514  0.64831143  0.65084951
  0.65284646  0.65338759  0.65794527  0.66096883  0.66161449  0.666078
  0.66694823  0.66852122  0.66861608  0.67047418  0.67115416  0.67138982
  0.67181718  0.67369224  0.67623033  0.67822728  0.67876841  0.67973995
  0.68003028  0.68107264  0.68133683  0.68467282  0.68487742  0.68584152
  0.68892074  0.69228707  0.69345577  0.69549443  0.69575048  0.69647941
  0.69653498  0.69663329  0.697198    0.69736323  0.69853193  0.69907306
  0.70107001  0.70161115  0.7036081   0.70414923  0.71176347  0.71305562
  0.71555834  0.7162985   0.71773831  0.71961337  0.71999326  0.72765498
  0.72886885  0.72892349  0.73220893  0.73266384  0.73267557  0.7353396
  0.73887939  0.73906866  0.73964937  0.74205146  0.7421256   0.74288348
  0.74720176  0.74732337  0.74795964  0.75008837  0.75475001  0.75557388
  0.7576846   0.75811197  0.76045652  0.76445     0.76750642  0.76826429
  0.76952564  0.77080238  0.7712826   0.77206372  0.7732374   0.77334046
  0.77587854  0.77841662  0.7809547   0.78335425  0.78397301  0.78603087
  0.78626652  0.78887498  0.78904917  0.79364511  0.79612762  0.79872128
  0.80040521  0.80125936  0.80149501  0.8040331   0.80627995  0.80736741
  0.80777695  0.81137868  0.81394977  0.81418542  0.81517863  0.81565678
  0.81679873  0.81897036  0.82031808  0.82052293  0.82179967  0.82539424
  0.83052782  0.831952    0.83225061  0.83422142  0.83673693  0.83775777
  0.83933059  0.83956624  0.84718049  0.84795257  0.85179688  0.85302874
  0.85325592  0.85733281  0.86212457  0.86318107  0.86566357  0.86696508
  0.86930398  0.87229265  0.87436624  0.88327522  0.88348572  0.89461532
  0.89612865  0.89871421  0.90379037  0.90594278  0.90632846  0.90656411
  0.91140462  0.91164028  0.9139427   0.91648078  0.91901887  0.92155695
  0.9217926   0.92433069  0.92657754  0.92663311  0.93170927  0.93194493
  0.93424736  0.93663939  0.93932352  0.93955918  0.9400394   0.94439968
  0.94693777  0.94717342  0.94822018  0.94947585  0.95201393  0.95831111
  0.96216626  0.96240191  0.96470434  0.96494     0.96709637  0.9697805
  0.97020935  0.97509232  0.97736174  0.9776304   0.98016849  0.98270657
  0.98318679  0.98524465  0.98778273  0.99252838  0.9928589   0.99539698
  0.99841528  1.00047314  1.0055493   1.01167222  1.01560678  1.02098168
  1.02324721  1.02829718  1.28353226]

  UserWarning,

2022-10-31 11:02:16,801:INFO:Calculating mean and std
2022-10-31 11:02:16,801:INFO:Creating metrics dataframe
2022-10-31 11:02:16,801:INFO:Uploading results into container
2022-10-31 11:02:16,801:INFO:Uploading model into container now
2022-10-31 11:02:16,801:INFO:master_model_container: 10
2022-10-31 11:02:16,801:INFO:display_container: 2
2022-10-31 11:02:16,801:INFO:Lars(random_state=3360)
2022-10-31 11:02:16,801:INFO:create_model() successfully completed......................................
2022-10-31 11:02:16,922:ERROR:create_model() for Lars(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:02:16,922:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:16,922:INFO:Initializing Lasso Least Angle Regression
2022-10-31 11:02:16,922:INFO:Total runtime is 0.7322049419085185 minutes
2022-10-31 11:02:16,922:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:16,922:INFO:Initializing create_model()
2022-10-31 11:02:16,922:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:16,922:INFO:Checking exceptions
2022-10-31 11:02:16,922:INFO:Importing libraries
2022-10-31 11:02:16,922:INFO:Copying training dataset
2022-10-31 11:02:16,922:INFO:Defining folds
2022-10-31 11:02:16,922:INFO:Declaring metric variables
2022-10-31 11:02:16,922:INFO:Importing untrained model
2022-10-31 11:02:16,922:INFO:Lasso Least Angle Regression Imported successfully
2022-10-31 11:02:16,922:INFO:Starting cross validation
2022-10-31 11:02:16,922:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:18,199:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,214:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,230:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,290:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,368:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,415:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:18,431:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:19,104:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50328707]

  UserWarning,

2022-10-31 11:02:19,151:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50401753]

  UserWarning,

2022-10-31 11:02:19,182:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50292184]

  UserWarning,

2022-10-31 11:02:19,198:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.49579985]

  UserWarning,

2022-10-31 11:02:19,261:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50356034]

  UserWarning,

2022-10-31 11:02:19,267:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50584368]

  UserWarning,

2022-10-31 11:02:19,290:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50465583]

  UserWarning,

2022-10-31 11:02:19,305:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50264743]

  UserWarning,

2022-10-31 11:02:19,782:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:19,782:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:20,120:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50045645]

  UserWarning,

2022-10-31 11:02:20,120:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50063904]

  UserWarning,

2022-10-31 11:02:20,135:INFO:Calculating mean and std
2022-10-31 11:02:20,135:INFO:Creating metrics dataframe
2022-10-31 11:02:20,135:INFO:Uploading results into container
2022-10-31 11:02:20,135:INFO:Uploading model into container now
2022-10-31 11:02:20,135:INFO:master_model_container: 11
2022-10-31 11:02:20,135:INFO:display_container: 2
2022-10-31 11:02:20,135:INFO:LassoLars(random_state=3360)
2022-10-31 11:02:20,135:INFO:create_model() successfully completed......................................
2022-10-31 11:02:20,256:WARNING:create_model() for LassoLars(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:20,257:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:20,258:INFO:Initializing create_model()
2022-10-31 11:02:20,258:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:20,258:INFO:Checking exceptions
2022-10-31 11:02:20,261:INFO:Importing libraries
2022-10-31 11:02:20,261:INFO:Copying training dataset
2022-10-31 11:02:20,266:INFO:Defining folds
2022-10-31 11:02:20,266:INFO:Declaring metric variables
2022-10-31 11:02:20,267:INFO:Importing untrained model
2022-10-31 11:02:20,267:INFO:Lasso Least Angle Regression Imported successfully
2022-10-31 11:02:20,268:INFO:Starting cross validation
2022-10-31 11:02:20,270:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:21,520:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,567:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,567:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,719:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,734:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,750:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,750:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:21,765:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:22,382:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50401753]

  UserWarning,

2022-10-31 11:02:22,429:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.49579985]

  UserWarning,

2022-10-31 11:02:22,429:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50328707]

  UserWarning,

2022-10-31 11:02:22,596:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50292184]

  UserWarning,

2022-10-31 11:02:22,612:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50584368]

  UserWarning,

2022-10-31 11:02:22,644:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50356034]

  UserWarning,

2022-10-31 11:02:22,644:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50264743]

  UserWarning,

2022-10-31 11:02:22,663:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50465583]

  UserWarning,

2022-10-31 11:02:23,103:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:23,103:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:02:23,427:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50045645]

  UserWarning,

2022-10-31 11:02:23,444:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50063904]

  UserWarning,

2022-10-31 11:02:23,444:INFO:Calculating mean and std
2022-10-31 11:02:23,444:INFO:Creating metrics dataframe
2022-10-31 11:02:23,455:INFO:Uploading results into container
2022-10-31 11:02:23,455:INFO:Uploading model into container now
2022-10-31 11:02:23,456:INFO:master_model_container: 12
2022-10-31 11:02:23,456:INFO:display_container: 2
2022-10-31 11:02:23,456:INFO:LassoLars(random_state=3360)
2022-10-31 11:02:23,456:INFO:create_model() successfully completed......................................
2022-10-31 11:02:23,565:ERROR:create_model() for LassoLars(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:02:23,565:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:23,565:INFO:Initializing Orthogonal Matching Pursuit
2022-10-31 11:02:23,565:INFO:Total runtime is 0.842910357316335 minutes
2022-10-31 11:02:23,565:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:23,565:INFO:Initializing create_model()
2022-10-31 11:02:23,565:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:23,565:INFO:Checking exceptions
2022-10-31 11:02:23,565:INFO:Importing libraries
2022-10-31 11:02:23,565:INFO:Copying training dataset
2022-10-31 11:02:23,565:INFO:Defining folds
2022-10-31 11:02:23,565:INFO:Declaring metric variables
2022-10-31 11:02:23,565:INFO:Importing untrained model
2022-10-31 11:02:23,565:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-31 11:02:23,565:INFO:Starting cross validation
2022-10-31 11:02:23,580:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:24,797:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:24,887:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:24,918:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:24,965:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:24,965:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:25,043:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:25,073:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:25,077:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:25,657:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03373907  0.37193828  0.77761564]

  UserWarning,

2022-10-31 11:02:25,779:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04426677  0.36404537  0.77235751]

  UserWarning,

2022-10-31 11:02:25,826:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03542007  0.37091491  0.7772499 ]

  UserWarning,

2022-10-31 11:02:25,866:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.02375642  0.37609056  0.77593754]

  UserWarning,

2022-10-31 11:02:25,873:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04827419  0.3669873   0.7822488 ]

  UserWarning,

2022-10-31 11:02:25,901:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03533586  0.37316338  0.78166262]

  UserWarning,

2022-10-31 11:02:25,916:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03493225  0.37147865  0.77788956]

  UserWarning,

2022-10-31 11:02:25,932:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04186383  0.36841106  0.77868595]

  UserWarning,

2022-10-31 11:02:26,363:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:26,410:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:26,700:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03265353  0.37013905  0.77293162]

  UserWarning,

2022-10-31 11:02:26,747:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04168747  0.36846473  0.77861694]

  UserWarning,

2022-10-31 11:02:26,747:INFO:Calculating mean and std
2022-10-31 11:02:26,747:INFO:Creating metrics dataframe
2022-10-31 11:02:26,747:INFO:Uploading results into container
2022-10-31 11:02:26,747:INFO:Uploading model into container now
2022-10-31 11:02:26,747:INFO:master_model_container: 13
2022-10-31 11:02:26,747:INFO:display_container: 2
2022-10-31 11:02:26,747:INFO:OrthogonalMatchingPursuit()
2022-10-31 11:02:26,747:INFO:create_model() successfully completed......................................
2022-10-31 11:02:26,872:WARNING:create_model() for OrthogonalMatchingPursuit() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:26,872:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:26,872:INFO:Initializing create_model()
2022-10-31 11:02:26,872:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:26,872:INFO:Checking exceptions
2022-10-31 11:02:26,876:INFO:Importing libraries
2022-10-31 11:02:26,876:INFO:Copying training dataset
2022-10-31 11:02:26,881:INFO:Defining folds
2022-10-31 11:02:26,881:INFO:Declaring metric variables
2022-10-31 11:02:26,881:INFO:Importing untrained model
2022-10-31 11:02:26,882:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-31 11:02:26,882:INFO:Starting cross validation
2022-10-31 11:02:26,885:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:28,132:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,132:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,163:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,276:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,285:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,332:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,347:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,347:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:28,993:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03373907  0.37193828  0.77761564]

  UserWarning,

2022-10-31 11:02:29,055:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04426677  0.36404537  0.77235751]

  UserWarning,

2022-10-31 11:02:29,099:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04827419  0.3669873   0.7822488 ]

  UserWarning,

2022-10-31 11:02:29,162:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03542007  0.37091491  0.7772499 ]

  UserWarning,

2022-10-31 11:02:29,177:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.02375642  0.37609056  0.77593754]

  UserWarning,

2022-10-31 11:02:29,193:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03533586  0.37316338  0.78166262]

  UserWarning,

2022-10-31 11:02:29,209:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04186383  0.36841106  0.77868595]

  UserWarning,

2022-10-31 11:02:29,240:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03493225  0.37147865  0.77788956]

  UserWarning,

2022-10-31 11:02:29,714:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:29,714:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:02:30,072:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.03265353  0.37013905  0.77293162]

  UserWarning,

2022-10-31 11:02:30,072:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.04168747  0.36846473  0.77861694]

  UserWarning,

2022-10-31 11:02:30,074:INFO:Calculating mean and std
2022-10-31 11:02:30,075:INFO:Creating metrics dataframe
2022-10-31 11:02:30,079:INFO:Uploading results into container
2022-10-31 11:02:30,079:INFO:Uploading model into container now
2022-10-31 11:02:30,079:INFO:master_model_container: 14
2022-10-31 11:02:30,080:INFO:display_container: 2
2022-10-31 11:02:30,080:INFO:OrthogonalMatchingPursuit()
2022-10-31 11:02:30,080:INFO:create_model() successfully completed......................................
2022-10-31 11:02:30,178:ERROR:create_model() for OrthogonalMatchingPursuit() raised an exception or returned all 0.0:
2022-10-31 11:02:30,178:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:30,178:INFO:Initializing Bayesian Ridge
2022-10-31 11:02:30,178:INFO:Total runtime is 0.9531274954477946 minutes
2022-10-31 11:02:30,178:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:30,178:INFO:Initializing create_model()
2022-10-31 11:02:30,178:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:30,178:INFO:Checking exceptions
2022-10-31 11:02:30,193:INFO:Importing libraries
2022-10-31 11:02:30,193:INFO:Copying training dataset
2022-10-31 11:02:30,193:INFO:Defining folds
2022-10-31 11:02:30,193:INFO:Declaring metric variables
2022-10-31 11:02:30,193:INFO:Importing untrained model
2022-10-31 11:02:30,193:INFO:Bayesian Ridge Imported successfully
2022-10-31 11:02:30,193:INFO:Starting cross validation
2022-10-31 11:02:30,193:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:32,344:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95422963 -0.94503222 -0.59332873 -0.5484015  -0.28746071 -0.20505621
 -0.18770158 -0.13383685 -0.12300769 -0.11891206 -0.11411188 -0.08360044
 -0.07321535 -0.05695978 -0.0295048  -0.01722776 -0.01049762 -0.0046217
 -0.00349913 -0.00221674  0.01733191  0.02205641  0.02742504  0.03356734
  0.03453241  0.03956776  0.04862843  0.05106527  0.05156649  0.05340592
  0.05476553  0.07395147  0.08023631  0.08116403  0.0812563   0.08265375
  0.08322122  0.08440536  0.0909668   0.09235316  0.09802814  0.10075575
  0.10122052  0.10131905  0.1110236   0.1173328   0.14784543  0.1589923
  0.16033235  0.1610757   0.16292677  0.16317024  0.16888569  0.17688061
  0.18511487  0.1864731   0.18835931  0.19204877  0.19278377  0.19360456
  0.19376523  0.19796415  0.20224326  0.2066736   0.20932832  0.20937602
  0.21879352  0.21957918  0.22377567  0.22642514  0.22738376  0.22861582
  0.22957222  0.23152126  0.23472499  0.23525404  0.23759038  0.23813706
  0.23819379  0.24082385  0.24127569  0.24376785  0.24555853  0.24918676
  0.25133677  0.25137445  0.25154077  0.25296254  0.2535039   0.25367058
  0.25370687  0.25571514  0.25632976  0.25646776  0.25845071  0.25943957
  0.25973399  0.2603881   0.26131083  0.26309011  0.26364942  0.26424576
  0.26524733  0.26733846  0.2681878   0.26880977  0.26971213  0.26990365
  0.27217542  0.27656103  0.27669452  0.27721579  0.27732946  0.27826587
  0.27921126  0.27950207  0.27982361  0.28040501  0.28063599  0.28071162
  0.28121711  0.28171602  0.28200177  0.2828876   0.28288871  0.28312463
  0.28394047  0.28426115  0.28506822  0.28516307  0.28559481  0.28661763
  0.28690423  0.28763487  0.29034448  0.29042581  0.29072816  0.29094529
  0.29113691  0.29117279  0.2922958   0.29265832  0.29280769  0.29304371
  0.29371049  0.29687648  0.29793983  0.29839465  0.29857785  0.29860623
  0.29930527  0.30007626  0.30018839  0.30259454  0.30298453  0.30326398
  0.30456973  0.30826801  0.30954176  0.30979103  0.31207736  0.31252971
  0.3142319   0.31499161  0.31650095  0.31658737  0.31672926  0.3175331
  0.31779999  0.31870814  0.31919463  0.31992272  0.32122653  0.32185856
  0.32254623  0.3234556   0.3251176   0.32513859  0.32608543  0.32724358
  0.32767188  0.33093626  0.3311579   0.331204    0.33131073  0.3336363
  0.33597143  0.33991813  0.34120356  0.34267729  0.34399366  0.34420008
  0.34498619  0.34534196  0.34653805  0.3493094   0.35004807  0.35020587
  0.35206469  0.35215027  0.35220586  0.35246093  0.35551059  0.35574595
  0.35798107  0.3594825   0.36172382  0.3621763   0.36227843  0.36256966
  0.36270827  0.36621185  0.36760017  0.36918694  0.37171282  0.37198799
  0.37249076  0.37398553  0.37444595  0.3745442   0.37471084  0.37491716
  0.3749338   0.37500491  0.37627185  0.37696956  0.37772486  0.37795001
  0.37967057  0.37989406  0.38348033  0.38488268  0.38686425  0.38759861
  0.38832892  0.38875874  0.38890388  0.39071291  0.39093657  0.39160509
  0.39183976  0.39256592  0.39316926  0.39473677  0.39474716  0.39486021
  0.3952272   0.39542813  0.39641055  0.39679888  0.39744203  0.3976201
  0.40049633  0.40257832  0.40346234  0.40466951  0.40498421  0.40584168
  0.40609645  0.40810257  0.40994376  0.4103676   0.41058578  0.4117843
  0.41213277  0.41243519  0.41264144  0.41319202  0.41452505  0.41571099
  0.41656648  0.41785545  0.42033216  0.42106776  0.42139529  0.42220779
  0.42369196  0.42398118  0.42527884  0.42529103  0.42545183  0.4300422
  0.43146685  0.43211336  0.43459881  0.43515719  0.43541825  0.43905635
  0.44380006  0.44438752  0.44523272  0.44527116  0.44754801  0.44940623
  0.44961399  0.45097718  0.45120016  0.45423812  0.45428166  0.45627564
  0.45813443  0.4590033   0.45928947  0.46183835  0.4620071   0.46357658
  0.4636062   0.46507762  0.4666603   0.46842328  0.47009617  0.47049966
  0.47165291  0.4735087   0.4762175   0.47763253  0.47941677  0.48508775
  0.48760311  0.48845481  0.48912421  0.48986614  0.4912592   0.49435714
  0.4945601   0.49669207  0.49948576  0.50038381  0.50086787  0.50654804
  0.50725175  0.51474023  0.52458292  0.53208833  0.53457199  0.5371246
  0.53752495  0.53948761  0.54130709  0.54947394  0.5497331   0.55617503
  0.55662249  0.55692491  0.55800098  0.56128337  0.56610966  0.56907625
  0.57052451  0.57195638  0.57386153  0.57980659  0.58069191  0.58349527
  0.58597893  0.59384456  0.5943479   0.59591356  0.59839722  0.59947625
  0.60266546  0.60306211  0.60336453  0.60584819  0.60632762  0.60763277
  0.61081551  0.61451311  0.61533828  0.61687081  0.61826648  0.61982664
  0.62075014  0.62109481  0.62440858  0.62525785  0.62868054  0.63068477
  0.63074626  0.63639408  0.63743667  0.63813575  0.63902117  0.63992033
  0.64061941  0.64297867  0.64310306  0.64807038  0.65055404  0.65060392
  0.65309031  0.65352062  0.65530171  0.65552135  0.65800501  0.65889043
  0.66015345  0.66048867  0.66137409  0.662022    0.66297233  0.66345176
  0.66345525  0.66475691  0.66545599  0.66641663  0.66793964  0.67227102
  0.67290696  0.67292678  0.67506292  0.67787428  0.67965886  0.68057542
  0.68079549  0.68101611  0.68448753  0.68780891  0.68816395  0.69029257
  0.69207715  0.69467126  0.69469948  0.69577718  0.69704446  0.69952812
  0.7002272   0.70166026  0.70237564  0.70314841  0.70449544  0.70519451
  0.70536306  0.70563207  0.7069791   0.70767817  0.70782701  0.70946275
  0.70982661  0.70983413  0.7108787   0.71727759  0.72011202  0.72338906
  0.7265842   0.72798499  0.7290296   0.72935188  0.73271966  0.73467071
  0.73963051  0.74040328  0.74161166  0.74223323  0.74254605  0.74331758
  0.74459783  0.7453706   0.74785426  0.74926272  0.75337741  0.75370699
  0.75453246  0.75768353  0.75778889  0.75817402  0.75941878  0.76185853
  0.76275621  0.76375171  0.76523986  0.76689959  0.76772352  0.76871903
  0.76876077  0.7689319   0.77020718  0.77108034  0.77178694  0.77625353
  0.77775357  0.77816746  0.78262547  0.78510913  0.78576942  0.79426079
  0.79504376  0.79548198  0.79752742  0.79967586  0.80427931  0.80497839
  0.80746205  0.80855142  0.81338258  0.81386243  0.8233574   0.82397257
  0.82737423  0.82893989  0.83693073  0.84401098  0.84471692  0.8457641
  0.84684343  0.84847249  0.84968424  0.8511173   0.85377647  0.85626013
  0.8637111   0.87359964  0.88588851  0.89103134  0.89265461  0.89538348
  0.9000667   0.90096597  0.90311441  0.90529565  0.90593329  0.90657973
  0.91304904  0.91635661  0.91835158  0.92331889  0.92524064  0.92828621
  0.92876564  0.93325353  0.93540197  0.93788562  0.93822084  0.94036928
  0.9407045   0.94272855  0.94285294  0.94318816  0.94322432  0.9532705
  0.95527123  0.95775489  0.95809011  0.9590835   0.96057377  0.96112397
  0.96156716  0.96228254  0.9627222   0.96340291  0.96554108  0.96768952
  0.96779805  0.96980932  0.97063792  0.97265684  0.9751405   0.97652289
  0.97799553  0.98010781  0.98507513  0.98755879  1.00742805  1.00991171
  1.01239537  1.01252488  1.01485736  1.01736268  1.03721028  1.06101793
  1.08603281  1.09498653  1.18686975  1.65284746]

  UserWarning,

2022-10-31 11:02:32,449:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.23953206e+00 -8.42291012e-01 -6.19405789e-01 -4.00621949e-01
 -3.82966146e-01 -2.37493102e-01 -1.68814969e-01 -1.02889683e-01
 -1.01084317e-01 -9.00304520e-02 -8.17229716e-02 -8.06768748e-02
 -6.09450377e-02 -4.96795631e-02 -3.76153244e-02 -2.97624180e-02
 -1.56562758e-02 -4.02814325e-03 -1.57158862e-04  8.00099222e-03
  1.77412442e-02  2.49904409e-02  2.51539418e-02  2.73155360e-02
  3.40545626e-02  5.60027264e-02  5.89099203e-02  6.63939546e-02
  6.71369705e-02  9.74759277e-02  1.05463220e-01  1.30967407e-01
  1.35813211e-01  1.35906136e-01  1.37628123e-01  1.44185443e-01
  1.44624023e-01  1.47221938e-01  1.51159145e-01  1.53440485e-01
  1.53850390e-01  1.56342972e-01  1.56569020e-01  1.58717512e-01
  1.60966176e-01  1.61667964e-01  1.61867582e-01  1.63393123e-01
  1.65240796e-01  1.66436024e-01  1.71460730e-01  1.71898004e-01
  1.76152739e-01  1.78392071e-01  1.80545345e-01  1.83218048e-01
  1.91732468e-01  1.92067747e-01  1.93569051e-01  1.95449489e-01
  1.96536575e-01  1.98463266e-01  1.99560873e-01  2.00065307e-01
  2.00253805e-01  2.01064246e-01  2.01332715e-01  2.07144690e-01
  2.08983532e-01  2.09574695e-01  2.09651961e-01  2.11483444e-01
  2.13939184e-01  2.16271229e-01  2.17980525e-01  2.19080067e-01
  2.19426661e-01  2.20008082e-01  2.20723156e-01  2.24476529e-01
  2.25921508e-01  2.26419940e-01  2.26712489e-01  2.27640994e-01
  2.28406475e-01  2.28851079e-01  2.29602922e-01  2.30593958e-01
  2.39550194e-01  2.41382333e-01  2.41441010e-01  2.42147923e-01
  2.42806779e-01  2.43184854e-01  2.44322815e-01  2.47544014e-01
  2.49527173e-01  2.49751749e-01  2.50764877e-01  2.52888045e-01
  2.52925644e-01  2.53597380e-01  2.59830144e-01  2.60915225e-01
  2.62468509e-01  2.62833683e-01  2.66001106e-01  2.66972792e-01
  2.67844700e-01  2.69282469e-01  2.69584222e-01  2.69700147e-01
  2.69763877e-01  2.70442661e-01  2.70730073e-01  2.70771548e-01
  2.71848962e-01  2.72291175e-01  2.72415612e-01  2.73312924e-01
  2.73323992e-01  2.73489073e-01  2.76031539e-01  2.76576879e-01
  2.81902838e-01  2.83996469e-01  2.84024280e-01  2.84506578e-01
  2.84536000e-01  2.85738189e-01  2.86402547e-01  2.86903871e-01
  2.87007886e-01  2.89227198e-01  2.89831077e-01  2.90138561e-01
  2.91317515e-01  2.91342973e-01  2.91479411e-01  2.91747330e-01
  2.91774153e-01  2.91807358e-01  2.92576772e-01  2.92743106e-01
  2.93588996e-01  2.93630430e-01  2.93829341e-01  2.96318524e-01
  2.96909645e-01  2.97171831e-01  2.98692254e-01  2.98739940e-01
  3.00167653e-01  3.00389467e-01  3.01103558e-01  3.01847083e-01
  3.05625979e-01  3.06353950e-01  3.06448038e-01  3.08045004e-01
  3.08327137e-01  3.08551052e-01  3.08969196e-01  3.10154365e-01
  3.10443598e-01  3.10971673e-01  3.11935105e-01  3.12827592e-01
  3.14893470e-01  3.15315237e-01  3.16062534e-01  3.16198826e-01
  3.16210479e-01  3.16780447e-01  3.20535470e-01  3.21005830e-01
  3.23841517e-01  3.23935178e-01  3.24435926e-01  3.26147577e-01
  3.26978752e-01  3.27904117e-01  3.28074588e-01  3.28262035e-01
  3.28642244e-01  3.28668101e-01  3.28879094e-01  3.29740985e-01
  3.31199467e-01  3.34498556e-01  3.35534458e-01  3.39415231e-01
  3.39592009e-01  3.40013587e-01  3.40822510e-01  3.40882578e-01
  3.41473241e-01  3.41546362e-01  3.42299225e-01  3.42476486e-01
  3.42509891e-01  3.42950939e-01  3.46389862e-01  3.47904324e-01
  3.49029509e-01  3.50110883e-01  3.50398715e-01  3.50886631e-01
  3.52824904e-01  3.54051060e-01  3.54179160e-01  3.54607142e-01
  3.55647282e-01  3.57040821e-01  3.57874044e-01  3.60948962e-01
  3.61506215e-01  3.62434929e-01  3.64504490e-01  3.64602618e-01
  3.64811527e-01  3.64984821e-01  3.65463293e-01  3.65603793e-01
  3.65671560e-01  3.65848147e-01  3.66054556e-01  3.66225477e-01
  3.67371605e-01  3.68733181e-01  3.70227589e-01  3.70330593e-01
  3.71461461e-01  3.74162542e-01  3.77444202e-01  3.78282741e-01
  3.78611836e-01  3.78622818e-01  3.79634414e-01  3.81986093e-01
  3.84578758e-01  3.86942158e-01  3.87321816e-01  3.88129869e-01
  3.88322841e-01  3.88857905e-01  3.88959549e-01  3.89134259e-01
  3.89305557e-01  3.89477786e-01  3.89722708e-01  3.90125953e-01
  3.90493269e-01  3.90985175e-01  3.91395728e-01  3.91544199e-01
  3.91668957e-01  3.93030487e-01  3.93580979e-01  3.93732295e-01
  3.94015511e-01  3.96049748e-01  3.96395757e-01  3.97095544e-01
  3.98005128e-01  3.98631790e-01  4.00297536e-01  4.01122556e-01
  4.01341112e-01  4.01438737e-01  4.02070022e-01  4.02868730e-01
  4.03443281e-01  4.05180760e-01  4.06636458e-01  4.06647906e-01
  4.07148598e-01  4.08073140e-01  4.08999078e-01  4.10576155e-01
  4.11986304e-01  4.12334721e-01  4.12987093e-01  4.14651476e-01
  4.15049072e-01  4.17096668e-01  4.22270686e-01  4.23487280e-01
  4.25472930e-01  4.26492791e-01  4.26863409e-01  4.28963315e-01
  4.30188513e-01  4.30536330e-01  4.34396072e-01  4.34429536e-01
  4.35623996e-01  4.38344182e-01  4.39236387e-01  4.39759561e-01
  4.39989929e-01  4.42007321e-01  4.42085783e-01  4.42353329e-01
  4.42864591e-01  4.43104664e-01  4.43318920e-01  4.45685504e-01
  4.47080128e-01  4.47643499e-01  4.49951357e-01  4.52697640e-01
  4.53593454e-01  4.53737053e-01  4.54170327e-01  4.55075565e-01
  4.55260383e-01  4.57311125e-01  4.60202553e-01  4.63766757e-01
  4.65069729e-01  4.67947196e-01  4.77522793e-01  4.78523941e-01
  4.78892453e-01  4.79548674e-01  4.79697191e-01  4.81276437e-01
  4.84420573e-01  4.86973437e-01  4.94512177e-01  4.95082443e-01
  4.95653549e-01  4.96913663e-01  4.98032013e-01  5.00380349e-01
  5.05904785e-01  5.09419234e-01  5.10461119e-01  5.11510395e-01
  5.14490225e-01  5.18009054e-01  5.18107058e-01  5.18724509e-01
  5.24014346e-01  5.31104545e-01  5.38517693e-01  5.39091180e-01
  5.41519674e-01  5.41884411e-01  5.43015407e-01  5.47648343e-01
  5.48466127e-01  5.57585614e-01  5.60411987e-01  5.63993146e-01
  5.64504147e-01  5.64860419e-01  5.67516979e-01  5.68812958e-01
  5.69842389e-01  5.71455883e-01  5.76553655e-01  5.78869878e-01
  5.79228221e-01  5.80515572e-01  5.83813272e-01  5.85242371e-01
  5.93943158e-01  5.97059370e-01  5.97963191e-01  5.98030958e-01
  6.08165296e-01  6.10785504e-01  6.12211356e-01  6.14320874e-01
  6.16870388e-01  6.18886841e-01  6.19301555e-01  6.21411073e-01
  6.21597188e-01  6.21603249e-01  6.32306265e-01  6.33481954e-01
  6.35845353e-01  6.37149173e-01  6.38538238e-01  6.40572153e-01
  6.41011075e-01  6.42935552e-01  6.47662352e-01  6.49944098e-01
  6.52389151e-01  6.52667512e-01  6.54752551e-01  6.57115951e-01
  6.57529678e-01  6.57887396e-01  6.59479350e-01  6.61242860e-01
  6.61842750e-01  6.62776580e-01  6.64206150e-01  6.64761776e-01
  6.68805315e-01  6.69117990e-01  6.71296349e-01  6.73659749e-01
  6.74298013e-01  6.76023148e-01  6.78132666e-01  6.78386548e-01
  6.79439526e-01  6.80496066e-01  6.80749948e-01  6.82859466e-01
  6.82892981e-01  6.83751611e-01  6.83884793e-01  6.84166325e-01
  6.85222865e-01  6.86529725e-01  6.86664459e-01  6.88963301e-01
  6.89949665e-01  6.92566947e-01  6.97039864e-01  6.98346723e-01
  6.99403264e-01  7.01674537e-01  7.02020545e-01  7.03073523e-01
  7.04037937e-01  7.04130063e-01  7.05155391e-01  7.06365829e-01
  7.08442149e-01  7.09105749e-01  7.09110745e-01  7.09594417e-01
  7.10163722e-01  7.12817868e-01  7.13917843e-01  7.16195948e-01
  7.17070216e-01  7.17253921e-01  7.18708627e-01  7.21912954e-01
  7.22152949e-01  7.25157416e-01  7.27412980e-01  7.27764060e-01
  7.28012947e-01  7.30376346e-01  7.31882885e-01  7.34547774e-01
  7.35103146e-01  7.39102345e-01  7.39229979e-01  7.41977974e-01
  7.43986979e-01  7.46920144e-01  7.49283544e-01  7.50341517e-01
  7.52704917e-01  7.53410377e-01  7.54010344e-01  7.58737143e-01
  7.61876984e-01  7.65827342e-01  7.68190742e-01  7.70554142e-01
  7.73924303e-01  7.75153307e-01  7.75280941e-01  7.75964201e-01
  7.77644341e-01  7.78001073e-01  7.78083263e-01  7.79433731e-01
  7.82371140e-01  7.87097940e-01  7.89115331e-01  7.89461339e-01
  7.90970891e-01  7.96551538e-01  7.98914938e-01  8.01278338e-01
  8.03641738e-01  8.09004021e-01  8.10301758e-01  8.10731937e-01
  8.17476128e-01  8.21356972e-01  8.24912335e-01  8.28047181e-01
  8.28328712e-01  8.39092733e-01  8.41456133e-01  8.43819533e-01
  8.45029901e-01  8.47296170e-01  8.54396286e-01  8.54880445e-01
  8.57779564e-01  8.61046785e-01  8.71183643e-01  8.75227183e-01
  8.76819137e-01  8.80323506e-01  8.82686906e-01  8.84115143e-01
  8.84680782e-01  8.85733759e-01  8.87044181e-01  8.89407581e-01
  8.89495573e-01  8.99444547e-01  9.05951379e-01  9.11203128e-01
  9.15404978e-01  9.18821355e-01  9.20131778e-01  9.29585376e-01
  9.30268636e-01  9.30510720e-01  9.32655746e-01  9.33585121e-01
  9.34312176e-01  9.39038975e-01  9.39522647e-01  9.40091953e-01
  9.41402375e-01  9.43765775e-01  9.47182152e-01  9.47696724e-01
  9.50917139e-01  9.51908951e-01  9.54272351e-01  9.54342527e-01
  9.56066445e-01  9.56635751e-01  9.59458950e-01  9.68452749e-01
  9.69763172e-01  9.70816149e-01  9.71938332e-01  9.73179549e-01
  9.76633004e-01  9.80269748e-01  9.82633148e-01  9.84996547e-01
  9.89723347e-01  9.91513430e-01  9.91805215e-01  9.94104138e-01
  9.94622375e-01  9.97369172e-01  9.99049312e-01  9.99176946e-01
  1.00576525e+00  1.01147762e+00  1.02016601e+00  1.02254032e+00
  1.02390100e+00  1.03079965e+00  1.03440757e+00  1.03720990e+00
  1.04500560e+00]

  UserWarning,

2022-10-31 11:02:32,463:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.68684337e-01 -4.16429792e-01 -3.47789102e-01 -3.05572986e-01
 -1.94576249e-01 -1.40155912e-01 -1.24286768e-01 -6.34578237e-02
 -3.27782442e-02 -3.24759866e-02 -2.66920852e-02 -6.60795114e-03
  1.62577994e-03  1.79327474e-02  2.83155712e-02  3.84570555e-02
  4.40216364e-02  4.57306973e-02  4.84423483e-02  4.88716041e-02
  5.79436589e-02  5.80979983e-02  7.08180104e-02  8.55664035e-02
  8.95545230e-02  1.02675368e-01  1.04971186e-01  1.06753377e-01
  1.08569972e-01  1.11512042e-01  1.13170422e-01  1.23691900e-01
  1.29472003e-01  1.30190550e-01  1.32399452e-01  1.36655946e-01
  1.37441671e-01  1.42192520e-01  1.43775919e-01  1.51659845e-01
  1.52967716e-01  1.53355406e-01  1.60523082e-01  1.61580802e-01
  1.63758811e-01  1.65431265e-01  1.67054134e-01  1.70937268e-01
  1.71325732e-01  1.73462915e-01  1.76078661e-01  1.84889012e-01
  1.85392016e-01  1.86549204e-01  1.87871043e-01  1.89284757e-01
  1.92577367e-01  1.94116157e-01  1.94772687e-01  1.95243444e-01
  1.98919554e-01  2.00487656e-01  2.02610178e-01  2.02875533e-01
  2.04559525e-01  2.09691791e-01  2.11042806e-01  2.14338212e-01
  2.17260020e-01  2.17591643e-01  2.21191130e-01  2.22034338e-01
  2.22481439e-01  2.23551548e-01  2.26399590e-01  2.29335951e-01
  2.36731058e-01  2.38123877e-01  2.38477432e-01  2.38574402e-01
  2.38659289e-01  2.47601336e-01  2.48068406e-01  2.48749633e-01
  2.49223884e-01  2.49227714e-01  2.50686150e-01  2.51060421e-01
  2.52628328e-01  2.54412437e-01  2.55201096e-01  2.57464802e-01
  2.58150766e-01  2.58236299e-01  2.58565848e-01  2.59033143e-01
  2.59153088e-01  2.62199819e-01  2.63224479e-01  2.65677534e-01
  2.65895488e-01  2.67060157e-01  2.68464583e-01  2.68730824e-01
  2.68820115e-01  2.70346831e-01  2.71484410e-01  2.72497803e-01
  2.72838946e-01  2.73561591e-01  2.74010482e-01  2.75176263e-01
  2.75542492e-01  2.76500256e-01  2.76957767e-01  2.77346547e-01
  2.77786942e-01  2.79212771e-01  2.79323543e-01  2.79900068e-01
  2.82772732e-01  2.83847216e-01  2.84170905e-01  2.85156239e-01
  2.85940838e-01  2.86311044e-01  2.87200559e-01  2.87431396e-01
  2.87766381e-01  2.88998454e-01  2.89196023e-01  2.89482509e-01
  2.90436913e-01  2.90512169e-01  2.91966734e-01  2.92021375e-01
  2.92964692e-01  2.95458279e-01  2.97268107e-01  2.97487108e-01
  2.97740613e-01  2.98353274e-01  2.98968725e-01  2.99843851e-01
  3.00404631e-01  3.00708902e-01  3.00769059e-01  3.01121714e-01
  3.01225322e-01  3.03347907e-01  3.03738592e-01  3.03922374e-01
  3.04314497e-01  3.04827964e-01  3.06344775e-01  3.06934325e-01
  3.07184370e-01  3.08168869e-01  3.08390532e-01  3.10567081e-01
  3.10581750e-01  3.11288642e-01  3.11922929e-01  3.11993378e-01
  3.13923040e-01  3.14736424e-01  3.16232107e-01  3.17077035e-01
  3.18334109e-01  3.21438011e-01  3.22433925e-01  3.25909764e-01
  3.26586649e-01  3.27533933e-01  3.28041984e-01  3.28523326e-01
  3.29254298e-01  3.29650582e-01  3.30277208e-01  3.30583598e-01
  3.30776744e-01  3.33636909e-01  3.34076981e-01  3.35107802e-01
  3.35922734e-01  3.36156313e-01  3.37721027e-01  3.38064737e-01
  3.38177010e-01  3.38376030e-01  3.39334313e-01  3.39531201e-01
  3.42003521e-01  3.44823200e-01  3.45861011e-01  3.47054701e-01
  3.47260606e-01  3.48337743e-01  3.48563790e-01  3.48715589e-01
  3.48963884e-01  3.52427990e-01  3.52638203e-01  3.53260570e-01
  3.53437790e-01  3.55024007e-01  3.56239282e-01  3.56698630e-01
  3.56759527e-01  3.57118347e-01  3.57985487e-01  3.58467505e-01
  3.58819917e-01  3.58823730e-01  3.59598778e-01  3.61008343e-01
  3.61634614e-01  3.62534237e-01  3.62969590e-01  3.63390266e-01
  3.65422650e-01  3.69017451e-01  3.69424422e-01  3.69490062e-01
  3.70692979e-01  3.71156260e-01  3.72906483e-01  3.74599073e-01
  3.76063299e-01  3.76269854e-01  3.77602325e-01  3.77603765e-01
  3.78758550e-01  3.79017276e-01  3.80509813e-01  3.80744099e-01
  3.80843038e-01  3.82979800e-01  3.83287881e-01  3.84660685e-01
  3.85138598e-01  3.85469224e-01  3.85953825e-01  3.86042820e-01
  3.86813955e-01  3.87313770e-01  3.87943203e-01  3.88508288e-01
  3.89434859e-01  3.89946838e-01  3.90762980e-01  3.91318860e-01
  3.92031957e-01  3.92339519e-01  3.92439041e-01  3.92497940e-01
  3.93695800e-01  3.94148070e-01  3.94302354e-01  3.94542215e-01
  3.94718187e-01  3.95391393e-01  3.95642728e-01  3.95864897e-01
  3.96811529e-01  3.97132619e-01  3.97409442e-01  3.97486950e-01
  4.00141489e-01  4.00367296e-01  4.01519455e-01  4.04641918e-01
  4.04729538e-01  4.05343159e-01  4.06129833e-01  4.08289160e-01
  4.09313384e-01  4.10260462e-01  4.10572260e-01  4.10856074e-01
  4.13039435e-01  4.14287527e-01  4.14821168e-01  4.16040207e-01
  4.16201682e-01  4.16976437e-01  4.17091296e-01  4.20173847e-01
  4.20398428e-01  4.21895097e-01  4.23141960e-01  4.23201055e-01
  4.23249323e-01  4.23575572e-01  4.24378473e-01  4.25540553e-01
  4.27392095e-01  4.28685495e-01  4.32329751e-01  4.32991476e-01
  4.33023144e-01  4.35217787e-01  4.36544738e-01  4.37837287e-01
  4.41145712e-01  4.44459349e-01  4.53214476e-01  4.53786687e-01
  4.55889216e-01  4.56905363e-01  4.57023303e-01  4.58410965e-01
  4.59446261e-01  4.59612952e-01  4.60215236e-01  4.62691470e-01
  4.63426980e-01  4.63443850e-01  4.64959983e-01  4.65573304e-01
  4.66301997e-01  4.66538038e-01  4.66806074e-01  4.67222738e-01
  4.67260655e-01  4.67617924e-01  4.69112714e-01  4.69711736e-01
  4.73505228e-01  4.74130846e-01  4.75131803e-01  4.81452779e-01
  4.84089557e-01  4.88062680e-01  4.91665514e-01  4.94399549e-01
  4.95453826e-01  4.98470157e-01  5.04070756e-01  5.05359469e-01
  5.06287112e-01  5.07236169e-01  5.08071120e-01  5.08482543e-01
  5.09615616e-01  5.10039296e-01  5.10088579e-01  5.13061120e-01
  5.13188831e-01  5.28441120e-01  5.30291918e-01  5.31429073e-01
  5.34597349e-01  5.40475232e-01  5.47974688e-01  5.48662630e-01
  5.49683638e-01  5.50834175e-01  5.50906003e-01  5.51451847e-01
  5.54788933e-01  5.55954951e-01  5.57751263e-01  5.61516519e-01
  5.61752607e-01  5.62167652e-01  5.62168440e-01  5.62640781e-01
  5.62891727e-01  5.63070269e-01  5.64736711e-01  5.66034179e-01
  5.68156784e-01  5.68230653e-01  5.76258197e-01  5.77296609e-01
  5.79863600e-01  5.81490359e-01  5.83718268e-01  5.85084625e-01
  5.86618241e-01  5.88494763e-01  5.90194840e-01  5.90228501e-01
  5.94564872e-01  5.97276523e-01  5.97311179e-01  5.99988174e-01
  6.08123127e-01  6.08562460e-01  6.10626608e-01  6.12361832e-01
  6.16258080e-01  6.18969731e-01  6.23100504e-01  6.25006051e-01
  6.25604032e-01  6.31027334e-01  6.32562642e-01  6.35239637e-01
  6.35803829e-01  6.35839922e-01  6.35893143e-01  6.36605994e-01
  6.37951288e-01  6.38722483e-01  6.40662939e-01  6.40697595e-01
  6.42029296e-01  6.44427860e-01  6.44740947e-01  6.48797892e-01
  6.51087687e-01  6.51509543e-01  6.51655318e-01  6.54221194e-01
  6.59644496e-01  6.61010853e-01  6.62317458e-01  6.62356147e-01
  6.64716248e-01  6.65067798e-01  6.67177135e-01  6.69145806e-01
  6.70491100e-01  6.70724908e-01  6.71857457e-01  6.72910728e-01
  6.73202751e-01  6.75764198e-01  6.76212097e-01  6.78626053e-01
  6.79576577e-01  6.80022595e-01  6.80592696e-01  6.82704061e-01
  6.83962690e-01  6.84298090e-01  6.85415712e-01  6.86339150e-01
  6.88127363e-01  6.88986768e-01  6.90839014e-01  6.93550665e-01
  6.93590748e-01  6.94480126e-01  6.96210902e-01  6.96262316e-01
  6.97256060e-01  6.98973967e-01  7.02727862e-01  7.03065568e-01
  7.04397269e-01  7.05140249e-01  7.05391013e-01  7.05674638e-01
  7.05742563e-01  7.07108920e-01  7.08102664e-01  7.09820571e-01
  7.10687239e-01  7.13461683e-01  7.13525966e-01  7.13877516e-01
  7.14932995e-01  7.15243873e-01  7.15917058e-01  7.16237617e-01
  7.21660919e-01  7.23360211e-01  7.26678261e-01  7.29089600e-01
  7.32870271e-01  7.35911086e-01  7.43296723e-01  7.44669070e-01
  7.45745219e-01  7.46720767e-01  7.47783685e-01  7.48456870e-01
  7.48777429e-01  7.51278303e-01  7.51569382e-01  7.51754382e-01
  7.52054122e-01  7.53528622e-01  7.54200731e-01  7.55502805e-01
  7.55595792e-01  7.56240766e-01  7.59481903e-01  7.64726776e-01
  7.65423341e-01  7.66568012e-01  7.72861729e-01  7.75231121e-01
  7.80996682e-01  7.83467243e-01  7.83530535e-01  7.83708333e-01
  7.89131635e-01  7.91843286e-01  7.94554937e-01  7.97266588e-01
  7.99562405e-01  7.99978239e-01  8.02689890e-01  8.07664106e-01
  8.08113192e-01  8.09319410e-01  8.18959796e-01  8.22216717e-01
  8.25749455e-01  8.27094749e-01  8.32333191e-01  8.33821257e-01
  8.33932222e-01  8.34813868e-01  8.35120977e-01  8.37941353e-01
  8.40301454e-01  8.40653004e-01  8.43364655e-01  8.48338871e-01
  8.50540737e-01  8.51390884e-01  8.56465373e-01  8.56507076e-01
  8.59218727e-01  8.62346212e-01  8.63635634e-01  8.64111777e-01
  8.70129615e-01  8.73462424e-01  8.83930696e-01  8.86566211e-01
  8.88246290e-01  8.88355641e-01  8.90055718e-01  9.02912253e-01
  9.06490571e-01  9.07643643e-01  9.08198411e-01  9.15868194e-01
  9.16118958e-01  9.19182159e-01  9.29677213e-01  9.32324580e-01
  9.32388864e-01  9.32740414e-01  9.39030383e-01  9.40523817e-01
  9.40875367e-01  9.43587018e-01  9.48588464e-01  9.49010320e-01
  9.59856924e-01  9.61966261e-01  9.63750184e-01  9.65810101e-01
  9.71570195e-01  9.73063629e-01  9.74274628e-01  9.81383034e-01
  9.83910233e-01  9.89333535e-01  9.92045186e-01  9.97468488e-01
  1.00018014e+00  1.00218552e+00  1.00289179e+00  1.00831509e+00
  1.01119343e+00  1.01874586e+00  1.01916170e+00  1.01956462e+00
  1.01985361e+00  1.02221371e+00  1.04567149e+00  1.06778655e+00
  1.07320986e+00  1.09017279e+00  1.42215981e+00  1.66927014e+00]

  UserWarning,

2022-10-31 11:02:32,482:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.15483526 -0.5764159  -0.49154652 -0.47224084 -0.35426769 -0.10808954
 -0.1070076  -0.09583039 -0.09230676 -0.08404107 -0.03836924 -0.02588125
 -0.01135945 -0.00217259  0.03584146  0.04893263  0.05913797  0.06468657
  0.07065817  0.07649725  0.0967695   0.09686226  0.10057674  0.11515608
  0.12858408  0.13027778  0.13501977  0.13926419  0.14119226  0.14796182
  0.15231174  0.16602923  0.16894026  0.17255909  0.18214188  0.19041573
  0.19104831  0.19643416  0.19651793  0.1989937   0.19955917  0.2006014
  0.2030259   0.20593951  0.20814231  0.20856282  0.21036423  0.21380376
  0.2141108   0.21806757  0.21927611  0.22071853  0.22163495  0.22419255
  0.22685082  0.22805664  0.22862903  0.22964744  0.23068108  0.23224827
  0.23253098  0.2331381   0.23595209  0.23640184  0.23652714  0.23798822
  0.23919763  0.24007001  0.24102042  0.24305708  0.24368253  0.24382366
  0.24408334  0.24827004  0.24855592  0.25034608  0.25049588  0.25062936
  0.25065178  0.25134718  0.25202441  0.25216579  0.25223788  0.25268518
  0.25268525  0.25395938  0.25412213  0.25446124  0.25597694  0.25633269
  0.25676818  0.25881645  0.25913348  0.25968375  0.26106252  0.26351505
  0.26507396  0.26508577  0.2654704   0.26618182  0.26656815  0.26841052
  0.26849647  0.26865685  0.27123972  0.27325913  0.27468162  0.27488318
  0.27656444  0.27910577  0.27999803  0.28002787  0.28064999  0.2814208
  0.28239394  0.2825525   0.28371545  0.28710953  0.28859028  0.28873076
  0.28882264  0.28892113  0.28929147  0.29100118  0.29365101  0.2948828
  0.29670941  0.29755368  0.29756729  0.29871087  0.29876124  0.29943603
  0.29973505  0.30067856  0.30248554  0.30280987  0.30301641  0.30315292
  0.30397827  0.3050466   0.30637175  0.30728939  0.31007555  0.31122262
  0.3117704   0.31214471  0.31221713  0.31271368  0.31372705  0.31598993
  0.31776517  0.31895896  0.31915142  0.32116746  0.32137314  0.32194333
  0.32359795  0.32565212  0.32668606  0.32682813  0.32777254  0.32781719
  0.32864472  0.32868223  0.32894821  0.32951981  0.33125879  0.33137831
  0.33204741  0.33234078  0.33272705  0.33361579  0.33384401  0.33525957
  0.33542964  0.33661768  0.33672759  0.33707246  0.33726388  0.33763943
  0.33836102  0.33887859  0.33896539  0.33939963  0.34165248  0.34186546
  0.34301103  0.34338991  0.34511768  0.34538565  0.34575086  0.34609109
  0.34631478  0.34660765  0.34675392  0.34758986  0.34767348  0.34893403
  0.34948994  0.35002045  0.35008084  0.35193107  0.35417918  0.35440533
  0.35560156  0.35612066  0.35644529  0.35646531  0.35669059  0.35705303
  0.35706206  0.36011004  0.36098444  0.36139533  0.36201716  0.3620628
  0.36230638  0.36237177  0.36251249  0.36326343  0.36343769  0.36443738
  0.36492797  0.36766288  0.36792711  0.36831219  0.36935325  0.36958591
  0.37084134  0.37192949  0.37321817  0.37375291  0.37402429  0.37537829
  0.37560442  0.37581338  0.37757551  0.37810832  0.37882444  0.37882755
  0.37963898  0.38118626  0.38264765  0.38321549  0.38535621  0.38577947
  0.38640401  0.38726712  0.38729635  0.38753221  0.38848574  0.3887007
  0.38915469  0.38933322  0.389823    0.38995061  0.39209133  0.39220634
  0.39233686  0.39310946  0.39327475  0.39563159  0.39571941  0.39690216
  0.39773681  0.39777751  0.39821339  0.39879284  0.4012544   0.40189825
  0.40202803  0.40212644  0.40222881  0.40368433  0.40839811  0.40946498
  0.4101268   0.41029252  0.41143132  0.41187201  0.41209248  0.41229425
  0.41258805  0.41289182  0.41822366  0.41829357  0.4183514   0.42140506
  0.42146147  0.42261058  0.42537368  0.42774381  0.42878168  0.43348845
  0.43431767  0.43658635  0.4371768   0.43834308  0.4389767   0.44101037
  0.44533178  0.44562317  0.44668312  0.44965708  0.45029482  0.45052009
  0.45060337  0.45196048  0.45273617  0.45669204  0.45713292  0.45746586
  0.45789233  0.45808067  0.45814386  0.46200634  0.4629307   0.46328816
  0.464994    0.46513894  0.46604109  0.47019723  0.47190049  0.47237664
  0.47504928  0.47551856  0.47793389  0.48052564  0.48498291  0.49454829
  0.50177679  0.50557058  0.5077669   0.51543142  0.51789436  0.51903782
  0.52041891  0.52320597  0.53478193  0.53690609  0.54111729  0.54266712
  0.54620177  0.54757383  0.55198868  0.5571585   0.56239754  0.56402898
  0.56630998  0.56951484  0.57292089  0.57374165  0.57852235  0.5809127
  0.58569339  0.58569749  0.58907868  0.5919042   0.59286444  0.59525478
  0.6021861   0.604272    0.60438366  0.61198722  0.61438166  0.61770486
  0.61915827  0.62161783  0.62321535  0.62393896  0.62632931  0.62659229
  0.62871966  0.62928847  0.62959735  0.63111001  0.63350036  0.63374536
  0.6358907   0.63609966  0.63828105  0.63849001  0.643385    0.64395112
  0.64524911  0.6454521   0.64645426  0.64873182  0.6498245   0.65201767
  0.65262314  0.65501349  0.65623671  0.65636654  0.65666275  0.65740384
  0.65764239  0.65803285  0.6590255   0.65979419  0.6610174   0.66114723
  0.66239349  0.66353758  0.66457488  0.66558055  0.66696523  0.66973367
  0.67174593  0.67233552  0.67472587  0.67517444  0.67652662  0.67700261
  0.67836549  0.68369767  0.68425155  0.68505072  0.68666685  0.68983141
  0.69099472  0.69258919  0.69325906  0.69330555  0.69461211  0.69504648
  0.69564941  0.69589442  0.69595828  0.69700246  0.6985348   0.69926298
  0.69975968  0.70104376  0.70178315  0.70656385  0.70667903  0.70817999
  0.70819075  0.70863931  0.7089542   0.70907117  0.70919291  0.71234671
  0.71297144  0.7137349   0.71536179  0.71612524  0.72253284  0.72626547
  0.72748482  0.72838586  0.72878215  0.72969312  0.7303375   0.73208347
  0.74164486  0.74241907  0.74355179  0.74404597  0.74449454  0.74719977
  0.74838388  0.74881591  0.7523262   0.75437082  0.75598695  0.7583773
  0.75901707  0.75976517  0.76335308  0.76436701  0.7657573   0.76676104
  0.76793869  0.77032904  0.77271939  0.77923612  0.77989043  0.78228078
  0.78467113  0.78706148  0.78765107  0.78795085  0.78923634  0.78945183
  0.79135876  0.79184217  0.79423252  0.79662287  0.79721247  0.7985298
  0.8001567   0.80140357  0.80395151  0.80427973  0.80438351  0.80618426
  0.80760805  0.80857461  0.80887228  0.82083523  0.82392236  0.82420496
  0.82710599  0.83545804  0.83725879  0.84270161  0.84786911  0.86180853
  0.87672758  0.87926043  0.88031856  0.88186759  0.8868606   0.89403164
  0.89642199  0.89896994  0.90120269  0.9012719   0.90359303  0.9045952
  0.90837373  0.91315443  0.91374402  0.91422984  0.91613437  0.91793922
  0.91852472  0.92271582  0.92328463  0.92510617  0.92749652  0.92988687
  0.93110547  0.93227721  0.93335263  0.93525716  0.93585003  0.93705791
  0.9423997   0.94284427  0.94692818  0.94959925  0.95276736  0.95379035
  0.95677029  0.95877911  0.95887992  0.9639654   0.96633169  0.96935565
  0.97350273  0.97350682  0.97608897  0.97904688  0.98440742  0.98545447
  0.98801431  0.98847844  0.99323917  0.99740621  0.99853893  0.99862943
  1.00218691  1.00457726  1.0069676   1.01038767  1.01229142  1.01260578
  1.04703104  1.05576804]

  UserWarning,

2022-10-31 11:02:32,513:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04565302 -0.84450285 -0.75904925 -0.72365144 -0.6623799  -0.56606793
 -0.518642   -0.41424126 -0.39203911 -0.38561188 -0.35563004 -0.3541531
 -0.28733916 -0.28332456 -0.26126767 -0.24413417 -0.23868457 -0.22597675
 -0.18887541 -0.15979404 -0.13853131 -0.10001982 -0.08167049 -0.07653521
 -0.04311978 -0.03073684 -0.02712962 -0.02612148 -0.02348756 -0.00612833
 -0.00291073 -0.00274286  0.02083868  0.04546981  0.0512354   0.06984162
  0.07739052  0.07869841  0.08629994  0.09402676  0.09895561  0.10503819
  0.10556503  0.11133615  0.11351778  0.11423749  0.11514491  0.12452678
  0.129196    0.14440033  0.15542486  0.15651655  0.16389472  0.16558137
  0.16803712  0.17131421  0.17277853  0.17458772  0.17595117  0.17738508
  0.17825577  0.17978118  0.19014312  0.19059802  0.19174173  0.19486267
  0.19555929  0.19661483  0.19766985  0.19857287  0.19919845  0.2018508
  0.20277896  0.20385289  0.20437823  0.21598864  0.22222799  0.22231116
  0.22234317  0.22286233  0.22551477  0.22626977  0.22739679  0.22784902
  0.22917827  0.2298159   0.23005999  0.23166408  0.23876981  0.23926827
  0.23955836  0.24229931  0.24329918  0.24356814  0.24402798  0.24441849
  0.24451974  0.24517934  0.24678     0.24855423  0.25643989  0.25827791
  0.25946858  0.25952195  0.26105657  0.2613075   0.26321193  0.26509744
  0.26585864  0.26624452  0.26642243  0.26768744  0.26908188  0.26959913
  0.27061408  0.27064281  0.27227215  0.27257147  0.27341317  0.27461015
  0.27475863  0.27480698  0.27559895  0.2762409   0.27728861  0.2773191
  0.27738006  0.27805155  0.27904445  0.28183365  0.2819277   0.28206675
  0.28293759  0.28433784  0.28453867  0.28611772  0.28704986  0.28800925
  0.28954406  0.29076086  0.29150754  0.29212756  0.29225719  0.29325983
  0.29329175  0.2934213   0.29402333  0.29708313  0.29850696  0.29946595
  0.29952466  0.2997281   0.3015712   0.30506818  0.30606322  0.30639113
  0.30890869  0.31041106  0.31098123  0.31132171  0.31341274  0.31392029
  0.31434711  0.31438836  0.31578916  0.3164962   0.31779741  0.31963131
  0.31968721  0.3208132   0.32161949  0.32166527  0.32272863  0.32426683
  0.32474214  0.32587329  0.32723912  0.32824943  0.32897434  0.32931364
  0.33053337  0.33090343  0.33111781  0.33189906  0.33310926  0.3352829
  0.33764929  0.33806085  0.33825633  0.33875621  0.33973073  0.33983875
  0.34003916  0.34142414  0.34332731  0.3436097   0.34416674  0.34418049
  0.34566275  0.34610752  0.346131    0.34772067  0.34845476  0.34847699
  0.34859123  0.34929021  0.34978397  0.35090522  0.35099121  0.35166142
  0.35282696  0.35390962  0.35456689  0.35509557  0.35826256  0.35968646
  0.36008581  0.36072799  0.3615944   0.36171256  0.36172126  0.36174753
  0.36196411  0.36214393  0.36258603  0.36305781  0.36381346  0.36535307
  0.36565201  0.36639157  0.36680794  0.36804359  0.36862304  0.36907476
  0.3693536   0.37050302  0.37237856  0.37302681  0.3731396   0.37341571
  0.37412487  0.3765459   0.37712748  0.37827611  0.37935564  0.37942167
  0.37990856  0.38090143  0.38108923  0.38217637  0.38251415  0.38347736
  0.38441944  0.38635666  0.38673385  0.3892504   0.39054181  0.39071658
  0.39684644  0.3973119   0.39942488  0.3995479   0.3999493   0.40022915
  0.40098052  0.40138945  0.40156206  0.40297685  0.40376054  0.40388691
  0.40433087  0.40477945  0.4054851   0.40707218  0.40825326  0.40835527
  0.41041723  0.41084962  0.41179221  0.41434562  0.41541181  0.41553028
  0.41584589  0.41596938  0.41638768  0.41710532  0.41741989  0.41803526
  0.41965241  0.42010631  0.42030235  0.42283349  0.42333382  0.42511786
  0.42543157  0.42584258  0.42643717  0.42749477  0.42804853  0.42963119
  0.4296529   0.43056825  0.43267566  0.43369075  0.43377554  0.4342061
  0.43528815  0.44054572  0.44091389  0.4433509   0.44386488  0.44636754
  0.44897937  0.4492261   0.44963318  0.44997483  0.46024119  0.46578214
  0.46926027  0.46995536  0.47286614  0.47524215  0.47709769  0.48148478
  0.49555316  0.49975815  0.49982201  0.50655652  0.50865469  0.5089376
  0.51047166  0.51240385  0.5156556   0.51807346  0.52387927  0.52456966
  0.52623237  0.53205804  0.53343997  0.53399079  0.53434946  0.53505663
  0.53934885  0.54106144  0.54159372  0.54176191  0.54471393  0.5456831
  0.54620635  0.54792082  0.54838387  0.54912823  0.56420488  0.56475003
  0.56818808  0.57061924  0.57123689  0.57416575  0.5744188   0.57501961
  0.57784912  0.58549471  0.586063    0.59770496  0.59863683  0.60257023
  0.60266956  0.60434516  0.60471477  0.60620438  0.61083542  0.61210829
  0.61359048  0.6151672   0.61800805  0.6191006   0.62126117  0.62185566
  0.62230708  0.62375686  0.62402086  0.62461073  0.62496493  0.62531689
  0.62736579  0.62894251  0.63287591  0.63364163  0.63394302  0.63563097
  0.63685439  0.63838603  0.64078779  0.64093152  0.64155327  0.64389616
  0.64547288  0.64591423  0.64708837  0.65216134  0.65491641  0.65504103
  0.65875253  0.65915553  0.66212708  0.66232773  0.66318159  0.66475831
  0.66557538  0.66646856  0.66869171  0.66933103  0.67039256  0.67064506
  0.67119674  0.67144678  0.67237508  0.67275812  0.67420184  0.6756859
  0.67638251  0.67645725  0.67654977  0.6769569   0.67785903  0.67916681
  0.67929934  0.67971196  0.68271069  0.68283351  0.68382308  0.68385281
  0.68404375  0.68603915  0.68890545  0.68987835  0.69073221  0.6919018
  0.69506399  0.69538847  0.69624233  0.69676454  0.69781905  0.69814353
  0.6989974   0.70089859  0.70101547  0.70175246  0.70332918  0.70365366
  0.70396237  0.70450752  0.70514684  0.70628251  0.70640872  0.70726258
  0.71035159  0.71159436  0.71174505  0.7127727   0.71341202  0.71352527
  0.71434033  0.71434943  0.71927636  0.72013555  0.72019221  0.72106918
  0.72224482  0.72879047  0.73120428  0.73449268  0.73679409  0.73820758
  0.73883621  0.73966349  0.74202063  0.74497958  0.74647277  0.74741017
  0.74867683  0.74922783  0.75060658  0.75292029  0.75749301  0.75806062
  0.75833274  0.76024807  0.76173214  0.76199402  0.76851326  0.77126832
  0.77316928  0.77478911  0.77642514  0.77677845  0.77804156  0.78080758
  0.78248898  0.78504363  0.7858607   0.79055375  0.801574    0.80432906
  0.80469047  0.81128589  0.81692603  0.81968109  0.82000557  0.82085944
  0.8236145   0.82519122  0.83078324  0.83114269  0.83389775  0.83896653
  0.84014487  0.84510984  0.84723171  0.85489692  0.85593825  0.86867223
  0.87300911  0.87969248  0.8852026   0.89622285  0.89779957  0.89897791
  0.89941506  0.89979498  0.90055463  0.90173297  0.90606475  0.90724309
  0.91216336  0.91550828  0.91679125  0.91952906  0.9285466   0.92928359
  0.93168535  0.93444041  0.93479371  0.93678285  0.93754878  0.9391255
  0.93975869  0.94463562  0.94581396  0.94802387  0.94856902  0.95014574
  0.95081148  0.95132408  0.95290081  0.95683421  0.95958927  0.96632281
  0.96667611  0.96710349  0.97060952  0.97218624  0.97611964  0.9788747
  0.98596155  0.98871661  0.99422673  0.99805089  0.99973686  1.01351217
  1.01749484  1.02177735  1.02655048  1.10560813  1.51583983]

  UserWarning,

2022-10-31 11:02:32,544:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81370778 -0.50647708 -0.43917599 -0.37842913 -0.34789335 -0.33925243
 -0.26553249 -0.25458835 -0.11978107 -0.11919051 -0.09813073 -0.08117799
 -0.07433806 -0.07197974 -0.03000322 -0.0269691  -0.02370193 -0.02107891
  0.01515899  0.01773204  0.02828308  0.03870956  0.04050178  0.04794799
  0.05413944  0.05781648  0.06813298  0.07071211  0.08595383  0.09445459
  0.10142875  0.10660706  0.10831802  0.11197795  0.11577419  0.11872771
  0.12087195  0.12287982  0.13154087  0.13464854  0.14251397  0.14555533
  0.15399703  0.15904685  0.16505429  0.16893607  0.17739895  0.18146106
  0.18391429  0.19475464  0.19732928  0.1996905   0.20048444  0.2011899
  0.2028927   0.20445425  0.20476729  0.20477257  0.20537017  0.20912024
  0.21000342  0.21121371  0.21542955  0.21913655  0.21971498  0.21997871
  0.22084562  0.22280159  0.22773374  0.22836873  0.23218661  0.23262133
  0.23401828  0.23789497  0.23996111  0.24015406  0.24190633  0.24301669
  0.2431457   0.24329667  0.24402898  0.24474953  0.24581402  0.24833657
  0.25019283  0.25095328  0.25313257  0.25324947  0.25353763  0.25401829
  0.25436448  0.25523654  0.2563114   0.2566605   0.25790328  0.25884079
  0.25963045  0.26073692  0.2612728   0.2622561   0.26250475  0.26336415
  0.26528018  0.26596061  0.26637308  0.26647007  0.26656986  0.2668753
  0.26730844  0.27027125  0.27054345  0.27127563  0.27130232  0.27249814
  0.27286314  0.27320497  0.27429729  0.27439361  0.27576155  0.27895196
  0.27937     0.2794039   0.28096531  0.28258325  0.28272613  0.28368132
  0.28375033  0.28567244  0.28608322  0.28618115  0.28620497  0.28729613
  0.28868582  0.28874749  0.28910214  0.28931972  0.28934053  0.28980738
  0.29100901  0.29143505  0.29165107  0.29165214  0.29216381  0.29268853
  0.29288814  0.29293037  0.29325937  0.2939705   0.29587775  0.29599301
  0.29993303  0.30054662  0.30108891  0.30177346  0.30227171  0.30323222
  0.30469305  0.30504908  0.30534906  0.30582599  0.30631962  0.30812484
  0.30870243  0.30936745  0.30949437  0.31339814  0.31481757  0.31583467
  0.31637241  0.31711636  0.3188218   0.32064155  0.32077333  0.32117053
  0.32162405  0.32219372  0.32337457  0.32471657  0.32472003  0.327009
  0.32705797  0.32727276  0.32831945  0.32878776  0.3288358   0.33063056
  0.33182528  0.33321715  0.33495595  0.33571334  0.33648659  0.33844449
  0.33945885  0.34072388  0.341175    0.34245183  0.34290716  0.34300845
  0.34423545  0.34509656  0.34514323  0.3456425   0.34654156  0.34703057
  0.34800727  0.34818642  0.35134038  0.35158345  0.35362502  0.35429794
  0.35482841  0.35706541  0.35853354  0.35887462  0.3597647   0.36130487
  0.36152525  0.36324338  0.36407688  0.36430129  0.36434281  0.36458944
  0.36463064  0.36522707  0.36696355  0.36771648  0.36852911  0.36910478
  0.36937711  0.37002385  0.37187178  0.37207934  0.37262363  0.37343725
  0.37525796  0.37542214  0.37605902  0.37636071  0.37638241  0.37770461
  0.37813964  0.37929636  0.38031815  0.38031994  0.38219681  0.38286914
  0.38385295  0.38408271  0.38568958  0.38569349  0.38634566  0.38879819
  0.38886515  0.3890174   0.39029798  0.39116654  0.39172283  0.39175678
  0.3921784   0.39275096  0.39342374  0.39343087  0.3947659   0.39544909
  0.39567342  0.39585303  0.39820696  0.39890392  0.39960468  0.40207562
  0.40217738  0.4027713   0.4034895   0.40512423  0.40588994  0.40735962
  0.40773093  0.41021322  0.41094185  0.41174458  0.41256018  0.41427814
  0.41581229  0.41590442  0.41609796  0.4169578   0.42005153  0.4214126
  0.42262032  0.42264037  0.42538355  0.42579639  0.42682808  0.42697097
  0.43011705  0.43195675  0.43232694  0.4326783   0.43404417  0.43517438
  0.43542637  0.43788452  0.43853412  0.43974583  0.43989294  0.44087699
  0.4412215   0.45008245  0.45037277  0.45292333  0.45497204  0.45738109
  0.4585455   0.45889894  0.45916712  0.46281118  0.46741158  0.46750356
  0.46865584  0.46986296  0.47080108  0.47566604  0.47695494  0.48134051
  0.48180395  0.48394764  0.48464044  0.48849731  0.49036251  0.4915884
  0.49307099  0.49309006  0.49522913  0.50353794  0.50373809  0.50583574
  0.50673808  0.50942808  0.51118232  0.51501012  0.51584177  0.52523415
  0.52698525  0.53124663  0.53264021  0.53383466  0.53926137  0.54418396
  0.55892348  0.56169407  0.56343043  0.56363703  0.56419871  0.56839254
  0.56848405  0.56852346  0.56976907  0.57694945  0.578352    0.58008557
  0.58184487  0.58685042  0.58731327  0.59302978  0.59516774  0.59737877
  0.59762977  0.5993101   0.60071858  0.60321992  0.6032977   0.60587683
  0.60755809  0.60906309  0.61551516  0.61593853  0.61610037  0.61619333
  0.6179039   0.6207293   0.62135158  0.62487403  0.62514765  0.62625412
  0.6272171   0.62762934  0.63354879  0.63424721  0.63429266  0.63682633
  0.63929225  0.63999067  0.64198458  0.64380303  0.64456371  0.64663315
  0.65230109  0.6525715   0.65450197  0.65488021  0.65582353  0.65714454
  0.65804454  0.65898508  0.65936029  0.66003846  0.66049501  0.66261759
  0.66377235  0.66519671  0.66578192  0.66708603  0.66777584  0.67293409
  0.67551322  0.67593701  0.67658928  0.67809234  0.67867755  0.67986823
  0.6844106   0.68510584  0.68610013  0.68899405  0.68924755  0.69060973
  0.69451042  0.69643734  0.69931055  0.70035215  0.70130447  0.70280694
  0.70393091  0.7044688   0.70540934  0.70595304  0.70636976  0.70646272
  0.70904185  0.70998517  0.71166955  0.71298808  0.7138853   0.7142001
  0.71514342  0.71524677  0.7208841   0.72090134  0.72262096  0.72413836
  0.72671749  0.72803905  0.72925423  0.7301783   0.73127484  0.73148037
  0.73187574  0.73257346  0.7355749   0.73610721  0.73689197  0.73799744
  0.73961312  0.74219224  0.74411347  0.74445657  0.74477137  0.74501281
  0.74507613  0.74622369  0.74992962  0.7501978   0.75603119  0.75766699
  0.75867775  0.76116519  0.76192516  0.76216735  0.76474648  0.7676687
  0.77056262  0.77314175  0.77394739  0.77400683  0.77572087  0.77740213
  0.7779852   0.7783      0.78087912  0.78414173  0.78603738  0.78672086
  0.78771863  0.78955982  0.78962853  0.79119563  0.79345996  0.79377475
  0.79846409  0.80151213  0.80409125  0.80606106  0.81698688  0.82436713
  0.82458746  0.82566758  0.82698859  0.82944063  0.83297306  0.83340496
  0.83642971  0.83769465  0.83795389  0.83914563  0.84246334  0.85224389
  0.85248532  0.85794023  0.8594478   0.86072196  0.86345846  0.87457644
  0.87834394  0.87836058  0.8871347   0.88903565  0.88971382  0.8925581
  0.89487207  0.90355277  0.90606875  0.91360943  0.91577023  0.9231324
  0.92324245  0.92582158  0.92808591  0.92950797  0.93124498  0.93355896
  0.93524021  0.93613808  0.9370814   0.93871721  0.94129633  0.9426628
  0.94297759  0.94392404  0.94435593  0.94645458  0.94997703  0.95419196
  0.95477717  0.95677109  0.95739961  0.96287266  0.96450846  0.96974148
  0.97323774  0.97482496  0.98350566  0.98689043  0.98866391  0.99382216
  0.99640129  0.99792704  0.99811186  1.00319535  1.00413867  1.00495137
  1.0122951   1.01703429  1.02219254  1.02224112  1.03129638  1.06503799
  1.06866538]

  UserWarning,

2022-10-31 11:02:32,560:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60750608 -0.84421162 -0.3080757  -0.23445272 -0.23346599 -0.18720504
 -0.17394825 -0.15959539 -0.06070921 -0.05281708 -0.04781285 -0.03847156
 -0.0126326  -0.00631742  0.00949194  0.01127895  0.01487019  0.02167722
  0.06493001  0.07566155  0.07934392  0.08516388  0.10282866  0.10433722
  0.11220724  0.12334835  0.12348349  0.12473277  0.14426859  0.14673399
  0.14679398  0.14778631  0.15585848  0.16416492  0.1646608   0.17813856
  0.18534472  0.18998408  0.19150023  0.19224553  0.19348705  0.19663704
  0.19683221  0.19834162  0.20007908  0.20185613  0.20230863  0.20689069
  0.21133577  0.21249233  0.21250185  0.21483344  0.21546066  0.21635078
  0.21854151  0.22035012  0.22072916  0.222621    0.2229098   0.22347002
  0.22558294  0.2270389   0.22801154  0.22855193  0.22882645  0.23082798
  0.2357576   0.23600845  0.23606699  0.23788247  0.23981814  0.2415881
  0.24184759  0.24250718  0.24842027  0.24860947  0.24894411  0.249758
  0.2506074   0.25126232  0.25288634  0.25362301  0.25489044  0.25504598
  0.25623979  0.25639517  0.25649004  0.2573984   0.25746251  0.25846259
  0.2588365   0.25944706  0.26028289  0.26058704  0.26152299  0.26398338
  0.26407278  0.26436766  0.26480439  0.26502941  0.26629567  0.26662859
  0.26764916  0.26775131  0.2680741   0.26858178  0.26960763  0.27050709
  0.27121072  0.27194989  0.27207402  0.27267269  0.27365773  0.2744986
  0.27635398  0.27652292  0.27722699  0.27755336  0.27816048  0.27948253
  0.28399481  0.2877089   0.28774215  0.28787684  0.2885625   0.28878548
  0.29016835  0.29068092  0.29116108  0.2917339   0.29175831  0.29199707
  0.29223517  0.29282926  0.29330353  0.29573099  0.29622894  0.29630178
  0.2966847   0.29704304  0.29763063  0.30066234  0.30200128  0.30223694
  0.30301364  0.30330059  0.30349613  0.30367652  0.30439739  0.30559441
  0.30593089  0.30809233  0.31018169  0.31053766  0.31357613  0.31456424
  0.31615798  0.31618545  0.31767032  0.31958052  0.3216803   0.32265803
  0.32313283  0.32454034  0.32471255  0.32478809  0.3260495   0.32606951
  0.32672067  0.32723912  0.32728051  0.32818188  0.32872658  0.32928348
  0.32954267  0.32973583  0.3305887   0.33061101  0.33064835  0.33076591
  0.3326874   0.33377569  0.33385257  0.33392663  0.33555304  0.33607904
  0.33634398  0.33994482  0.34016919  0.34038641  0.34137225  0.34175589
  0.34189663  0.34200221  0.34281432  0.34298619  0.34378299  0.34443457
  0.3448892   0.34562403  0.34728947  0.34836812  0.34873628  0.3489332
  0.34919343  0.34979344  0.35108862  0.35191588  0.35240562  0.35258938
  0.35333611  0.3552483   0.35591559  0.35645237  0.35741814  0.35768328
  0.35799999  0.35856033  0.35993991  0.36040773  0.36142029  0.36170288
  0.36196947  0.36445415  0.36450265  0.36821776  0.369225    0.36938018
  0.36977612  0.37136872  0.37266499  0.37315825  0.37430881  0.37446511
  0.37636     0.37753995  0.37851037  0.38057139  0.38098305  0.3842502
  0.384346    0.38462658  0.38465731  0.38518272  0.38527659  0.38533332
  0.38702977  0.38747816  0.38759346  0.38761262  0.38921027  0.38936071
  0.39030356  0.39031166  0.39046124  0.39131719  0.39187833  0.39414713
  0.39578248  0.39595703  0.39651958  0.3971787   0.398487    0.39897474
  0.39897569  0.40003589  0.40126449  0.40159994  0.40248412  0.40272637
  0.40320349  0.4032735   0.40407735  0.40413688  0.40598697  0.40600939
  0.40617112  0.40762913  0.40876248  0.40896754  0.41295351  0.4140173
  0.41419919  0.41475464  0.41645958  0.41722427  0.41849519  0.41941261
  0.41958624  0.42181861  0.4221941   0.42262303  0.42380187  0.42434666
  0.42571813  0.42586994  0.42611336  0.42614551  0.42885931  0.42903844
  0.42927837  0.43511108  0.43516488  0.43875486  0.43915215  0.43980655
  0.44000994  0.44291982  0.44329164  0.44363148  0.44749468  0.44844996
  0.45322459  0.45606175  0.45857625  0.46709114  0.46829457  0.47457668
  0.47548637  0.47561822  0.47770854  0.47856873  0.47922949  0.48080281
  0.48289575  0.48295486  0.48343071  0.48427643  0.4849202   0.48754679
  0.49800359  0.50185324  0.50287623  0.50396622  0.50441431  0.51004941
  0.51049928  0.51511116  0.51525789  0.51886553  0.52013964  0.52172151
  0.52205916  0.53021081  0.53178306  0.53259803  0.5376627   0.53879522
  0.5388357   0.5408729   0.5459355   0.54656097  0.55560015  0.55842323
  0.56469122  0.57144842  0.57437077  0.57525294  0.57777574  0.57977531
  0.58465572  0.58643849  0.59163758  0.59897426  0.60103436  0.6011274
  0.60463057  0.60707582  0.61061721  0.61291652  0.61298966  0.61944915
  0.62018351  0.62315374  0.62437262  0.62485193  0.62572529  0.62674507
  0.62679875  0.62708251  0.62722438  0.62945497  0.62959684  0.632111
  0.63331135  0.63368387  0.6361022   0.63867199  0.64190333  0.6436422
  0.6441707   0.64857646  0.65132179  0.65201854  0.65332137  0.65555768
  0.66006672  0.66043873  0.66215331  0.66518364  0.66631436  0.66687909
  0.66714143  0.66852994  0.67020406  0.67026772  0.67030142  0.672301
  0.67467345  0.67645825  0.6770459   0.68137615  0.68179081  0.68220788
  0.68348626  0.68649194  0.68690859  0.6882503   0.68928105  0.69107947
  0.69297607  0.69772098  0.70077044  0.70246589  0.70551534  0.7059912
  0.70958325  0.71022917  0.71026025  0.71124628  0.71254109  0.71260162
  0.71300558  0.71404418  0.71432815  0.71455878  0.71497407  0.71567942
  0.71670061  0.7195162   0.71964363  0.72021802  0.72027094  0.72144551
  0.72322216  0.72619042  0.72627978  0.72683634  0.72920879  0.72961276
  0.72999349  0.73079346  0.73137847  0.73720597  0.73869861  0.73940396
  0.74302886  0.74466596  0.74561319  0.74818842  0.74933338  0.75001277
  0.75125288  0.75146383  0.75308007  0.75310342  0.75344098  0.75406405
  0.75703232  0.75767823  0.7580822   0.75940477  0.76005069  0.76123535
  0.76527649  0.76964406  0.76991338  0.77428541  0.77472964  0.77599999
  0.77903031  0.78140277  0.78163378  0.78293468  0.78377522  0.78547067
  0.78614767  0.78917106  0.79244446  0.79326503  0.79349604  0.79616041
  0.79660994  0.79800994  0.80038239  0.80179921  0.8032307   0.80431239
  0.80994293  0.81320447  0.81698957  0.81736245  0.81936202  0.8197349
  0.82173447  0.83122429  0.83461492  0.84661258  0.847584    0.8510649
  0.85902443  0.86309314  0.8716846   0.87780174  0.87945289  0.88117441
  0.89000635  0.89165025  0.89578201  0.89656271  0.9046335   0.90631576
  0.9072714   0.9120163   0.92026439  0.92625102  0.92669525  0.92862348
  0.93099593  0.93336838  0.93574084  0.94048574  0.94281642  0.94401177
  0.94993378  0.95234801  0.95272089  0.95472046  0.95705114  0.9574658
  0.95946537  0.96932806  0.97141654  0.97170052  0.97644542  0.97926211
  0.98119033  0.98180348  0.98314813  0.99068014  0.99133423  0.99155736
  0.99620575  0.99641108  0.9977975   1.00016996  1.00491486  1.00598492
  1.00700335  1.02086417  1.02267561  1.02488052  1.02731999  1.02794996
  1.04800584  1.05114505  1.05588996  1.0612661   1.11806734  1.26616715
  1.29663438  2.51866165]

  UserWarning,

2022-10-31 11:02:32,576:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-7.73119087e-01 -7.51434568e-01 -5.02883827e-01 -3.85864802e-01
 -2.76146539e-01 -2.56141016e-01 -1.87563167e-01 -1.65706464e-01
 -1.47288801e-01 -8.33930772e-02 -3.98721460e-02  8.16843935e-04
  2.24595810e-02  2.29873171e-02  4.31577725e-02  7.51877021e-02
  7.63480694e-02  8.10352480e-02  9.87306967e-02  1.05126414e-01
  1.16867488e-01  1.20029962e-01  1.29312070e-01  1.32726168e-01
  1.40373964e-01  1.42529818e-01  1.47276948e-01  1.56019645e-01
  1.56420135e-01  1.59526741e-01  1.61159603e-01  1.63440286e-01
  1.63553103e-01  1.66716317e-01  1.66997922e-01  1.70951295e-01
  1.71280461e-01  1.81455142e-01  1.85810518e-01  1.86696854e-01
  1.86978134e-01  1.88782314e-01  1.88924484e-01  1.89815518e-01
  1.90994689e-01  1.91098708e-01  1.96374360e-01  2.01257833e-01
  2.01681085e-01  2.02091619e-01  2.02148559e-01  2.07412840e-01
  2.15716067e-01  2.17902527e-01  2.18414655e-01  2.21302684e-01
  2.21446826e-01  2.23518058e-01  2.23689739e-01  2.24033067e-01
  2.24121004e-01  2.29750509e-01  2.30923511e-01  2.32956926e-01
  2.33583428e-01  2.34367355e-01  2.37635028e-01  2.40219962e-01
  2.40787064e-01  2.42261376e-01  2.42989606e-01  2.45079502e-01
  2.45642509e-01  2.46807010e-01  2.47966343e-01  2.49791821e-01
  2.49913696e-01  2.49943089e-01  2.49964622e-01  2.49975133e-01
  2.51342057e-01  2.53686735e-01  2.54513584e-01  2.55133827e-01
  2.56555366e-01  2.57024595e-01  2.57441299e-01  2.57767500e-01
  2.59115146e-01  2.61700704e-01  2.63320753e-01  2.65030646e-01
  2.65737146e-01  2.66174530e-01  2.66592563e-01  2.67965529e-01
  2.68452612e-01  2.68539557e-01  2.68687188e-01  2.69889740e-01
  2.71407864e-01  2.72080765e-01  2.72485102e-01  2.73999277e-01
  2.74974329e-01  2.76789423e-01  2.77754367e-01  2.79544876e-01
  2.80757008e-01  2.81030424e-01  2.82252795e-01  2.82811309e-01
  2.83049384e-01  2.84435551e-01  2.84950720e-01  2.84981436e-01
  2.85351857e-01  2.86169519e-01  2.86835996e-01  2.87514201e-01
  2.88553133e-01  2.89806167e-01  2.89914817e-01  2.91168345e-01
  2.92364857e-01  2.93857712e-01  2.96936806e-01  2.98693192e-01
  2.98999915e-01  2.99073131e-01  2.99198811e-01  2.99405365e-01
  2.99663869e-01  3.00020326e-01  3.00120684e-01  3.04163838e-01
  3.04480750e-01  3.05194523e-01  3.06702806e-01  3.06741964e-01
  3.08110886e-01  3.08628182e-01  3.10211813e-01  3.10349236e-01
  3.10519484e-01  3.10636053e-01  3.11579001e-01  3.11866120e-01
  3.11993123e-01  3.12676881e-01  3.15686126e-01  3.16305902e-01
  3.16887113e-01  3.18446131e-01  3.19332957e-01  3.19836403e-01
  3.19964331e-01  3.20337008e-01  3.20387880e-01  3.22150061e-01
  3.23926024e-01  3.24265701e-01  3.25307251e-01  3.28841962e-01
  3.30023295e-01  3.30965001e-01  3.30976603e-01  3.30990831e-01
  3.31529559e-01  3.31823897e-01  3.32274272e-01  3.32558619e-01
  3.35064771e-01  3.35959948e-01  3.36194873e-01  3.36370381e-01
  3.37366153e-01  3.38654142e-01  3.39436009e-01  3.39565228e-01
  3.40963668e-01  3.41010398e-01  3.41046284e-01  3.41634425e-01
  3.41822978e-01  3.42041996e-01  3.42237494e-01  3.43100501e-01
  3.43260684e-01  3.44063808e-01  3.45072186e-01  3.45723306e-01
  3.46247420e-01  3.46473531e-01  3.48281410e-01  3.48747962e-01
  3.49499637e-01  3.50023757e-01  3.51045994e-01  3.52791009e-01
  3.52900852e-01  3.52946430e-01  3.53502751e-01  3.53687494e-01
  3.54168724e-01  3.55059923e-01  3.55266827e-01  3.55714271e-01
  3.55843780e-01  3.55895158e-01  3.55979457e-01  3.56201691e-01
  3.57161553e-01  3.57336663e-01  3.57360738e-01  3.57729844e-01
  3.57936073e-01  3.60249196e-01  3.60821940e-01  3.61550486e-01
  3.61592796e-01  3.62724800e-01  3.63364789e-01  3.63781130e-01
  3.64762066e-01  3.65253767e-01  3.66714176e-01  3.67921206e-01
  3.67977919e-01  3.70458768e-01  3.71289886e-01  3.71884177e-01
  3.72233823e-01  3.73350788e-01  3.73781523e-01  3.73909379e-01
  3.73974434e-01  3.75350718e-01  3.76254328e-01  3.78589013e-01
  3.79592043e-01  3.79915588e-01  3.81541223e-01  3.82656394e-01
  3.83011157e-01  3.83058684e-01  3.85391071e-01  3.85464656e-01
  3.86233226e-01  3.87181455e-01  3.87651666e-01  3.88822492e-01
  3.89041105e-01  3.89232780e-01  3.90503228e-01  3.93579226e-01
  3.93803131e-01  3.95776752e-01  3.96242492e-01  3.96421420e-01
  3.98337526e-01  3.98879254e-01  4.00531823e-01  4.00702446e-01
  4.03432663e-01  4.03687037e-01  4.04152777e-01  4.04419705e-01
  4.04464802e-01  4.05048552e-01  4.05397995e-01  4.07397202e-01
  4.08406996e-01  4.08960560e-01  4.09426301e-01  4.10993277e-01
  4.12071874e-01  4.14715817e-01  4.14800024e-01  4.16020697e-01
  4.17885708e-01  4.19047343e-01  4.20382274e-01  4.20780868e-01
  4.24668494e-01  4.26558524e-01  4.31313343e-01  4.31796612e-01
  4.32186515e-01  4.35834439e-01  4.36096550e-01  4.37599544e-01
  4.39056217e-01  4.40973259e-01  4.42980791e-01  4.47802145e-01
  4.48303936e-01  4.50163996e-01  4.53612450e-01  4.55324026e-01
  4.63482778e-01  4.64274168e-01  4.64462185e-01  4.65296721e-01
  4.66848089e-01  4.68596264e-01  4.68705372e-01  4.69333217e-01
  4.70104503e-01  4.70865407e-01  4.72526744e-01  4.72691756e-01
  4.81747321e-01  4.83448000e-01  4.88683198e-01  4.92504095e-01
  4.95951960e-01  5.01157585e-01  5.05143308e-01  5.07399261e-01
  5.08395229e-01  5.10609240e-01  5.15012360e-01  5.19101122e-01
  5.22262829e-01  5.22724807e-01  5.23101808e-01  5.30745489e-01
  5.33270525e-01  5.37637681e-01  5.40038707e-01  5.41692295e-01
  5.44999549e-01  5.49892132e-01  5.50913632e-01  5.52139277e-01
  5.52444361e-01  5.53057324e-01  5.54214924e-01  5.57629216e-01
  5.58849440e-01  5.61132800e-01  5.62665615e-01  5.66082433e-01
  5.69043085e-01  5.69636296e-01  5.71679847e-01  5.71945042e-01
  5.79590132e-01  5.80220965e-01  5.81062085e-01  5.83184302e-01
  5.90607090e-01  5.92773940e-01  5.93064070e-01  5.94245894e-01
  5.99519417e-01  6.00295453e-01  6.05957748e-01  6.08594510e-01
  6.11231271e-01  6.12426128e-01  6.14320476e-01  6.16504795e-01
  6.16823006e-01  6.21778318e-01  6.24415080e-01  6.25124003e-01
  6.27404814e-01  6.31510388e-01  6.34188659e-01  6.34962126e-01
  6.35460552e-01  6.38347412e-01  6.39411758e-01  6.40734075e-01
  6.45509173e-01  6.47322043e-01  6.47398591e-01  6.48145934e-01
  6.49303899e-01  6.50316955e-01  6.50782696e-01  6.51576285e-01
  6.52645990e-01  6.53419457e-01  6.54553972e-01  6.58692981e-01
  6.61329742e-01  6.63966504e-01  6.66055955e-01  6.70515168e-01
  6.71839346e-01  6.71876789e-01  6.72195000e-01  6.72416811e-01
  6.73479467e-01  6.73517318e-01  6.73740083e-01  6.74513551e-01
  6.75107926e-01  6.75623195e-01  6.77150312e-01  6.79787074e-01
  6.85954022e-01  6.86873467e-01  6.87697359e-01  6.89966123e-01
  6.91443771e-01  6.92895515e-01  6.94217833e-01  6.95607644e-01
  6.95685569e-01  6.95699788e-01  6.96106070e-01  6.96401233e-01
  6.96717288e-01  6.97420514e-01  6.98013964e-01  7.00057276e-01
  7.02694037e-01  7.04016355e-01  7.04946628e-01  7.05330799e-01
  7.06154690e-01  7.06948256e-01  7.07727436e-01  7.09289878e-01
  7.10604322e-01  7.12406493e-01  7.12498360e-01  7.14064975e-01
  7.14858564e-01  7.15570246e-01  7.15877845e-01  7.17410661e-01
  7.20344023e-01  7.20685628e-01  7.21151369e-01  7.21229294e-01
  7.22003702e-01  7.22473686e-01  7.23084905e-01  7.25405611e-01
  7.26424892e-01  7.28223102e-01  7.28358428e-01  7.29061654e-01
  7.30271535e-01  7.31429500e-01  7.33020733e-01  7.33680272e-01
  7.34335177e-01  7.35493142e-01  7.35952657e-01  7.38402804e-01
  7.38589419e-01  7.43862942e-01  7.44015800e-01  7.45158804e-01
  7.45510919e-01  7.46204541e-01  7.47165770e-01  7.47523831e-01
  7.51489692e-01  7.51773227e-01  7.54562846e-01  7.57046750e-01
  7.59683512e-01  7.61559370e-01  7.62873814e-01  7.64661872e-01
  7.65321411e-01  7.65343025e-01  7.66887474e-01  7.67298634e-01
  7.67593797e-01  7.69126612e-01  7.70230558e-01  7.70513685e-01
  7.72867320e-01  7.76252606e-01  7.86051128e-01  7.88687890e-01
  7.91324651e-01  7.92799826e-01  7.99234936e-01  8.01871698e-01
  8.04508459e-01  8.06539701e-01  8.10712069e-01  8.11576271e-01
  8.12418744e-01  8.15845805e-01  8.17308604e-01  8.17692268e-01
  8.21949686e-01  8.22350529e-01  8.23464217e-01  8.24075436e-01
  8.26342436e-01  8.33512837e-01  8.36140672e-01  8.36149599e-01
  8.46230905e-01  8.48867666e-01  8.51504428e-01  8.59697520e-01
  8.63015641e-01  8.64474482e-01  8.65077853e-01  8.77011305e-01
  8.88261397e-01  8.90974283e-01  8.96718992e-01  8.99355754e-01
  9.06718729e-01  9.09518629e-01  9.09935486e-01  9.12539562e-01
  9.18311511e-01  9.20948273e-01  9.22227949e-01  9.23086609e-01
  9.23626631e-01  9.28360132e-01  9.28858558e-01  9.29108656e-01
  9.31179099e-01  9.33633655e-01  9.36768843e-01  9.41543940e-01
  9.42042366e-01  9.45798158e-01  9.46817463e-01  9.49952651e-01
  9.55226174e-01  9.57364510e-01  9.60001271e-01  9.65773221e-01
  9.66590779e-01  9.67087665e-01  9.72289582e-01  9.72637769e-01
  9.73683506e-01  9.77911293e-01  9.78957029e-01  9.79805733e-01
  9.81593791e-01  9.82205009e-01  9.82405646e-01  9.84230552e-01
  9.84979077e-01  9.85502464e-01  9.86867314e-01  9.89196452e-01
  9.90824320e-01  9.91675096e-01  9.92140837e-01  1.00043711e+00
  1.00268788e+00  1.00485890e+00  1.01059817e+00  1.01268762e+00
  1.01990601e+00  1.02114522e+00  1.03514088e+00  1.04019074e+00
  1.04432876e+00  1.31622560e+00  1.44533592e+00]

  UserWarning,

2022-10-31 11:02:33,393:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4106432  -0.40131823 -0.35261898 -0.23299735 -0.21444307 -0.18296433
 -0.13885849 -0.13142853 -0.10105376 -0.03473708 -0.02818374 -0.0194759
 -0.01093454 -0.00586488  0.02752858  0.0399851   0.0454719   0.04803761
  0.05082278  0.05263528  0.0789575   0.07903334  0.08710848  0.08881402
  0.10114859  0.10432017  0.10898431  0.11706806  0.12689001  0.1283329
  0.13307604  0.13433481  0.1376852   0.13867701  0.14912017  0.15343985
  0.1534842   0.15451012  0.15651104  0.15653556  0.16253026  0.16372426
  0.16754243  0.18865565  0.19243534  0.19311153  0.19426708  0.19712022
  0.20339877  0.20348327  0.20470718  0.20504853  0.20864831  0.20935819
  0.21049551  0.21220841  0.21545213  0.21556539  0.21568886  0.21757685
  0.22068849  0.22096726  0.22167896  0.2222569   0.22229638  0.2229421
  0.22374995  0.22507046  0.22638921  0.22944369  0.2307092   0.23093642
  0.23220598  0.2354758   0.23598922  0.2366699   0.23714539  0.23778019
  0.23827292  0.23860712  0.24313975  0.2442204   0.24445982  0.24577852
  0.24675473  0.2469225   0.24946462  0.25171647  0.25328449  0.25356874
  0.25636398  0.25645534  0.25894554  0.26138083  0.26540976  0.26831453
  0.26879317  0.2701771   0.27076042  0.27105065  0.27154388  0.2727519
  0.27324657  0.27349716  0.27433863  0.27440154  0.27504837  0.27741528
  0.27760293  0.27826632  0.2798925   0.2812407   0.28231444  0.2843083
  0.28496696  0.28654196  0.28830458  0.28852345  0.28873265  0.28891195
  0.28911975  0.28989269  0.29149131  0.29154132  0.29163293  0.2917512
  0.29227915  0.29278825  0.29362803  0.29373089  0.29435115  0.2952679
  0.2956108   0.29772652  0.29840171  0.29850787  0.29857437  0.299234
  0.29925025  0.29941388  0.29952028  0.30024467  0.30082877  0.30103045
  0.3010332   0.30192556  0.30260246  0.30267848  0.30310535  0.30404994
  0.30467536  0.30476523  0.30570504  0.30578787  0.30665327  0.30719297
  0.30747594  0.30839616  0.30957443  0.30975214  0.31033631  0.31145182
  0.31271336  0.31384079  0.31516266  0.31561576  0.31568192  0.31731052
  0.31791294  0.31847418  0.31963915  0.32170443  0.32222866  0.3228453
  0.32350558  0.3237165   0.32378217  0.3260509   0.32659135  0.32862938
  0.3290624   0.33002924  0.33097663  0.33115974  0.33162076  0.33291064
  0.3329665   0.33340028  0.33585173  0.3369787   0.33837245  0.3384365
  0.33874071  0.33875442  0.33985085  0.34033037  0.34054604  0.34169061
  0.34305652  0.34381094  0.34403542  0.34422728  0.34662468  0.34665198
  0.34714261  0.34904421  0.34975724  0.35069687  0.35144427  0.35146096
  0.35190441  0.35255177  0.35405172  0.35484484  0.3555142   0.35589995
  0.35677711  0.35683954  0.35718296  0.3582759   0.35879627  0.35917961
  0.36146872  0.36304422  0.36352769  0.36455344  0.36491354  0.36495438
  0.36520073  0.36651732  0.36690559  0.36691655  0.36756205  0.3688487
  0.36901921  0.36920965  0.36926791  0.37297169  0.374149    0.3745814
  0.37576277  0.37669263  0.37675875  0.3787315   0.37917807  0.37963735
  0.38175529  0.38190605  0.3829224   0.38393822  0.38545373  0.38558233
  0.38679878  0.38798506  0.38864697  0.39003742  0.39121422  0.3920099
  0.39299486  0.39304772  0.39317364  0.39335191  0.39339965  0.39465289
  0.39546236  0.39573022  0.39811038  0.39811328  0.39913898  0.39941402
  0.39985984  0.40064171  0.40311278  0.40317304  0.4044167   0.40490789
  0.40565151  0.40570437  0.40594114  0.40613164  0.40671949  0.40788615
  0.4082357   0.41076703  0.41138636  0.41148731  0.41224251  0.4198815
  0.42077142  0.42107925  0.42181616  0.42325933  0.42391774  0.42633314
  0.42689986  0.42748383  0.43039799  0.43481894  0.43671808  0.43814831
  0.4393802   0.44282126  0.44330383  0.44678659  0.4480482   0.44938987
  0.44983643  0.45089782  0.45236776  0.45489909  0.45743042  0.45937696
  0.45996175  0.46102314  0.46317287  0.46500284  0.4660858   0.4670366
  0.47333869  0.47360626  0.47507425  0.47581718  0.48291069  0.48418108
  0.48489516  0.49260319  0.49467164  0.49724242  0.50019718  0.51394039
  0.51467759  0.5153415   0.51754402  0.51760404  0.51781185  0.52161254
  0.52224253  0.5225805   0.53310447  0.5356358   0.54570826  0.54576112
  0.54783255  0.55045833  0.55648749  0.56156683  0.56356718  0.57015516
  0.5760871   0.5762136   0.58274071  0.58727642  0.58732806  0.58736185
  0.590885    0.59127508  0.59311762  0.59762229  0.59967729  0.6011428
  0.60385867  0.6046013   0.60529008  0.60774632  0.61023953  0.61035274
  0.61069867  0.61405705  0.61658838  0.6185137   0.61911971  0.62165104
  0.6250194   0.62863902  0.62924503  0.63430769  0.63686776  0.63937035
  0.64190168  0.64432006  0.64443301  0.64696434  0.64949567  0.64976044
  0.65133821  0.652027    0.65468999  0.65757133  0.66140663  0.66431315
  0.66468365  0.66721498  0.66974631  0.67018466  0.67019612  0.6705799
  0.67084575  0.67227764  0.67480897  0.67611041  0.67665151  0.6773403
  0.67853615  0.68051792  0.68165244  0.68321921  0.6842455   0.68432828
  0.68746562  0.69183949  0.69192227  0.6938516   0.69482358  0.69500675
  0.69505961  0.6958932   0.69690215  0.69698493  0.69759094  0.69779603
  0.69943348  0.70012227  0.70196481  0.7026536   0.70750672  0.71024759
  0.71462146  0.71532807  0.71724329  0.71853691  0.71894102  0.72626916
  0.72818486  0.72869585  0.72987333  0.73127896  0.73205306  0.73297752
  0.73316122  0.73772173  0.73844737  0.74013692  0.74145715  0.74160336
  0.74278439  0.74559791  0.74651981  0.74662204  0.7541138   0.75577655
  0.75664513  0.75691097  0.75891955  0.76290364  0.76303503  0.76677045
  0.76930178  0.77011     0.77051288  0.77164198  0.77183311  0.77304421
  0.77436444  0.77689577  0.7794271   0.78268395  0.78276665  0.78448976
  0.7855892   0.78631043  0.78782931  0.79208375  0.79456222  0.79714641
  0.79960421  0.79967774  0.80077718  0.80330851  0.80468754  0.80487206
  0.80739481  0.80859924  0.80952941  0.81233439  0.81343383  0.81465618
  0.81602824  0.81635761  0.81734419  0.81971884  0.82102782  0.82223892
  0.82357741  0.82765403  0.83075822  0.83115314  0.8350635   0.83535716
  0.83764769  0.83874713  0.84458568  0.84634112  0.84913795  0.84964834
  0.85151297  0.85646644  0.85977366  0.86225213  0.86361398  0.86508378
  0.86562398  0.86757719  0.87071524  0.8800243   0.88130285  0.89091302
  0.89254269  0.89521229  0.90027495  0.901568    0.90280628  0.90390572
  0.90786894  0.90896838  0.91040027  0.9129316   0.91546293  0.91799426
  0.9190937   0.92162503  0.92300406  0.92305692  0.92811958  0.92921902
  0.93065091  0.93306359  0.93571357  0.93681301  0.93762124  0.94077623
  0.94330756  0.944407    0.94465446  0.94583889  0.94837022  0.95521031
  0.95849554  0.95959498  0.96102687  0.96212631  0.96343955  0.96590902
  0.96608953  0.96875252  0.97225163  0.97478296  0.97731429  0.97984562
  0.98065385  0.98237695  0.98490828  0.98596967  0.98997094  0.99250227
  0.99584183  0.99756493  1.00262759  1.00793771  1.00985109  1.01934439
  1.0210751   1.02250774  1.28330982]

  UserWarning,

2022-10-31 11:02:33,424:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95227898 -0.78157472 -0.48119186 -0.39358498 -0.33397497 -0.3143276
 -0.31081464 -0.30887746 -0.26864586 -0.25947918 -0.25675931 -0.25581714
 -0.12154502 -0.11867552 -0.10201774 -0.09434295 -0.05578859 -0.04737465
 -0.0340599  -0.03026718  0.00295308  0.01119338  0.02331091  0.03058403
  0.03457226  0.04094351  0.05513802  0.06027344  0.06781999  0.06890992
  0.08334548  0.0855771   0.08623848  0.09402878  0.10131812  0.10714207
  0.10982025  0.12698994  0.12848897  0.12851052  0.13651895  0.15065335
  0.15644648  0.16347079  0.16985765  0.1713272   0.1763457   0.17796296
  0.17812915  0.1811501   0.18671478  0.19215118  0.19730775  0.20278156
  0.20630153  0.20713441  0.20718308  0.21086975  0.2111338   0.2113237
  0.21237601  0.21348378  0.21368212  0.21397818  0.21398956  0.21697067
  0.21813487  0.21911686  0.21952921  0.22038451  0.22187544  0.22452416
  0.22743713  0.23326631  0.23693902  0.23780146  0.23791093  0.24075849
  0.24089617  0.24210658  0.24323937  0.2437666   0.24654752  0.24673273
  0.24934707  0.25048647  0.25055818  0.25118449  0.25135534  0.25167388
  0.25416154  0.2572368   0.25920414  0.25922009  0.25967082  0.26088765
  0.26420897  0.26484771  0.26487714  0.26583555  0.26691453  0.26774033
  0.26784978  0.27037406  0.27110047  0.27146809  0.27254407  0.27367048
  0.27371797  0.27433296  0.2744804   0.27520669  0.27556389  0.27564164
  0.27662147  0.27684452  0.27808654  0.27852799  0.27855239  0.28004997
  0.28070116  0.28080504  0.28318103  0.28390424  0.28500708  0.28542284
  0.28570032  0.28578395  0.28637646  0.28745258  0.28756513  0.28798319
  0.28890387  0.29151648  0.29184559  0.29197619  0.29205054  0.29292925
  0.29324298  0.29389413  0.29538328  0.29688802  0.29793604  0.29875405
  0.29921306  0.29935219  0.29962617  0.29979959  0.30356149  0.30516118
  0.30575447  0.30815266  0.30922841  0.3116514   0.31173568  0.31324541
  0.31385718  0.31463023  0.31484838  0.3152341   0.3158038   0.31620544
  0.31917379  0.31964278  0.32027617  0.32089717  0.32098192  0.32176669
  0.32268656  0.32299074  0.3270686   0.32833841  0.32890927  0.33023999
  0.33048228  0.33213933  0.33310957  0.33499579  0.33613371  0.33711267
  0.33739478  0.33841519  0.33864253  0.34003519  0.34118161  0.34194122
  0.34227995  0.34389656  0.34408943  0.3442494   0.34517836  0.34757599
  0.34794175  0.34799752  0.3491925   0.35119877  0.35131262  0.35250175
  0.3526402   0.35299447  0.35487027  0.35518106  0.35627004  0.35681399
  0.36107015  0.36162633  0.36546676  0.36844694  0.36901218  0.37430334
  0.37450897  0.37520745  0.37535421  0.37543616  0.37822891  0.37857501
  0.37891588  0.37936551  0.38009362  0.38068682  0.38069262  0.38082769
  0.38203874  0.38229444  0.3823644   0.38268126  0.38323905  0.38435
  0.38484162  0.38491941  0.38498733  0.38608722  0.38735447  0.38806064
  0.38867864  0.38909332  0.38917121  0.39002136  0.39025385  0.39111702
  0.39185343  0.39225715  0.39239647  0.39433048  0.39512408  0.3952011
  0.39567329  0.39572293  0.39649285  0.39658642  0.39717996  0.39907679
  0.40373244  0.40379416  0.40385434  0.40556058  0.40647533  0.40815045
  0.40883384  0.40936168  0.40958605  0.4097172   0.41003058  0.41050934
  0.41074032  0.41180908  0.41193987  0.41238222  0.41361783  0.41387682
  0.41516071  0.41580234  0.41625742  0.41685202  0.41697657  0.41888822
  0.41926956  0.42269748  0.42387567  0.42508147  0.42666823  0.42700553
  0.43042906  0.43227951  0.43372232  0.43561292  0.43859385  0.44118372
  0.44636346  0.44720305  0.44793021  0.44831186  0.45008131  0.45024764
  0.45183455  0.45439141  0.4544545   0.45599504  0.45671312  0.45704775
  0.45855279  0.4592358   0.45931281  0.45953335  0.46114279  0.46331229
  0.46393427  0.46593097  0.48341917  0.48406006  0.48603502  0.48838191
  0.48841606  0.49324078  0.49547458  0.49889312  0.50139247  0.50218916
  0.5025018   0.50455463  0.5047882   0.5073689   0.50827497  0.50856799
  0.51082375  0.5117249   0.51560162  0.52083044  0.52194247  0.53175142
  0.53218206  0.53446147  0.53844735  0.54620594  0.55003864  0.55041549
  0.55344863  0.55474714  0.55514096  0.55579549  0.55657644  0.5634996
  0.56826245  0.56874316  0.57324264  0.57424095  0.5757612   0.57593735
  0.58111709  0.58370696  0.58558421  0.59111826  0.59329774  0.59406644
  0.59455072  0.59478945  0.59784243  0.60683873  0.60692465  0.6070158
  0.61210439  0.61737528  0.62773476  0.63097982  0.63282335  0.6329145
  0.63550437  0.63809424  0.63869061  0.64658345  0.64902554  0.6510436
  0.65136182  0.65212255  0.65241563  0.65260736  0.65355645  0.65363347
  0.65622334  0.65646139  0.65881321  0.66076803  0.66219969  0.66274076
  0.6633579   0.66399295  0.66454867  0.66478956  0.66658282  0.66737741
  0.66737943  0.66756849  0.66917269  0.67007698  0.67011346  0.67176256
  0.67435244  0.67514904  0.6757454   0.67773891  0.67889712  0.67921111
  0.67953218  0.68291866  0.68407686  0.68467322  0.68616371  0.6880984
  0.69303725  0.69327814  0.69443635  0.69827036  0.69845788  0.6988723
  0.69961609  0.70060253  0.70220596  0.70227068  0.70284101  0.70353867
  0.70363762  0.70535387  0.70543089  0.70622749  0.70808175  0.70881736
  0.71312348  0.7132005   0.71399711  0.71512579  0.71515531  0.71878671
  0.72067615  0.72254787  0.7321262   0.73704755  0.73731269  0.73990256
  0.74023315  0.74126622  0.74240867  0.7450823   0.74684843  0.74767217
  0.7502553   0.75026204  0.7527749   0.75320248  0.7591061   0.75918312
  0.76043284  0.76152581  0.7632114   0.76580127  0.76799425  0.76896657
  0.76928462  0.77098101  0.77330182  0.77496268  0.77706503  0.7805718
  0.783188    0.7834025   0.78372206  0.78581712  0.78652024  0.78911011
  0.79081862  0.79494503  0.79584778  0.79687972  0.80205946  0.80464933
  0.8072392   0.80982907  0.81043699  0.81347714  0.81448763  0.81500881
  0.82277843  0.82404248  0.82558919  0.8316887   0.83613425  0.83686844
  0.83831765  0.84011124  0.84164171  0.84722792  0.84729034  0.85836811
  0.86276714  0.86487353  0.86802143  0.86884036  0.8788818   0.8921492
  0.90153819  0.90161521  0.90242937  0.90316355  0.90357002  0.90420508
  0.91126495  0.91197469  0.9127713   0.91662844  0.91707742  0.91715443
  0.9197443   0.92294102  0.93002677  0.9320586   0.9378734   0.93811738
  0.94038625  0.94234339  0.94241808  0.94269483  0.94305314  0.94493094
  0.94500795  0.94564301  0.94625768  0.94759783  0.9501877   0.95139818
  0.95402729  0.95536744  0.95795731  0.95859236  0.96054718  0.96306004
  0.96636198  0.96697664  0.96895185  0.96953001  0.97154172  0.97415172
  0.9760864   0.97867627  0.98126615  0.98232159  0.98385602  0.98644589
  0.98903576  0.99287535  0.9942155   0.99995328  1.00162288  1.00198511
  1.00386723  1.00457498  1.00845061  1.01262134  1.02072887  1.0239538
  1.02744795  1.06734655  1.06918805  1.07454965  1.22456555  1.25484376
  1.26811931  1.32706393  1.39296634]

  UserWarning,

2022-10-31 11:02:33,424:INFO:Calculating mean and std
2022-10-31 11:02:33,424:INFO:Creating metrics dataframe
2022-10-31 11:02:33,441:INFO:Uploading results into container
2022-10-31 11:02:33,441:INFO:Uploading model into container now
2022-10-31 11:02:33,441:INFO:master_model_container: 15
2022-10-31 11:02:33,441:INFO:display_container: 2
2022-10-31 11:02:33,441:INFO:BayesianRidge()
2022-10-31 11:02:33,441:INFO:create_model() successfully completed......................................
2022-10-31 11:02:33,544:WARNING:create_model() for BayesianRidge() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:33,544:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:33,544:INFO:Initializing create_model()
2022-10-31 11:02:33,544:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:33,544:INFO:Checking exceptions
2022-10-31 11:02:33,544:INFO:Importing libraries
2022-10-31 11:02:33,544:INFO:Copying training dataset
2022-10-31 11:02:33,559:INFO:Defining folds
2022-10-31 11:02:33,559:INFO:Declaring metric variables
2022-10-31 11:02:33,559:INFO:Importing untrained model
2022-10-31 11:02:33,559:INFO:Bayesian Ridge Imported successfully
2022-10-31 11:02:33,559:INFO:Starting cross validation
2022-10-31 11:02:33,559:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:35,638:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.68684337e-01 -4.16429792e-01 -3.47789102e-01 -3.05572986e-01
 -1.94576249e-01 -1.40155912e-01 -1.24286768e-01 -6.34578237e-02
 -3.27782442e-02 -3.24759866e-02 -2.66920852e-02 -6.60795114e-03
  1.62577994e-03  1.79327474e-02  2.83155712e-02  3.84570555e-02
  4.40216364e-02  4.57306973e-02  4.84423483e-02  4.88716041e-02
  5.79436589e-02  5.80979983e-02  7.08180104e-02  8.55664035e-02
  8.95545230e-02  1.02675368e-01  1.04971186e-01  1.06753377e-01
  1.08569972e-01  1.11512042e-01  1.13170422e-01  1.23691900e-01
  1.29472003e-01  1.30190550e-01  1.32399452e-01  1.36655946e-01
  1.37441671e-01  1.42192520e-01  1.43775919e-01  1.51659845e-01
  1.52967716e-01  1.53355406e-01  1.60523082e-01  1.61580802e-01
  1.63758811e-01  1.65431265e-01  1.67054134e-01  1.70937268e-01
  1.71325732e-01  1.73462915e-01  1.76078661e-01  1.84889012e-01
  1.85392016e-01  1.86549204e-01  1.87871043e-01  1.89284757e-01
  1.92577367e-01  1.94116157e-01  1.94772687e-01  1.95243444e-01
  1.98919554e-01  2.00487656e-01  2.02610178e-01  2.02875533e-01
  2.04559525e-01  2.09691791e-01  2.11042806e-01  2.14338212e-01
  2.17260020e-01  2.17591643e-01  2.21191130e-01  2.22034338e-01
  2.22481439e-01  2.23551548e-01  2.26399590e-01  2.29335951e-01
  2.36731058e-01  2.38123877e-01  2.38477432e-01  2.38574402e-01
  2.38659289e-01  2.47601336e-01  2.48068406e-01  2.48749633e-01
  2.49223884e-01  2.49227714e-01  2.50686150e-01  2.51060421e-01
  2.52628328e-01  2.54412437e-01  2.55201096e-01  2.57464802e-01
  2.58150766e-01  2.58236299e-01  2.58565848e-01  2.59033143e-01
  2.59153088e-01  2.62199819e-01  2.63224479e-01  2.65677534e-01
  2.65895488e-01  2.67060157e-01  2.68464583e-01  2.68730824e-01
  2.68820115e-01  2.70346831e-01  2.71484410e-01  2.72497803e-01
  2.72838946e-01  2.73561591e-01  2.74010482e-01  2.75176263e-01
  2.75542492e-01  2.76500256e-01  2.76957767e-01  2.77346547e-01
  2.77786942e-01  2.79212771e-01  2.79323543e-01  2.79900068e-01
  2.82772732e-01  2.83847216e-01  2.84170905e-01  2.85156239e-01
  2.85940838e-01  2.86311044e-01  2.87200559e-01  2.87431396e-01
  2.87766381e-01  2.88998454e-01  2.89196023e-01  2.89482509e-01
  2.90436913e-01  2.90512169e-01  2.91966734e-01  2.92021375e-01
  2.92964692e-01  2.95458279e-01  2.97268107e-01  2.97487108e-01
  2.97740613e-01  2.98353274e-01  2.98968725e-01  2.99843851e-01
  3.00404631e-01  3.00708902e-01  3.00769059e-01  3.01121714e-01
  3.01225322e-01  3.03347907e-01  3.03738592e-01  3.03922374e-01
  3.04314497e-01  3.04827964e-01  3.06344775e-01  3.06934325e-01
  3.07184370e-01  3.08168869e-01  3.08390532e-01  3.10567081e-01
  3.10581750e-01  3.11288642e-01  3.11922929e-01  3.11993378e-01
  3.13923040e-01  3.14736424e-01  3.16232107e-01  3.17077035e-01
  3.18334109e-01  3.21438011e-01  3.22433925e-01  3.25909764e-01
  3.26586649e-01  3.27533933e-01  3.28041984e-01  3.28523326e-01
  3.29254298e-01  3.29650582e-01  3.30277208e-01  3.30583598e-01
  3.30776744e-01  3.33636909e-01  3.34076981e-01  3.35107802e-01
  3.35922734e-01  3.36156313e-01  3.37721027e-01  3.38064737e-01
  3.38177010e-01  3.38376030e-01  3.39334313e-01  3.39531201e-01
  3.42003521e-01  3.44823200e-01  3.45861011e-01  3.47054701e-01
  3.47260606e-01  3.48337743e-01  3.48563790e-01  3.48715589e-01
  3.48963884e-01  3.52427990e-01  3.52638203e-01  3.53260570e-01
  3.53437790e-01  3.55024007e-01  3.56239282e-01  3.56698630e-01
  3.56759527e-01  3.57118347e-01  3.57985487e-01  3.58467505e-01
  3.58819917e-01  3.58823730e-01  3.59598778e-01  3.61008343e-01
  3.61634614e-01  3.62534237e-01  3.62969590e-01  3.63390266e-01
  3.65422650e-01  3.69017451e-01  3.69424422e-01  3.69490062e-01
  3.70692979e-01  3.71156260e-01  3.72906483e-01  3.74599073e-01
  3.76063299e-01  3.76269854e-01  3.77602325e-01  3.77603765e-01
  3.78758550e-01  3.79017276e-01  3.80509813e-01  3.80744099e-01
  3.80843038e-01  3.82979800e-01  3.83287881e-01  3.84660685e-01
  3.85138598e-01  3.85469224e-01  3.85953825e-01  3.86042820e-01
  3.86813955e-01  3.87313770e-01  3.87943203e-01  3.88508288e-01
  3.89434859e-01  3.89946838e-01  3.90762980e-01  3.91318860e-01
  3.92031957e-01  3.92339519e-01  3.92439041e-01  3.92497940e-01
  3.93695800e-01  3.94148070e-01  3.94302354e-01  3.94542215e-01
  3.94718187e-01  3.95391393e-01  3.95642728e-01  3.95864897e-01
  3.96811529e-01  3.97132619e-01  3.97409442e-01  3.97486950e-01
  4.00141489e-01  4.00367296e-01  4.01519455e-01  4.04641918e-01
  4.04729538e-01  4.05343159e-01  4.06129833e-01  4.08289160e-01
  4.09313384e-01  4.10260462e-01  4.10572260e-01  4.10856074e-01
  4.13039435e-01  4.14287527e-01  4.14821168e-01  4.16040207e-01
  4.16201682e-01  4.16976437e-01  4.17091296e-01  4.20173847e-01
  4.20398428e-01  4.21895097e-01  4.23141960e-01  4.23201055e-01
  4.23249323e-01  4.23575572e-01  4.24378473e-01  4.25540553e-01
  4.27392095e-01  4.28685495e-01  4.32329751e-01  4.32991476e-01
  4.33023144e-01  4.35217787e-01  4.36544738e-01  4.37837287e-01
  4.41145712e-01  4.44459349e-01  4.53214476e-01  4.53786687e-01
  4.55889216e-01  4.56905363e-01  4.57023303e-01  4.58410965e-01
  4.59446261e-01  4.59612952e-01  4.60215236e-01  4.62691470e-01
  4.63426980e-01  4.63443850e-01  4.64959983e-01  4.65573304e-01
  4.66301997e-01  4.66538038e-01  4.66806074e-01  4.67222738e-01
  4.67260655e-01  4.67617924e-01  4.69112714e-01  4.69711736e-01
  4.73505228e-01  4.74130846e-01  4.75131803e-01  4.81452779e-01
  4.84089557e-01  4.88062680e-01  4.91665514e-01  4.94399549e-01
  4.95453826e-01  4.98470157e-01  5.04070756e-01  5.05359469e-01
  5.06287112e-01  5.07236169e-01  5.08071120e-01  5.08482543e-01
  5.09615616e-01  5.10039296e-01  5.10088579e-01  5.13061120e-01
  5.13188831e-01  5.28441120e-01  5.30291918e-01  5.31429073e-01
  5.34597349e-01  5.40475232e-01  5.47974688e-01  5.48662630e-01
  5.49683638e-01  5.50834175e-01  5.50906003e-01  5.51451847e-01
  5.54788933e-01  5.55954951e-01  5.57751263e-01  5.61516519e-01
  5.61752607e-01  5.62167652e-01  5.62168440e-01  5.62640781e-01
  5.62891727e-01  5.63070269e-01  5.64736711e-01  5.66034179e-01
  5.68156784e-01  5.68230653e-01  5.76258197e-01  5.77296609e-01
  5.79863600e-01  5.81490359e-01  5.83718268e-01  5.85084625e-01
  5.86618241e-01  5.88494763e-01  5.90194840e-01  5.90228501e-01
  5.94564872e-01  5.97276523e-01  5.97311179e-01  5.99988174e-01
  6.08123127e-01  6.08562460e-01  6.10626608e-01  6.12361832e-01
  6.16258080e-01  6.18969731e-01  6.23100504e-01  6.25006051e-01
  6.25604032e-01  6.31027334e-01  6.32562642e-01  6.35239637e-01
  6.35803829e-01  6.35839922e-01  6.35893143e-01  6.36605994e-01
  6.37951288e-01  6.38722483e-01  6.40662939e-01  6.40697595e-01
  6.42029296e-01  6.44427860e-01  6.44740947e-01  6.48797892e-01
  6.51087687e-01  6.51509543e-01  6.51655318e-01  6.54221194e-01
  6.59644496e-01  6.61010853e-01  6.62317458e-01  6.62356147e-01
  6.64716248e-01  6.65067798e-01  6.67177135e-01  6.69145806e-01
  6.70491100e-01  6.70724908e-01  6.71857457e-01  6.72910728e-01
  6.73202751e-01  6.75764198e-01  6.76212097e-01  6.78626053e-01
  6.79576577e-01  6.80022595e-01  6.80592696e-01  6.82704061e-01
  6.83962690e-01  6.84298090e-01  6.85415712e-01  6.86339150e-01
  6.88127363e-01  6.88986768e-01  6.90839014e-01  6.93550665e-01
  6.93590748e-01  6.94480126e-01  6.96210902e-01  6.96262316e-01
  6.97256060e-01  6.98973967e-01  7.02727862e-01  7.03065568e-01
  7.04397269e-01  7.05140249e-01  7.05391013e-01  7.05674638e-01
  7.05742563e-01  7.07108920e-01  7.08102664e-01  7.09820571e-01
  7.10687239e-01  7.13461683e-01  7.13525966e-01  7.13877516e-01
  7.14932995e-01  7.15243873e-01  7.15917058e-01  7.16237617e-01
  7.21660919e-01  7.23360211e-01  7.26678261e-01  7.29089600e-01
  7.32870271e-01  7.35911086e-01  7.43296723e-01  7.44669070e-01
  7.45745219e-01  7.46720767e-01  7.47783685e-01  7.48456870e-01
  7.48777429e-01  7.51278303e-01  7.51569382e-01  7.51754382e-01
  7.52054122e-01  7.53528622e-01  7.54200731e-01  7.55502805e-01
  7.55595792e-01  7.56240766e-01  7.59481903e-01  7.64726776e-01
  7.65423341e-01  7.66568012e-01  7.72861729e-01  7.75231121e-01
  7.80996682e-01  7.83467243e-01  7.83530535e-01  7.83708333e-01
  7.89131635e-01  7.91843286e-01  7.94554937e-01  7.97266588e-01
  7.99562405e-01  7.99978239e-01  8.02689890e-01  8.07664106e-01
  8.08113192e-01  8.09319410e-01  8.18959796e-01  8.22216717e-01
  8.25749455e-01  8.27094749e-01  8.32333191e-01  8.33821257e-01
  8.33932222e-01  8.34813868e-01  8.35120977e-01  8.37941353e-01
  8.40301454e-01  8.40653004e-01  8.43364655e-01  8.48338871e-01
  8.50540737e-01  8.51390884e-01  8.56465373e-01  8.56507076e-01
  8.59218727e-01  8.62346212e-01  8.63635634e-01  8.64111777e-01
  8.70129615e-01  8.73462424e-01  8.83930696e-01  8.86566211e-01
  8.88246290e-01  8.88355641e-01  8.90055718e-01  9.02912253e-01
  9.06490571e-01  9.07643643e-01  9.08198411e-01  9.15868194e-01
  9.16118958e-01  9.19182159e-01  9.29677213e-01  9.32324580e-01
  9.32388864e-01  9.32740414e-01  9.39030383e-01  9.40523817e-01
  9.40875367e-01  9.43587018e-01  9.48588464e-01  9.49010320e-01
  9.59856924e-01  9.61966261e-01  9.63750184e-01  9.65810101e-01
  9.71570195e-01  9.73063629e-01  9.74274628e-01  9.81383034e-01
  9.83910233e-01  9.89333535e-01  9.92045186e-01  9.97468488e-01
  1.00018014e+00  1.00218552e+00  1.00289179e+00  1.00831509e+00
  1.01119343e+00  1.01874586e+00  1.01916170e+00  1.01956462e+00
  1.01985361e+00  1.02221371e+00  1.04567149e+00  1.06778655e+00
  1.07320986e+00  1.09017279e+00  1.42215981e+00  1.66927014e+00]

  UserWarning,

2022-10-31 11:02:35,744:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.15483526 -0.5764159  -0.49154652 -0.47224084 -0.35426769 -0.10808954
 -0.1070076  -0.09583039 -0.09230676 -0.08404107 -0.03836924 -0.02588125
 -0.01135945 -0.00217259  0.03584146  0.04893263  0.05913797  0.06468657
  0.07065817  0.07649725  0.0967695   0.09686226  0.10057674  0.11515608
  0.12858408  0.13027778  0.13501977  0.13926419  0.14119226  0.14796182
  0.15231174  0.16602923  0.16894026  0.17255909  0.18214188  0.19041573
  0.19104831  0.19643416  0.19651793  0.1989937   0.19955917  0.2006014
  0.2030259   0.20593951  0.20814231  0.20856282  0.21036423  0.21380376
  0.2141108   0.21806757  0.21927611  0.22071853  0.22163495  0.22419255
  0.22685082  0.22805664  0.22862903  0.22964744  0.23068108  0.23224827
  0.23253098  0.2331381   0.23595209  0.23640184  0.23652714  0.23798822
  0.23919763  0.24007001  0.24102042  0.24305708  0.24368253  0.24382366
  0.24408334  0.24827004  0.24855592  0.25034608  0.25049588  0.25062936
  0.25065178  0.25134718  0.25202441  0.25216579  0.25223788  0.25268518
  0.25268525  0.25395938  0.25412213  0.25446124  0.25597694  0.25633269
  0.25676818  0.25881645  0.25913348  0.25968375  0.26106252  0.26351505
  0.26507396  0.26508577  0.2654704   0.26618182  0.26656815  0.26841052
  0.26849647  0.26865685  0.27123972  0.27325913  0.27468162  0.27488318
  0.27656444  0.27910577  0.27999803  0.28002787  0.28064999  0.2814208
  0.28239394  0.2825525   0.28371545  0.28710953  0.28859028  0.28873076
  0.28882264  0.28892113  0.28929147  0.29100118  0.29365101  0.2948828
  0.29670941  0.29755368  0.29756729  0.29871087  0.29876124  0.29943603
  0.29973505  0.30067856  0.30248554  0.30280987  0.30301641  0.30315292
  0.30397827  0.3050466   0.30637175  0.30728939  0.31007555  0.31122262
  0.3117704   0.31214471  0.31221713  0.31271368  0.31372705  0.31598993
  0.31776517  0.31895896  0.31915142  0.32116746  0.32137314  0.32194333
  0.32359795  0.32565212  0.32668606  0.32682813  0.32777254  0.32781719
  0.32864472  0.32868223  0.32894821  0.32951981  0.33125879  0.33137831
  0.33204741  0.33234078  0.33272705  0.33361579  0.33384401  0.33525957
  0.33542964  0.33661768  0.33672759  0.33707246  0.33726388  0.33763943
  0.33836102  0.33887859  0.33896539  0.33939963  0.34165248  0.34186546
  0.34301103  0.34338991  0.34511768  0.34538565  0.34575086  0.34609109
  0.34631478  0.34660765  0.34675392  0.34758986  0.34767348  0.34893403
  0.34948994  0.35002045  0.35008084  0.35193107  0.35417918  0.35440533
  0.35560156  0.35612066  0.35644529  0.35646531  0.35669059  0.35705303
  0.35706206  0.36011004  0.36098444  0.36139533  0.36201716  0.3620628
  0.36230638  0.36237177  0.36251249  0.36326343  0.36343769  0.36443738
  0.36492797  0.36766288  0.36792711  0.36831219  0.36935325  0.36958591
  0.37084134  0.37192949  0.37321817  0.37375291  0.37402429  0.37537829
  0.37560442  0.37581338  0.37757551  0.37810832  0.37882444  0.37882755
  0.37963898  0.38118626  0.38264765  0.38321549  0.38535621  0.38577947
  0.38640401  0.38726712  0.38729635  0.38753221  0.38848574  0.3887007
  0.38915469  0.38933322  0.389823    0.38995061  0.39209133  0.39220634
  0.39233686  0.39310946  0.39327475  0.39563159  0.39571941  0.39690216
  0.39773681  0.39777751  0.39821339  0.39879284  0.4012544   0.40189825
  0.40202803  0.40212644  0.40222881  0.40368433  0.40839811  0.40946498
  0.4101268   0.41029252  0.41143132  0.41187201  0.41209248  0.41229425
  0.41258805  0.41289182  0.41822366  0.41829357  0.4183514   0.42140506
  0.42146147  0.42261058  0.42537368  0.42774381  0.42878168  0.43348845
  0.43431767  0.43658635  0.4371768   0.43834308  0.4389767   0.44101037
  0.44533178  0.44562317  0.44668312  0.44965708  0.45029482  0.45052009
  0.45060337  0.45196048  0.45273617  0.45669204  0.45713292  0.45746586
  0.45789233  0.45808067  0.45814386  0.46200634  0.4629307   0.46328816
  0.464994    0.46513894  0.46604109  0.47019723  0.47190049  0.47237664
  0.47504928  0.47551856  0.47793389  0.48052564  0.48498291  0.49454829
  0.50177679  0.50557058  0.5077669   0.51543142  0.51789436  0.51903782
  0.52041891  0.52320597  0.53478193  0.53690609  0.54111729  0.54266712
  0.54620177  0.54757383  0.55198868  0.5571585   0.56239754  0.56402898
  0.56630998  0.56951484  0.57292089  0.57374165  0.57852235  0.5809127
  0.58569339  0.58569749  0.58907868  0.5919042   0.59286444  0.59525478
  0.6021861   0.604272    0.60438366  0.61198722  0.61438166  0.61770486
  0.61915827  0.62161783  0.62321535  0.62393896  0.62632931  0.62659229
  0.62871966  0.62928847  0.62959735  0.63111001  0.63350036  0.63374536
  0.6358907   0.63609966  0.63828105  0.63849001  0.643385    0.64395112
  0.64524911  0.6454521   0.64645426  0.64873182  0.6498245   0.65201767
  0.65262314  0.65501349  0.65623671  0.65636654  0.65666275  0.65740384
  0.65764239  0.65803285  0.6590255   0.65979419  0.6610174   0.66114723
  0.66239349  0.66353758  0.66457488  0.66558055  0.66696523  0.66973367
  0.67174593  0.67233552  0.67472587  0.67517444  0.67652662  0.67700261
  0.67836549  0.68369767  0.68425155  0.68505072  0.68666685  0.68983141
  0.69099472  0.69258919  0.69325906  0.69330555  0.69461211  0.69504648
  0.69564941  0.69589442  0.69595828  0.69700246  0.6985348   0.69926298
  0.69975968  0.70104376  0.70178315  0.70656385  0.70667903  0.70817999
  0.70819075  0.70863931  0.7089542   0.70907117  0.70919291  0.71234671
  0.71297144  0.7137349   0.71536179  0.71612524  0.72253284  0.72626547
  0.72748482  0.72838586  0.72878215  0.72969312  0.7303375   0.73208347
  0.74164486  0.74241907  0.74355179  0.74404597  0.74449454  0.74719977
  0.74838388  0.74881591  0.7523262   0.75437082  0.75598695  0.7583773
  0.75901707  0.75976517  0.76335308  0.76436701  0.7657573   0.76676104
  0.76793869  0.77032904  0.77271939  0.77923612  0.77989043  0.78228078
  0.78467113  0.78706148  0.78765107  0.78795085  0.78923634  0.78945183
  0.79135876  0.79184217  0.79423252  0.79662287  0.79721247  0.7985298
  0.8001567   0.80140357  0.80395151  0.80427973  0.80438351  0.80618426
  0.80760805  0.80857461  0.80887228  0.82083523  0.82392236  0.82420496
  0.82710599  0.83545804  0.83725879  0.84270161  0.84786911  0.86180853
  0.87672758  0.87926043  0.88031856  0.88186759  0.8868606   0.89403164
  0.89642199  0.89896994  0.90120269  0.9012719   0.90359303  0.9045952
  0.90837373  0.91315443  0.91374402  0.91422984  0.91613437  0.91793922
  0.91852472  0.92271582  0.92328463  0.92510617  0.92749652  0.92988687
  0.93110547  0.93227721  0.93335263  0.93525716  0.93585003  0.93705791
  0.9423997   0.94284427  0.94692818  0.94959925  0.95276736  0.95379035
  0.95677029  0.95877911  0.95887992  0.9639654   0.96633169  0.96935565
  0.97350273  0.97350682  0.97608897  0.97904688  0.98440742  0.98545447
  0.98801431  0.98847844  0.99323917  0.99740621  0.99853893  0.99862943
  1.00218691  1.00457726  1.0069676   1.01038767  1.01229142  1.01260578
  1.04703104  1.05576804]

  UserWarning,

2022-10-31 11:02:35,759:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95422963 -0.94503222 -0.59332873 -0.5484015  -0.28746071 -0.20505621
 -0.18770158 -0.13383685 -0.12300769 -0.11891206 -0.11411188 -0.08360044
 -0.07321535 -0.05695978 -0.0295048  -0.01722776 -0.01049762 -0.0046217
 -0.00349913 -0.00221674  0.01733191  0.02205641  0.02742504  0.03356734
  0.03453241  0.03956776  0.04862843  0.05106527  0.05156649  0.05340592
  0.05476553  0.07395147  0.08023631  0.08116403  0.0812563   0.08265375
  0.08322122  0.08440536  0.0909668   0.09235316  0.09802814  0.10075575
  0.10122052  0.10131905  0.1110236   0.1173328   0.14784543  0.1589923
  0.16033235  0.1610757   0.16292677  0.16317024  0.16888569  0.17688061
  0.18511487  0.1864731   0.18835931  0.19204877  0.19278377  0.19360456
  0.19376523  0.19796415  0.20224326  0.2066736   0.20932832  0.20937602
  0.21879352  0.21957918  0.22377567  0.22642514  0.22738376  0.22861582
  0.22957222  0.23152126  0.23472499  0.23525404  0.23759038  0.23813706
  0.23819379  0.24082385  0.24127569  0.24376785  0.24555853  0.24918676
  0.25133677  0.25137445  0.25154077  0.25296254  0.2535039   0.25367058
  0.25370687  0.25571514  0.25632976  0.25646776  0.25845071  0.25943957
  0.25973399  0.2603881   0.26131083  0.26309011  0.26364942  0.26424576
  0.26524733  0.26733846  0.2681878   0.26880977  0.26971213  0.26990365
  0.27217542  0.27656103  0.27669452  0.27721579  0.27732946  0.27826587
  0.27921126  0.27950207  0.27982361  0.28040501  0.28063599  0.28071162
  0.28121711  0.28171602  0.28200177  0.2828876   0.28288871  0.28312463
  0.28394047  0.28426115  0.28506822  0.28516307  0.28559481  0.28661763
  0.28690423  0.28763487  0.29034448  0.29042581  0.29072816  0.29094529
  0.29113691  0.29117279  0.2922958   0.29265832  0.29280769  0.29304371
  0.29371049  0.29687648  0.29793983  0.29839465  0.29857785  0.29860623
  0.29930527  0.30007626  0.30018839  0.30259454  0.30298453  0.30326398
  0.30456973  0.30826801  0.30954176  0.30979103  0.31207736  0.31252971
  0.3142319   0.31499161  0.31650095  0.31658737  0.31672926  0.3175331
  0.31779999  0.31870814  0.31919463  0.31992272  0.32122653  0.32185856
  0.32254623  0.3234556   0.3251176   0.32513859  0.32608543  0.32724358
  0.32767188  0.33093626  0.3311579   0.331204    0.33131073  0.3336363
  0.33597143  0.33991813  0.34120356  0.34267729  0.34399366  0.34420008
  0.34498619  0.34534196  0.34653805  0.3493094   0.35004807  0.35020587
  0.35206469  0.35215027  0.35220586  0.35246093  0.35551059  0.35574595
  0.35798107  0.3594825   0.36172382  0.3621763   0.36227843  0.36256966
  0.36270827  0.36621185  0.36760017  0.36918694  0.37171282  0.37198799
  0.37249076  0.37398553  0.37444595  0.3745442   0.37471084  0.37491716
  0.3749338   0.37500491  0.37627185  0.37696956  0.37772486  0.37795001
  0.37967057  0.37989406  0.38348033  0.38488268  0.38686425  0.38759861
  0.38832892  0.38875874  0.38890388  0.39071291  0.39093657  0.39160509
  0.39183976  0.39256592  0.39316926  0.39473677  0.39474716  0.39486021
  0.3952272   0.39542813  0.39641055  0.39679888  0.39744203  0.3976201
  0.40049633  0.40257832  0.40346234  0.40466951  0.40498421  0.40584168
  0.40609645  0.40810257  0.40994376  0.4103676   0.41058578  0.4117843
  0.41213277  0.41243519  0.41264144  0.41319202  0.41452505  0.41571099
  0.41656648  0.41785545  0.42033216  0.42106776  0.42139529  0.42220779
  0.42369196  0.42398118  0.42527884  0.42529103  0.42545183  0.4300422
  0.43146685  0.43211336  0.43459881  0.43515719  0.43541825  0.43905635
  0.44380006  0.44438752  0.44523272  0.44527116  0.44754801  0.44940623
  0.44961399  0.45097718  0.45120016  0.45423812  0.45428166  0.45627564
  0.45813443  0.4590033   0.45928947  0.46183835  0.4620071   0.46357658
  0.4636062   0.46507762  0.4666603   0.46842328  0.47009617  0.47049966
  0.47165291  0.4735087   0.4762175   0.47763253  0.47941677  0.48508775
  0.48760311  0.48845481  0.48912421  0.48986614  0.4912592   0.49435714
  0.4945601   0.49669207  0.49948576  0.50038381  0.50086787  0.50654804
  0.50725175  0.51474023  0.52458292  0.53208833  0.53457199  0.5371246
  0.53752495  0.53948761  0.54130709  0.54947394  0.5497331   0.55617503
  0.55662249  0.55692491  0.55800098  0.56128337  0.56610966  0.56907625
  0.57052451  0.57195638  0.57386153  0.57980659  0.58069191  0.58349527
  0.58597893  0.59384456  0.5943479   0.59591356  0.59839722  0.59947625
  0.60266546  0.60306211  0.60336453  0.60584819  0.60632762  0.60763277
  0.61081551  0.61451311  0.61533828  0.61687081  0.61826648  0.61982664
  0.62075014  0.62109481  0.62440858  0.62525785  0.62868054  0.63068477
  0.63074626  0.63639408  0.63743667  0.63813575  0.63902117  0.63992033
  0.64061941  0.64297867  0.64310306  0.64807038  0.65055404  0.65060392
  0.65309031  0.65352062  0.65530171  0.65552135  0.65800501  0.65889043
  0.66015345  0.66048867  0.66137409  0.662022    0.66297233  0.66345176
  0.66345525  0.66475691  0.66545599  0.66641663  0.66793964  0.67227102
  0.67290696  0.67292678  0.67506292  0.67787428  0.67965886  0.68057542
  0.68079549  0.68101611  0.68448753  0.68780891  0.68816395  0.69029257
  0.69207715  0.69467126  0.69469948  0.69577718  0.69704446  0.69952812
  0.7002272   0.70166026  0.70237564  0.70314841  0.70449544  0.70519451
  0.70536306  0.70563207  0.7069791   0.70767817  0.70782701  0.70946275
  0.70982661  0.70983413  0.7108787   0.71727759  0.72011202  0.72338906
  0.7265842   0.72798499  0.7290296   0.72935188  0.73271966  0.73467071
  0.73963051  0.74040328  0.74161166  0.74223323  0.74254605  0.74331758
  0.74459783  0.7453706   0.74785426  0.74926272  0.75337741  0.75370699
  0.75453246  0.75768353  0.75778889  0.75817402  0.75941878  0.76185853
  0.76275621  0.76375171  0.76523986  0.76689959  0.76772352  0.76871903
  0.76876077  0.7689319   0.77020718  0.77108034  0.77178694  0.77625353
  0.77775357  0.77816746  0.78262547  0.78510913  0.78576942  0.79426079
  0.79504376  0.79548198  0.79752742  0.79967586  0.80427931  0.80497839
  0.80746205  0.80855142  0.81338258  0.81386243  0.8233574   0.82397257
  0.82737423  0.82893989  0.83693073  0.84401098  0.84471692  0.8457641
  0.84684343  0.84847249  0.84968424  0.8511173   0.85377647  0.85626013
  0.8637111   0.87359964  0.88588851  0.89103134  0.89265461  0.89538348
  0.9000667   0.90096597  0.90311441  0.90529565  0.90593329  0.90657973
  0.91304904  0.91635661  0.91835158  0.92331889  0.92524064  0.92828621
  0.92876564  0.93325353  0.93540197  0.93788562  0.93822084  0.94036928
  0.9407045   0.94272855  0.94285294  0.94318816  0.94322432  0.9532705
  0.95527123  0.95775489  0.95809011  0.9590835   0.96057377  0.96112397
  0.96156716  0.96228254  0.9627222   0.96340291  0.96554108  0.96768952
  0.96779805  0.96980932  0.97063792  0.97265684  0.9751405   0.97652289
  0.97799553  0.98010781  0.98507513  0.98755879  1.00742805  1.00991171
  1.01239537  1.01252488  1.01485736  1.01736268  1.03721028  1.06101793
  1.08603281  1.09498653  1.18686975  1.65284746]

  UserWarning,

2022-10-31 11:02:35,790:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.23953206e+00 -8.42291012e-01 -6.19405789e-01 -4.00621949e-01
 -3.82966146e-01 -2.37493102e-01 -1.68814969e-01 -1.02889683e-01
 -1.01084317e-01 -9.00304520e-02 -8.17229716e-02 -8.06768748e-02
 -6.09450377e-02 -4.96795631e-02 -3.76153244e-02 -2.97624180e-02
 -1.56562758e-02 -4.02814325e-03 -1.57158862e-04  8.00099222e-03
  1.77412442e-02  2.49904409e-02  2.51539418e-02  2.73155360e-02
  3.40545626e-02  5.60027264e-02  5.89099203e-02  6.63939546e-02
  6.71369705e-02  9.74759277e-02  1.05463220e-01  1.30967407e-01
  1.35813211e-01  1.35906136e-01  1.37628123e-01  1.44185443e-01
  1.44624023e-01  1.47221938e-01  1.51159145e-01  1.53440485e-01
  1.53850390e-01  1.56342972e-01  1.56569020e-01  1.58717512e-01
  1.60966176e-01  1.61667964e-01  1.61867582e-01  1.63393123e-01
  1.65240796e-01  1.66436024e-01  1.71460730e-01  1.71898004e-01
  1.76152739e-01  1.78392071e-01  1.80545345e-01  1.83218048e-01
  1.91732468e-01  1.92067747e-01  1.93569051e-01  1.95449489e-01
  1.96536575e-01  1.98463266e-01  1.99560873e-01  2.00065307e-01
  2.00253805e-01  2.01064246e-01  2.01332715e-01  2.07144690e-01
  2.08983532e-01  2.09574695e-01  2.09651961e-01  2.11483444e-01
  2.13939184e-01  2.16271229e-01  2.17980525e-01  2.19080067e-01
  2.19426661e-01  2.20008082e-01  2.20723156e-01  2.24476529e-01
  2.25921508e-01  2.26419940e-01  2.26712489e-01  2.27640994e-01
  2.28406475e-01  2.28851079e-01  2.29602922e-01  2.30593958e-01
  2.39550194e-01  2.41382333e-01  2.41441010e-01  2.42147923e-01
  2.42806779e-01  2.43184854e-01  2.44322815e-01  2.47544014e-01
  2.49527173e-01  2.49751749e-01  2.50764877e-01  2.52888045e-01
  2.52925644e-01  2.53597380e-01  2.59830144e-01  2.60915225e-01
  2.62468509e-01  2.62833683e-01  2.66001106e-01  2.66972792e-01
  2.67844700e-01  2.69282469e-01  2.69584222e-01  2.69700147e-01
  2.69763877e-01  2.70442661e-01  2.70730073e-01  2.70771548e-01
  2.71848962e-01  2.72291175e-01  2.72415612e-01  2.73312924e-01
  2.73323992e-01  2.73489073e-01  2.76031539e-01  2.76576879e-01
  2.81902838e-01  2.83996469e-01  2.84024280e-01  2.84506578e-01
  2.84536000e-01  2.85738189e-01  2.86402547e-01  2.86903871e-01
  2.87007886e-01  2.89227198e-01  2.89831077e-01  2.90138561e-01
  2.91317515e-01  2.91342973e-01  2.91479411e-01  2.91747330e-01
  2.91774153e-01  2.91807358e-01  2.92576772e-01  2.92743106e-01
  2.93588996e-01  2.93630430e-01  2.93829341e-01  2.96318524e-01
  2.96909645e-01  2.97171831e-01  2.98692254e-01  2.98739940e-01
  3.00167653e-01  3.00389467e-01  3.01103558e-01  3.01847083e-01
  3.05625979e-01  3.06353950e-01  3.06448038e-01  3.08045004e-01
  3.08327137e-01  3.08551052e-01  3.08969196e-01  3.10154365e-01
  3.10443598e-01  3.10971673e-01  3.11935105e-01  3.12827592e-01
  3.14893470e-01  3.15315237e-01  3.16062534e-01  3.16198826e-01
  3.16210479e-01  3.16780447e-01  3.20535470e-01  3.21005830e-01
  3.23841517e-01  3.23935178e-01  3.24435926e-01  3.26147577e-01
  3.26978752e-01  3.27904117e-01  3.28074588e-01  3.28262035e-01
  3.28642244e-01  3.28668101e-01  3.28879094e-01  3.29740985e-01
  3.31199467e-01  3.34498556e-01  3.35534458e-01  3.39415231e-01
  3.39592009e-01  3.40013587e-01  3.40822510e-01  3.40882578e-01
  3.41473241e-01  3.41546362e-01  3.42299225e-01  3.42476486e-01
  3.42509891e-01  3.42950939e-01  3.46389862e-01  3.47904324e-01
  3.49029509e-01  3.50110883e-01  3.50398715e-01  3.50886631e-01
  3.52824904e-01  3.54051060e-01  3.54179160e-01  3.54607142e-01
  3.55647282e-01  3.57040821e-01  3.57874044e-01  3.60948962e-01
  3.61506215e-01  3.62434929e-01  3.64504490e-01  3.64602618e-01
  3.64811527e-01  3.64984821e-01  3.65463293e-01  3.65603793e-01
  3.65671560e-01  3.65848147e-01  3.66054556e-01  3.66225477e-01
  3.67371605e-01  3.68733181e-01  3.70227589e-01  3.70330593e-01
  3.71461461e-01  3.74162542e-01  3.77444202e-01  3.78282741e-01
  3.78611836e-01  3.78622818e-01  3.79634414e-01  3.81986093e-01
  3.84578758e-01  3.86942158e-01  3.87321816e-01  3.88129869e-01
  3.88322841e-01  3.88857905e-01  3.88959549e-01  3.89134259e-01
  3.89305557e-01  3.89477786e-01  3.89722708e-01  3.90125953e-01
  3.90493269e-01  3.90985175e-01  3.91395728e-01  3.91544199e-01
  3.91668957e-01  3.93030487e-01  3.93580979e-01  3.93732295e-01
  3.94015511e-01  3.96049748e-01  3.96395757e-01  3.97095544e-01
  3.98005128e-01  3.98631790e-01  4.00297536e-01  4.01122556e-01
  4.01341112e-01  4.01438737e-01  4.02070022e-01  4.02868730e-01
  4.03443281e-01  4.05180760e-01  4.06636458e-01  4.06647906e-01
  4.07148598e-01  4.08073140e-01  4.08999078e-01  4.10576155e-01
  4.11986304e-01  4.12334721e-01  4.12987093e-01  4.14651476e-01
  4.15049072e-01  4.17096668e-01  4.22270686e-01  4.23487280e-01
  4.25472930e-01  4.26492791e-01  4.26863409e-01  4.28963315e-01
  4.30188513e-01  4.30536330e-01  4.34396072e-01  4.34429536e-01
  4.35623996e-01  4.38344182e-01  4.39236387e-01  4.39759561e-01
  4.39989929e-01  4.42007321e-01  4.42085783e-01  4.42353329e-01
  4.42864591e-01  4.43104664e-01  4.43318920e-01  4.45685504e-01
  4.47080128e-01  4.47643499e-01  4.49951357e-01  4.52697640e-01
  4.53593454e-01  4.53737053e-01  4.54170327e-01  4.55075565e-01
  4.55260383e-01  4.57311125e-01  4.60202553e-01  4.63766757e-01
  4.65069729e-01  4.67947196e-01  4.77522793e-01  4.78523941e-01
  4.78892453e-01  4.79548674e-01  4.79697191e-01  4.81276437e-01
  4.84420573e-01  4.86973437e-01  4.94512177e-01  4.95082443e-01
  4.95653549e-01  4.96913663e-01  4.98032013e-01  5.00380349e-01
  5.05904785e-01  5.09419234e-01  5.10461119e-01  5.11510395e-01
  5.14490225e-01  5.18009054e-01  5.18107058e-01  5.18724509e-01
  5.24014346e-01  5.31104545e-01  5.38517693e-01  5.39091180e-01
  5.41519674e-01  5.41884411e-01  5.43015407e-01  5.47648343e-01
  5.48466127e-01  5.57585614e-01  5.60411987e-01  5.63993146e-01
  5.64504147e-01  5.64860419e-01  5.67516979e-01  5.68812958e-01
  5.69842389e-01  5.71455883e-01  5.76553655e-01  5.78869878e-01
  5.79228221e-01  5.80515572e-01  5.83813272e-01  5.85242371e-01
  5.93943158e-01  5.97059370e-01  5.97963191e-01  5.98030958e-01
  6.08165296e-01  6.10785504e-01  6.12211356e-01  6.14320874e-01
  6.16870388e-01  6.18886841e-01  6.19301555e-01  6.21411073e-01
  6.21597188e-01  6.21603249e-01  6.32306265e-01  6.33481954e-01
  6.35845353e-01  6.37149173e-01  6.38538238e-01  6.40572153e-01
  6.41011075e-01  6.42935552e-01  6.47662352e-01  6.49944098e-01
  6.52389151e-01  6.52667512e-01  6.54752551e-01  6.57115951e-01
  6.57529678e-01  6.57887396e-01  6.59479350e-01  6.61242860e-01
  6.61842750e-01  6.62776580e-01  6.64206150e-01  6.64761776e-01
  6.68805315e-01  6.69117990e-01  6.71296349e-01  6.73659749e-01
  6.74298013e-01  6.76023148e-01  6.78132666e-01  6.78386548e-01
  6.79439526e-01  6.80496066e-01  6.80749948e-01  6.82859466e-01
  6.82892981e-01  6.83751611e-01  6.83884793e-01  6.84166325e-01
  6.85222865e-01  6.86529725e-01  6.86664459e-01  6.88963301e-01
  6.89949665e-01  6.92566947e-01  6.97039864e-01  6.98346723e-01
  6.99403264e-01  7.01674537e-01  7.02020545e-01  7.03073523e-01
  7.04037937e-01  7.04130063e-01  7.05155391e-01  7.06365829e-01
  7.08442149e-01  7.09105749e-01  7.09110745e-01  7.09594417e-01
  7.10163722e-01  7.12817868e-01  7.13917843e-01  7.16195948e-01
  7.17070216e-01  7.17253921e-01  7.18708627e-01  7.21912954e-01
  7.22152949e-01  7.25157416e-01  7.27412980e-01  7.27764060e-01
  7.28012947e-01  7.30376346e-01  7.31882885e-01  7.34547774e-01
  7.35103146e-01  7.39102345e-01  7.39229979e-01  7.41977974e-01
  7.43986979e-01  7.46920144e-01  7.49283544e-01  7.50341517e-01
  7.52704917e-01  7.53410377e-01  7.54010344e-01  7.58737143e-01
  7.61876984e-01  7.65827342e-01  7.68190742e-01  7.70554142e-01
  7.73924303e-01  7.75153307e-01  7.75280941e-01  7.75964201e-01
  7.77644341e-01  7.78001073e-01  7.78083263e-01  7.79433731e-01
  7.82371140e-01  7.87097940e-01  7.89115331e-01  7.89461339e-01
  7.90970891e-01  7.96551538e-01  7.98914938e-01  8.01278338e-01
  8.03641738e-01  8.09004021e-01  8.10301758e-01  8.10731937e-01
  8.17476128e-01  8.21356972e-01  8.24912335e-01  8.28047181e-01
  8.28328712e-01  8.39092733e-01  8.41456133e-01  8.43819533e-01
  8.45029901e-01  8.47296170e-01  8.54396286e-01  8.54880445e-01
  8.57779564e-01  8.61046785e-01  8.71183643e-01  8.75227183e-01
  8.76819137e-01  8.80323506e-01  8.82686906e-01  8.84115143e-01
  8.84680782e-01  8.85733759e-01  8.87044181e-01  8.89407581e-01
  8.89495573e-01  8.99444547e-01  9.05951379e-01  9.11203128e-01
  9.15404978e-01  9.18821355e-01  9.20131778e-01  9.29585376e-01
  9.30268636e-01  9.30510720e-01  9.32655746e-01  9.33585121e-01
  9.34312176e-01  9.39038975e-01  9.39522647e-01  9.40091953e-01
  9.41402375e-01  9.43765775e-01  9.47182152e-01  9.47696724e-01
  9.50917139e-01  9.51908951e-01  9.54272351e-01  9.54342527e-01
  9.56066445e-01  9.56635751e-01  9.59458950e-01  9.68452749e-01
  9.69763172e-01  9.70816149e-01  9.71938332e-01  9.73179549e-01
  9.76633004e-01  9.80269748e-01  9.82633148e-01  9.84996547e-01
  9.89723347e-01  9.91513430e-01  9.91805215e-01  9.94104138e-01
  9.94622375e-01  9.97369172e-01  9.99049312e-01  9.99176946e-01
  1.00576525e+00  1.01147762e+00  1.02016601e+00  1.02254032e+00
  1.02390100e+00  1.03079965e+00  1.03440757e+00  1.03720990e+00
  1.04500560e+00]

  UserWarning,

2022-10-31 11:02:35,895:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.60750608 -0.84421162 -0.3080757  -0.23445272 -0.23346599 -0.18720504
 -0.17394825 -0.15959539 -0.06070921 -0.05281708 -0.04781285 -0.03847156
 -0.0126326  -0.00631742  0.00949194  0.01127895  0.01487019  0.02167722
  0.06493001  0.07566155  0.07934392  0.08516388  0.10282866  0.10433722
  0.11220724  0.12334835  0.12348349  0.12473277  0.14426859  0.14673399
  0.14679398  0.14778631  0.15585848  0.16416492  0.1646608   0.17813856
  0.18534472  0.18998408  0.19150023  0.19224553  0.19348705  0.19663704
  0.19683221  0.19834162  0.20007908  0.20185613  0.20230863  0.20689069
  0.21133577  0.21249233  0.21250185  0.21483344  0.21546066  0.21635078
  0.21854151  0.22035012  0.22072916  0.222621    0.2229098   0.22347002
  0.22558294  0.2270389   0.22801154  0.22855193  0.22882645  0.23082798
  0.2357576   0.23600845  0.23606699  0.23788247  0.23981814  0.2415881
  0.24184759  0.24250718  0.24842027  0.24860947  0.24894411  0.249758
  0.2506074   0.25126232  0.25288634  0.25362301  0.25489044  0.25504598
  0.25623979  0.25639517  0.25649004  0.2573984   0.25746251  0.25846259
  0.2588365   0.25944706  0.26028289  0.26058704  0.26152299  0.26398338
  0.26407278  0.26436766  0.26480439  0.26502941  0.26629567  0.26662859
  0.26764916  0.26775131  0.2680741   0.26858178  0.26960763  0.27050709
  0.27121072  0.27194989  0.27207402  0.27267269  0.27365773  0.2744986
  0.27635398  0.27652292  0.27722699  0.27755336  0.27816048  0.27948253
  0.28399481  0.2877089   0.28774215  0.28787684  0.2885625   0.28878548
  0.29016835  0.29068092  0.29116108  0.2917339   0.29175831  0.29199707
  0.29223517  0.29282926  0.29330353  0.29573099  0.29622894  0.29630178
  0.2966847   0.29704304  0.29763063  0.30066234  0.30200128  0.30223694
  0.30301364  0.30330059  0.30349613  0.30367652  0.30439739  0.30559441
  0.30593089  0.30809233  0.31018169  0.31053766  0.31357613  0.31456424
  0.31615798  0.31618545  0.31767032  0.31958052  0.3216803   0.32265803
  0.32313283  0.32454034  0.32471255  0.32478809  0.3260495   0.32606951
  0.32672067  0.32723912  0.32728051  0.32818188  0.32872658  0.32928348
  0.32954267  0.32973583  0.3305887   0.33061101  0.33064835  0.33076591
  0.3326874   0.33377569  0.33385257  0.33392663  0.33555304  0.33607904
  0.33634398  0.33994482  0.34016919  0.34038641  0.34137225  0.34175589
  0.34189663  0.34200221  0.34281432  0.34298619  0.34378299  0.34443457
  0.3448892   0.34562403  0.34728947  0.34836812  0.34873628  0.3489332
  0.34919343  0.34979344  0.35108862  0.35191588  0.35240562  0.35258938
  0.35333611  0.3552483   0.35591559  0.35645237  0.35741814  0.35768328
  0.35799999  0.35856033  0.35993991  0.36040773  0.36142029  0.36170288
  0.36196947  0.36445415  0.36450265  0.36821776  0.369225    0.36938018
  0.36977612  0.37136872  0.37266499  0.37315825  0.37430881  0.37446511
  0.37636     0.37753995  0.37851037  0.38057139  0.38098305  0.3842502
  0.384346    0.38462658  0.38465731  0.38518272  0.38527659  0.38533332
  0.38702977  0.38747816  0.38759346  0.38761262  0.38921027  0.38936071
  0.39030356  0.39031166  0.39046124  0.39131719  0.39187833  0.39414713
  0.39578248  0.39595703  0.39651958  0.3971787   0.398487    0.39897474
  0.39897569  0.40003589  0.40126449  0.40159994  0.40248412  0.40272637
  0.40320349  0.4032735   0.40407735  0.40413688  0.40598697  0.40600939
  0.40617112  0.40762913  0.40876248  0.40896754  0.41295351  0.4140173
  0.41419919  0.41475464  0.41645958  0.41722427  0.41849519  0.41941261
  0.41958624  0.42181861  0.4221941   0.42262303  0.42380187  0.42434666
  0.42571813  0.42586994  0.42611336  0.42614551  0.42885931  0.42903844
  0.42927837  0.43511108  0.43516488  0.43875486  0.43915215  0.43980655
  0.44000994  0.44291982  0.44329164  0.44363148  0.44749468  0.44844996
  0.45322459  0.45606175  0.45857625  0.46709114  0.46829457  0.47457668
  0.47548637  0.47561822  0.47770854  0.47856873  0.47922949  0.48080281
  0.48289575  0.48295486  0.48343071  0.48427643  0.4849202   0.48754679
  0.49800359  0.50185324  0.50287623  0.50396622  0.50441431  0.51004941
  0.51049928  0.51511116  0.51525789  0.51886553  0.52013964  0.52172151
  0.52205916  0.53021081  0.53178306  0.53259803  0.5376627   0.53879522
  0.5388357   0.5408729   0.5459355   0.54656097  0.55560015  0.55842323
  0.56469122  0.57144842  0.57437077  0.57525294  0.57777574  0.57977531
  0.58465572  0.58643849  0.59163758  0.59897426  0.60103436  0.6011274
  0.60463057  0.60707582  0.61061721  0.61291652  0.61298966  0.61944915
  0.62018351  0.62315374  0.62437262  0.62485193  0.62572529  0.62674507
  0.62679875  0.62708251  0.62722438  0.62945497  0.62959684  0.632111
  0.63331135  0.63368387  0.6361022   0.63867199  0.64190333  0.6436422
  0.6441707   0.64857646  0.65132179  0.65201854  0.65332137  0.65555768
  0.66006672  0.66043873  0.66215331  0.66518364  0.66631436  0.66687909
  0.66714143  0.66852994  0.67020406  0.67026772  0.67030142  0.672301
  0.67467345  0.67645825  0.6770459   0.68137615  0.68179081  0.68220788
  0.68348626  0.68649194  0.68690859  0.6882503   0.68928105  0.69107947
  0.69297607  0.69772098  0.70077044  0.70246589  0.70551534  0.7059912
  0.70958325  0.71022917  0.71026025  0.71124628  0.71254109  0.71260162
  0.71300558  0.71404418  0.71432815  0.71455878  0.71497407  0.71567942
  0.71670061  0.7195162   0.71964363  0.72021802  0.72027094  0.72144551
  0.72322216  0.72619042  0.72627978  0.72683634  0.72920879  0.72961276
  0.72999349  0.73079346  0.73137847  0.73720597  0.73869861  0.73940396
  0.74302886  0.74466596  0.74561319  0.74818842  0.74933338  0.75001277
  0.75125288  0.75146383  0.75308007  0.75310342  0.75344098  0.75406405
  0.75703232  0.75767823  0.7580822   0.75940477  0.76005069  0.76123535
  0.76527649  0.76964406  0.76991338  0.77428541  0.77472964  0.77599999
  0.77903031  0.78140277  0.78163378  0.78293468  0.78377522  0.78547067
  0.78614767  0.78917106  0.79244446  0.79326503  0.79349604  0.79616041
  0.79660994  0.79800994  0.80038239  0.80179921  0.8032307   0.80431239
  0.80994293  0.81320447  0.81698957  0.81736245  0.81936202  0.8197349
  0.82173447  0.83122429  0.83461492  0.84661258  0.847584    0.8510649
  0.85902443  0.86309314  0.8716846   0.87780174  0.87945289  0.88117441
  0.89000635  0.89165025  0.89578201  0.89656271  0.9046335   0.90631576
  0.9072714   0.9120163   0.92026439  0.92625102  0.92669525  0.92862348
  0.93099593  0.93336838  0.93574084  0.94048574  0.94281642  0.94401177
  0.94993378  0.95234801  0.95272089  0.95472046  0.95705114  0.9574658
  0.95946537  0.96932806  0.97141654  0.97170052  0.97644542  0.97926211
  0.98119033  0.98180348  0.98314813  0.99068014  0.99133423  0.99155736
  0.99620575  0.99641108  0.9977975   1.00016996  1.00491486  1.00598492
  1.00700335  1.02086417  1.02267561  1.02488052  1.02731999  1.02794996
  1.04800584  1.05114505  1.05588996  1.0612661   1.11806734  1.26616715
  1.29663438  2.51866165]

  UserWarning,

2022-10-31 11:02:35,895:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.81370778 -0.50647708 -0.43917599 -0.37842913 -0.34789335 -0.33925243
 -0.26553249 -0.25458835 -0.11978107 -0.11919051 -0.09813073 -0.08117799
 -0.07433806 -0.07197974 -0.03000322 -0.0269691  -0.02370193 -0.02107891
  0.01515899  0.01773204  0.02828308  0.03870956  0.04050178  0.04794799
  0.05413944  0.05781648  0.06813298  0.07071211  0.08595383  0.09445459
  0.10142875  0.10660706  0.10831802  0.11197795  0.11577419  0.11872771
  0.12087195  0.12287982  0.13154087  0.13464854  0.14251397  0.14555533
  0.15399703  0.15904685  0.16505429  0.16893607  0.17739895  0.18146106
  0.18391429  0.19475464  0.19732928  0.1996905   0.20048444  0.2011899
  0.2028927   0.20445425  0.20476729  0.20477257  0.20537017  0.20912024
  0.21000342  0.21121371  0.21542955  0.21913655  0.21971498  0.21997871
  0.22084562  0.22280159  0.22773374  0.22836873  0.23218661  0.23262133
  0.23401828  0.23789497  0.23996111  0.24015406  0.24190633  0.24301669
  0.2431457   0.24329667  0.24402898  0.24474953  0.24581402  0.24833657
  0.25019283  0.25095328  0.25313257  0.25324947  0.25353763  0.25401829
  0.25436448  0.25523654  0.2563114   0.2566605   0.25790328  0.25884079
  0.25963045  0.26073692  0.2612728   0.2622561   0.26250475  0.26336415
  0.26528018  0.26596061  0.26637308  0.26647007  0.26656986  0.2668753
  0.26730844  0.27027125  0.27054345  0.27127563  0.27130232  0.27249814
  0.27286314  0.27320497  0.27429729  0.27439361  0.27576155  0.27895196
  0.27937     0.2794039   0.28096531  0.28258325  0.28272613  0.28368132
  0.28375033  0.28567244  0.28608322  0.28618115  0.28620497  0.28729613
  0.28868582  0.28874749  0.28910214  0.28931972  0.28934053  0.28980738
  0.29100901  0.29143505  0.29165107  0.29165214  0.29216381  0.29268853
  0.29288814  0.29293037  0.29325937  0.2939705   0.29587775  0.29599301
  0.29993303  0.30054662  0.30108891  0.30177346  0.30227171  0.30323222
  0.30469305  0.30504908  0.30534906  0.30582599  0.30631962  0.30812484
  0.30870243  0.30936745  0.30949437  0.31339814  0.31481757  0.31583467
  0.31637241  0.31711636  0.3188218   0.32064155  0.32077333  0.32117053
  0.32162405  0.32219372  0.32337457  0.32471657  0.32472003  0.327009
  0.32705797  0.32727276  0.32831945  0.32878776  0.3288358   0.33063056
  0.33182528  0.33321715  0.33495595  0.33571334  0.33648659  0.33844449
  0.33945885  0.34072388  0.341175    0.34245183  0.34290716  0.34300845
  0.34423545  0.34509656  0.34514323  0.3456425   0.34654156  0.34703057
  0.34800727  0.34818642  0.35134038  0.35158345  0.35362502  0.35429794
  0.35482841  0.35706541  0.35853354  0.35887462  0.3597647   0.36130487
  0.36152525  0.36324338  0.36407688  0.36430129  0.36434281  0.36458944
  0.36463064  0.36522707  0.36696355  0.36771648  0.36852911  0.36910478
  0.36937711  0.37002385  0.37187178  0.37207934  0.37262363  0.37343725
  0.37525796  0.37542214  0.37605902  0.37636071  0.37638241  0.37770461
  0.37813964  0.37929636  0.38031815  0.38031994  0.38219681  0.38286914
  0.38385295  0.38408271  0.38568958  0.38569349  0.38634566  0.38879819
  0.38886515  0.3890174   0.39029798  0.39116654  0.39172283  0.39175678
  0.3921784   0.39275096  0.39342374  0.39343087  0.3947659   0.39544909
  0.39567342  0.39585303  0.39820696  0.39890392  0.39960468  0.40207562
  0.40217738  0.4027713   0.4034895   0.40512423  0.40588994  0.40735962
  0.40773093  0.41021322  0.41094185  0.41174458  0.41256018  0.41427814
  0.41581229  0.41590442  0.41609796  0.4169578   0.42005153  0.4214126
  0.42262032  0.42264037  0.42538355  0.42579639  0.42682808  0.42697097
  0.43011705  0.43195675  0.43232694  0.4326783   0.43404417  0.43517438
  0.43542637  0.43788452  0.43853412  0.43974583  0.43989294  0.44087699
  0.4412215   0.45008245  0.45037277  0.45292333  0.45497204  0.45738109
  0.4585455   0.45889894  0.45916712  0.46281118  0.46741158  0.46750356
  0.46865584  0.46986296  0.47080108  0.47566604  0.47695494  0.48134051
  0.48180395  0.48394764  0.48464044  0.48849731  0.49036251  0.4915884
  0.49307099  0.49309006  0.49522913  0.50353794  0.50373809  0.50583574
  0.50673808  0.50942808  0.51118232  0.51501012  0.51584177  0.52523415
  0.52698525  0.53124663  0.53264021  0.53383466  0.53926137  0.54418396
  0.55892348  0.56169407  0.56343043  0.56363703  0.56419871  0.56839254
  0.56848405  0.56852346  0.56976907  0.57694945  0.578352    0.58008557
  0.58184487  0.58685042  0.58731327  0.59302978  0.59516774  0.59737877
  0.59762977  0.5993101   0.60071858  0.60321992  0.6032977   0.60587683
  0.60755809  0.60906309  0.61551516  0.61593853  0.61610037  0.61619333
  0.6179039   0.6207293   0.62135158  0.62487403  0.62514765  0.62625412
  0.6272171   0.62762934  0.63354879  0.63424721  0.63429266  0.63682633
  0.63929225  0.63999067  0.64198458  0.64380303  0.64456371  0.64663315
  0.65230109  0.6525715   0.65450197  0.65488021  0.65582353  0.65714454
  0.65804454  0.65898508  0.65936029  0.66003846  0.66049501  0.66261759
  0.66377235  0.66519671  0.66578192  0.66708603  0.66777584  0.67293409
  0.67551322  0.67593701  0.67658928  0.67809234  0.67867755  0.67986823
  0.6844106   0.68510584  0.68610013  0.68899405  0.68924755  0.69060973
  0.69451042  0.69643734  0.69931055  0.70035215  0.70130447  0.70280694
  0.70393091  0.7044688   0.70540934  0.70595304  0.70636976  0.70646272
  0.70904185  0.70998517  0.71166955  0.71298808  0.7138853   0.7142001
  0.71514342  0.71524677  0.7208841   0.72090134  0.72262096  0.72413836
  0.72671749  0.72803905  0.72925423  0.7301783   0.73127484  0.73148037
  0.73187574  0.73257346  0.7355749   0.73610721  0.73689197  0.73799744
  0.73961312  0.74219224  0.74411347  0.74445657  0.74477137  0.74501281
  0.74507613  0.74622369  0.74992962  0.7501978   0.75603119  0.75766699
  0.75867775  0.76116519  0.76192516  0.76216735  0.76474648  0.7676687
  0.77056262  0.77314175  0.77394739  0.77400683  0.77572087  0.77740213
  0.7779852   0.7783      0.78087912  0.78414173  0.78603738  0.78672086
  0.78771863  0.78955982  0.78962853  0.79119563  0.79345996  0.79377475
  0.79846409  0.80151213  0.80409125  0.80606106  0.81698688  0.82436713
  0.82458746  0.82566758  0.82698859  0.82944063  0.83297306  0.83340496
  0.83642971  0.83769465  0.83795389  0.83914563  0.84246334  0.85224389
  0.85248532  0.85794023  0.8594478   0.86072196  0.86345846  0.87457644
  0.87834394  0.87836058  0.8871347   0.88903565  0.88971382  0.8925581
  0.89487207  0.90355277  0.90606875  0.91360943  0.91577023  0.9231324
  0.92324245  0.92582158  0.92808591  0.92950797  0.93124498  0.93355896
  0.93524021  0.93613808  0.9370814   0.93871721  0.94129633  0.9426628
  0.94297759  0.94392404  0.94435593  0.94645458  0.94997703  0.95419196
  0.95477717  0.95677109  0.95739961  0.96287266  0.96450846  0.96974148
  0.97323774  0.97482496  0.98350566  0.98689043  0.98866391  0.99382216
  0.99640129  0.99792704  0.99811186  1.00319535  1.00413867  1.00495137
  1.0122951   1.01703429  1.02219254  1.02224112  1.03129638  1.06503799
  1.06866538]

  UserWarning,

2022-10-31 11:02:35,895:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.04565302 -0.84450285 -0.75904925 -0.72365144 -0.6623799  -0.56606793
 -0.518642   -0.41424126 -0.39203911 -0.38561188 -0.35563004 -0.3541531
 -0.28733916 -0.28332456 -0.26126767 -0.24413417 -0.23868457 -0.22597675
 -0.18887541 -0.15979404 -0.13853131 -0.10001982 -0.08167049 -0.07653521
 -0.04311978 -0.03073684 -0.02712962 -0.02612148 -0.02348756 -0.00612833
 -0.00291073 -0.00274286  0.02083868  0.04546981  0.0512354   0.06984162
  0.07739052  0.07869841  0.08629994  0.09402676  0.09895561  0.10503819
  0.10556503  0.11133615  0.11351778  0.11423749  0.11514491  0.12452678
  0.129196    0.14440033  0.15542486  0.15651655  0.16389472  0.16558137
  0.16803712  0.17131421  0.17277853  0.17458772  0.17595117  0.17738508
  0.17825577  0.17978118  0.19014312  0.19059802  0.19174173  0.19486267
  0.19555929  0.19661483  0.19766985  0.19857287  0.19919845  0.2018508
  0.20277896  0.20385289  0.20437823  0.21598864  0.22222799  0.22231116
  0.22234317  0.22286233  0.22551477  0.22626977  0.22739679  0.22784902
  0.22917827  0.2298159   0.23005999  0.23166408  0.23876981  0.23926827
  0.23955836  0.24229931  0.24329918  0.24356814  0.24402798  0.24441849
  0.24451974  0.24517934  0.24678     0.24855423  0.25643989  0.25827791
  0.25946858  0.25952195  0.26105657  0.2613075   0.26321193  0.26509744
  0.26585864  0.26624452  0.26642243  0.26768744  0.26908188  0.26959913
  0.27061408  0.27064281  0.27227215  0.27257147  0.27341317  0.27461015
  0.27475863  0.27480698  0.27559895  0.2762409   0.27728861  0.2773191
  0.27738006  0.27805155  0.27904445  0.28183365  0.2819277   0.28206675
  0.28293759  0.28433784  0.28453867  0.28611772  0.28704986  0.28800925
  0.28954406  0.29076086  0.29150754  0.29212756  0.29225719  0.29325983
  0.29329175  0.2934213   0.29402333  0.29708313  0.29850696  0.29946595
  0.29952466  0.2997281   0.3015712   0.30506818  0.30606322  0.30639113
  0.30890869  0.31041106  0.31098123  0.31132171  0.31341274  0.31392029
  0.31434711  0.31438836  0.31578916  0.3164962   0.31779741  0.31963131
  0.31968721  0.3208132   0.32161949  0.32166527  0.32272863  0.32426683
  0.32474214  0.32587329  0.32723912  0.32824943  0.32897434  0.32931364
  0.33053337  0.33090343  0.33111781  0.33189906  0.33310926  0.3352829
  0.33764929  0.33806085  0.33825633  0.33875621  0.33973073  0.33983875
  0.34003916  0.34142414  0.34332731  0.3436097   0.34416674  0.34418049
  0.34566275  0.34610752  0.346131    0.34772067  0.34845476  0.34847699
  0.34859123  0.34929021  0.34978397  0.35090522  0.35099121  0.35166142
  0.35282696  0.35390962  0.35456689  0.35509557  0.35826256  0.35968646
  0.36008581  0.36072799  0.3615944   0.36171256  0.36172126  0.36174753
  0.36196411  0.36214393  0.36258603  0.36305781  0.36381346  0.36535307
  0.36565201  0.36639157  0.36680794  0.36804359  0.36862304  0.36907476
  0.3693536   0.37050302  0.37237856  0.37302681  0.3731396   0.37341571
  0.37412487  0.3765459   0.37712748  0.37827611  0.37935564  0.37942167
  0.37990856  0.38090143  0.38108923  0.38217637  0.38251415  0.38347736
  0.38441944  0.38635666  0.38673385  0.3892504   0.39054181  0.39071658
  0.39684644  0.3973119   0.39942488  0.3995479   0.3999493   0.40022915
  0.40098052  0.40138945  0.40156206  0.40297685  0.40376054  0.40388691
  0.40433087  0.40477945  0.4054851   0.40707218  0.40825326  0.40835527
  0.41041723  0.41084962  0.41179221  0.41434562  0.41541181  0.41553028
  0.41584589  0.41596938  0.41638768  0.41710532  0.41741989  0.41803526
  0.41965241  0.42010631  0.42030235  0.42283349  0.42333382  0.42511786
  0.42543157  0.42584258  0.42643717  0.42749477  0.42804853  0.42963119
  0.4296529   0.43056825  0.43267566  0.43369075  0.43377554  0.4342061
  0.43528815  0.44054572  0.44091389  0.4433509   0.44386488  0.44636754
  0.44897937  0.4492261   0.44963318  0.44997483  0.46024119  0.46578214
  0.46926027  0.46995536  0.47286614  0.47524215  0.47709769  0.48148478
  0.49555316  0.49975815  0.49982201  0.50655652  0.50865469  0.5089376
  0.51047166  0.51240385  0.5156556   0.51807346  0.52387927  0.52456966
  0.52623237  0.53205804  0.53343997  0.53399079  0.53434946  0.53505663
  0.53934885  0.54106144  0.54159372  0.54176191  0.54471393  0.5456831
  0.54620635  0.54792082  0.54838387  0.54912823  0.56420488  0.56475003
  0.56818808  0.57061924  0.57123689  0.57416575  0.5744188   0.57501961
  0.57784912  0.58549471  0.586063    0.59770496  0.59863683  0.60257023
  0.60266956  0.60434516  0.60471477  0.60620438  0.61083542  0.61210829
  0.61359048  0.6151672   0.61800805  0.6191006   0.62126117  0.62185566
  0.62230708  0.62375686  0.62402086  0.62461073  0.62496493  0.62531689
  0.62736579  0.62894251  0.63287591  0.63364163  0.63394302  0.63563097
  0.63685439  0.63838603  0.64078779  0.64093152  0.64155327  0.64389616
  0.64547288  0.64591423  0.64708837  0.65216134  0.65491641  0.65504103
  0.65875253  0.65915553  0.66212708  0.66232773  0.66318159  0.66475831
  0.66557538  0.66646856  0.66869171  0.66933103  0.67039256  0.67064506
  0.67119674  0.67144678  0.67237508  0.67275812  0.67420184  0.6756859
  0.67638251  0.67645725  0.67654977  0.6769569   0.67785903  0.67916681
  0.67929934  0.67971196  0.68271069  0.68283351  0.68382308  0.68385281
  0.68404375  0.68603915  0.68890545  0.68987835  0.69073221  0.6919018
  0.69506399  0.69538847  0.69624233  0.69676454  0.69781905  0.69814353
  0.6989974   0.70089859  0.70101547  0.70175246  0.70332918  0.70365366
  0.70396237  0.70450752  0.70514684  0.70628251  0.70640872  0.70726258
  0.71035159  0.71159436  0.71174505  0.7127727   0.71341202  0.71352527
  0.71434033  0.71434943  0.71927636  0.72013555  0.72019221  0.72106918
  0.72224482  0.72879047  0.73120428  0.73449268  0.73679409  0.73820758
  0.73883621  0.73966349  0.74202063  0.74497958  0.74647277  0.74741017
  0.74867683  0.74922783  0.75060658  0.75292029  0.75749301  0.75806062
  0.75833274  0.76024807  0.76173214  0.76199402  0.76851326  0.77126832
  0.77316928  0.77478911  0.77642514  0.77677845  0.77804156  0.78080758
  0.78248898  0.78504363  0.7858607   0.79055375  0.801574    0.80432906
  0.80469047  0.81128589  0.81692603  0.81968109  0.82000557  0.82085944
  0.8236145   0.82519122  0.83078324  0.83114269  0.83389775  0.83896653
  0.84014487  0.84510984  0.84723171  0.85489692  0.85593825  0.86867223
  0.87300911  0.87969248  0.8852026   0.89622285  0.89779957  0.89897791
  0.89941506  0.89979498  0.90055463  0.90173297  0.90606475  0.90724309
  0.91216336  0.91550828  0.91679125  0.91952906  0.9285466   0.92928359
  0.93168535  0.93444041  0.93479371  0.93678285  0.93754878  0.9391255
  0.93975869  0.94463562  0.94581396  0.94802387  0.94856902  0.95014574
  0.95081148  0.95132408  0.95290081  0.95683421  0.95958927  0.96632281
  0.96667611  0.96710349  0.97060952  0.97218624  0.97611964  0.9788747
  0.98596155  0.98871661  0.99422673  0.99805089  0.99973686  1.01351217
  1.01749484  1.02177735  1.02655048  1.10560813  1.51583983]

  UserWarning,

2022-10-31 11:02:35,927:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-7.73119087e-01 -7.51434568e-01 -5.02883827e-01 -3.85864802e-01
 -2.76146539e-01 -2.56141016e-01 -1.87563167e-01 -1.65706464e-01
 -1.47288801e-01 -8.33930772e-02 -3.98721460e-02  8.16843935e-04
  2.24595810e-02  2.29873171e-02  4.31577725e-02  7.51877021e-02
  7.63480694e-02  8.10352480e-02  9.87306967e-02  1.05126414e-01
  1.16867488e-01  1.20029962e-01  1.29312070e-01  1.32726168e-01
  1.40373964e-01  1.42529818e-01  1.47276948e-01  1.56019645e-01
  1.56420135e-01  1.59526741e-01  1.61159603e-01  1.63440286e-01
  1.63553103e-01  1.66716317e-01  1.66997922e-01  1.70951295e-01
  1.71280461e-01  1.81455142e-01  1.85810518e-01  1.86696854e-01
  1.86978134e-01  1.88782314e-01  1.88924484e-01  1.89815518e-01
  1.90994689e-01  1.91098708e-01  1.96374360e-01  2.01257833e-01
  2.01681085e-01  2.02091619e-01  2.02148559e-01  2.07412840e-01
  2.15716067e-01  2.17902527e-01  2.18414655e-01  2.21302684e-01
  2.21446826e-01  2.23518058e-01  2.23689739e-01  2.24033067e-01
  2.24121004e-01  2.29750509e-01  2.30923511e-01  2.32956926e-01
  2.33583428e-01  2.34367355e-01  2.37635028e-01  2.40219962e-01
  2.40787064e-01  2.42261376e-01  2.42989606e-01  2.45079502e-01
  2.45642509e-01  2.46807010e-01  2.47966343e-01  2.49791821e-01
  2.49913696e-01  2.49943089e-01  2.49964622e-01  2.49975133e-01
  2.51342057e-01  2.53686735e-01  2.54513584e-01  2.55133827e-01
  2.56555366e-01  2.57024595e-01  2.57441299e-01  2.57767500e-01
  2.59115146e-01  2.61700704e-01  2.63320753e-01  2.65030646e-01
  2.65737146e-01  2.66174530e-01  2.66592563e-01  2.67965529e-01
  2.68452612e-01  2.68539557e-01  2.68687188e-01  2.69889740e-01
  2.71407864e-01  2.72080765e-01  2.72485102e-01  2.73999277e-01
  2.74974329e-01  2.76789423e-01  2.77754367e-01  2.79544876e-01
  2.80757008e-01  2.81030424e-01  2.82252795e-01  2.82811309e-01
  2.83049384e-01  2.84435551e-01  2.84950720e-01  2.84981436e-01
  2.85351857e-01  2.86169519e-01  2.86835996e-01  2.87514201e-01
  2.88553133e-01  2.89806167e-01  2.89914817e-01  2.91168345e-01
  2.92364857e-01  2.93857712e-01  2.96936806e-01  2.98693192e-01
  2.98999915e-01  2.99073131e-01  2.99198811e-01  2.99405365e-01
  2.99663869e-01  3.00020326e-01  3.00120684e-01  3.04163838e-01
  3.04480750e-01  3.05194523e-01  3.06702806e-01  3.06741964e-01
  3.08110886e-01  3.08628182e-01  3.10211813e-01  3.10349236e-01
  3.10519484e-01  3.10636053e-01  3.11579001e-01  3.11866120e-01
  3.11993123e-01  3.12676881e-01  3.15686126e-01  3.16305902e-01
  3.16887113e-01  3.18446131e-01  3.19332957e-01  3.19836403e-01
  3.19964331e-01  3.20337008e-01  3.20387880e-01  3.22150061e-01
  3.23926024e-01  3.24265701e-01  3.25307251e-01  3.28841962e-01
  3.30023295e-01  3.30965001e-01  3.30976603e-01  3.30990831e-01
  3.31529559e-01  3.31823897e-01  3.32274272e-01  3.32558619e-01
  3.35064771e-01  3.35959948e-01  3.36194873e-01  3.36370381e-01
  3.37366153e-01  3.38654142e-01  3.39436009e-01  3.39565228e-01
  3.40963668e-01  3.41010398e-01  3.41046284e-01  3.41634425e-01
  3.41822978e-01  3.42041996e-01  3.42237494e-01  3.43100501e-01
  3.43260684e-01  3.44063808e-01  3.45072186e-01  3.45723306e-01
  3.46247420e-01  3.46473531e-01  3.48281410e-01  3.48747962e-01
  3.49499637e-01  3.50023757e-01  3.51045994e-01  3.52791009e-01
  3.52900852e-01  3.52946430e-01  3.53502751e-01  3.53687494e-01
  3.54168724e-01  3.55059923e-01  3.55266827e-01  3.55714271e-01
  3.55843780e-01  3.55895158e-01  3.55979457e-01  3.56201691e-01
  3.57161553e-01  3.57336663e-01  3.57360738e-01  3.57729844e-01
  3.57936073e-01  3.60249196e-01  3.60821940e-01  3.61550486e-01
  3.61592796e-01  3.62724800e-01  3.63364789e-01  3.63781130e-01
  3.64762066e-01  3.65253767e-01  3.66714176e-01  3.67921206e-01
  3.67977919e-01  3.70458768e-01  3.71289886e-01  3.71884177e-01
  3.72233823e-01  3.73350788e-01  3.73781523e-01  3.73909379e-01
  3.73974434e-01  3.75350718e-01  3.76254328e-01  3.78589013e-01
  3.79592043e-01  3.79915588e-01  3.81541223e-01  3.82656394e-01
  3.83011157e-01  3.83058684e-01  3.85391071e-01  3.85464656e-01
  3.86233226e-01  3.87181455e-01  3.87651666e-01  3.88822492e-01
  3.89041105e-01  3.89232780e-01  3.90503228e-01  3.93579226e-01
  3.93803131e-01  3.95776752e-01  3.96242492e-01  3.96421420e-01
  3.98337526e-01  3.98879254e-01  4.00531823e-01  4.00702446e-01
  4.03432663e-01  4.03687037e-01  4.04152777e-01  4.04419705e-01
  4.04464802e-01  4.05048552e-01  4.05397995e-01  4.07397202e-01
  4.08406996e-01  4.08960560e-01  4.09426301e-01  4.10993277e-01
  4.12071874e-01  4.14715817e-01  4.14800024e-01  4.16020697e-01
  4.17885708e-01  4.19047343e-01  4.20382274e-01  4.20780868e-01
  4.24668494e-01  4.26558524e-01  4.31313343e-01  4.31796612e-01
  4.32186515e-01  4.35834439e-01  4.36096550e-01  4.37599544e-01
  4.39056217e-01  4.40973259e-01  4.42980791e-01  4.47802145e-01
  4.48303936e-01  4.50163996e-01  4.53612450e-01  4.55324026e-01
  4.63482778e-01  4.64274168e-01  4.64462185e-01  4.65296721e-01
  4.66848089e-01  4.68596264e-01  4.68705372e-01  4.69333217e-01
  4.70104503e-01  4.70865407e-01  4.72526744e-01  4.72691756e-01
  4.81747321e-01  4.83448000e-01  4.88683198e-01  4.92504095e-01
  4.95951960e-01  5.01157585e-01  5.05143308e-01  5.07399261e-01
  5.08395229e-01  5.10609240e-01  5.15012360e-01  5.19101122e-01
  5.22262829e-01  5.22724807e-01  5.23101808e-01  5.30745489e-01
  5.33270525e-01  5.37637681e-01  5.40038707e-01  5.41692295e-01
  5.44999549e-01  5.49892132e-01  5.50913632e-01  5.52139277e-01
  5.52444361e-01  5.53057324e-01  5.54214924e-01  5.57629216e-01
  5.58849440e-01  5.61132800e-01  5.62665615e-01  5.66082433e-01
  5.69043085e-01  5.69636296e-01  5.71679847e-01  5.71945042e-01
  5.79590132e-01  5.80220965e-01  5.81062085e-01  5.83184302e-01
  5.90607090e-01  5.92773940e-01  5.93064070e-01  5.94245894e-01
  5.99519417e-01  6.00295453e-01  6.05957748e-01  6.08594510e-01
  6.11231271e-01  6.12426128e-01  6.14320476e-01  6.16504795e-01
  6.16823006e-01  6.21778318e-01  6.24415080e-01  6.25124003e-01
  6.27404814e-01  6.31510388e-01  6.34188659e-01  6.34962126e-01
  6.35460552e-01  6.38347412e-01  6.39411758e-01  6.40734075e-01
  6.45509173e-01  6.47322043e-01  6.47398591e-01  6.48145934e-01
  6.49303899e-01  6.50316955e-01  6.50782696e-01  6.51576285e-01
  6.52645990e-01  6.53419457e-01  6.54553972e-01  6.58692981e-01
  6.61329742e-01  6.63966504e-01  6.66055955e-01  6.70515168e-01
  6.71839346e-01  6.71876789e-01  6.72195000e-01  6.72416811e-01
  6.73479467e-01  6.73517318e-01  6.73740083e-01  6.74513551e-01
  6.75107926e-01  6.75623195e-01  6.77150312e-01  6.79787074e-01
  6.85954022e-01  6.86873467e-01  6.87697359e-01  6.89966123e-01
  6.91443771e-01  6.92895515e-01  6.94217833e-01  6.95607644e-01
  6.95685569e-01  6.95699788e-01  6.96106070e-01  6.96401233e-01
  6.96717288e-01  6.97420514e-01  6.98013964e-01  7.00057276e-01
  7.02694037e-01  7.04016355e-01  7.04946628e-01  7.05330799e-01
  7.06154690e-01  7.06948256e-01  7.07727436e-01  7.09289878e-01
  7.10604322e-01  7.12406493e-01  7.12498360e-01  7.14064975e-01
  7.14858564e-01  7.15570246e-01  7.15877845e-01  7.17410661e-01
  7.20344023e-01  7.20685628e-01  7.21151369e-01  7.21229294e-01
  7.22003702e-01  7.22473686e-01  7.23084905e-01  7.25405611e-01
  7.26424892e-01  7.28223102e-01  7.28358428e-01  7.29061654e-01
  7.30271535e-01  7.31429500e-01  7.33020733e-01  7.33680272e-01
  7.34335177e-01  7.35493142e-01  7.35952657e-01  7.38402804e-01
  7.38589419e-01  7.43862942e-01  7.44015800e-01  7.45158804e-01
  7.45510919e-01  7.46204541e-01  7.47165770e-01  7.47523831e-01
  7.51489692e-01  7.51773227e-01  7.54562846e-01  7.57046750e-01
  7.59683512e-01  7.61559370e-01  7.62873814e-01  7.64661872e-01
  7.65321411e-01  7.65343025e-01  7.66887474e-01  7.67298634e-01
  7.67593797e-01  7.69126612e-01  7.70230558e-01  7.70513685e-01
  7.72867320e-01  7.76252606e-01  7.86051128e-01  7.88687890e-01
  7.91324651e-01  7.92799826e-01  7.99234936e-01  8.01871698e-01
  8.04508459e-01  8.06539701e-01  8.10712069e-01  8.11576271e-01
  8.12418744e-01  8.15845805e-01  8.17308604e-01  8.17692268e-01
  8.21949686e-01  8.22350529e-01  8.23464217e-01  8.24075436e-01
  8.26342436e-01  8.33512837e-01  8.36140672e-01  8.36149599e-01
  8.46230905e-01  8.48867666e-01  8.51504428e-01  8.59697520e-01
  8.63015641e-01  8.64474482e-01  8.65077853e-01  8.77011305e-01
  8.88261397e-01  8.90974283e-01  8.96718992e-01  8.99355754e-01
  9.06718729e-01  9.09518629e-01  9.09935486e-01  9.12539562e-01
  9.18311511e-01  9.20948273e-01  9.22227949e-01  9.23086609e-01
  9.23626631e-01  9.28360132e-01  9.28858558e-01  9.29108656e-01
  9.31179099e-01  9.33633655e-01  9.36768843e-01  9.41543940e-01
  9.42042366e-01  9.45798158e-01  9.46817463e-01  9.49952651e-01
  9.55226174e-01  9.57364510e-01  9.60001271e-01  9.65773221e-01
  9.66590779e-01  9.67087665e-01  9.72289582e-01  9.72637769e-01
  9.73683506e-01  9.77911293e-01  9.78957029e-01  9.79805733e-01
  9.81593791e-01  9.82205009e-01  9.82405646e-01  9.84230552e-01
  9.84979077e-01  9.85502464e-01  9.86867314e-01  9.89196452e-01
  9.90824320e-01  9.91675096e-01  9.92140837e-01  1.00043711e+00
  1.00268788e+00  1.00485890e+00  1.01059817e+00  1.01268762e+00
  1.01990601e+00  1.02114522e+00  1.03514088e+00  1.04019074e+00
  1.04432876e+00  1.31622560e+00  1.44533592e+00]

  UserWarning,

2022-10-31 11:02:36,696:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.4106432  -0.40131823 -0.35261898 -0.23299735 -0.21444307 -0.18296433
 -0.13885849 -0.13142853 -0.10105376 -0.03473708 -0.02818374 -0.0194759
 -0.01093454 -0.00586488  0.02752858  0.0399851   0.0454719   0.04803761
  0.05082278  0.05263528  0.0789575   0.07903334  0.08710848  0.08881402
  0.10114859  0.10432017  0.10898431  0.11706806  0.12689001  0.1283329
  0.13307604  0.13433481  0.1376852   0.13867701  0.14912017  0.15343985
  0.1534842   0.15451012  0.15651104  0.15653556  0.16253026  0.16372426
  0.16754243  0.18865565  0.19243534  0.19311153  0.19426708  0.19712022
  0.20339877  0.20348327  0.20470718  0.20504853  0.20864831  0.20935819
  0.21049551  0.21220841  0.21545213  0.21556539  0.21568886  0.21757685
  0.22068849  0.22096726  0.22167896  0.2222569   0.22229638  0.2229421
  0.22374995  0.22507046  0.22638921  0.22944369  0.2307092   0.23093642
  0.23220598  0.2354758   0.23598922  0.2366699   0.23714539  0.23778019
  0.23827292  0.23860712  0.24313975  0.2442204   0.24445982  0.24577852
  0.24675473  0.2469225   0.24946462  0.25171647  0.25328449  0.25356874
  0.25636398  0.25645534  0.25894554  0.26138083  0.26540976  0.26831453
  0.26879317  0.2701771   0.27076042  0.27105065  0.27154388  0.2727519
  0.27324657  0.27349716  0.27433863  0.27440154  0.27504837  0.27741528
  0.27760293  0.27826632  0.2798925   0.2812407   0.28231444  0.2843083
  0.28496696  0.28654196  0.28830458  0.28852345  0.28873265  0.28891195
  0.28911975  0.28989269  0.29149131  0.29154132  0.29163293  0.2917512
  0.29227915  0.29278825  0.29362803  0.29373089  0.29435115  0.2952679
  0.2956108   0.29772652  0.29840171  0.29850787  0.29857437  0.299234
  0.29925025  0.29941388  0.29952028  0.30024467  0.30082877  0.30103045
  0.3010332   0.30192556  0.30260246  0.30267848  0.30310535  0.30404994
  0.30467536  0.30476523  0.30570504  0.30578787  0.30665327  0.30719297
  0.30747594  0.30839616  0.30957443  0.30975214  0.31033631  0.31145182
  0.31271336  0.31384079  0.31516266  0.31561576  0.31568192  0.31731052
  0.31791294  0.31847418  0.31963915  0.32170443  0.32222866  0.3228453
  0.32350558  0.3237165   0.32378217  0.3260509   0.32659135  0.32862938
  0.3290624   0.33002924  0.33097663  0.33115974  0.33162076  0.33291064
  0.3329665   0.33340028  0.33585173  0.3369787   0.33837245  0.3384365
  0.33874071  0.33875442  0.33985085  0.34033037  0.34054604  0.34169061
  0.34305652  0.34381094  0.34403542  0.34422728  0.34662468  0.34665198
  0.34714261  0.34904421  0.34975724  0.35069687  0.35144427  0.35146096
  0.35190441  0.35255177  0.35405172  0.35484484  0.3555142   0.35589995
  0.35677711  0.35683954  0.35718296  0.3582759   0.35879627  0.35917961
  0.36146872  0.36304422  0.36352769  0.36455344  0.36491354  0.36495438
  0.36520073  0.36651732  0.36690559  0.36691655  0.36756205  0.3688487
  0.36901921  0.36920965  0.36926791  0.37297169  0.374149    0.3745814
  0.37576277  0.37669263  0.37675875  0.3787315   0.37917807  0.37963735
  0.38175529  0.38190605  0.3829224   0.38393822  0.38545373  0.38558233
  0.38679878  0.38798506  0.38864697  0.39003742  0.39121422  0.3920099
  0.39299486  0.39304772  0.39317364  0.39335191  0.39339965  0.39465289
  0.39546236  0.39573022  0.39811038  0.39811328  0.39913898  0.39941402
  0.39985984  0.40064171  0.40311278  0.40317304  0.4044167   0.40490789
  0.40565151  0.40570437  0.40594114  0.40613164  0.40671949  0.40788615
  0.4082357   0.41076703  0.41138636  0.41148731  0.41224251  0.4198815
  0.42077142  0.42107925  0.42181616  0.42325933  0.42391774  0.42633314
  0.42689986  0.42748383  0.43039799  0.43481894  0.43671808  0.43814831
  0.4393802   0.44282126  0.44330383  0.44678659  0.4480482   0.44938987
  0.44983643  0.45089782  0.45236776  0.45489909  0.45743042  0.45937696
  0.45996175  0.46102314  0.46317287  0.46500284  0.4660858   0.4670366
  0.47333869  0.47360626  0.47507425  0.47581718  0.48291069  0.48418108
  0.48489516  0.49260319  0.49467164  0.49724242  0.50019718  0.51394039
  0.51467759  0.5153415   0.51754402  0.51760404  0.51781185  0.52161254
  0.52224253  0.5225805   0.53310447  0.5356358   0.54570826  0.54576112
  0.54783255  0.55045833  0.55648749  0.56156683  0.56356718  0.57015516
  0.5760871   0.5762136   0.58274071  0.58727642  0.58732806  0.58736185
  0.590885    0.59127508  0.59311762  0.59762229  0.59967729  0.6011428
  0.60385867  0.6046013   0.60529008  0.60774632  0.61023953  0.61035274
  0.61069867  0.61405705  0.61658838  0.6185137   0.61911971  0.62165104
  0.6250194   0.62863902  0.62924503  0.63430769  0.63686776  0.63937035
  0.64190168  0.64432006  0.64443301  0.64696434  0.64949567  0.64976044
  0.65133821  0.652027    0.65468999  0.65757133  0.66140663  0.66431315
  0.66468365  0.66721498  0.66974631  0.67018466  0.67019612  0.6705799
  0.67084575  0.67227764  0.67480897  0.67611041  0.67665151  0.6773403
  0.67853615  0.68051792  0.68165244  0.68321921  0.6842455   0.68432828
  0.68746562  0.69183949  0.69192227  0.6938516   0.69482358  0.69500675
  0.69505961  0.6958932   0.69690215  0.69698493  0.69759094  0.69779603
  0.69943348  0.70012227  0.70196481  0.7026536   0.70750672  0.71024759
  0.71462146  0.71532807  0.71724329  0.71853691  0.71894102  0.72626916
  0.72818486  0.72869585  0.72987333  0.73127896  0.73205306  0.73297752
  0.73316122  0.73772173  0.73844737  0.74013692  0.74145715  0.74160336
  0.74278439  0.74559791  0.74651981  0.74662204  0.7541138   0.75577655
  0.75664513  0.75691097  0.75891955  0.76290364  0.76303503  0.76677045
  0.76930178  0.77011     0.77051288  0.77164198  0.77183311  0.77304421
  0.77436444  0.77689577  0.7794271   0.78268395  0.78276665  0.78448976
  0.7855892   0.78631043  0.78782931  0.79208375  0.79456222  0.79714641
  0.79960421  0.79967774  0.80077718  0.80330851  0.80468754  0.80487206
  0.80739481  0.80859924  0.80952941  0.81233439  0.81343383  0.81465618
  0.81602824  0.81635761  0.81734419  0.81971884  0.82102782  0.82223892
  0.82357741  0.82765403  0.83075822  0.83115314  0.8350635   0.83535716
  0.83764769  0.83874713  0.84458568  0.84634112  0.84913795  0.84964834
  0.85151297  0.85646644  0.85977366  0.86225213  0.86361398  0.86508378
  0.86562398  0.86757719  0.87071524  0.8800243   0.88130285  0.89091302
  0.89254269  0.89521229  0.90027495  0.901568    0.90280628  0.90390572
  0.90786894  0.90896838  0.91040027  0.9129316   0.91546293  0.91799426
  0.9190937   0.92162503  0.92300406  0.92305692  0.92811958  0.92921902
  0.93065091  0.93306359  0.93571357  0.93681301  0.93762124  0.94077623
  0.94330756  0.944407    0.94465446  0.94583889  0.94837022  0.95521031
  0.95849554  0.95959498  0.96102687  0.96212631  0.96343955  0.96590902
  0.96608953  0.96875252  0.97225163  0.97478296  0.97731429  0.97984562
  0.98065385  0.98237695  0.98490828  0.98596967  0.98997094  0.99250227
  0.99584183  0.99756493  1.00262759  1.00793771  1.00985109  1.01934439
  1.0210751   1.02250774  1.28330982]

  UserWarning,

2022-10-31 11:02:36,743:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.95227898 -0.78157472 -0.48119186 -0.39358498 -0.33397497 -0.3143276
 -0.31081464 -0.30887746 -0.26864586 -0.25947918 -0.25675931 -0.25581714
 -0.12154502 -0.11867552 -0.10201774 -0.09434295 -0.05578859 -0.04737465
 -0.0340599  -0.03026718  0.00295308  0.01119338  0.02331091  0.03058403
  0.03457226  0.04094351  0.05513802  0.06027344  0.06781999  0.06890992
  0.08334548  0.0855771   0.08623848  0.09402878  0.10131812  0.10714207
  0.10982025  0.12698994  0.12848897  0.12851052  0.13651895  0.15065335
  0.15644648  0.16347079  0.16985765  0.1713272   0.1763457   0.17796296
  0.17812915  0.1811501   0.18671478  0.19215118  0.19730775  0.20278156
  0.20630153  0.20713441  0.20718308  0.21086975  0.2111338   0.2113237
  0.21237601  0.21348378  0.21368212  0.21397818  0.21398956  0.21697067
  0.21813487  0.21911686  0.21952921  0.22038451  0.22187544  0.22452416
  0.22743713  0.23326631  0.23693902  0.23780146  0.23791093  0.24075849
  0.24089617  0.24210658  0.24323937  0.2437666   0.24654752  0.24673273
  0.24934707  0.25048647  0.25055818  0.25118449  0.25135534  0.25167388
  0.25416154  0.2572368   0.25920414  0.25922009  0.25967082  0.26088765
  0.26420897  0.26484771  0.26487714  0.26583555  0.26691453  0.26774033
  0.26784978  0.27037406  0.27110047  0.27146809  0.27254407  0.27367048
  0.27371797  0.27433296  0.2744804   0.27520669  0.27556389  0.27564164
  0.27662147  0.27684452  0.27808654  0.27852799  0.27855239  0.28004997
  0.28070116  0.28080504  0.28318103  0.28390424  0.28500708  0.28542284
  0.28570032  0.28578395  0.28637646  0.28745258  0.28756513  0.28798319
  0.28890387  0.29151648  0.29184559  0.29197619  0.29205054  0.29292925
  0.29324298  0.29389413  0.29538328  0.29688802  0.29793604  0.29875405
  0.29921306  0.29935219  0.29962617  0.29979959  0.30356149  0.30516118
  0.30575447  0.30815266  0.30922841  0.3116514   0.31173568  0.31324541
  0.31385718  0.31463023  0.31484838  0.3152341   0.3158038   0.31620544
  0.31917379  0.31964278  0.32027617  0.32089717  0.32098192  0.32176669
  0.32268656  0.32299074  0.3270686   0.32833841  0.32890927  0.33023999
  0.33048228  0.33213933  0.33310957  0.33499579  0.33613371  0.33711267
  0.33739478  0.33841519  0.33864253  0.34003519  0.34118161  0.34194122
  0.34227995  0.34389656  0.34408943  0.3442494   0.34517836  0.34757599
  0.34794175  0.34799752  0.3491925   0.35119877  0.35131262  0.35250175
  0.3526402   0.35299447  0.35487027  0.35518106  0.35627004  0.35681399
  0.36107015  0.36162633  0.36546676  0.36844694  0.36901218  0.37430334
  0.37450897  0.37520745  0.37535421  0.37543616  0.37822891  0.37857501
  0.37891588  0.37936551  0.38009362  0.38068682  0.38069262  0.38082769
  0.38203874  0.38229444  0.3823644   0.38268126  0.38323905  0.38435
  0.38484162  0.38491941  0.38498733  0.38608722  0.38735447  0.38806064
  0.38867864  0.38909332  0.38917121  0.39002136  0.39025385  0.39111702
  0.39185343  0.39225715  0.39239647  0.39433048  0.39512408  0.3952011
  0.39567329  0.39572293  0.39649285  0.39658642  0.39717996  0.39907679
  0.40373244  0.40379416  0.40385434  0.40556058  0.40647533  0.40815045
  0.40883384  0.40936168  0.40958605  0.4097172   0.41003058  0.41050934
  0.41074032  0.41180908  0.41193987  0.41238222  0.41361783  0.41387682
  0.41516071  0.41580234  0.41625742  0.41685202  0.41697657  0.41888822
  0.41926956  0.42269748  0.42387567  0.42508147  0.42666823  0.42700553
  0.43042906  0.43227951  0.43372232  0.43561292  0.43859385  0.44118372
  0.44636346  0.44720305  0.44793021  0.44831186  0.45008131  0.45024764
  0.45183455  0.45439141  0.4544545   0.45599504  0.45671312  0.45704775
  0.45855279  0.4592358   0.45931281  0.45953335  0.46114279  0.46331229
  0.46393427  0.46593097  0.48341917  0.48406006  0.48603502  0.48838191
  0.48841606  0.49324078  0.49547458  0.49889312  0.50139247  0.50218916
  0.5025018   0.50455463  0.5047882   0.5073689   0.50827497  0.50856799
  0.51082375  0.5117249   0.51560162  0.52083044  0.52194247  0.53175142
  0.53218206  0.53446147  0.53844735  0.54620594  0.55003864  0.55041549
  0.55344863  0.55474714  0.55514096  0.55579549  0.55657644  0.5634996
  0.56826245  0.56874316  0.57324264  0.57424095  0.5757612   0.57593735
  0.58111709  0.58370696  0.58558421  0.59111826  0.59329774  0.59406644
  0.59455072  0.59478945  0.59784243  0.60683873  0.60692465  0.6070158
  0.61210439  0.61737528  0.62773476  0.63097982  0.63282335  0.6329145
  0.63550437  0.63809424  0.63869061  0.64658345  0.64902554  0.6510436
  0.65136182  0.65212255  0.65241563  0.65260736  0.65355645  0.65363347
  0.65622334  0.65646139  0.65881321  0.66076803  0.66219969  0.66274076
  0.6633579   0.66399295  0.66454867  0.66478956  0.66658282  0.66737741
  0.66737943  0.66756849  0.66917269  0.67007698  0.67011346  0.67176256
  0.67435244  0.67514904  0.6757454   0.67773891  0.67889712  0.67921111
  0.67953218  0.68291866  0.68407686  0.68467322  0.68616371  0.6880984
  0.69303725  0.69327814  0.69443635  0.69827036  0.69845788  0.6988723
  0.69961609  0.70060253  0.70220596  0.70227068  0.70284101  0.70353867
  0.70363762  0.70535387  0.70543089  0.70622749  0.70808175  0.70881736
  0.71312348  0.7132005   0.71399711  0.71512579  0.71515531  0.71878671
  0.72067615  0.72254787  0.7321262   0.73704755  0.73731269  0.73990256
  0.74023315  0.74126622  0.74240867  0.7450823   0.74684843  0.74767217
  0.7502553   0.75026204  0.7527749   0.75320248  0.7591061   0.75918312
  0.76043284  0.76152581  0.7632114   0.76580127  0.76799425  0.76896657
  0.76928462  0.77098101  0.77330182  0.77496268  0.77706503  0.7805718
  0.783188    0.7834025   0.78372206  0.78581712  0.78652024  0.78911011
  0.79081862  0.79494503  0.79584778  0.79687972  0.80205946  0.80464933
  0.8072392   0.80982907  0.81043699  0.81347714  0.81448763  0.81500881
  0.82277843  0.82404248  0.82558919  0.8316887   0.83613425  0.83686844
  0.83831765  0.84011124  0.84164171  0.84722792  0.84729034  0.85836811
  0.86276714  0.86487353  0.86802143  0.86884036  0.8788818   0.8921492
  0.90153819  0.90161521  0.90242937  0.90316355  0.90357002  0.90420508
  0.91126495  0.91197469  0.9127713   0.91662844  0.91707742  0.91715443
  0.9197443   0.92294102  0.93002677  0.9320586   0.9378734   0.93811738
  0.94038625  0.94234339  0.94241808  0.94269483  0.94305314  0.94493094
  0.94500795  0.94564301  0.94625768  0.94759783  0.9501877   0.95139818
  0.95402729  0.95536744  0.95795731  0.95859236  0.96054718  0.96306004
  0.96636198  0.96697664  0.96895185  0.96953001  0.97154172  0.97415172
  0.9760864   0.97867627  0.98126615  0.98232159  0.98385602  0.98644589
  0.98903576  0.99287535  0.9942155   0.99995328  1.00162288  1.00198511
  1.00386723  1.00457498  1.00845061  1.01262134  1.02072887  1.0239538
  1.02744795  1.06734655  1.06918805  1.07454965  1.22456555  1.25484376
  1.26811931  1.32706393  1.39296634]

  UserWarning,

2022-10-31 11:02:36,743:INFO:Calculating mean and std
2022-10-31 11:02:36,743:INFO:Creating metrics dataframe
2022-10-31 11:02:36,743:INFO:Uploading results into container
2022-10-31 11:02:36,743:INFO:Uploading model into container now
2022-10-31 11:02:36,743:INFO:master_model_container: 16
2022-10-31 11:02:36,743:INFO:display_container: 2
2022-10-31 11:02:36,743:INFO:BayesianRidge()
2022-10-31 11:02:36,743:INFO:create_model() successfully completed......................................
2022-10-31 11:02:36,867:ERROR:create_model() for BayesianRidge() raised an exception or returned all 0.0:
2022-10-31 11:02:36,867:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:36,867:INFO:Initializing Passive Aggressive Regressor
2022-10-31 11:02:36,867:INFO:Total runtime is 1.0646253466606141 minutes
2022-10-31 11:02:36,868:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:36,868:INFO:Initializing create_model()
2022-10-31 11:02:36,868:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:36,868:INFO:Checking exceptions
2022-10-31 11:02:36,871:INFO:Importing libraries
2022-10-31 11:02:36,871:INFO:Copying training dataset
2022-10-31 11:02:36,876:INFO:Defining folds
2022-10-31 11:02:36,876:INFO:Declaring metric variables
2022-10-31 11:02:36,876:INFO:Importing untrained model
2022-10-31 11:02:36,877:INFO:Passive Aggressive Regressor Imported successfully
2022-10-31 11:02:36,878:INFO:Starting cross validation
2022-10-31 11:02:36,880:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:39,086:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.56774523e-01 -2.54804588e-01 -2.47394484e-01 -2.38446023e-01
 -2.29929139e-01 -2.13570574e-01 -2.02671622e-01 -1.85186832e-01
 -1.82419704e-01 -1.77776728e-01 -1.73902820e-01 -1.60066051e-01
 -1.57826019e-01 -1.57544255e-01 -1.49392986e-01 -1.48595794e-01
 -1.42161350e-01 -1.40032393e-01 -1.40012947e-01 -1.17651517e-01
 -1.15137499e-01 -1.12801948e-01 -1.08722500e-01 -9.74384882e-02
 -9.33666671e-02 -8.96625140e-02 -8.59565638e-02 -6.80596410e-02
 -6.65212829e-02 -6.60897056e-02 -5.75728215e-02 -5.58386037e-02
 -5.35655481e-02 -5.01627181e-02 -4.73580248e-02 -4.12142567e-02
 -3.96758986e-02 -2.33173339e-02 -1.79581787e-02 -1.43688725e-02
 -5.42041110e-03 -2.57527384e-03  3.52805031e-03  8.88720548e-03
  1.57490875e-02  2.03557578e-02  2.26913084e-02  2.67841283e-02
  2.70330996e-02  3.03734345e-02  3.10656101e-02  5.72188188e-02
  6.61672802e-02  7.51157416e-02  7.70856769e-02  8.01931329e-02
  8.40642030e-02  8.56025611e-02  9.06015003e-02  9.08832642e-02
  9.30126644e-02  9.45510225e-02  9.49825997e-02  1.01961126e-01
  1.10909587e-01  1.12860077e-01  1.16517714e-01  1.25217204e-01
  1.25466175e-01  1.26897827e-01  1.28806510e-01  1.30753111e-01
  1.34165665e-01  1.35432000e-01  1.43340119e-01  1.43677597e-01
  1.47121400e-01  1.52062588e-01  1.55651894e-01  1.61011049e-01
  1.62171465e-01  1.73966784e-01  1.78907972e-01  1.89612688e-01
  1.93212693e-01  1.96804895e-01  2.05753356e-01  2.10316602e-01
  2.18291124e-01  2.20241614e-01  2.23650279e-01  2.27239586e-01
  2.41547202e-01  2.47908635e-01  2.54084970e-01  2.71981893e-01
  2.80930354e-01  2.83594925e-01  2.98827277e-01  3.22083355e-01
  3.31031816e-01  3.39815382e-01  3.48928739e-01  3.52936012e-01
  3.56902313e-01  3.65513296e-01  3.68948281e-01  3.71992843e-01
  3.76841634e-01  3.78603565e-01  3.80067173e-01  3.83156655e-01
  3.86850674e-01  3.88391266e-01  3.91628193e-01  3.92265175e-01
  4.12935795e-01  4.15157275e-01  4.17140624e-01  4.24527717e-01
  4.28877136e-01  4.34905382e-01  4.40614598e-01  4.42447650e-01
  4.44676339e-01  4.46753136e-01  4.50857672e-01  4.63902901e-01
  4.64151314e-01  4.71250696e-01  4.75179868e-01  4.77690805e-01
  4.78815843e-01  4.80958425e-01  4.89525508e-01  4.92035210e-01
  4.99740218e-01  5.04922552e-01  5.07984816e-01  5.08546361e-01
  5.12987927e-01  5.13923769e-01  5.31296562e-01  5.36064926e-01
  5.40083533e-01  5.44291586e-01  5.45013387e-01  5.45444965e-01
  5.51882974e-01  5.52141727e-01  5.53961849e-01  5.62670843e-01
  5.63197866e-01  5.65390559e-01  5.67806396e-01  5.70320413e-01
  5.71858772e-01  5.78297104e-01  5.80308827e-01  5.80388544e-01
  5.96884034e-01  5.96907045e-01  6.00711316e-01  6.05855506e-01
  6.10391619e-01  6.12727170e-01  6.14441503e-01  6.15230835e-01
  6.16601079e-01  6.17032656e-01  6.23035522e-01  6.23752429e-01
  6.25018677e-01  6.26259922e-01  6.27500030e-01  6.27793537e-01
  6.28612574e-01  6.32959643e-01  6.33008656e-01  6.41649352e-01
  6.45854188e-01  6.50290295e-01  6.50597813e-01  6.50856566e-01
  6.55797754e-01  6.59546274e-01  6.59805028e-01  6.61084633e-01
  6.61343386e-01  6.62005692e-01  6.68005816e-01  6.68494736e-01
  6.68753489e-01  6.69599790e-01  6.70703979e-01  6.71566242e-01
  6.72261782e-01  6.73058532e-01  6.77443197e-01  6.77701950e-01
  6.81709224e-01  6.84521012e-01  6.86054759e-01  6.86391659e-01
  6.86650412e-01  6.88188770e-01  6.89924903e-01  6.95340120e-01
  7.04288582e-01  7.14186586e-01  7.22444257e-01  7.23982616e-01
  7.30003371e-01  7.30045522e-01  7.31032060e-01  7.31133966e-01
  7.40082427e-01  7.40341180e-01  7.47045309e-01  7.49030889e-01
  7.50555977e-01  7.50828000e-01  7.55040681e-01  7.58238103e-01
  7.66246080e-01  7.67186564e-01  7.74043221e-01  7.76561893e-01
  7.77414631e-01  7.84824734e-01  7.85083487e-01  7.88693696e-01
  7.89248877e-01  7.93773196e-01  7.93899569e-01  7.94031949e-01
  7.99049033e-01  8.01071459e-01  8.03724186e-01  8.03920465e-01
  8.04260015e-01  8.04518768e-01  8.06999017e-01  8.17278245e-01
  8.20618580e-01  8.20877333e-01  8.25338944e-01  8.33213709e-01
  8.34444955e-01  8.35184950e-01  8.40053861e-01  8.45889666e-01
  8.47463964e-01  8.48057570e-01  8.54053864e-01  8.57645118e-01
  8.63700632e-01  8.67157998e-01  8.76106459e-01  8.77994633e-01
  8.82714215e-01  8.83257810e-01  8.85054921e-01  8.92206271e-01
  8.92383869e-01  8.98010656e-01  9.02257571e-01  9.09082224e-01
  9.10192499e-01  9.12066966e-01  9.15770787e-01  9.21127502e-01
  9.24025494e-01  9.27832755e-01  9.29877660e-01  9.44522046e-01
  9.57122151e-01  9.60168252e-01  9.61781706e-01  9.62375175e-01
  9.70256140e-01  9.72742424e-01  9.75019074e-01  9.76738621e-01
  9.79956898e-01  9.82852774e-01  9.91702179e-01  9.99143746e-01
  1.00032586e+00  1.00380485e+00  1.01074551e+00  1.01077201e+00
  1.01740101e+00  1.02193175e+00  1.03300006e+00  1.03326350e+00
  1.03538155e+00  1.04817455e+00  1.05043396e+00  1.05327858e+00
  1.06069412e+00  1.06125100e+00  1.07984206e+00  1.09905700e+00
  1.10349706e+00  1.10759190e+00  1.10774549e+00  1.11343726e+00
  1.12053089e+00  1.13315033e+00  1.13600273e+00  1.14888536e+00
  1.17933318e+00  1.19368688e+00  1.21073601e+00  1.21899390e+00
  1.22126219e+00  1.22183171e+00  1.22338846e+00  1.23494321e+00
  1.23982959e+00  1.24116894e+00  1.25509449e+00  1.26382300e+00
  1.26445234e+00  1.26475612e+00  1.26772876e+00  1.27454807e+00
  1.28653003e+00  1.29473262e+00  1.29858797e+00  1.30113786e+00
  1.30310205e+00  1.31318074e+00  1.31784955e+00  1.33272029e+00
  1.33357804e+00  1.33963833e+00  1.35092017e+00  1.35409830e+00
  1.35932011e+00  1.38183878e+00  1.38279912e+00  1.40053161e+00
  1.40395782e+00  1.40668585e+00  1.40769789e+00  1.41080758e+00
  1.41333481e+00  1.42029509e+00  1.42097250e+00  1.43339076e+00
  1.44103003e+00  1.44937777e+00  1.44983276e+00  1.45153896e+00
  1.47204674e+00  1.47397276e+00  1.48393992e+00  1.49373971e+00
  1.50382357e+00  1.51016068e+00  1.51528256e+00  1.51628384e+00
  1.51932761e+00  1.51945309e+00  1.52837360e+00  1.52970072e+00
  1.53467861e+00  1.54814341e+00  1.55779492e+00  1.56276925e+00
  1.57255526e+00  1.58267032e+00  1.58771904e+00  1.59458682e+00
  1.59902563e+00  1.60654884e+00  1.61314037e+00  1.62141301e+00
  1.63303807e+00  1.65408153e+00  1.65459740e+00  1.65813817e+00
  1.67265313e+00  1.67558999e+00  1.68131061e+00  1.69783046e+00
  1.69982091e+00  1.70280601e+00  1.70672910e+00  1.71568904e+00
  1.73156053e+00  1.74610079e+00  1.74845228e+00  1.76144604e+00
  1.77420525e+00  1.79591683e+00  1.79825127e+00  1.81570323e+00
  1.81993430e+00  1.84188105e+00  1.84786193e+00  1.86433998e+00
  1.87001713e+00  1.88596795e+00  1.89175040e+00  1.89610942e+00
  1.90419723e+00  1.90653431e+00  1.91247623e+00  1.92235519e+00
  1.93154752e+00  1.96293177e+00  1.97805634e+00  1.98605729e+00
  1.98640641e+00  2.04529350e+00  2.07037275e+00  2.08890753e+00
  2.10545216e+00  2.12663973e+00  2.14953184e+00  2.19703496e+00
  2.22347518e+00  2.23219379e+00  2.24889468e+00  2.29079205e+00
  2.29140529e+00  2.29159184e+00  2.29328879e+00  2.29540148e+00
  2.29725317e+00  2.30349908e+00  2.30917108e+00  2.31165445e+00
  2.31550657e+00  2.31599105e+00  2.32339303e+00  2.34224556e+00
  2.36308250e+00  2.38847504e+00  2.39087157e+00  2.42878880e+00
  2.46342515e+00  2.46576737e+00  2.52899444e+00  2.54865742e+00
  2.55785434e+00  2.60711482e+00  2.61991636e+00  2.62352462e+00
  2.62878709e+00  2.65290641e+00  2.65821999e+00  2.67652912e+00
  2.69167132e+00  2.76530811e+00  2.83973174e+00  2.88116863e+00
  2.88459476e+00  2.88569996e+00  2.88820190e+00  2.90665043e+00
  2.93880957e+00  2.94741500e+00  2.95160051e+00  3.00797003e+00
  3.00941155e+00  3.05571018e+00  3.07709756e+00  3.17439217e+00
  3.22135966e+00  3.22713894e+00  3.22850229e+00  3.31461860e+00
  3.33695858e+00  3.34532661e+00  3.35892368e+00  3.38188352e+00
  3.38991129e+00  3.41803299e+00  3.43642932e+00  3.46028482e+00
  3.54349076e+00  3.63772785e+00  3.67191863e+00  3.67257780e+00
  3.69908345e+00  3.71753203e+00  3.72011410e+00  3.78245787e+00
  3.80349966e+00  3.82143283e+00  3.83601892e+00  3.84512133e+00
  4.03505589e+00  4.08228911e+00  4.17286202e+00  4.17463064e+00
  4.24326688e+00  4.40603494e+00  4.41851130e+00  4.54042034e+00
  4.56279837e+00  4.72489486e+00  4.87992253e+00  4.92491923e+00
  5.09458315e+00  5.19681139e+00  5.20372078e+00  5.25308096e+00
  5.30173337e+00  5.36371924e+00  5.38430774e+00  5.39951350e+00
  5.47113273e+00  5.48550207e+00  5.49594663e+00  5.53198745e+00
  5.54491411e+00  5.55101178e+00  5.57505597e+00  5.68783567e+00
  5.78136823e+00  5.80592477e+00  6.04859636e+00  6.06904885e+00
  6.40324907e+00  6.41507745e+00  6.43979568e+00  6.50553113e+00
  6.54858749e+00  6.58932637e+00  6.62133043e+00  6.73201885e+00
  6.75600008e+00  6.81974148e+00  7.06252071e+00  7.07050425e+00
  7.36050259e+00  7.71871737e+00  7.77963539e+00  8.49839571e+00
  8.55816328e+00  8.70090199e+00  8.81510448e+00  8.85837164e+00
  8.93316536e+00  9.77259902e+00  9.97483294e+00  1.00279874e+01
  1.00836010e+01  1.06769584e+01  1.07922577e+01  1.12353861e+01
  1.17841667e+01  1.18292062e+01  1.20461664e+01  1.20967630e+01
  1.21121265e+01  1.45514456e+01  1.50411144e+01  1.55312878e+01
  1.61207678e+01  1.62253634e+01  1.63176685e+01  1.65116760e+01
  1.88582269e+01  1.93555883e+01  2.71425011e+01  3.19000114e+01]

  UserWarning,

2022-10-31 11:02:39,093:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-11.51373608  -7.59213999  -5.82086408  -5.10621082  -4.99915015
  -4.9877377   -4.98573885  -4.8811111   -3.76951158  -3.61686949
  -3.03538806  -2.94577698  -2.65684367  -2.41071215  -2.18072972
  -1.97552269  -1.91681491  -1.71703362  -1.48513316  -1.42636136
  -1.40973653  -1.29406136  -1.22968193  -1.18243431  -1.10218686
  -1.06465198  -1.01638438  -1.00126297  -0.82621351  -0.79717602
  -0.78268771  -0.77756636  -0.76948076  -0.74008759  -0.73708136
  -0.71773522  -0.6917214   -0.66229791  -0.54356897  -0.49573627
  -0.43130122  -0.38862933  -0.37300131  -0.350051    -0.34288161
  -0.33278025  -0.30628356  -0.28711751  -0.21280254  -0.20370181
  -0.18734508  -0.17656467  -0.15884721  -0.09806416  -0.06707124
  -0.05210005  -0.02456184  -0.02048724   0.01197318   0.01928575
   0.02069015   0.11489152   0.11701176   0.13215769   0.1330253
   0.13603817   0.13996165   0.14161695   0.14554353   0.14699794
   0.15176546   0.15935402   0.15943642   0.179743     0.18160727
   0.18425998   0.19819038   0.21337776   0.2199443    0.24972828
   0.25195295   0.25960434   0.26388232   0.26719081   0.28925823
   0.29678553   0.29980296   0.3098232    0.32127988   0.32231916
   0.34394439   0.34967888   0.35113325   0.35710798   0.37383675
   0.37607823   0.38889173   0.39732384   0.39771684   0.39893447
   0.3999383    0.40031661   0.40225607   0.41506014   0.42731419
   0.43082614   0.43418401   0.43619955   0.43957225   0.44151318
   0.4433636    0.4534995    0.45858018   0.45914128   0.46568613
   0.46931208   0.47339268   0.48328412   0.48703265   0.49824043
   0.50363037   0.51032582   0.51134661   0.51507737   0.51653787
   0.51716029   0.5179463    0.52182715   0.52854371   0.52995813
   0.53204503   0.53253982   0.53652237   0.53722971   0.55244975
   0.55757886   0.56278632   0.56336315   0.56522209   0.56778886
   0.57136838   0.57193156   0.58086831   0.58094272   0.58340543
   0.58613369   0.58622597   0.59398451   0.59739375   0.59865856
   0.59945872   0.60394786   0.60878921   0.60897375   0.61480673
   0.61941442   0.62154322   0.62934658   0.63011451   0.63298052
   0.63308901   0.63363621   0.63655496   0.63662643   0.63703872
   0.63963248   0.64220045   0.64424741   0.65160911   0.65656281
   0.65713536   0.65982068   0.66284362   0.66674489   0.6671846
   0.66907135   0.67097005   0.67103232   0.67129805   0.67221265
   0.67315666   0.67969341   0.68363286   0.68481847   0.68601136
   0.69074846   0.69174269   0.69520479   0.69556885   0.69652141
   0.69746315   0.6986669    0.70028182   0.70166956   0.70559111
   0.70905322   0.71263081   0.71371847   0.71389011   0.71437159
   0.71597743   0.71616847   0.71625486   0.71873539   0.72031808
   0.72664118   0.72767843   0.72934023   0.72982585   0.7341665
   0.73478099   0.73484812   0.73554911   0.73743082   0.73860671
   0.74109071   0.74374264   0.74455282   0.74571177   0.74730991
   0.74893553   0.75211155   0.75233942   0.75247556   0.75373692
   0.75493914   0.75500141   0.75567686   0.75681672   0.7645157
   0.77137112   0.77164856   0.77224967   0.77274591   0.77680478
   0.77790658   0.77918292   0.78090129   0.78175744   0.78279578
   0.78485549   0.78521954   0.79033762   0.7912203    0.7913993
   0.79222512   0.7930223    0.79308026   0.79379972   0.79456278
   0.79891376   0.79999282   0.80253007   0.8060867    0.80875091
   0.81009585   0.81215074   0.81725704   0.81835005   0.82101081
   0.82204408   0.82380609   0.82442414   0.82512062   0.82583722
   0.82711826   0.82918384   0.82978495   0.83012465   0.83156043
   0.83362023   0.83420352   0.83532663   0.8364675    0.83670916
   0.83787419   0.84005892   0.84129902   0.8413899    0.84324926
   0.84346503   0.84380104   0.84391081   0.85083502   0.85089729
   0.85090032   0.85149878   0.85489824   0.8549605    0.85523832
   0.85526229   0.85754368   0.85880231   0.85989532   0.86231869
   0.86237792   0.86528455   0.86714341   0.86750148   0.86784142
   0.86820782   0.86943152   0.87133022   0.87427201   0.87825443
   0.87919525   0.87957495   0.88139286   0.88171654   0.88224469
   0.88296001   0.88485497   0.88613511   0.88855264   0.88864075
   0.88913055   0.89210285   0.89260671   0.89990561   0.90248917
   0.90682982   0.90763592   0.91029193   0.91078817   0.9121798
   0.91721614   0.91903329   0.91922788   0.92067825   0.9214493
   0.92414035   0.92905292   0.93106457   0.93452667   0.93798878
   0.94403444   0.94458726   0.9483751    0.9504101    0.9518372
   0.95453289   0.95529931   0.9561869    0.95789534   0.96876022
   0.97607194   0.97953405   0.98019004   0.98051867   0.98243994
   0.98302092   0.983943     0.98992037   0.99244943   0.99685362
   0.99773217   0.99817981   0.99855749   1.00008398   1.00030668
   1.00376879   1.00518011   1.00541739   1.00594756   1.0072309
   1.02057924   1.02064092   1.02107932   1.02416859   1.02548675
   1.0267347    1.02878799   1.0348463    1.03498924   1.03510554
   1.04291793   1.04531406   1.04925728   1.05607884   1.05614111
   1.0566928    1.059905     1.06315827   1.06377004   1.06683791
   1.06717098   1.06740636   1.06899823   1.06953921   1.06998953
   1.07019469   1.07162661   1.07447859   1.07683714   1.08160047
   1.08230267   1.08377569   1.0843768    1.08729442   1.08790117
   1.09136328   1.09380955   1.09531275   1.09806604   1.09857626
   1.10053267   1.10080878   1.10861154   1.11131059   1.11207365
   1.11503666   1.11534343   1.11553575   1.11906013   1.11911129
   1.11922479   1.12330525   1.12459705   1.128782     1.12938418
   1.13505357   1.13784405   1.13889195   1.1397705    1.14143868
   1.14266353   1.1460936    1.14644835   1.14718377   1.15007525
   1.15082051   1.15290514   1.15338502   1.15361892   1.15535221
   1.15850368   1.15887414   1.16054313   1.16189623   1.1620003
   1.16232293   1.1631369    1.16348736   1.16400524   1.1640675
   1.16746734   1.16843114   1.17092945   1.17178464   1.17192004
   1.17439155   1.1761512    1.17635957   1.17677869   1.17684142
   1.17725255   1.17785366   1.17862932   1.18055271   1.18186951
   1.18477787   1.18500436   1.18570889   1.18823998   1.19413359
   1.19701796   1.1986263    1.20540392   1.20901261   1.21652513
   1.21703745   1.22536376   1.22565724   1.22978525   1.23061341
   1.23324735   1.23340725   1.23405017   1.23642158   1.23688846
   1.24413704   1.24667431   1.24967934   1.25723475   1.25822151
   1.25845324   1.26448326   1.26757997   1.27133052   1.27239416
   1.2860525    1.28905672   1.28923059   1.29075431   1.29100112
   1.29809624   1.3018438    1.30794799   1.30877707   1.31321696
   1.31443679   1.31910112   1.32196211   1.33324613   1.3364378
   1.36064639   1.36344694   1.3675706    1.3710327    1.37156535
   1.37449481   1.37516551   1.37675469   1.38141902   1.38488113
   1.38750468   1.38834323   1.38927111   1.39180534   1.39526744
   1.39872955   1.40190276   1.40219166   1.40488735   1.40571603
   1.40586356   1.41330463   1.41604008   1.41950219   1.42000964
   1.4298885    1.43570149   1.43696795   1.43790572   1.43930352
   1.43932914   1.44145677   1.44303091   1.45066114   1.45119946
   1.45412324   1.45437539   1.46104746   1.47254855   1.4938997
   1.51358834   1.52256758   1.52538455   1.52616882   1.52682747
   1.53091025   1.53708069   1.54025163   1.56357681   1.56699283
   1.56764293   1.57351709   1.57729004   1.59045549   1.60261292
   1.63859443   1.65759206   1.66742906   1.71706734   1.71889971
   1.72640688   1.72901455   1.74893362   1.79823478   1.81052412
   1.86884252   1.89488314   1.9248575    1.95117574   1.96275389
   2.03106079   2.10639977   2.27528564   2.27985135   2.38515663
   3.07325719   3.23708093   4.8804758    6.35100041]

  UserWarning,

2022-10-31 11:02:39,095:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.10200358 -4.26814219 -3.69281014 -2.48327842 -2.40074346 -2.3870153
 -2.2350873  -2.21002295 -2.13920414 -2.12133852 -2.02564335 -1.99664998
 -1.96829758 -1.60544288 -1.49351958 -1.4291065  -1.328609   -1.23560519
 -1.18417751 -1.1575717  -1.1416605  -1.12475622 -1.08264886 -1.05440975
 -1.02827333 -0.98628171 -0.96600721 -0.80032782 -0.79012411 -0.70076568
 -0.66647754 -0.65137977 -0.61855441 -0.57771591 -0.54391464 -0.50267498
 -0.46564001 -0.40793506 -0.33136495 -0.3270434  -0.3207827  -0.31388241
 -0.31249641 -0.31156086 -0.28577608 -0.27381284 -0.25702741 -0.18942226
 -0.17579604 -0.13867445 -0.08414862 -0.08062515 -0.07813892 -0.04376263
 -0.03990891 -0.03923105  0.02863841  0.04057491  0.04481901  0.0583712
  0.07361377  0.09106553  0.09154163  0.09552292  0.1164942   0.1319999
  0.13432355  0.13987098  0.1418973   0.1682275   0.17376508  0.18174714
  0.18822719  0.1939691   0.20845874  0.23203009  0.23531307  0.24812113
  0.25172928  0.25750948  0.27192044  0.27878085  0.28089166  0.31102229
  0.32189989  0.33502855  0.33927144  0.34589525  0.35023746  0.36046078
  0.3636493   0.36479148  0.3652948   0.36969989  0.36996346  0.37670361
  0.37745095  0.38024646  0.38849048  0.389962    0.39039394  0.39824596
  0.4150139   0.42288418  0.43720367  0.4413516   0.44377702  0.44379175
  0.44993376  0.45109717  0.45212339  0.45871805  0.47333524  0.47472737
  0.47654567  0.49398293  0.49477076  0.49869434  0.49994762  0.50143775
  0.5139551   0.52816061  0.53901391  0.55867772  0.56111623  0.56218437
  0.5622824   0.5647521   0.57029205  0.57390562  0.57600442  0.57655877
  0.57789397  0.58458658  0.58539351  0.58633888  0.58787727  0.59099871
  0.59250863  0.59318834  0.5984192   0.60601922  0.61178747  0.61200026
  0.61779746  0.63576063  0.64742468  0.64880848  0.65484518  0.67416121
  0.67456042  0.67680108  0.67690794  0.68007064  0.69488113  0.69743854
  0.69836016  0.70136856  0.7061001   0.70809986  0.71508114  0.71841965
  0.72565153  0.72809451  0.73657921  0.73698264  0.74067204  0.74293746
  0.74557079  0.75110378  0.75306969  0.75927784  0.75978116  0.76518867
  0.7667836   0.77529715  0.78479788  0.78544953  0.7882024   0.79109503
  0.79294135  0.7965947   0.79780164  0.81005841  0.81573311  0.81635793
  0.81711579  0.81813411  0.82696266  0.8282237   0.82885325  0.83006292
  0.83825555  0.83850348  0.84264707  0.84442012  0.84837748  0.85352399
  0.85553149  0.86225293  0.8662413   0.86766184  0.87099071  0.87257705
  0.87345565  0.87572576  0.87956694  0.88513308  0.88555332  0.88602821
  0.88844759  0.88945583  0.89017149  0.89492946  0.8989964   0.89913333
  0.90101634  0.90429717  0.90542292  0.90972103  0.90994997  0.9124123
  0.91354322  0.91928112  0.9194232   0.92071241  0.92264697  0.925874
  0.92762618  0.93570163  0.94389211  0.94762287  0.95135544  0.97515372
  0.97907294  0.97956843  0.98171591  0.98688505  0.98833708  0.98890974
  0.9924198   0.99870377  1.00026108  1.00313905  1.00778141  1.01075551
  1.01153259  1.01369805  1.01433819  1.01803485  1.02010212  1.02238895
  1.02328843  1.02365221  1.02730774  1.0359944   1.03680632  1.03974128
  1.04191104  1.04991128  1.05025277  1.05282841  1.05318235  1.0566291
  1.05674406  1.05792641  1.05912118  1.06090489  1.06101068  1.06357191
  1.06410156  1.06420739  1.07208962  1.07458992  1.07763672  1.07848701
  1.08681518  1.08789226  1.09242038  1.10779923  1.10811795  1.10974532
  1.11335753  1.11648796  1.12695181  1.13824899  1.13911371  1.1394844
  1.14520004  1.14884636  1.15635836  1.16386687  1.16877643  1.17705935
  1.17769948  1.18191571  1.18220807  1.18226229  1.1830847   1.18538434
  1.18652772  1.1894506   1.19340726  1.19445687  1.20475246  1.20576446
  1.20745272  1.21358304  1.21993392  1.22523365  1.23348532  1.23413285
  1.23621688  1.23660073  1.24850583  1.24945121  1.25556361  1.26169831
  1.26759884  1.26804011  1.27201762  1.27501294  1.27903564  1.28239097
  1.28933451  1.2899113   1.29302671  1.30099007  1.30324843  1.30413748
  1.30724863  1.30969566  1.31881215  1.3212397   1.32404093  1.3331448
  1.33546162  1.34153683  1.35382326  1.36016989  1.37446941  1.37455027
  1.37940305  1.38294848  1.38982616  1.3918876   1.39883199  1.40051012
  1.40226777  1.4098433   1.41858173  1.41939654  1.42010058  1.42089918
  1.42868012  1.43048076  1.43848408  1.4412723   1.44161975  1.44349969
  1.44720018  1.45668769  1.46131927  1.46906867  1.47124522  1.47652656
  1.47732515  1.48046762  1.48179703  1.48473736  1.48476577  1.48722927
  1.48740222  1.49051763  1.50553814  1.51511973  1.51561521  1.51873062
  1.52153185  1.52823742  1.53375113  1.54333272  1.54561901  1.54694361
  1.54876501  1.5509709   1.55650737  1.57019322  1.57346227  1.5757932
  1.57899035  1.58499584  1.58748391  1.60025418  1.60062246  1.60336959
  1.61253715  1.61759151  1.61974722  1.61994975  1.62032306  1.62928973
  1.63148048  1.63307292  1.63763104  1.64269375  1.65510773  1.65618467
  1.65668015  1.65919798  1.65979556  1.66052648  1.66896894  1.67161981
  1.67443222  1.67490571  1.67737281  1.68438983  1.68439766  1.68488702
  1.68489314  1.68793624  1.69823028  1.70164518  1.7127379   1.71594185
  1.71622154  1.71686168  1.73398033  1.73571392  1.73882933  1.74131912
  1.75364005  1.75594369  1.75927659  1.75945504  1.76797598  1.76953211
  1.77264752  1.77947569  1.77986163  1.7814729   1.78366045  1.78686944
  1.78766803  1.79747496  1.8048878   1.80822322  1.81276561  1.81688044
  1.8254626   1.82913673  1.83300161  1.83550375  1.84048312  1.84409401
  1.84683918  1.84989222  1.85367559  1.85477311  1.86643774  1.87566232
  1.87691731  1.88188858  1.88238406  1.88511562  1.89510403  1.90051998
  1.90206669  1.9051303   1.90638056  1.90733588  1.9080698   1.91010157
  1.91059705  1.91371246  1.92753129  1.92873297  1.94261419  1.94322541
  1.96652755  1.96897161  1.96927682  1.97013844  1.97543625  1.97646781
  1.97875516  1.98515895  1.98564238  1.98747577  2.00163024  2.01598568
  2.04158493  2.04510025  2.05678555  2.06899932  2.0793795   2.08299039
  2.09054758  2.09530564  2.09721231  2.09801091  2.10759249  2.10808797
  2.12142511  2.12622389  2.12823587  2.13459243  2.14341093  2.16401847
  2.16433215  2.1709523   2.17704253  2.18339453  2.19430866  2.21703268
  2.22093993  2.23778177  2.25506956  2.26367795  2.26728884  2.27064417
  2.2729117   2.27439344  2.27881901  2.29550183  2.30508341  2.3097075
  2.31928314  2.32371482  2.3275679   2.33194931  2.3519278   2.36486472
  2.39333327  2.39866353  2.41357877  2.43576818  2.47797224  2.49219415
  2.50618523  2.54280818  2.55738208  2.55900031  2.5872133   2.6058447
  2.68998035  2.71869666  2.78831511  2.81255668  2.8411302   2.84474109
  2.87363775  2.94718354  2.97861438  3.07583682  3.11820373  3.26815094
  3.27646829  3.43968407  4.16695055  4.1975614   4.26880641  4.27921617
  4.34094539  4.37070057  5.05546045  5.30992908  5.58393033  6.34405341
  6.71851606  7.20375262 11.29504126 12.37558946]

  UserWarning,

2022-10-31 11:02:39,111:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [ 0.48166673  0.52890319  0.61062807  0.61142614  0.63283718  0.65198407
  0.66373798  0.6678724   0.6821734   0.68516783  0.70239208  0.70429591
  0.71795094  0.72455905  0.74323072  0.76074514  0.76551999  0.77059786
  0.77105194  0.77336395  0.77365785  0.78073812  0.78541474  0.79024181
  0.79136104  0.7939289   0.79504813  0.79700002  0.79761598  0.79803355
  0.79967697  0.80130307  0.80725917  0.80753725  0.80806127  0.80867724
  0.8160514   0.81780043  0.8185985   0.82753022  0.82935004  0.83079974
  0.83446606  0.83490439  0.840721    0.84199555  0.84440809  0.85150418
  0.85178226  0.85246701  0.85546934  0.86284351  0.87021768  0.87063524
  0.87390476  0.87759185  0.88127893  0.88496601  0.88503918  0.8886531
  0.89275775  0.89971435  0.90340144  0.90455121  0.90708852  0.90750608
  0.9107756   0.91119317  0.91346822  0.91437608  0.91446269  0.91488025
  0.919269    0.92037094  0.92808951  0.93076365  0.93289811  0.94068984
  0.94087042  0.94283784  0.94570838  0.94590896  0.94652493  0.94703048
  0.94764645  0.95071757  0.95389909  0.95543818  0.9587077   0.96281235
  0.96325125  0.96373071  0.96496035  0.96856443  0.97529917  0.97588981
  0.97721628  0.97756068  0.97856869  0.98493485  0.98594286  0.98962995
  0.99230902  0.9959961   0.99618757  0.99733203  0.99968319  1.00101465
  1.00724883  1.00806537  1.00838882  1.01024027  1.01040603  1.01543954
  1.01777722  1.02294075  1.02314162  1.02568425  1.02650079  1.02691835
  1.02937133  1.0367455   1.03756204  1.04024111  1.04525966  1.0469972
  1.04842489  1.05438576  1.05518092  1.05632091  1.05755502  1.058868
  1.06010211  1.06391624  1.0657123   1.06624217  1.07116336  1.07361634
  1.07677355  1.07853753  1.08153423  1.0821305   1.08222462  1.08467759
  1.0859117   1.08836468  1.08918122  1.09205176  1.09260004  1.09266188
  1.0928683   1.09908153  1.09942593  1.09988305  1.10190185  1.10311301
  1.10379777  1.10434712  1.1080342   1.10837456  1.11048718  1.11172129
  1.11786135  1.12093247  1.12154844  1.12278254  1.12307955  1.12493445
  1.12646963  1.13015671  1.13260969  1.13629677  1.13688613  1.13753088
  1.13765793  1.13883802  1.13949356  1.13998386  1.14367094  1.14428908
  1.14636356  1.14696603  1.15119999  1.15227922  1.15373772  1.15473219
  1.16261053  1.16517748  1.16526357  1.16895065  1.16965714  1.17217315
  1.17263774  1.17469972  1.17573765  1.17632482  1.1768547   1.18241829
  1.18369899  1.19476024  1.196472    1.19758287  1.19844733  1.1989772
  1.20039724  1.20266429  1.20546688  1.2058215   1.20635137  1.20805381
  1.21220348  1.22019936  1.22457119  1.23413026  1.24108775  1.24142679
  1.24269234  1.24833487  1.2488786   1.26441192  1.27053193  1.28603928
  1.28607289  1.28876745  1.29334038  1.29486657  1.3017068   1.30286719
  1.31316017  1.32284698  1.32748473  1.32917549  1.32945073  1.3317448
  1.33313781  1.33663731  1.34084928  1.34218655  1.35731711  1.36065352
  1.36134217  1.36814184  1.38581696  1.39063253  1.4030574   1.41936953
  1.42072235  1.42999908  1.43283797  1.43463876  1.43867024  1.43949468
  1.44177131  1.44260297  1.44309116  1.4463927   1.46587303  1.46624238
  1.47164301  1.47493234  1.4761314   1.48363943  1.49589961  1.50110658
  1.50140397  1.5089224   1.52840227  1.53017665  1.53317974  1.54796919
  1.54899849  1.56201524  1.58525707  1.58624206  1.5923946   1.59672682
  1.59714186  1.59944092  1.61432441  1.61555752  1.61747605  1.61753321
  1.62104912  1.62138703  1.63280914  1.63653399  1.64253531  1.64573621
  1.64690826  1.64819657  1.64909867  1.6565279   1.65930136  1.66463745
  1.66836279  1.6723553   1.6729212   1.6741875   1.67814179  1.69324485
  1.69623661  1.69736378  1.699247    1.70575345  1.70664812  1.72286228
  1.72376911  1.72520732  1.72829154  1.73507045  1.73741388  1.73796721
  1.73822766  1.74182955  1.74703002  1.74784271  1.77378983  1.77633849
  1.78753609  1.79069314  1.79265472  1.79281466  1.79406417  1.79536997
  1.80127197  1.80429525  1.811233    1.81652709  1.82004712  1.82601848
  1.8264347   1.83225221  1.83651305  1.83789308  1.84115739  1.8463908
  1.84830237  1.84835712  1.8562613   1.85798938  1.85978932  1.86246653
  1.867429    1.86878483  1.86903007  1.87295122  1.87415684  1.88136877
  1.88823432  1.89842874  1.93320395  1.93882854  1.94572381  1.95657593
  1.95817205  1.95961724  1.96119915  1.96460599  1.96942346  1.97502884
  1.97540841  1.98358736  2.00384247  2.00811813  2.01448455  2.01780087
  2.02067174  2.04815232  2.05913689  2.06052231  2.06948573  2.07138179
  2.07606352  2.09209392  2.09317276  2.09461196  2.11996332  2.12455744
  2.12497408  2.12795398  2.12958684  2.13461072  2.16507934  2.16752081
  2.1714295   2.17247824  2.18285608  2.19699764  2.19823217  2.20220137
  2.20681302  2.21285499  2.21877761  2.24401478  2.26498635  2.31981214
  2.32336589  2.3401844   2.34809363  2.34951661  2.35976461  2.36046117
  2.38057314  2.38229661  2.40256448  2.41460443  2.41554351  2.42665233
  2.47824273  2.48166632  2.48190825  2.48754926  2.49336347  2.50582527
  2.5200884   2.54830925  2.55051801  2.56862215  2.57013366  2.58618309
  2.61283138  2.61561674  2.66342064  2.66969001  2.67216661  2.69011524
  2.69118645  2.70048978  2.72078248  2.73524218  2.7435528   2.74465461
  2.75221513  2.76644931  2.7762528   2.77980319  2.78598702  2.79854917
  2.82590995  2.89534725  2.91016682  2.9666467   2.99700317  3.10222901
  3.10523259  3.14595949  3.17324889  3.18397245  3.18748988  3.19513969
  3.20304338  3.20732815  3.23168679  3.25306942  3.25952434  3.29392417
  3.32608543  3.34040379  3.38367298  3.40435848  3.44565203  3.45839617
  3.45928794  3.5735145   3.59413687  3.59679068  3.63364945  3.65600776
  3.69576592  3.7270489   3.78460748  3.8018692   3.94718711  3.96781289
  3.99094944  3.99553551  4.01754959  4.0677185   4.08483264  4.21066178
  4.21748697  4.23075821  4.23176301  4.24318744  4.25268742  4.27926117
  4.32967623  4.35623212  4.40320442  4.40778991  4.40874438  4.44440633
  4.44586746  4.45485663  4.56144208  4.60173366  4.6193721   4.67107756
  4.73492977  4.81148992  4.81439651  4.85103032  4.86168628  4.93515543
  4.97851581  4.98408214  5.02121635  5.2101356   5.24033402  5.27046041
  5.27652126  5.33231286  5.36913085  5.40993316  5.56068012  5.60271221
  5.66545862  5.68709265  5.97301145  6.03080924  6.03442174  6.05867138
  6.07479626  6.17233057  6.18110154  6.41366985  6.5240556   6.69512361
  7.02552453  7.13248657  7.55644495  7.72008991  7.92259984  7.96853647
  8.11753913  8.26243344  8.55503912  8.72289149  8.78450129  8.88994031
  9.14408873  9.1552964   9.19655808  9.61947182  9.79615855  9.91033445
 10.16218191 10.39511856 11.05787558 11.40126321 11.55239073 12.09843946
 12.89310254 13.49026881 13.51546926 13.68191387 14.77408044 14.86458012
 14.86639772 15.72564572 15.98016573 16.54949798 17.76677895 19.72046433
 20.13182818 22.04795744 22.1185273  28.34085293 30.39443383]

  UserWarning,

2022-10-31 11:02:39,189:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.19081510e+01 -9.86240019e+00 -9.10092164e+00 -9.03080899e+00
 -8.32920877e+00 -7.59928361e+00 -7.55170380e+00 -7.40771201e+00
 -6.47002607e+00 -6.38019750e+00 -6.25803071e+00 -6.21580296e+00
 -6.18112689e+00 -5.53632376e+00 -5.43539027e+00 -5.01518760e+00
 -4.86489720e+00 -4.23444555e+00 -4.08795678e+00 -4.00345311e+00
 -3.96624266e+00 -3.80695317e+00 -3.78025118e+00 -3.53296453e+00
 -3.52106941e+00 -3.44604862e+00 -3.41472457e+00 -3.35197864e+00
 -3.17684860e+00 -3.13831459e+00 -3.01436121e+00 -2.71716350e+00
 -2.68096658e+00 -2.56577628e+00 -2.54725715e+00 -2.52928066e+00
 -2.51622420e+00 -2.47805052e+00 -2.34102063e+00 -2.30454267e+00
 -2.14023741e+00 -2.12212304e+00 -2.10603868e+00 -2.05075847e+00
 -1.87125696e+00 -1.79976876e+00 -1.76571928e+00 -1.71035927e+00
 -1.65414713e+00 -1.60640760e+00 -1.58518607e+00 -1.43042939e+00
 -1.42652643e+00 -1.39762096e+00 -1.32242471e+00 -1.26661570e+00
 -1.25958659e+00 -1.22540604e+00 -1.14215880e+00 -1.11687222e+00
 -1.11421772e+00 -1.09279392e+00 -1.01986344e+00 -9.96920613e-01
 -9.10493213e-01 -8.66039733e-01 -8.29886209e-01 -8.02664962e-01
 -7.39569011e-01 -7.26784511e-01 -7.12413674e-01 -6.89550286e-01
 -6.79334417e-01 -6.76055391e-01 -6.74198676e-01 -6.66133153e-01
 -6.16735635e-01 -5.80770251e-01 -5.62459420e-01 -5.61052570e-01
 -5.22519267e-01 -5.21363660e-01 -4.81457748e-01 -4.69633587e-01
 -4.57321343e-01 -4.50950853e-01 -4.42860830e-01 -4.14989489e-01
 -3.80348447e-01 -3.53411975e-01 -3.41862604e-01 -3.41592108e-01
 -3.38864238e-01 -3.32151452e-01 -3.23293002e-01 -3.21689521e-01
 -2.99784622e-01 -2.95709694e-01 -2.87908070e-01 -2.77679171e-01
 -2.76269972e-01 -2.70022948e-01 -2.59615265e-01 -2.46528513e-01
 -2.42449537e-01 -2.38792885e-01 -2.27820861e-01 -2.20951358e-01
 -2.16047593e-01 -2.08857010e-01 -2.05847616e-01 -1.70126787e-01
 -1.46188675e-01 -1.31353057e-01 -1.06276329e-01 -8.43323179e-02
 -8.29804262e-02 -7.76122229e-02 -5.43115030e-02 -3.74812512e-02
 -2.67509462e-02 -2.55305041e-02 -1.58058458e-02 -1.22580162e-02
 -1.07049037e-02  4.20068769e-03  7.08482198e-03  9.13084996e-03
  9.96167720e-03  1.20970553e-02  1.81186249e-02  1.99729058e-02
  2.07281403e-02  2.07322940e-02  2.93806897e-02  3.05337095e-02
  3.52164707e-02  4.12875950e-02  5.52049339e-02  6.47040812e-02
  6.69971211e-02  7.17117738e-02  7.67333377e-02  8.14494337e-02
  8.46449388e-02  8.50527529e-02  9.42680441e-02  9.65011678e-02
  1.01012857e-01  1.03781176e-01  1.06630262e-01  1.08776618e-01
  1.19392528e-01  1.24914159e-01  1.26843052e-01  1.29047078e-01
  1.40157675e-01  1.42092195e-01  1.42256521e-01  1.52380359e-01
  1.55316281e-01  1.55711015e-01  1.60078020e-01  1.62391637e-01
  1.65037740e-01  1.66569114e-01  1.74592354e-01  1.74886185e-01
  1.77237687e-01  1.85885263e-01  2.02267423e-01  2.07829743e-01
  2.18457333e-01  2.21374232e-01  2.33848708e-01  2.40324462e-01
  2.41684318e-01  2.54009959e-01  2.57472786e-01  2.58548481e-01
  2.60002319e-01  2.60379548e-01  2.62017259e-01  2.74317384e-01
  2.80144988e-01  2.82341641e-01  2.84612991e-01  2.97178604e-01
  3.00012485e-01  3.09489711e-01  3.11848906e-01  3.15922614e-01
  3.21081193e-01  3.34737203e-01  3.39157277e-01  3.45824118e-01
  3.48970980e-01  3.53894045e-01  3.58809556e-01  3.58865808e-01
  3.65040056e-01  3.65492871e-01  3.66140588e-01  3.74540609e-01
  3.75410389e-01  3.85916482e-01  3.96929206e-01  3.99893129e-01
  4.01178256e-01  4.03965419e-01  4.07517455e-01  4.13419073e-01
  4.17967088e-01  4.21231688e-01  4.25449841e-01  4.26474764e-01
  4.29918269e-01  4.30895182e-01  4.32260685e-01  4.32987739e-01
  4.35259958e-01  4.44099210e-01  4.44906819e-01  4.47583214e-01
  4.48330341e-01  4.50438563e-01  4.54986782e-01  4.57435689e-01
  4.58818554e-01  4.60075632e-01  4.61116516e-01  4.63002877e-01
  4.66386456e-01  4.67375124e-01  4.74057615e-01  4.74780657e-01
  4.81982661e-01  4.82807191e-01  4.86459195e-01  4.86490559e-01
  4.90839348e-01  4.92356209e-01  4.93337594e-01  4.95745646e-01
  4.97019481e-01  4.98010539e-01  5.17155916e-01  5.22291296e-01
  5.25461715e-01  5.28405575e-01  5.29881857e-01  5.30110642e-01
  5.34587756e-01  5.39946455e-01  5.40825005e-01  5.41054221e-01
  5.43705018e-01  5.45812340e-01  5.52320675e-01  5.53241210e-01
  5.53332679e-01  5.59285031e-01  5.68515044e-01  5.68917542e-01
  5.71963406e-01  5.72363404e-01  5.79973757e-01  5.82632241e-01
  5.89009524e-01  5.89923515e-01  5.92009301e-01  5.97918866e-01
  5.98639158e-01  6.01067470e-01  6.03085747e-01  6.11930059e-01
  6.13606337e-01  6.18442553e-01  6.19879211e-01  6.24852562e-01
  6.25236811e-01  6.26439353e-01  6.31446469e-01  6.34411518e-01
  6.35840994e-01  6.36468872e-01  6.36649725e-01  6.37270749e-01
  6.37633802e-01  6.40751727e-01  6.41725163e-01  6.42706440e-01
  6.46550929e-01  6.47438804e-01  6.48039261e-01  6.48332216e-01
  6.48365291e-01  6.49235812e-01  6.54155950e-01  6.54798841e-01
  6.55695676e-01  6.59814854e-01  6.61708181e-01  6.62345482e-01
  6.62881168e-01  6.63080832e-01  6.64084841e-01  6.67695839e-01
  6.70137220e-01  6.70192192e-01  6.71261298e-01  6.72457849e-01
  6.74263456e-01  6.75219378e-01  6.76925852e-01  6.81540858e-01
  6.83174326e-01  6.83427446e-01  6.86155865e-01  6.87848638e-01
  6.90388751e-01  6.90770871e-01  6.98518802e-01  7.00000885e-01
  7.00147888e-01  7.00358571e-01  7.01283548e-01  7.01567664e-01
  7.02497238e-01  7.03464268e-01  7.03481243e-01  7.04615891e-01
  7.09230898e-01  7.10325924e-01  7.11469723e-01  7.18460911e-01
  7.23222921e-01  7.23899240e-01  7.26411518e-01  7.32305930e-01
  7.32418842e-01  7.33181877e-01  7.33375874e-01  7.33714498e-01
  7.35394469e-01  7.35891713e-01  7.36920937e-01  7.38457963e-01
  7.41127882e-01  7.44621295e-01  7.47154959e-01  7.49486550e-01
  7.50765957e-01  7.50912960e-01  7.52281539e-01  7.53305381e-01
  7.54343901e-01  7.57293245e-01  7.57718036e-01  7.59995970e-01
  7.60763043e-01  7.60871916e-01  7.61018920e-01  7.64610976e-01
  7.66148002e-01  7.67946577e-01  7.69225983e-01  7.69891943e-01
  7.73840989e-01  7.74713852e-01  7.76789434e-01  7.77176590e-01
  7.78455996e-01  7.78743369e-01  7.78785477e-01  7.79716014e-01
  7.81721344e-01  7.83218006e-01  7.85549437e-01  7.86167324e-01
  7.86171376e-01  7.89825058e-01  7.90921842e-01  7.91887988e-01
  7.94440064e-01  7.95636616e-01  7.96916022e-01  7.99782860e-01
  8.00143040e-01  8.00251623e-01  8.04866629e-01  8.05177846e-01
  8.05578026e-01  8.06146035e-01  8.07575702e-01  8.09571105e-01
  8.10761042e-01  8.11765051e-01  8.14096642e-01  8.15376048e-01
  8.15523052e-01  8.18324493e-01  8.19991055e-01  8.24294716e-01
  8.24606061e-01  8.27941662e-01  8.28088666e-01  8.28174777e-01
  8.30159492e-01  8.32703672e-01  8.33836075e-01  8.33983078e-01
  8.38598085e-01  8.39340875e-01  8.40113667e-01  8.41933685e-01
  8.43382531e-01  8.47018542e-01  8.47828098e-01  8.49006803e-01
  8.49123137e-01  8.51016695e-01  8.53488738e-01  8.55691639e-01
  8.57915117e-01  8.60246708e-01  8.60800941e-01  8.61673118e-01
  8.64861714e-01  8.66003004e-01  8.66288124e-01  8.67145130e-01
  8.67810159e-01  8.68760985e-01  8.71760136e-01  8.75279035e-01
  8.76375143e-01  8.77477151e-01  8.77657186e-01  8.77849568e-01
  8.78706734e-01  8.80990149e-01  8.81655179e-01  8.83321740e-01
  8.83792831e-01  8.85605156e-01  8.87936747e-01  8.90518689e-01
  8.97166760e-01  8.97313764e-01  8.98397577e-01  9.01781767e-01
  9.06543777e-01  9.09275240e-01  9.13490214e-01  9.15773790e-01
  9.18037884e-01  9.20241793e-01  9.20388797e-01  9.20830640e-01
  9.21668203e-01  9.27140215e-01  9.27483486e-01  9.29471806e-01
  9.30898216e-01  9.34086813e-01  9.43463830e-01  9.46741895e-01
  9.48467079e-01  9.50215248e-01  9.52693843e-01  9.53640317e-01
  9.60919686e-01  9.64041818e-01  9.67395522e-01  9.75768875e-01
  9.80383882e-01  9.83506199e-01  9.89510422e-01  9.89613895e-01
  9.95928546e-01  9.98843908e-01  1.00168914e+00  1.00257673e+00
  1.00413235e+00  1.00649595e+00  1.01142404e+00  1.02191894e+00
  1.02591720e+00  1.03180737e+00  1.03552617e+00  1.03576396e+00
  1.04350128e+00  1.04413681e+00  1.04932408e+00  1.05341409e+00
  1.05508065e+00  1.05734630e+00  1.06385827e+00  1.06951420e+00
  1.09768481e+00  1.09885300e+00  1.09888136e+00  1.10669240e+00
  1.10746749e+00  1.11272638e+00  1.11734139e+00  1.12170795e+00
  1.12604546e+00  1.13118641e+00  1.13272343e+00  1.13467464e+00
  1.13580142e+00  1.14084579e+00  1.14257608e+00  1.14964644e+00
  1.15242136e+00  1.15382798e+00  1.15426144e+00  1.15622462e+00
  1.15816110e+00  1.16015911e+00  1.16349145e+00  1.16750339e+00
  1.17694646e+00  1.17733647e+00  1.18020331e+00  1.18030139e+00
  1.18656649e+00  1.19245738e+00  1.19904391e+00  1.20502651e+00
  1.20964152e+00  1.21087560e+00  1.22611198e+00  1.23106891e+00
  1.23194777e+00  1.23324350e+00  1.23733156e+00  1.24194657e+00
  1.26055360e+00  1.26516860e+00  1.26963661e+00  1.27439862e+00
  1.27720006e+00  1.28092605e+00  1.29525106e+00  1.29586972e+00
  1.29747365e+00  1.29833065e+00  1.30060259e+00  1.31131867e+00
  1.31826511e+00  1.32288011e+00  1.35980016e+00  1.36106208e+00
  1.36224120e+00  1.49610203e+00  1.53870609e+00  1.56483630e+00
  1.57860723e+00  1.59451164e+00  1.67639928e+00  1.69828384e+00
  1.98062504e+00  2.06888755e+00  2.11369817e+00  2.14640270e+00
  2.21617390e+00  2.70993427e+00  2.90440822e+00  2.98095627e+00
  3.15043535e+00  3.16709144e+00  3.20088076e+00]

  UserWarning,

2022-10-31 11:02:39,204:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.62027726e+01 -1.31138485e+01 -1.17849096e+01 -9.41506453e+00
 -8.36597121e+00 -8.22967679e+00 -8.01895272e+00 -5.24684407e+00
 -5.10490587e+00 -4.73260542e+00 -4.15305637e+00 -4.07644911e+00
 -3.96376620e+00 -3.68165601e+00 -3.56018290e+00 -2.97744717e+00
 -2.76268112e+00 -2.71044274e+00 -2.66491051e+00 -2.44763488e+00
 -2.40206114e+00 -2.31655393e+00 -2.30619807e+00 -2.13107386e+00
 -2.09169457e+00 -2.07795882e+00 -2.04304470e+00 -1.96792499e+00
 -1.95593630e+00 -1.91744124e+00 -1.84250786e+00 -1.81768357e+00
 -1.77211896e+00 -1.75392787e+00 -1.74172512e+00 -1.73898516e+00
 -1.68241379e+00 -1.65572460e+00 -1.63027230e+00 -1.53587351e+00
 -1.51953453e+00 -1.50655823e+00 -1.50123993e+00 -1.47372746e+00
 -1.46000067e+00 -1.45882027e+00 -1.44579650e+00 -1.42509780e+00
 -1.40413988e+00 -1.36966617e+00 -1.31462409e+00 -1.28442147e+00
 -1.28144636e+00 -1.25440430e+00 -1.25073970e+00 -1.24670585e+00
 -1.22903766e+00 -1.21180894e+00 -1.20072310e+00 -1.18166050e+00
 -1.17031992e+00 -1.16030096e+00 -1.15324339e+00 -1.12930566e+00
 -1.12893032e+00 -1.12474058e+00 -1.11693001e+00 -1.10592314e+00
 -1.10102482e+00 -1.07538566e+00 -1.07158773e+00 -1.06574498e+00
 -1.05478898e+00 -1.05284564e+00 -1.04710458e+00 -1.01577775e+00
 -1.01263700e+00 -1.01223047e+00 -1.00896614e+00 -1.00841588e+00
 -1.00144399e+00 -9.78406256e-01 -9.60619092e-01 -9.52164566e-01
 -9.51722184e-01 -9.44434107e-01 -9.18731577e-01 -9.10134642e-01
 -9.04774037e-01 -9.03804962e-01 -8.93036790e-01 -8.85589985e-01
 -8.72072450e-01 -8.47712420e-01 -8.46821537e-01 -8.41131940e-01
 -8.40617340e-01 -8.27670157e-01 -8.22041386e-01 -7.97306280e-01
 -7.94532540e-01 -7.91564423e-01 -7.91483283e-01 -7.86613708e-01
 -7.75999514e-01 -7.70610039e-01 -7.63788467e-01 -7.59075230e-01
 -7.57459579e-01 -7.56382443e-01 -7.47844324e-01 -7.32475812e-01
 -7.31947989e-01 -7.29101514e-01 -6.92787819e-01 -6.83231827e-01
 -6.78644556e-01 -6.62648394e-01 -6.47985870e-01 -6.45887051e-01
 -6.44765405e-01 -6.35345799e-01 -6.34796081e-01 -6.27193453e-01
 -6.24926533e-01 -6.21084775e-01 -6.17143853e-01 -6.16395552e-01
 -6.08683999e-01 -5.99650714e-01 -5.97902949e-01 -5.83098640e-01
 -5.83042071e-01 -5.80588113e-01 -5.69382430e-01 -5.62065159e-01
 -5.53414377e-01 -5.36248401e-01 -5.29428714e-01 -5.28233280e-01
 -5.18252718e-01 -5.13280388e-01 -5.04024068e-01 -4.93001336e-01
 -4.83204218e-01 -4.76864551e-01 -4.74728694e-01 -4.69986626e-01
 -4.68128773e-01 -4.62017125e-01 -4.52903761e-01 -4.36455569e-01
 -4.30673418e-01 -4.30529068e-01 -4.16009597e-01 -4.07343496e-01
 -3.92436023e-01 -3.77078397e-01 -3.68469028e-01 -3.67262600e-01
 -3.61444879e-01 -3.61107159e-01 -3.49406427e-01 -3.41801180e-01
 -3.39696420e-01 -3.30984520e-01 -3.30592924e-01 -3.18268837e-01
 -3.13472101e-01 -3.05912189e-01 -3.03350100e-01 -3.02536212e-01
 -2.84684586e-01 -2.76660784e-01 -2.75996353e-01 -2.73479468e-01
 -2.72130716e-01 -2.68634484e-01 -2.65823082e-01 -2.65793976e-01
 -2.59630591e-01 -2.58177126e-01 -2.38508177e-01 -2.32078412e-01
 -2.29447010e-01 -2.23032303e-01 -2.22212375e-01 -2.11490705e-01
 -1.99616800e-01 -1.99491732e-01 -1.82280306e-01 -1.82055158e-01
 -1.72161002e-01 -1.69595872e-01 -1.66056033e-01 -1.57778892e-01
 -1.51469966e-01 -1.51138977e-01 -1.49866193e-01 -1.49843002e-01
 -1.38164672e-01 -1.36043968e-01 -1.34245043e-01 -1.29925473e-01
 -1.25839178e-01 -1.24945128e-01 -1.18224741e-01 -1.12700636e-01
 -1.11395415e-01 -1.01115796e-01 -9.66266921e-02 -9.06175992e-02
 -8.84895818e-02 -8.36891354e-02 -8.19496969e-02 -8.08486266e-02
 -7.72462829e-02 -6.77799105e-02 -6.15760102e-02 -5.39351477e-02
 -5.20017381e-02 -4.48146671e-02 -4.35712938e-02 -4.27543730e-02
 -3.99913521e-02 -3.82460883e-02 -3.74527981e-02 -2.59501371e-02
 -2.10357600e-02 -1.74349193e-02 -1.48910102e-02 -1.45463828e-02
 -4.50066148e-03 -4.46177964e-03  7.57239241e-04  1.42167019e-03
  4.19219134e-03  5.72987329e-03  1.92408978e-02  2.05830524e-02
  2.15400618e-02  2.20751649e-02  2.90148776e-02  3.34715294e-02
  4.97897491e-02  5.50198764e-02  5.54266410e-02  5.80323752e-02
  7.91696649e-02  7.96909645e-02  8.44336567e-02  8.84733135e-02
  8.97691850e-02  9.31079408e-02  9.38943447e-02  1.01240547e-01
  1.11713572e-01  1.16790393e-01  1.19075441e-01  1.27071296e-01
  1.34049550e-01  1.35465818e-01  1.40130682e-01  1.48959584e-01
  1.54335027e-01  1.59514507e-01  1.70398966e-01  1.73133780e-01
  1.73656802e-01  1.76472864e-01  1.78491020e-01  1.83648878e-01
  1.84603311e-01  1.89727047e-01  1.89939638e-01  1.93728888e-01
  2.04285028e-01  2.06799873e-01  2.08018588e-01  2.24393569e-01
  2.32925809e-01  2.43462166e-01  2.48177316e-01  2.49334963e-01
  2.51665957e-01  2.57784453e-01  2.62871641e-01  2.69806409e-01
  2.70348927e-01  2.71652623e-01  2.76715505e-01  2.78839694e-01
  2.86086692e-01  2.88008139e-01  2.88313035e-01  2.89081207e-01
  2.98224095e-01  3.00492502e-01  3.04966496e-01  3.05348999e-01
  3.12345241e-01  3.13449659e-01  3.17183206e-01  3.17888960e-01
  3.24364748e-01  3.25076031e-01  3.26941066e-01  3.32757180e-01
  3.37139089e-01  3.40496419e-01  3.42895258e-01  3.44138632e-01
  3.55245703e-01  3.63645906e-01  3.63952854e-01  3.70519078e-01
  3.74085944e-01  3.75313175e-01  3.76192772e-01  3.77432586e-01
  3.77880947e-01  3.78244261e-01  3.82265749e-01  3.87805232e-01
  3.89230377e-01  3.90741557e-01  3.99488914e-01  4.04326869e-01
  4.05408710e-01  4.17548705e-01  4.18610173e-01  4.19498661e-01
  4.24458360e-01  4.24910574e-01  4.25787903e-01  4.26625290e-01
  4.35298724e-01  4.35466714e-01  4.36075683e-01  4.36611306e-01
  4.42382657e-01  4.44326937e-01  4.45892275e-01  4.46767939e-01
  4.50107723e-01  4.50335033e-01  4.52686316e-01  4.59743770e-01
  4.63785043e-01  4.65028416e-01  4.67289922e-01  4.67488701e-01
  4.69372083e-01  4.71146912e-01  4.82235712e-01  4.82847643e-01
  4.85589858e-01  4.90749253e-01  5.02089607e-01  5.04108499e-01
  5.10021380e-01  5.11264753e-01  5.13985234e-01  5.17383249e-01
  5.17946452e-01  5.18999128e-01  5.19410490e-01  5.24270563e-01
  5.25308321e-01  5.36835796e-01  5.37774059e-01  5.49665996e-01
  5.56257717e-01  5.56707800e-01  5.62826296e-01  5.63619586e-01
  5.66632085e-01  5.70540417e-01  5.71184504e-01  5.83780132e-01
  5.95902333e-01  5.98492087e-01  6.00810999e-01  6.09855923e-01
  6.16568952e-01  6.21556655e-01  6.23940842e-01  6.27258778e-01
  6.34026100e-01  6.38911801e-01  6.48730392e-01  6.49973765e-01
  6.53773315e-01  6.56253864e-01  6.66870843e-01  6.74842419e-01
  6.90564087e-01  6.94966729e-01  6.96210102e-01  6.97029430e-01
  6.97660694e-01  7.02328598e-01  7.12503106e-01  7.24309447e-01
  7.33401515e-01  7.35935513e-01  7.41203066e-01  7.42446439e-01
  7.42532510e-01  7.49972527e-01  7.85756348e-01  7.88682776e-01
  7.94801272e-01  7.95605118e-01  7.97641781e-01  8.05093307e-01
  8.13594621e-01  8.14217635e-01  8.18106160e-01  8.18971227e-01
  8.36127288e-01  8.36463119e-01  8.40244319e-01  8.41037609e-01
  8.48412695e-01  8.51516837e-01  8.52738341e-01  8.59278321e-01
  8.63405229e-01  8.63944025e-01  8.71317237e-01  8.72110527e-01
  8.81155451e-01  8.88871082e-01  8.99491527e-01  9.15045679e-01
  9.26419186e-01  9.34314129e-01  9.34455990e-01  9.39119009e-01
  9.45211016e-01  9.56673634e-01  9.69794855e-01  9.79746621e-01
  9.82926005e-01  9.89651332e-01  9.92977975e-01  9.95546516e-01
  9.98976342e-01  1.00872693e+00  1.01986446e+00  1.02073538e+00
  1.02598296e+00  1.02758009e+00  1.03314204e+00  1.03494196e+00
  1.04018730e+00  1.04440632e+00  1.05885405e+00  1.06610080e+00
  1.06670274e+00  1.07440394e+00  1.07786240e+00  1.09136673e+00
  1.09344266e+00  1.09594227e+00  1.10091589e+00  1.10206048e+00
  1.11015861e+00  1.11326616e+00  1.11716066e+00  1.13025515e+00
  1.15174933e+00  1.15733010e+00  1.16655449e+00  1.18956672e+00
  1.19202278e+00  1.20480981e+00  1.21050655e+00  1.21451982e+00
  1.22903209e+00  1.24980278e+00  1.25716464e+00  1.26270303e+00
  1.28263320e+00  1.28314959e+00  1.28614496e+00  1.28846512e+00
  1.29553141e+00  1.29603911e+00  1.30084257e+00  1.31618605e+00
  1.31802486e+00  1.32297871e+00  1.33604835e+00  1.33843540e+00
  1.34046253e+00  1.35123445e+00  1.35669486e+00  1.36583999e+00
  1.36600959e+00  1.36690390e+00  1.37651156e+00  1.38196373e+00
  1.38771850e+00  1.38851179e+00  1.38975516e+00  1.39009734e+00
  1.40463927e+00  1.41323830e+00  1.42079453e+00  1.42265189e+00
  1.42658247e+00  1.43221584e+00  1.44131670e+00  1.44811904e+00
  1.45106900e+00  1.51295199e+00  1.51645929e+00  1.52813836e+00
  1.55445068e+00  1.56647518e+00  1.57054235e+00  1.57800367e+00
  1.64831671e+00  1.71873473e+00  1.74662747e+00  1.77258897e+00
  1.77612550e+00  1.78257968e+00  1.81172936e+00  1.85324244e+00
  1.86107754e+00  1.87609800e+00  1.89500610e+00  1.89835487e+00
  1.94223869e+00  1.96927990e+00  1.96967256e+00  2.02172671e+00
  2.02581580e+00  2.06531028e+00  2.08519653e+00  2.08659479e+00
  2.13791610e+00  2.22138823e+00  2.22280978e+00  2.27089762e+00
  2.29279608e+00  2.29808508e+00  2.33274877e+00  2.47294023e+00
  2.49062684e+00  2.50270739e+00  2.63168206e+00  2.67754878e+00
  2.72686089e+00  2.74133313e+00  2.86057184e+00  2.86529582e+00
  2.91446542e+00  3.22435263e+00  3.23377034e+00  3.24812528e+00
  3.39466133e+00  3.66796953e+00  3.76075791e+00  4.13121036e+00
  4.15807269e+00  5.05814722e+00  5.07719690e+00  5.29836491e+00
  5.82284093e+00  5.99491742e+00  6.54866583e+00  8.88813894e+00
  9.75889156e+00]

  UserWarning,

2022-10-31 11:02:39,220:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.02949591e+00 -4.02151209e+00 -3.47315515e+00 -3.02907006e+00
 -2.93371210e+00 -2.92861315e+00 -2.92150684e+00 -2.91809476e+00
 -2.88317942e+00 -2.84567135e+00 -2.80641876e+00 -2.73142095e+00
 -2.69597263e+00 -2.66399057e+00 -2.57300573e+00 -2.54304456e+00
 -2.49209548e+00 -2.49057396e+00 -2.47042249e+00 -2.46169942e+00
 -2.44949416e+00 -2.42466509e+00 -2.41594203e+00 -2.41116674e+00
 -2.33556173e+00 -2.28056354e+00 -2.26915573e+00 -2.22237395e+00
 -2.17849803e+00 -2.14356122e+00 -2.14144522e+00 -2.12454751e+00
 -2.02008280e+00 -1.94127007e+00 -1.88522204e+00 -1.88140711e+00
 -1.87172369e+00 -1.86373986e+00 -1.80429330e+00 -1.78739560e+00
 -1.75010528e+00 -1.71996522e+00 -1.68333503e+00 -1.68293089e+00
 -1.67223280e+00 -1.66943254e+00 -1.65253484e+00 -1.60677744e+00
 -1.57359882e+00 -1.54807013e+00 -1.53934706e+00 -1.53819590e+00
 -1.51767407e+00 -1.47938278e+00 -1.46714139e+00 -1.39706325e+00
 -1.38281331e+00 -1.35741220e+00 -1.34893507e+00 -1.33228063e+00
 -1.32727553e+00 -1.32466814e+00 -1.31538293e+00 -1.30193923e+00
 -1.29954794e+00 -1.27834860e+00 -1.26485025e+00 -1.20219515e+00
 -1.17703997e+00 -1.09269917e+00 -1.08896099e+00 -1.07605745e+00
 -1.07128217e+00 -1.06255910e+00 -1.05746016e+00 -1.05457528e+00
 -1.04586020e+00 -1.04566140e+00 -1.01891771e+00 -9.95128720e-01
 -9.67811935e-01 -9.64967190e-01 -9.59088865e-01 -9.41196689e-01
 -9.32473620e-01 -9.29814339e-01 -9.27698338e-01 -9.19523705e-01
 -9.11863354e-01 -9.03899869e-01 -8.73766307e-01 -8.41125803e-01
 -8.32765706e-01 -8.06335925e-01 -7.97612856e-01 -7.87193772e-01
 -7.78470703e-01 -7.75939872e-01 -7.75811422e-01 -7.63734612e-01
 -7.60104887e-01 -7.51146929e-01 -7.42761253e-01 -7.38905543e-01
 -7.17232559e-01 -7.14810866e-01 -7.14347489e-01 -7.08509489e-01
 -6.95245661e-01 -6.83351805e-01 -6.62752091e-01 -6.60092811e-01
 -6.57369598e-01 -6.49967731e-01 -6.42665117e-01 -6.41079107e-01
 -6.28873848e-01 -6.05630789e-01 -6.04044779e-01 -5.92662429e-01
 -5.90546428e-01 -5.82371795e-01 -5.73648725e-01 -5.69814375e-01
 -5.67349243e-01 -5.29726713e-01 -5.25232047e-01 -5.23116045e-01
 -4.74302713e-01 -4.65369086e-01 -4.60460945e-01 -4.57902765e-01
 -4.57801665e-01 -4.55685663e-01 -4.38787961e-01 -4.17993054e-01
 -4.06033129e-01 -3.94413487e-01 -3.71357579e-01 -3.67875389e-01
 -3.34323250e-01 -3.25600181e-01 -3.23094061e-01 -3.22940900e-01
 -3.03927197e-01 -2.69189672e-01 -2.68478878e-01 -2.53394517e-01
 -2.45971470e-01 -2.45219884e-01 -2.16104199e-01 -2.12611443e-01
 -2.12147130e-01 -2.01308316e-01 -1.98987515e-01 -1.88080136e-01
 -1.85964135e-01 -1.82981190e-01 -1.60359297e-01 -1.33369782e-01
 -1.32032104e-01 -1.29707352e-01 -1.29043106e-01 -1.20653449e-01
 -1.18533753e-01 -1.08000131e-01 -1.01636051e-01 -8.58010666e-02
 -8.29485125e-02 -6.76387965e-02 -5.98264404e-02 -5.58786527e-02
 -5.48053819e-02 -5.32193721e-02 -4.84556055e-02 -3.42056687e-02
 -2.63219860e-02 -2.00751664e-02  5.33485265e-03  1.15517294e-02
  2.45016439e-02  3.32247134e-02  3.84491155e-02  4.90596975e-02
  5.41028587e-02  5.67791755e-02  7.05150313e-02  8.16413920e-02
  8.37573932e-02  9.14695112e-02  9.19095546e-02  1.00655095e-01
  1.18287439e-01  1.30514133e-01  1.37689424e-01  1.49071774e-01
  1.80851143e-01  1.84656183e-01  2.18618157e-01  2.28861107e-01
  2.43942728e-01  2.49237984e-01  2.60480226e-01  2.67980242e-01
  2.71220168e-01  2.80123404e-01  2.83932538e-01  3.02946242e-01
  3.05898869e-01  3.06607972e-01  3.08730749e-01  3.29716409e-01
  3.32715634e-01  3.57649255e-01  3.61653554e-01  3.80862044e-01
  3.98267549e-01  4.06080932e-01  4.07666942e-01  4.07978388e-01
  4.08448656e-01  4.15925855e-01  4.19828528e-01  4.54941083e-01
  4.72773918e-01  4.75097324e-01  5.05237388e-01  5.27928068e-01
  5.36937356e-01  5.80019134e-01  5.84034584e-01  5.95552485e-01
  6.07195172e-01  6.08372078e-01  6.09958088e-01  6.21074327e-01
  6.23200450e-01  6.26938629e-01  6.35379690e-01  6.37361655e-01
  6.46992417e-01  6.51464568e-01  6.67920523e-01  6.73976395e-01
  6.90630832e-01  7.25512318e-01  7.28927762e-01  7.36095783e-01
  7.39307903e-01  7.43232842e-01  7.44818852e-01  7.52437399e-01
  7.53258284e-01  7.64649837e-01  7.65211468e-01  7.74958916e-01
  7.75305213e-01  7.87208116e-01  8.04361286e-01  8.23071162e-01
  8.26214386e-01  8.33666229e-01  8.35552940e-01  8.50853800e-01
  8.52874719e-01  8.62017216e-01  8.69370537e-01  8.71134204e-01
  8.81500562e-01  8.89538320e-01  8.95810024e-01  9.01013393e-01
  9.01096611e-01  9.06197344e-01  9.30801654e-01  9.34974854e-01
  9.42206055e-01  9.58766350e-01  9.64090539e-01  9.72442178e-01
  9.77250062e-01  9.94230603e-01  9.96056668e-01  1.02408073e+00
  1.02619673e+00  1.03163450e+00  1.03243906e+00  1.04710351e+00
  1.04926272e+00  1.05788068e+00  1.05991918e+00  1.06384195e+00
  1.08045804e+00  1.09362711e+00  1.09619419e+00  1.09895130e+00
  1.10886815e+00  1.12766761e+00  1.15619221e+00  1.16759298e+00
  1.19202672e+00  1.19583778e+00  1.19896388e+00  1.21390556e+00
  1.21636765e+00  1.22457111e+00  1.22743607e+00  1.22866911e+00
  1.26497565e+00  1.27056864e+00  1.28206665e+00  1.30323395e+00
  1.30907162e+00  1.31211138e+00  1.31425594e+00  1.32353336e+00
  1.37130926e+00  1.38487641e+00  1.38694972e+00  1.39231778e+00
  1.39532035e+00  1.41772774e+00  1.43480023e+00  1.43761978e+00
  1.47183729e+00  1.47407717e+00  1.48708006e+00  1.49202682e+00
  1.51117955e+00  1.53989725e+00  1.54767486e+00  1.55862682e+00
  1.56262686e+00  1.57526964e+00  1.57705318e+00  1.59659015e+00
  1.61527795e+00  1.62186686e+00  1.62766927e+00  1.63253559e+00
  1.63293253e+00  1.63867300e+00  1.68374712e+00  1.70767483e+00
  1.74710530e+00  1.75050665e+00  1.75638105e+00  1.75789591e+00
  1.78092177e+00  1.79747343e+00  1.82288113e+00  1.86839882e+00
  1.86972719e+00  1.89268518e+00  1.89275757e+00  1.90196168e+00
  1.92077478e+00  1.93304599e+00  1.95434293e+00  1.96340858e+00
  2.00064210e+00  2.01961692e+00  2.02382500e+00  2.02704750e+00
  2.03301342e+00  2.03899535e+00  2.04530910e+00  2.05363441e+00
  2.05515292e+00  2.06034885e+00  2.06867623e+00  2.13004159e+00
  2.13129888e+00  2.13196126e+00  2.14735304e+00  2.16197524e+00
  2.17366072e+00  2.20607881e+00  2.21656775e+00  2.22211831e+00
  2.22323955e+00  2.23221476e+00  2.24446583e+00  2.25036875e+00
  2.25058854e+00  2.26241879e+00  2.27129808e+00  2.28294082e+00
  2.31087145e+00  2.33624927e+00  2.40812157e+00  2.45731853e+00
  2.46012630e+00  2.46239909e+00  2.51185000e+00  2.54768589e+00
  2.56605115e+00  2.56748178e+00  2.57925857e+00  2.59914308e+00
  2.62255705e+00  2.64368433e+00  2.64851444e+00  2.64871223e+00
  2.69202765e+00  2.71619102e+00  2.74015498e+00  2.74031806e+00
  2.74601362e+00  2.76483097e+00  2.76894436e+00  2.79152729e+00
  2.81080297e+00  2.82375742e+00  2.85158486e+00  2.89006415e+00
  2.89201870e+00  2.90628736e+00  2.90918619e+00  2.97441803e+00
  2.98195450e+00  2.99360147e+00  2.99674456e+00  3.00222499e+00
  3.01859093e+00  3.03940722e+00  3.04826718e+00  3.07588559e+00
  3.10118247e+00  3.11894859e+00  3.17052738e+00  3.19510801e+00
  3.21813589e+00  3.29109162e+00  3.32186663e+00  3.35528771e+00
  3.40155215e+00  3.41430760e+00  3.46927731e+00  3.49925741e+00
  3.50786707e+00  3.51639166e+00  3.61622480e+00  3.61736949e+00
  3.67358572e+00  3.76276986e+00  3.77211185e+00  3.88117849e+00
  3.93976036e+00  4.00260889e+00  4.05912410e+00  4.06840583e+00
  4.13071670e+00  4.17986223e+00  4.25419983e+00  4.26640045e+00
  4.31152694e+00  4.35847876e+00  4.41612930e+00  4.55269990e+00
  4.58872069e+00  4.64561044e+00  4.66046619e+00  4.75747283e+00
  4.76430214e+00  4.77217730e+00  4.78171717e+00  4.91652638e+00
  4.96078946e+00  5.12718874e+00  5.14645366e+00  5.25144344e+00
  5.30747248e+00  5.49254472e+00  5.56820564e+00  5.64640918e+00
  5.81061028e+00  5.87176882e+00  6.00134352e+00  6.04409064e+00
  6.29095844e+00  6.52034304e+00  6.52634606e+00  6.53544816e+00
  6.70818294e+00  6.80919393e+00  6.81594862e+00  6.82862614e+00
  7.10980925e+00  7.17451410e+00  7.21840000e+00  7.41513512e+00
  7.47546510e+00  7.50927025e+00  7.57801426e+00  7.64571015e+00
  7.94032953e+00  7.98860523e+00  8.01906953e+00  8.30789565e+00
  8.33996333e+00  9.03140291e+00  9.03162230e+00  9.31763607e+00
  9.47881378e+00  9.89139097e+00  1.05237704e+01  1.10399212e+01
  1.15443761e+01  1.15443790e+01  1.15749028e+01  1.16321327e+01
  1.16618040e+01  1.17376356e+01  1.21508152e+01  1.23855945e+01
  1.27006193e+01  1.27457739e+01  1.30154625e+01  1.30308635e+01
  1.30370326e+01  1.41319687e+01  1.42746320e+01  1.47289039e+01
  1.47739849e+01  1.48884110e+01  1.51002106e+01  1.51710376e+01
  1.55600405e+01  1.55737430e+01  1.61704925e+01  1.65642207e+01
  1.71919070e+01  1.76646587e+01  1.76807927e+01  1.78851516e+01
  1.80121819e+01  1.81326050e+01  1.84930031e+01  1.94543528e+01
  1.98107459e+01  1.99275483e+01  2.00284696e+01  2.08392178e+01
  2.13040017e+01  2.17263334e+01  2.21971700e+01  2.30002532e+01
  2.30054649e+01  2.32971542e+01  2.39433200e+01  2.47136202e+01
  2.48512517e+01  2.48690728e+01  2.50374198e+01  2.51911108e+01
  2.97605393e+01  2.98751229e+01  3.46087220e+01  3.96238765e+01
  4.14811630e+01  4.57195892e+01  4.99969123e+01  5.20027460e+01
  5.36921222e+01  5.81334574e+01  6.15283876e+01  6.57446476e+01
  7.19602913e+01  7.64821248e+01  1.26331783e+02]

  UserWarning,

2022-10-31 11:02:39,278:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.33896520e+01 -6.29108353e+00 -4.70986901e+00 -4.38955757e+00
 -4.35595557e+00 -3.37695929e+00 -3.23553939e+00 -2.82894506e+00
 -2.36178487e+00 -2.32738275e+00 -2.32651319e+00 -2.23566609e+00
 -2.09435073e+00 -2.08198879e+00 -2.03907132e+00 -1.97639984e+00
 -1.96101543e+00 -1.79040183e+00 -1.77889420e+00 -1.76369946e+00
 -1.75435103e+00 -1.72514257e+00 -1.72500828e+00 -1.59935909e+00
 -1.59079614e+00 -1.54018931e+00 -1.52813091e+00 -1.51023832e+00
 -1.49192383e+00 -1.47591587e+00 -1.45089329e+00 -1.44918212e+00
 -1.32588996e+00 -1.25685901e+00 -1.25262908e+00 -1.24129234e+00
 -1.21253951e+00 -1.20543309e+00 -1.20157778e+00 -1.17913488e+00
 -1.17522917e+00 -1.17471724e+00 -1.16899151e+00 -1.13824451e+00
 -1.12842956e+00 -1.11426843e+00 -1.10404316e+00 -1.09099790e+00
 -1.06608220e+00 -1.00977055e+00 -1.00669406e+00 -9.91324206e-01
 -9.88665453e-01 -9.76209125e-01 -9.61856256e-01 -9.60577388e-01
 -9.34687641e-01 -9.09746128e-01 -8.96653852e-01 -8.94520914e-01
 -8.93905301e-01 -8.81632469e-01 -8.69721211e-01 -8.62545422e-01
 -8.52799370e-01 -8.51269762e-01 -8.49774376e-01 -8.40089935e-01
 -8.36261795e-01 -8.32001918e-01 -8.30122460e-01 -8.20461292e-01
 -8.15537705e-01 -7.76511109e-01 -7.66954381e-01 -7.63562717e-01
 -7.62073770e-01 -7.59615307e-01 -7.58655031e-01 -7.54806818e-01
 -7.47896694e-01 -7.45153065e-01 -7.28159233e-01 -7.26847323e-01
 -7.17802222e-01 -7.05339150e-01 -7.04404132e-01 -6.99507316e-01
 -6.95688442e-01 -6.92232551e-01 -6.66858752e-01 -6.61312386e-01
 -6.49848346e-01 -6.45955596e-01 -6.33332331e-01 -6.26410562e-01
 -6.26179344e-01 -6.24846521e-01 -6.17165967e-01 -6.15568912e-01
 -6.14170279e-01 -6.13503490e-01 -6.03072825e-01 -5.97822548e-01
 -5.85962373e-01 -5.83256044e-01 -5.80865446e-01 -5.74320016e-01
 -5.74244355e-01 -5.72237923e-01 -5.51752062e-01 -5.46640754e-01
 -5.39791828e-01 -5.36507951e-01 -5.31009890e-01 -5.26974817e-01
 -5.23210799e-01 -5.21614847e-01 -5.14576149e-01 -5.07619160e-01
 -4.97669861e-01 -4.86383978e-01 -4.73546972e-01 -4.62926298e-01
 -4.62331456e-01 -4.49398626e-01 -4.46503940e-01 -4.43751396e-01
 -4.36965824e-01 -4.36029789e-01 -4.19230818e-01 -4.15612266e-01
 -4.10559656e-01 -4.07719243e-01 -4.06365500e-01 -4.01599607e-01
 -3.98409387e-01 -3.95581599e-01 -3.94058456e-01 -3.91275133e-01
 -3.83982054e-01 -3.83721424e-01 -3.82611672e-01 -3.79344891e-01
 -3.75039541e-01 -3.67271684e-01 -3.56408873e-01 -3.56291956e-01
 -3.56135739e-01 -3.54818820e-01 -3.54560551e-01 -3.51839610e-01
 -3.50211792e-01 -3.43273234e-01 -3.31089431e-01 -3.30821185e-01
 -3.26188454e-01 -3.24710853e-01 -3.24600408e-01 -3.23770641e-01
 -3.05422747e-01 -2.89356290e-01 -2.71217245e-01 -2.70397130e-01
 -2.63950086e-01 -2.59841243e-01 -2.58369345e-01 -2.55481291e-01
 -2.50335913e-01 -2.38332876e-01 -2.30451796e-01 -2.28232773e-01
 -2.13246782e-01 -2.12922433e-01 -2.05474979e-01 -2.04563707e-01
 -2.02746558e-01 -2.00166284e-01 -1.97536553e-01 -1.96089157e-01
 -1.93340650e-01 -1.85914228e-01 -1.84936236e-01 -1.83960066e-01
 -1.81480475e-01 -1.79776876e-01 -1.78920238e-01 -1.70204010e-01
 -1.65030105e-01 -1.60260656e-01 -1.58270677e-01 -1.56686478e-01
 -1.53436865e-01 -1.52892461e-01 -1.46293126e-01 -1.45287509e-01
 -1.42576810e-01 -1.35476218e-01 -1.26668187e-01 -1.25208370e-01
 -1.20863815e-01 -1.12444271e-01 -1.07653504e-01 -9.32227824e-02
 -8.31575550e-02 -7.46379526e-02 -6.99673873e-02 -6.92014447e-02
 -6.63737013e-02 -6.49781257e-02 -6.22911475e-02 -6.01321475e-02
 -4.79521026e-02 -4.68466415e-02 -2.26177638e-02 -2.11586652e-02
 -2.08880052e-02 -1.96877161e-02 -9.77951673e-03 -5.48044007e-03
 -3.99096692e-03 -6.60748867e-04  3.31956138e-03  8.40323714e-03
  1.06844345e-02  1.20099250e-02  1.68833355e-02  2.02653353e-02
  2.24640724e-02  3.12867477e-02  4.08987021e-02  4.25372357e-02
  4.37430817e-02  4.64730847e-02  5.40476731e-02  5.93323444e-02
  6.67038218e-02  7.02148956e-02  7.27425662e-02  7.47224591e-02
  7.52453823e-02  8.58685535e-02  8.96279969e-02  9.59391484e-02
  9.73703754e-02  1.04170304e-01  1.04538779e-01  1.05197605e-01
  1.05444866e-01  1.10324618e-01  1.10663085e-01  1.10998424e-01
  1.13595128e-01  1.13840656e-01  1.19615398e-01  1.26832037e-01
  1.27314820e-01  1.28275096e-01  1.30510760e-01  1.34866341e-01
  1.37800706e-01  1.42105043e-01  1.45206189e-01  1.47177389e-01
  1.49976288e-01  1.57799817e-01  1.65879868e-01  1.75825521e-01
  1.82553233e-01  1.85654378e-01  1.86034443e-01  1.90315440e-01
  1.92788290e-01  1.94737036e-01  1.95116110e-01  2.08951187e-01
  2.11141247e-01  2.12101523e-01  2.23109428e-01  2.28290290e-01
  2.31683305e-01  2.33236480e-01  2.36434826e-01  2.39682604e-01
  2.41309418e-01  2.41433952e-01  2.46180954e-01  2.47812681e-01
  2.48332864e-01  2.49619665e-01  2.53127843e-01  2.60970021e-01
  2.61392449e-01  2.65153211e-01  2.65480584e-01  2.72269911e-01
  2.75633415e-01  2.77626166e-01  2.85190378e-01  2.87952098e-01
  2.91629411e-01  2.92037627e-01  2.92997902e-01  2.96114668e-01
  2.96463948e-01  3.02291637e-01  3.06411252e-01  3.08148152e-01
  3.12579684e-01  3.19073996e-01  3.20348172e-01  3.21817042e-01
  3.25467997e-01  3.26905079e-01  3.28583806e-01  3.29090702e-01
  3.30441316e-01  3.31592868e-01  3.32208348e-01  3.32356371e-01
  3.36232986e-01  3.43440095e-01  3.48683354e-01  3.55877034e-01
  3.62936881e-01  3.64050068e-01  3.65212399e-01  3.68007969e-01
  3.70964234e-01  3.72649046e-01  3.74300094e-01  3.79180823e-01
  3.80526086e-01  3.80617022e-01  3.80917043e-01  3.88354920e-01
  3.90350249e-01  3.96977985e-01  4.05660589e-01  4.13382196e-01
  4.19601368e-01  4.23532431e-01  4.25242371e-01  4.26249291e-01
  4.31546145e-01  4.32816069e-01  4.36774480e-01  4.46495391e-01
  4.48249649e-01  4.54267281e-01  4.54529087e-01  4.59570585e-01
  4.77713100e-01  4.77874364e-01  4.79963796e-01  4.87376650e-01
  4.88697838e-01  4.94278575e-01  4.95238851e-01  4.98030703e-01
  5.01378712e-01  5.06138751e-01  5.10139240e-01  5.10225050e-01
  5.13577227e-01  5.15144983e-01  5.18322554e-01  5.25534209e-01
  5.26405047e-01  5.29641876e-01  5.32375782e-01  5.35525777e-01
  5.35687041e-01  5.38874065e-01  5.46586940e-01  5.49429515e-01
  5.53555502e-01  5.55268823e-01  5.59628514e-01  5.62842521e-01
  5.66339202e-01  5.69510813e-01  5.74359791e-01  5.78462328e-01
  5.84095656e-01  5.87035130e-01  5.87189727e-01  5.92552132e-01
  5.93638197e-01  5.95717013e-01  5.97941691e-01  5.98117837e-01
  5.99218933e-01  6.10042408e-01  6.12693097e-01  6.15404022e-01
  6.15623145e-01  6.16583420e-01  6.27368215e-01  6.27483320e-01
  6.35625788e-01  6.36165202e-01  6.40337766e-01  6.45995507e-01
  6.49148218e-01  6.50490597e-01  6.56071334e-01  6.63174151e-01
  6.64605308e-01  6.67022553e-01  6.76613392e-01  6.78166209e-01
  6.80115313e-01  6.80147262e-01  6.83687073e-01  6.86687724e-01
  6.91141308e-01  6.96519524e-01  6.97479800e-01  7.10563022e-01
  7.14599392e-01  7.17061582e-01  7.17195599e-01  7.31386977e-01
  7.43247152e-01  7.48827889e-01  7.63065839e-01  7.65278260e-01
  7.67320432e-01  7.68773900e-01  7.77415904e-01  7.79123247e-01
  7.80554791e-01  7.87860156e-01  7.97957961e-01  7.98282311e-01
  7.98940589e-01  8.12283356e-01  8.15894321e-01  8.22672583e-01
  8.23420161e-01  8.23884960e-01  8.29724269e-01  8.35947839e-01
  8.48164519e-01  8.55382235e-01  8.57541956e-01  8.59272559e-01
  8.68701509e-01  8.70172458e-01  8.71410623e-01  8.85346035e-01
  8.92065724e-01  8.95610906e-01  8.99246036e-01  9.00592309e-01
  9.09869433e-01  9.11950641e-01  9.14046143e-01  9.20956873e-01
  9.21490282e-01  9.39903541e-01  9.58281648e-01  9.67397813e-01
  9.75036391e-01  9.77078846e-01  9.78763509e-01  9.80617128e-01
  9.89647796e-01  9.93124464e-01  1.00016907e+00  1.00019891e+00
  1.00868500e+00  1.01001499e+00  1.01091249e+00  1.01673597e+00
  1.02106532e+00  1.02997705e+00  1.04064710e+00  1.05628054e+00
  1.06885570e+00  1.07224778e+00  1.09180887e+00  1.10100142e+00
  1.10196170e+00  1.10669244e+00  1.11711116e+00  1.11718092e+00
  1.12017719e+00  1.12154348e+00  1.12596263e+00  1.12984560e+00
  1.14224862e+00  1.14772905e+00  1.15240083e+00  1.15330979e+00
  1.17289263e+00  1.18189780e+00  1.18322779e+00  1.19261138e+00
  1.19375798e+00  1.19963881e+00  1.20594178e+00  1.21243080e+00
  1.21720292e+00  1.21792805e+00  1.22699384e+00  1.22715448e+00
  1.23420617e+00  1.24638997e+00  1.25161681e+00  1.25264135e+00
  1.26279418e+00  1.28366059e+00  1.31463425e+00  1.32035012e+00
  1.33567708e+00  1.35001126e+00  1.35362050e+00  1.35424350e+00
  1.38950036e+00  1.39599893e+00  1.41335815e+00  1.48906772e+00
  1.49508927e+00  1.51643470e+00  1.56304328e+00  1.56762199e+00
  1.60783509e+00  1.66173660e+00  1.68224664e+00  1.68243990e+00
  1.73386117e+00  1.84143515e+00  1.89202375e+00  1.92942634e+00
  1.96959308e+00  2.21055856e+00  2.22541611e+00  2.32408908e+00
  2.34978665e+00  2.43958427e+00  2.44747470e+00  2.54839802e+00
  2.58951154e+00  2.64071910e+00  2.86644175e+00  2.95884930e+00
  3.06734365e+00  3.14293538e+00  3.14343625e+00  3.29780441e+00
  3.71164843e+00  3.82217621e+00  4.32376511e+00  4.40037981e+00
  4.59839529e+00  4.66611149e+00  4.93068069e+00  5.08084113e+00
  5.93213579e+00  6.01400739e+00  6.77981708e+00  7.60407772e+00
  7.85597636e+00  8.44963926e+00  9.04563811e+00  1.11443471e+01
  1.25249005e+01  1.56036453e+01  1.79162084e+01  1.82428379e+01]

  UserWarning,

2022-10-31 11:02:40,093:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.88146270e+01 -3.64976326e+01 -3.55351265e+01 -2.90304584e+01
 -2.21047597e+01 -1.62323138e+01 -1.62138232e+01 -1.58699297e+01
 -1.54496855e+01 -1.45922030e+01 -1.43492202e+01 -1.34843683e+01
 -1.32139227e+01 -1.31055609e+01 -1.30821427e+01 -1.30344789e+01
 -1.30178133e+01 -1.27094738e+01 -1.26720717e+01 -1.26150712e+01
 -1.25600685e+01 -1.25379058e+01 -1.20449873e+01 -1.20333760e+01
 -1.19901871e+01 -1.18519297e+01 -1.17404041e+01 -1.15930771e+01
 -1.15380656e+01 -1.14644245e+01 -1.14193158e+01 -1.13885902e+01
 -1.12930276e+01 -1.11503426e+01 -1.11089237e+01 -1.10947469e+01
 -1.10101879e+01 -1.09192233e+01 -1.06675784e+01 -1.06217168e+01
 -1.06026159e+01 -1.05939412e+01 -1.05918752e+01 -1.05635839e+01
 -1.05617678e+01 -1.05012708e+01 -1.04875509e+01 -1.04773069e+01
 -1.01523125e+01 -1.00774799e+01 -9.87244439e+00 -9.86923977e+00
 -9.84089930e+00 -9.62928209e+00 -9.55369158e+00 -9.54355801e+00
 -9.53156198e+00 -9.44019941e+00 -9.43188103e+00 -9.40318992e+00
 -9.37524159e+00 -9.29091940e+00 -9.28628896e+00 -9.18338071e+00
 -9.16548451e+00 -9.14945375e+00 -9.14866302e+00 -9.05230242e+00
 -9.02968853e+00 -9.00541553e+00 -8.95925882e+00 -8.95763029e+00
 -8.93622558e+00 -8.93085761e+00 -8.87952827e+00 -8.81227296e+00
 -8.79011721e+00 -8.78574690e+00 -8.76438267e+00 -8.74147402e+00
 -8.71894778e+00 -8.66162599e+00 -8.65050768e+00 -8.60723747e+00
 -8.50764683e+00 -8.50263271e+00 -8.41218797e+00 -8.34749179e+00
 -8.33946065e+00 -8.32816714e+00 -8.19716648e+00 -8.19389555e+00
 -8.17817505e+00 -8.04651952e+00 -8.02712496e+00 -8.00925642e+00
 -7.99918624e+00 -7.99213099e+00 -7.92494159e+00 -7.90255319e+00
 -7.89664553e+00 -7.83007364e+00 -7.78709550e+00 -7.76609692e+00
 -7.72881141e+00 -7.71247486e+00 -7.59204600e+00 -7.58220688e+00
 -7.55024494e+00 -7.54633124e+00 -7.54618437e+00 -7.53205755e+00
 -7.53098451e+00 -7.44829285e+00 -7.43219178e+00 -7.43158891e+00
 -7.39885861e+00 -7.38715738e+00 -7.38701051e+00 -7.37515080e+00
 -7.36790970e+00 -7.35799824e+00 -7.34114887e+00 -7.34103242e+00
 -7.33231509e+00 -7.20698299e+00 -7.18197502e+00 -7.17556238e+00
 -7.16312595e+00 -7.12942616e+00 -7.11393939e+00 -7.10719363e+00
 -7.07597014e+00 -7.07351105e+00 -7.04780913e+00 -7.03799767e+00
 -7.02134206e+00 -6.97693953e+00 -6.96173619e+00 -6.95683431e+00
 -6.94273291e+00 -6.92158562e+00 -6.91095345e+00 -6.88214082e+00
 -6.87693791e+00 -6.85166613e+00 -6.84277364e+00 -6.78984512e+00
 -6.76191804e+00 -6.76151179e+00 -6.74501459e+00 -6.73959978e+00
 -6.72604240e+00 -6.70780281e+00 -6.70407853e+00 -6.69712265e+00
 -6.69373272e+00 -6.64566593e+00 -6.62061718e+00 -6.59608235e+00
 -6.56686854e+00 -6.55688254e+00 -6.52796279e+00 -6.43099009e+00
 -6.42991678e+00 -6.40598704e+00 -6.38684102e+00 -6.37804311e+00
 -6.36354636e+00 -6.36274751e+00 -6.36183305e+00 -6.35500419e+00
 -6.33901067e+00 -6.31611829e+00 -6.31597141e+00 -6.29843594e+00
 -6.18055024e+00 -6.15679756e+00 -6.15456220e+00 -6.14681156e+00
 -6.14479097e+00 -6.12396602e+00 -6.11093592e+00 -6.06541269e+00
 -6.05305415e+00 -6.04202623e+00 -6.00146804e+00 -5.97677003e+00
 -5.96156509e+00 -5.95190894e+00 -5.95035296e+00 -5.93975548e+00
 -5.91999081e+00 -5.88890424e+00 -5.87140019e+00 -5.87043048e+00
 -5.86774124e+00 -5.82712086e+00 -5.82634373e+00 -5.77794766e+00
 -5.77173454e+00 -5.76756874e+00 -5.75046046e+00 -5.74216714e+00
 -5.73471999e+00 -5.73152324e+00 -5.72051503e+00 -5.70271109e+00
 -5.66327445e+00 -5.61256068e+00 -5.57186253e+00 -5.56669905e+00
 -5.53774094e+00 -5.51948623e+00 -5.50286625e+00 -5.48823968e+00
 -5.44409421e+00 -5.44066730e+00 -5.41626148e+00 -5.40471490e+00
 -5.39048382e+00 -5.37317079e+00 -5.30382405e+00 -5.29094083e+00
 -5.28251560e+00 -5.24773708e+00 -5.20629102e+00 -5.20248970e+00
 -5.18846833e+00 -5.18001454e+00 -5.17017299e+00 -5.15527688e+00
 -5.13142677e+00 -5.11961351e+00 -5.09946011e+00 -5.09271435e+00
 -5.07240069e+00 -5.06850793e+00 -5.05683871e+00 -5.05227683e+00
 -5.04127222e+00 -5.03925164e+00 -5.03176535e+00 -5.02132615e+00
 -5.00700539e+00 -5.00121608e+00 -4.97775593e+00 -4.97098713e+00
 -4.92658460e+00 -4.91659861e+00 -4.91457802e+00 -4.88268288e+00
 -4.86965543e+00 -4.80070947e+00 -4.78777808e+00 -4.76583510e+00
 -4.76306117e+00 -4.74402495e+00 -4.72154911e+00 -4.71156311e+00
 -4.67568747e+00 -4.63291318e+00 -4.59566157e+00 -4.54417621e+00
 -4.54152159e+00 -4.51651362e+00 -4.50450704e+00 -4.49085429e+00
 -4.47079886e+00 -4.45155117e+00 -4.43840053e+00 -4.38234773e+00
 -4.38116678e+00 -4.33663297e+00 -4.33648609e+00 -4.31547848e+00
 -4.31147813e+00 -4.30696164e+00 -4.29757863e+00 -4.27824240e+00
 -4.26561649e+00 -4.22633274e+00 -4.22436077e+00 -4.20387616e+00
 -4.13281356e+00 -4.13145060e+00 -4.10644263e+00 -4.10041031e+00
 -4.09645664e+00 -4.09443605e+00 -4.07664439e+00 -4.07314952e+00
 -4.06169828e+00 -4.06058100e+00 -4.05974939e+00 -4.02008155e+00
 -3.97227675e+00 -3.92721196e+00 -3.92641511e+00 -3.90140714e+00
 -3.88286024e+00 -3.82654472e+00 -3.80253021e+00 -3.77794842e+00
 -3.76724125e+00 -3.76495882e+00 -3.74054724e+00 -3.71884691e+00
 -3.71560240e+00 -3.68786282e+00 -3.68638565e+00 -3.68436507e+00
 -3.66054121e+00 -3.65746590e+00 -3.64136258e+00 -3.60505901e+00
 -3.59510951e+00 -3.55387248e+00 -3.53165483e+00 -3.52705129e+00
 -3.52455690e+00 -3.51634413e+00 -3.50310162e+00 -3.49133616e+00
 -3.48135016e+00 -3.47712304e+00 -3.42808789e+00 -3.42033226e+00
 -3.38093200e+00 -3.37874792e+00 -3.36683284e+00 -3.34540798e+00
 -3.34112820e+00 -3.31130863e+00 -3.29679035e+00 -3.27631467e+00
 -3.27429408e+00 -3.25281499e+00 -3.24739492e+00 -3.23059653e+00
 -3.20512532e+00 -3.19905569e+00 -3.14901273e+00 -3.14330376e+00
 -3.13955981e+00 -3.13681653e+00 -3.11662540e+00 -3.10627314e+00
 -3.10105581e+00 -3.07127918e+00 -3.06858725e+00 -3.06552710e+00
 -3.04912343e+00 -3.04153017e+00 -3.03889297e+00 -3.03540354e+00
 -3.02766314e+00 -3.02423628e+00 -3.00955031e+00 -2.99882462e+00
 -2.96583455e+00 -2.95518853e+00 -2.90123765e+00 -2.89249134e+00
 -2.87622968e+00 -2.87422170e+00 -2.86422310e+00 -2.84922980e+00
 -2.78570418e+00 -2.74206379e+00 -2.74025807e+00 -2.71196088e+00
 -2.69620216e+00 -2.67119419e+00 -2.66739817e+00 -2.66120819e+00
 -2.65918761e+00 -2.65445370e+00 -2.61977066e+00 -2.51148393e+00
 -2.51056123e+00 -2.50384916e+00 -2.45415211e+00 -2.43503616e+00
 -2.42979566e+00 -2.42029706e+00 -2.40146252e+00 -2.35138737e+00
 -2.32608075e+00 -2.32101750e+00 -2.30691222e+00 -2.30030840e+00
 -2.28613117e+00 -2.26931286e+00 -2.21526157e+00 -2.17736808e+00
 -2.14454827e+00 -2.08822462e+00 -2.05559820e+00 -2.04610171e+00
 -2.04408113e+00 -2.01022608e+00 -2.00314386e+00 -1.98551581e+00
 -1.96665155e+00 -1.95425253e+00 -1.94197373e+00 -1.91451313e+00
 -1.84997849e+00 -1.84714044e+00 -1.77148605e+00 -1.77102632e+00
 -1.74831475e+00 -1.72159549e+00 -1.69157613e+00 -1.68862281e+00
 -1.64225182e+00 -1.63462610e+00 -1.57319525e+00 -1.53129034e+00
 -1.49498744e+00 -1.47065215e+00 -1.43706945e+00 -1.39891026e+00
 -1.14960453e+00 -1.11321565e+00 -1.02714534e+00 -1.02699847e+00
 -9.74503723e-01 -9.64834481e-01 -8.88002346e-01 -8.80201040e-01
 -8.67824611e-01 -8.15888760e-01 -7.86969009e-01 -7.78381213e-01
 -7.25918341e-01 -7.20716681e-01 -6.99987218e-01 -6.76402685e-01
 -6.70536332e-01 -6.64129083e-01 -6.62789119e-01 -6.62341444e-01
 -6.08832683e-01 -5.80417940e-01 -5.31075908e-01 -4.95111009e-01
 -4.40811742e-01 -3.80169400e-01 -3.76473759e-01 -3.48724829e-01
 -3.03034337e-01 -2.79075495e-01 -2.64462472e-01 -2.54378997e-01
 -2.52718134e-01 -2.35923123e-01 -2.35776249e-01 -2.06856499e-01
 -1.49544272e-01 -1.29250147e-01 -5.75544917e-02 -3.59042362e-02
 -1.82100633e-03  3.12635540e-02  1.21257831e-01  1.66720152e-01
  2.03214486e-01  2.11162413e-01  2.45017466e-01  2.63792868e-01
  2.72124177e-01  2.93184905e-01  3.40877287e-01  3.79183354e-01
  3.79330227e-01  3.97382309e-01  4.06748991e-01  4.40872803e-01
  4.73567770e-01  4.77901049e-01  6.48279437e-01  7.03753279e-01
  7.12076814e-01  7.34131012e-01  7.43392703e-01  7.43539577e-01
  7.50033282e-01  7.72312454e-01  7.72459327e-01  7.84416186e-01
  8.26415763e-01  8.51008988e-01  8.71526713e-01  8.90972027e-01
  9.13958525e-01  9.16830502e-01  9.41187089e-01  9.83569035e-01
  1.02882512e+00  1.12726011e+00  1.17649713e+00  1.32509162e+00
  1.41131242e+00  1.68868150e+00  1.81659785e+00  1.82455805e+00
  1.82791496e+00  1.96627869e+00  2.11530374e+00  2.12368342e+00
  2.45495367e+00  2.62372628e+00  2.62463727e+00  2.88149484e+00
  3.17227891e+00  3.40806793e+00  3.65813345e+00  4.11234176e+00
  4.21892778e+00  4.28027920e+00  4.48230009e+00  4.58150003e+00
  4.90984034e+00  5.26491520e+00  5.26673350e+00  5.35405631e+00
  5.36086110e+00  5.40985872e+00  5.71523422e+00  6.46208041e+00
  7.62044278e+00  8.12671660e+00  8.22913885e+00  8.73482369e+00
  9.35481703e+00  9.92783967e+00  1.01966118e+01  1.11354442e+01
  1.22314103e+01  1.30305788e+01  1.32808640e+01  1.38474192e+01
  1.53608044e+01  1.66253448e+01  1.68423622e+01  1.73097993e+01
  2.02280915e+01  2.05706354e+01  2.16870121e+01  2.42557264e+01
  2.59289907e+01  2.69831830e+01  3.17220503e+01  3.45124557e+01
  3.75315796e+01  3.87191173e+01  4.24744510e+01  5.00889032e+01
  5.22780870e+01  5.49675696e+01  7.77217712e+01  7.89302134e+01
  8.00919475e+01  1.10505813e+02  1.34761967e+02]

  UserWarning,

2022-10-31 11:02:40,093:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.83769746e+00 -3.80257713e+00 -3.32194929e+00 -3.18122204e+00
 -3.13406121e+00 -3.09847766e+00 -3.07529588e+00 -3.03222306e+00
 -3.02493134e+00 -2.97258463e+00 -2.93501084e+00 -2.89976755e+00
 -2.83752710e+00 -2.82956229e+00 -2.81686651e+00 -2.79428359e+00
 -2.75528080e+00 -2.60578399e+00 -2.51282910e+00 -2.44246547e+00
 -2.41534536e+00 -2.38855373e+00 -2.37921410e+00 -2.33685855e+00
 -2.28969772e+00 -2.23137460e+00 -2.14912884e+00 -2.14320751e+00
 -2.05801588e+00 -2.05195764e+00 -2.02028373e+00 -2.01349332e+00
 -2.00597312e+00 -1.92553853e+00 -1.91467681e+00 -1.90275927e+00
 -1.86751598e+00 -1.83138472e+00 -1.82952256e+00 -1.81984847e+00
 -1.80919285e+00 -1.80543335e+00 -1.78207274e+00 -1.77394956e+00
 -1.72678873e+00 -1.71170911e+00 -1.70847115e+00 -1.70358593e+00
 -1.70297441e+00 -1.69531419e+00 -1.67170879e+00 -1.64134549e+00
 -1.59709105e+00 -1.57098187e+00 -1.57019095e+00 -1.56285868e+00
 -1.51569786e+00 -1.51312478e+00 -1.49924597e+00 -1.49249506e+00
 -1.48060973e+00 -1.48057753e+00 -1.47497293e+00 -1.46549791e+00
 -1.44533423e+00 -1.42478684e+00 -1.42213144e+00 -1.41021390e+00
 -1.40656798e+00 -1.38701111e+00 -1.35989099e+00 -1.35851873e+00
 -1.35176781e+00 -1.33537289e+00 -1.32381464e+00 -1.31664748e+00
 -1.30726775e+00 -1.29884402e+00 -1.29218277e+00 -1.28140419e+00
 -1.22848040e+00 -1.21916374e+00 -1.21555491e+00 -1.21341419e+00
 -1.19811210e+00 -1.18273341e+00 -1.16387973e+00 -1.14083530e+00
 -1.14067694e+00 -1.14022378e+00 -1.12875941e+00 -1.12299644e+00
 -1.12170162e+00 -1.11914613e+00 -1.11117957e+00 -1.09351611e+00
 -1.07843650e+00 -1.07296871e+00 -1.05391840e+00 -1.04564666e+00
 -1.03652955e+00 -1.03519299e+00 -1.02315249e+00 -1.01640157e+00
 -1.00807287e+00 -9.99949690e-01 -9.64829363e-01 -9.52788861e-01
 -9.37709247e-01 -9.29586066e-01 -9.17668533e-01 -9.02754940e-01
 -8.94465738e-01 -8.90855748e-01 -8.90708782e-01 -8.82425236e-01
 -8.67345622e-01 -8.50893741e-01 -8.50853640e-01 -8.26634545e-01
 -8.24102114e-01 -8.12061612e-01 -7.96981998e-01 -7.88858817e-01
 -7.77007731e-01 -7.64041599e-01 -7.53738490e-01 -7.26618374e-01
 -7.02931734e-01 -6.83374865e-01 -6.71334363e-01 -6.48131569e-01
 -6.36214036e-01 -6.33340945e-01 -6.28288754e-01 -6.22596490e-01
 -6.17375593e-01 -5.96691384e-01 -5.77767945e-01 -5.52103829e-01
 -5.50936819e-01 -5.48243553e-01 -5.42647617e-01 -5.35605307e-01
 -5.20564797e-01 -5.14212195e-01 -5.11241722e-01 -5.07404320e-01
 -5.03714323e-01 -4.91840861e-01 -4.72283993e-01 -4.60761996e-01
 -4.60243491e-01 -4.46136620e-01 -4.37040696e-01 -4.25123163e-01
 -4.10209570e-01 -4.08147554e-01 -3.94878059e-01 -3.90038232e-01
 -3.89749859e-01 -3.78471952e-01 -3.74800252e-01 -3.54125977e-01
 -3.31556744e-01 -3.19516242e-01 -3.04436628e-01 -3.03121322e-01
 -2.80749988e-01 -2.49152618e-01 -2.34073003e-01 -2.14032290e-01
 -2.11159199e-01 -2.03526981e-01 -2.02004553e-01 -1.97941742e-01
 -1.90829495e-01 -1.88938394e-01 -1.79356112e-01 -1.75041030e-01
 -1.70626534e-01 -1.63709379e-01 -1.55586199e-01 -1.44250356e-01
 -1.42657732e-01 -1.40022739e-01 -1.28589051e-01 -1.26843595e-01
 -1.08583734e-01 -1.00719223e-01 -9.57133362e-02 -9.33457548e-02
 -8.52225743e-02 -8.09432191e-02 -7.31126994e-02 -7.15336371e-02
 -6.96591150e-02 -4.63427459e-02 -2.30192235e-02 -2.29821305e-02
 -2.05085185e-02 -7.41083626e-03  7.04509318e-04  3.08717854e-02
  5.55046743e-02  5.72320191e-02  6.01413138e-02  6.37764135e-02
  6.59921131e-02  6.74222073e-02  7.97256711e-02  8.23652444e-02
  1.02665504e-01  1.05547505e-01  1.06900935e-01  1.16723420e-01
  1.34140038e-01  1.35646433e-01  1.43759564e-01  1.45836925e-01
  1.58084369e-01  1.61066567e-01  1.70366426e-01  2.04503662e-01
  2.05818968e-01  2.06719362e-01  2.08515821e-01  2.20696640e-01
  2.41855513e-01  2.43234387e-01  2.58472367e-01  2.61784456e-01
  2.94772545e-01  3.07455184e-01  3.13037269e-01  3.42610005e-01
  3.45072545e-01  3.45230911e-01  3.52522631e-01  3.61423888e-01
  3.85941988e-01  3.91672331e-01  4.11602444e-01  4.26599580e-01
  4.30771212e-01  4.69404874e-01  4.69563240e-01  4.85958159e-01
  4.94081340e-01  4.98623541e-01  5.06071781e-01  5.11301591e-01
  5.18546056e-01  5.18768382e-01  5.27829357e-01  5.35624081e-01
  5.39926864e-01  5.56321784e-01  5.82590666e-01  5.86633287e-01
  5.93870096e-01  6.10290488e-01  6.11605794e-01  6.18850896e-01
  6.26685408e-01  6.32153189e-01  6.43214355e-01  6.47749485e-01
  6.56676474e-01  6.61805736e-01  6.80654113e-01  6.81969418e-01
  6.96859850e-01  6.96890667e-01  6.97049032e-01  7.05172213e-01
  7.32169360e-01  7.35779757e-01  7.67412657e-01  7.84342271e-01
  8.00000554e-01  8.02532984e-01  8.06301747e-01  8.07743750e-01
  8.25078795e-01  8.37776281e-01  8.45899461e-01  8.51890309e-01
  8.57816994e-01  8.58838120e-01  8.71386042e-01  8.72896609e-01
  8.75572156e-01  8.80786361e-01  8.91744986e-01  9.24332883e-01
  9.26865313e-01  9.27390385e-01  9.30783317e-01  9.37234187e-01
  9.40727802e-01  9.40784764e-01  9.43260233e-01  9.53073015e-01
  9.63265550e-01  9.63423915e-01  9.95493524e-01  1.01848016e+00
  1.03308376e+00  1.08765359e+00  1.08834199e+00  1.11657538e+00
  1.12705926e+00  1.13441024e+00  1.14049856e+00  1.14605285e+00
  1.16235443e+00  1.18290342e+00  1.20058833e+00  1.21328104e+00
  1.22711238e+00  1.23788303e+00  1.26073310e+00  1.27080787e+00
  1.29169749e+00  1.29777898e+00  1.29783052e+00  1.31386971e+00
  1.32353788e+00  1.34595637e+00  1.34864625e+00  1.36774181e+00
  1.37795207e+00  1.38068485e+00  1.38527782e+00  1.38843154e+00
  1.39198487e+00  1.41425770e+00  1.43998248e+00  1.45016866e+00
  1.47426505e+00  1.49835445e+00  1.49836878e+00  1.52201245e+00
  1.54165302e+00  1.55360923e+00  1.56702211e+00  1.58785450e+00
  1.58972056e+00  1.60650941e+00  1.61327505e+00  1.61745088e+00
  1.62870576e+00  1.65271341e+00  1.67694088e+00  1.68024449e+00
  1.69539161e+00  1.71188929e+00  1.73284651e+00  1.73965389e+00
  1.76966029e+00  1.79361939e+00  1.83110466e+00  1.83366829e+00
  1.83455167e+00  1.84647178e+00  1.86582430e+00  1.88796058e+00
  1.89755859e+00  1.89875148e+00  1.90022906e+00  1.92236278e+00
  1.93505899e+00  1.93581256e+00  1.93974840e+00  1.95178710e+00
  1.96189248e+00  1.96532871e+00  1.96718075e+00  1.97870321e+00
  1.99190268e+00  2.00002848e+00  2.00329403e+00  2.00560825e+00
  2.05435829e+00  2.08120711e+00  2.10066019e+00  2.10794444e+00
  2.13144304e+00  2.13191165e+00  2.17001831e+00  2.17269132e+00
  2.19563843e+00  2.23782462e+00  2.27483881e+00  2.28465363e+00
  2.28748093e+00  2.28970112e+00  2.29677270e+00  2.36175473e+00
  2.36193166e+00  2.38530406e+00  2.44953190e+00  2.45687060e+00
  2.47639302e+00  2.52827637e+00  2.53645965e+00  2.54683054e+00
  2.60379744e+00  2.60406121e+00  2.61223061e+00  2.70208026e+00
  2.71383864e+00  2.72324210e+00  2.74912927e+00  2.79404206e+00
  2.79420589e+00  2.81981093e+00  2.84008955e+00  2.90539831e+00
  2.99083802e+00  2.99880203e+00  3.07111084e+00  3.07496746e+00
  3.07930489e+00  3.18869912e+00  3.19137404e+00  3.22779440e+00
  3.23065126e+00  3.25565533e+00  3.27292317e+00  3.33065254e+00
  3.34893215e+00  3.35880501e+00  3.42852715e+00  3.43458682e+00
  3.45559033e+00  3.46395491e+00  3.64248441e+00  3.65295979e+00
  3.66782912e+00  3.89841969e+00  3.89859659e+00  3.90335774e+00
  3.90741748e+00  3.90855374e+00  3.94754154e+00  3.98576740e+00
  4.08084672e+00  4.09173182e+00  4.12528030e+00  4.14348296e+00
  4.19066120e+00  4.20785675e+00  4.37102325e+00  4.48079292e+00
  4.48932896e+00  4.49141683e+00  4.53230500e+00  4.60629852e+00
  4.66718614e+00  4.68153800e+00  4.71135007e+00  4.75536459e+00
  4.75743510e+00  4.87098115e+00  4.88810746e+00  4.90912690e+00
  4.95809601e+00  4.99664804e+00  5.10132216e+00  5.10739159e+00
  5.31584417e+00  5.40879071e+00  5.53224951e+00  5.53489391e+00
  5.63462233e+00  5.65961571e+00  6.06095166e+00  6.10669742e+00
  6.27969250e+00  6.32397605e+00  6.40391972e+00  6.61592899e+00
  6.73279085e+00  6.84524314e+00  6.95585530e+00  7.02262313e+00
  7.05146229e+00  7.31491165e+00  7.34394283e+00  7.57693463e+00
  7.66472242e+00  7.79882285e+00  7.81005929e+00  8.19057562e+00
  8.28309026e+00  8.29530663e+00  8.45728314e+00  8.56530157e+00
  8.60251846e+00  8.62322405e+00  8.75878612e+00  8.77487675e+00
  8.81974920e+00  8.85454016e+00  8.93453591e+00  9.01115887e+00
  9.08895183e+00  9.19887919e+00  9.25631500e+00  9.34497336e+00
  9.38351979e+00  9.52758851e+00  9.63603854e+00  9.97567171e+00
  1.02648875e+01  1.03190222e+01  1.04264126e+01  1.08851220e+01
  1.10378884e+01  1.19156571e+01  1.31783574e+01  1.33808797e+01
  1.36669049e+01  1.44413490e+01  1.44933720e+01  1.48655795e+01
  1.49803060e+01  1.50377175e+01  1.55257944e+01  1.56869779e+01
  1.57205310e+01  1.67076816e+01  1.67389712e+01  1.71707603e+01
  1.73021979e+01  1.75797462e+01  1.80590283e+01  1.81432761e+01
  1.86331241e+01  1.86789700e+01  1.90588332e+01  1.93729777e+01
  1.97796863e+01  1.98037771e+01  2.10635933e+01  2.10870818e+01
  2.25413302e+01  2.29313202e+01  2.29679464e+01  2.31306734e+01
  2.34441226e+01  2.37684057e+01  2.54785390e+01  2.62717703e+01
  2.65852707e+01  2.80297517e+01  2.81343689e+01  2.87778309e+01
  2.91635457e+01  2.97836376e+01  3.02686401e+01  3.03169456e+01
  3.34447517e+01  3.59734894e+01  4.04136791e+01  4.83138231e+01
  5.33055907e+01  5.83707160e+01  7.59863542e+01  7.91274357e+01
  8.41818469e+01]

  UserWarning,

2022-10-31 11:02:40,093:INFO:Calculating mean and std
2022-10-31 11:02:40,093:INFO:Creating metrics dataframe
2022-10-31 11:02:40,093:INFO:Uploading results into container
2022-10-31 11:02:40,093:INFO:Uploading model into container now
2022-10-31 11:02:40,093:INFO:master_model_container: 17
2022-10-31 11:02:40,093:INFO:display_container: 2
2022-10-31 11:02:40,093:INFO:PassiveAggressiveRegressor(random_state=3360)
2022-10-31 11:02:40,093:INFO:create_model() successfully completed......................................
2022-10-31 11:02:40,234:WARNING:create_model() for PassiveAggressiveRegressor(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:40,234:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:40,234:INFO:Initializing create_model()
2022-10-31 11:02:40,234:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:40,234:INFO:Checking exceptions
2022-10-31 11:02:40,234:INFO:Importing libraries
2022-10-31 11:02:40,234:INFO:Copying training dataset
2022-10-31 11:02:40,234:INFO:Defining folds
2022-10-31 11:02:40,234:INFO:Declaring metric variables
2022-10-31 11:02:40,234:INFO:Importing untrained model
2022-10-31 11:02:40,234:INFO:Passive Aggressive Regressor Imported successfully
2022-10-31 11:02:40,234:INFO:Starting cross validation
2022-10-31 11:02:40,251:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:42,434:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.56774523e-01 -2.54804588e-01 -2.47394484e-01 -2.38446023e-01
 -2.29929139e-01 -2.13570574e-01 -2.02671622e-01 -1.85186832e-01
 -1.82419704e-01 -1.77776728e-01 -1.73902820e-01 -1.60066051e-01
 -1.57826019e-01 -1.57544255e-01 -1.49392986e-01 -1.48595794e-01
 -1.42161350e-01 -1.40032393e-01 -1.40012947e-01 -1.17651517e-01
 -1.15137499e-01 -1.12801948e-01 -1.08722500e-01 -9.74384882e-02
 -9.33666671e-02 -8.96625140e-02 -8.59565638e-02 -6.80596410e-02
 -6.65212829e-02 -6.60897056e-02 -5.75728215e-02 -5.58386037e-02
 -5.35655481e-02 -5.01627181e-02 -4.73580248e-02 -4.12142567e-02
 -3.96758986e-02 -2.33173339e-02 -1.79581787e-02 -1.43688725e-02
 -5.42041110e-03 -2.57527384e-03  3.52805031e-03  8.88720548e-03
  1.57490875e-02  2.03557578e-02  2.26913084e-02  2.67841283e-02
  2.70330996e-02  3.03734345e-02  3.10656101e-02  5.72188188e-02
  6.61672802e-02  7.51157416e-02  7.70856769e-02  8.01931329e-02
  8.40642030e-02  8.56025611e-02  9.06015003e-02  9.08832642e-02
  9.30126644e-02  9.45510225e-02  9.49825997e-02  1.01961126e-01
  1.10909587e-01  1.12860077e-01  1.16517714e-01  1.25217204e-01
  1.25466175e-01  1.26897827e-01  1.28806510e-01  1.30753111e-01
  1.34165665e-01  1.35432000e-01  1.43340119e-01  1.43677597e-01
  1.47121400e-01  1.52062588e-01  1.55651894e-01  1.61011049e-01
  1.62171465e-01  1.73966784e-01  1.78907972e-01  1.89612688e-01
  1.93212693e-01  1.96804895e-01  2.05753356e-01  2.10316602e-01
  2.18291124e-01  2.20241614e-01  2.23650279e-01  2.27239586e-01
  2.41547202e-01  2.47908635e-01  2.54084970e-01  2.71981893e-01
  2.80930354e-01  2.83594925e-01  2.98827277e-01  3.22083355e-01
  3.31031816e-01  3.39815382e-01  3.48928739e-01  3.52936012e-01
  3.56902313e-01  3.65513296e-01  3.68948281e-01  3.71992843e-01
  3.76841634e-01  3.78603565e-01  3.80067173e-01  3.83156655e-01
  3.86850674e-01  3.88391266e-01  3.91628193e-01  3.92265175e-01
  4.12935795e-01  4.15157275e-01  4.17140624e-01  4.24527717e-01
  4.28877136e-01  4.34905382e-01  4.40614598e-01  4.42447650e-01
  4.44676339e-01  4.46753136e-01  4.50857672e-01  4.63902901e-01
  4.64151314e-01  4.71250696e-01  4.75179868e-01  4.77690805e-01
  4.78815843e-01  4.80958425e-01  4.89525508e-01  4.92035210e-01
  4.99740218e-01  5.04922552e-01  5.07984816e-01  5.08546361e-01
  5.12987927e-01  5.13923769e-01  5.31296562e-01  5.36064926e-01
  5.40083533e-01  5.44291586e-01  5.45013387e-01  5.45444965e-01
  5.51882974e-01  5.52141727e-01  5.53961849e-01  5.62670843e-01
  5.63197866e-01  5.65390559e-01  5.67806396e-01  5.70320413e-01
  5.71858772e-01  5.78297104e-01  5.80308827e-01  5.80388544e-01
  5.96884034e-01  5.96907045e-01  6.00711316e-01  6.05855506e-01
  6.10391619e-01  6.12727170e-01  6.14441503e-01  6.15230835e-01
  6.16601079e-01  6.17032656e-01  6.23035522e-01  6.23752429e-01
  6.25018677e-01  6.26259922e-01  6.27500030e-01  6.27793537e-01
  6.28612574e-01  6.32959643e-01  6.33008656e-01  6.41649352e-01
  6.45854188e-01  6.50290295e-01  6.50597813e-01  6.50856566e-01
  6.55797754e-01  6.59546274e-01  6.59805028e-01  6.61084633e-01
  6.61343386e-01  6.62005692e-01  6.68005816e-01  6.68494736e-01
  6.68753489e-01  6.69599790e-01  6.70703979e-01  6.71566242e-01
  6.72261782e-01  6.73058532e-01  6.77443197e-01  6.77701950e-01
  6.81709224e-01  6.84521012e-01  6.86054759e-01  6.86391659e-01
  6.86650412e-01  6.88188770e-01  6.89924903e-01  6.95340120e-01
  7.04288582e-01  7.14186586e-01  7.22444257e-01  7.23982616e-01
  7.30003371e-01  7.30045522e-01  7.31032060e-01  7.31133966e-01
  7.40082427e-01  7.40341180e-01  7.47045309e-01  7.49030889e-01
  7.50555977e-01  7.50828000e-01  7.55040681e-01  7.58238103e-01
  7.66246080e-01  7.67186564e-01  7.74043221e-01  7.76561893e-01
  7.77414631e-01  7.84824734e-01  7.85083487e-01  7.88693696e-01
  7.89248877e-01  7.93773196e-01  7.93899569e-01  7.94031949e-01
  7.99049033e-01  8.01071459e-01  8.03724186e-01  8.03920465e-01
  8.04260015e-01  8.04518768e-01  8.06999017e-01  8.17278245e-01
  8.20618580e-01  8.20877333e-01  8.25338944e-01  8.33213709e-01
  8.34444955e-01  8.35184950e-01  8.40053861e-01  8.45889666e-01
  8.47463964e-01  8.48057570e-01  8.54053864e-01  8.57645118e-01
  8.63700632e-01  8.67157998e-01  8.76106459e-01  8.77994633e-01
  8.82714215e-01  8.83257810e-01  8.85054921e-01  8.92206271e-01
  8.92383869e-01  8.98010656e-01  9.02257571e-01  9.09082224e-01
  9.10192499e-01  9.12066966e-01  9.15770787e-01  9.21127502e-01
  9.24025494e-01  9.27832755e-01  9.29877660e-01  9.44522046e-01
  9.57122151e-01  9.60168252e-01  9.61781706e-01  9.62375175e-01
  9.70256140e-01  9.72742424e-01  9.75019074e-01  9.76738621e-01
  9.79956898e-01  9.82852774e-01  9.91702179e-01  9.99143746e-01
  1.00032586e+00  1.00380485e+00  1.01074551e+00  1.01077201e+00
  1.01740101e+00  1.02193175e+00  1.03300006e+00  1.03326350e+00
  1.03538155e+00  1.04817455e+00  1.05043396e+00  1.05327858e+00
  1.06069412e+00  1.06125100e+00  1.07984206e+00  1.09905700e+00
  1.10349706e+00  1.10759190e+00  1.10774549e+00  1.11343726e+00
  1.12053089e+00  1.13315033e+00  1.13600273e+00  1.14888536e+00
  1.17933318e+00  1.19368688e+00  1.21073601e+00  1.21899390e+00
  1.22126219e+00  1.22183171e+00  1.22338846e+00  1.23494321e+00
  1.23982959e+00  1.24116894e+00  1.25509449e+00  1.26382300e+00
  1.26445234e+00  1.26475612e+00  1.26772876e+00  1.27454807e+00
  1.28653003e+00  1.29473262e+00  1.29858797e+00  1.30113786e+00
  1.30310205e+00  1.31318074e+00  1.31784955e+00  1.33272029e+00
  1.33357804e+00  1.33963833e+00  1.35092017e+00  1.35409830e+00
  1.35932011e+00  1.38183878e+00  1.38279912e+00  1.40053161e+00
  1.40395782e+00  1.40668585e+00  1.40769789e+00  1.41080758e+00
  1.41333481e+00  1.42029509e+00  1.42097250e+00  1.43339076e+00
  1.44103003e+00  1.44937777e+00  1.44983276e+00  1.45153896e+00
  1.47204674e+00  1.47397276e+00  1.48393992e+00  1.49373971e+00
  1.50382357e+00  1.51016068e+00  1.51528256e+00  1.51628384e+00
  1.51932761e+00  1.51945309e+00  1.52837360e+00  1.52970072e+00
  1.53467861e+00  1.54814341e+00  1.55779492e+00  1.56276925e+00
  1.57255526e+00  1.58267032e+00  1.58771904e+00  1.59458682e+00
  1.59902563e+00  1.60654884e+00  1.61314037e+00  1.62141301e+00
  1.63303807e+00  1.65408153e+00  1.65459740e+00  1.65813817e+00
  1.67265313e+00  1.67558999e+00  1.68131061e+00  1.69783046e+00
  1.69982091e+00  1.70280601e+00  1.70672910e+00  1.71568904e+00
  1.73156053e+00  1.74610079e+00  1.74845228e+00  1.76144604e+00
  1.77420525e+00  1.79591683e+00  1.79825127e+00  1.81570323e+00
  1.81993430e+00  1.84188105e+00  1.84786193e+00  1.86433998e+00
  1.87001713e+00  1.88596795e+00  1.89175040e+00  1.89610942e+00
  1.90419723e+00  1.90653431e+00  1.91247623e+00  1.92235519e+00
  1.93154752e+00  1.96293177e+00  1.97805634e+00  1.98605729e+00
  1.98640641e+00  2.04529350e+00  2.07037275e+00  2.08890753e+00
  2.10545216e+00  2.12663973e+00  2.14953184e+00  2.19703496e+00
  2.22347518e+00  2.23219379e+00  2.24889468e+00  2.29079205e+00
  2.29140529e+00  2.29159184e+00  2.29328879e+00  2.29540148e+00
  2.29725317e+00  2.30349908e+00  2.30917108e+00  2.31165445e+00
  2.31550657e+00  2.31599105e+00  2.32339303e+00  2.34224556e+00
  2.36308250e+00  2.38847504e+00  2.39087157e+00  2.42878880e+00
  2.46342515e+00  2.46576737e+00  2.52899444e+00  2.54865742e+00
  2.55785434e+00  2.60711482e+00  2.61991636e+00  2.62352462e+00
  2.62878709e+00  2.65290641e+00  2.65821999e+00  2.67652912e+00
  2.69167132e+00  2.76530811e+00  2.83973174e+00  2.88116863e+00
  2.88459476e+00  2.88569996e+00  2.88820190e+00  2.90665043e+00
  2.93880957e+00  2.94741500e+00  2.95160051e+00  3.00797003e+00
  3.00941155e+00  3.05571018e+00  3.07709756e+00  3.17439217e+00
  3.22135966e+00  3.22713894e+00  3.22850229e+00  3.31461860e+00
  3.33695858e+00  3.34532661e+00  3.35892368e+00  3.38188352e+00
  3.38991129e+00  3.41803299e+00  3.43642932e+00  3.46028482e+00
  3.54349076e+00  3.63772785e+00  3.67191863e+00  3.67257780e+00
  3.69908345e+00  3.71753203e+00  3.72011410e+00  3.78245787e+00
  3.80349966e+00  3.82143283e+00  3.83601892e+00  3.84512133e+00
  4.03505589e+00  4.08228911e+00  4.17286202e+00  4.17463064e+00
  4.24326688e+00  4.40603494e+00  4.41851130e+00  4.54042034e+00
  4.56279837e+00  4.72489486e+00  4.87992253e+00  4.92491923e+00
  5.09458315e+00  5.19681139e+00  5.20372078e+00  5.25308096e+00
  5.30173337e+00  5.36371924e+00  5.38430774e+00  5.39951350e+00
  5.47113273e+00  5.48550207e+00  5.49594663e+00  5.53198745e+00
  5.54491411e+00  5.55101178e+00  5.57505597e+00  5.68783567e+00
  5.78136823e+00  5.80592477e+00  6.04859636e+00  6.06904885e+00
  6.40324907e+00  6.41507745e+00  6.43979568e+00  6.50553113e+00
  6.54858749e+00  6.58932637e+00  6.62133043e+00  6.73201885e+00
  6.75600008e+00  6.81974148e+00  7.06252071e+00  7.07050425e+00
  7.36050259e+00  7.71871737e+00  7.77963539e+00  8.49839571e+00
  8.55816328e+00  8.70090199e+00  8.81510448e+00  8.85837164e+00
  8.93316536e+00  9.77259902e+00  9.97483294e+00  1.00279874e+01
  1.00836010e+01  1.06769584e+01  1.07922577e+01  1.12353861e+01
  1.17841667e+01  1.18292062e+01  1.20461664e+01  1.20967630e+01
  1.21121265e+01  1.45514456e+01  1.50411144e+01  1.55312878e+01
  1.61207678e+01  1.62253634e+01  1.63176685e+01  1.65116760e+01
  1.88582269e+01  1.93555883e+01  2.71425011e+01  3.19000114e+01]

  UserWarning,

2022-10-31 11:02:42,434:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-11.51373608  -7.59213999  -5.82086408  -5.10621082  -4.99915015
  -4.9877377   -4.98573885  -4.8811111   -3.76951158  -3.61686949
  -3.03538806  -2.94577698  -2.65684367  -2.41071215  -2.18072972
  -1.97552269  -1.91681491  -1.71703362  -1.48513316  -1.42636136
  -1.40973653  -1.29406136  -1.22968193  -1.18243431  -1.10218686
  -1.06465198  -1.01638438  -1.00126297  -0.82621351  -0.79717602
  -0.78268771  -0.77756636  -0.76948076  -0.74008759  -0.73708136
  -0.71773522  -0.6917214   -0.66229791  -0.54356897  -0.49573627
  -0.43130122  -0.38862933  -0.37300131  -0.350051    -0.34288161
  -0.33278025  -0.30628356  -0.28711751  -0.21280254  -0.20370181
  -0.18734508  -0.17656467  -0.15884721  -0.09806416  -0.06707124
  -0.05210005  -0.02456184  -0.02048724   0.01197318   0.01928575
   0.02069015   0.11489152   0.11701176   0.13215769   0.1330253
   0.13603817   0.13996165   0.14161695   0.14554353   0.14699794
   0.15176546   0.15935402   0.15943642   0.179743     0.18160727
   0.18425998   0.19819038   0.21337776   0.2199443    0.24972828
   0.25195295   0.25960434   0.26388232   0.26719081   0.28925823
   0.29678553   0.29980296   0.3098232    0.32127988   0.32231916
   0.34394439   0.34967888   0.35113325   0.35710798   0.37383675
   0.37607823   0.38889173   0.39732384   0.39771684   0.39893447
   0.3999383    0.40031661   0.40225607   0.41506014   0.42731419
   0.43082614   0.43418401   0.43619955   0.43957225   0.44151318
   0.4433636    0.4534995    0.45858018   0.45914128   0.46568613
   0.46931208   0.47339268   0.48328412   0.48703265   0.49824043
   0.50363037   0.51032582   0.51134661   0.51507737   0.51653787
   0.51716029   0.5179463    0.52182715   0.52854371   0.52995813
   0.53204503   0.53253982   0.53652237   0.53722971   0.55244975
   0.55757886   0.56278632   0.56336315   0.56522209   0.56778886
   0.57136838   0.57193156   0.58086831   0.58094272   0.58340543
   0.58613369   0.58622597   0.59398451   0.59739375   0.59865856
   0.59945872   0.60394786   0.60878921   0.60897375   0.61480673
   0.61941442   0.62154322   0.62934658   0.63011451   0.63298052
   0.63308901   0.63363621   0.63655496   0.63662643   0.63703872
   0.63963248   0.64220045   0.64424741   0.65160911   0.65656281
   0.65713536   0.65982068   0.66284362   0.66674489   0.6671846
   0.66907135   0.67097005   0.67103232   0.67129805   0.67221265
   0.67315666   0.67969341   0.68363286   0.68481847   0.68601136
   0.69074846   0.69174269   0.69520479   0.69556885   0.69652141
   0.69746315   0.6986669    0.70028182   0.70166956   0.70559111
   0.70905322   0.71263081   0.71371847   0.71389011   0.71437159
   0.71597743   0.71616847   0.71625486   0.71873539   0.72031808
   0.72664118   0.72767843   0.72934023   0.72982585   0.7341665
   0.73478099   0.73484812   0.73554911   0.73743082   0.73860671
   0.74109071   0.74374264   0.74455282   0.74571177   0.74730991
   0.74893553   0.75211155   0.75233942   0.75247556   0.75373692
   0.75493914   0.75500141   0.75567686   0.75681672   0.7645157
   0.77137112   0.77164856   0.77224967   0.77274591   0.77680478
   0.77790658   0.77918292   0.78090129   0.78175744   0.78279578
   0.78485549   0.78521954   0.79033762   0.7912203    0.7913993
   0.79222512   0.7930223    0.79308026   0.79379972   0.79456278
   0.79891376   0.79999282   0.80253007   0.8060867    0.80875091
   0.81009585   0.81215074   0.81725704   0.81835005   0.82101081
   0.82204408   0.82380609   0.82442414   0.82512062   0.82583722
   0.82711826   0.82918384   0.82978495   0.83012465   0.83156043
   0.83362023   0.83420352   0.83532663   0.8364675    0.83670916
   0.83787419   0.84005892   0.84129902   0.8413899    0.84324926
   0.84346503   0.84380104   0.84391081   0.85083502   0.85089729
   0.85090032   0.85149878   0.85489824   0.8549605    0.85523832
   0.85526229   0.85754368   0.85880231   0.85989532   0.86231869
   0.86237792   0.86528455   0.86714341   0.86750148   0.86784142
   0.86820782   0.86943152   0.87133022   0.87427201   0.87825443
   0.87919525   0.87957495   0.88139286   0.88171654   0.88224469
   0.88296001   0.88485497   0.88613511   0.88855264   0.88864075
   0.88913055   0.89210285   0.89260671   0.89990561   0.90248917
   0.90682982   0.90763592   0.91029193   0.91078817   0.9121798
   0.91721614   0.91903329   0.91922788   0.92067825   0.9214493
   0.92414035   0.92905292   0.93106457   0.93452667   0.93798878
   0.94403444   0.94458726   0.9483751    0.9504101    0.9518372
   0.95453289   0.95529931   0.9561869    0.95789534   0.96876022
   0.97607194   0.97953405   0.98019004   0.98051867   0.98243994
   0.98302092   0.983943     0.98992037   0.99244943   0.99685362
   0.99773217   0.99817981   0.99855749   1.00008398   1.00030668
   1.00376879   1.00518011   1.00541739   1.00594756   1.0072309
   1.02057924   1.02064092   1.02107932   1.02416859   1.02548675
   1.0267347    1.02878799   1.0348463    1.03498924   1.03510554
   1.04291793   1.04531406   1.04925728   1.05607884   1.05614111
   1.0566928    1.059905     1.06315827   1.06377004   1.06683791
   1.06717098   1.06740636   1.06899823   1.06953921   1.06998953
   1.07019469   1.07162661   1.07447859   1.07683714   1.08160047
   1.08230267   1.08377569   1.0843768    1.08729442   1.08790117
   1.09136328   1.09380955   1.09531275   1.09806604   1.09857626
   1.10053267   1.10080878   1.10861154   1.11131059   1.11207365
   1.11503666   1.11534343   1.11553575   1.11906013   1.11911129
   1.11922479   1.12330525   1.12459705   1.128782     1.12938418
   1.13505357   1.13784405   1.13889195   1.1397705    1.14143868
   1.14266353   1.1460936    1.14644835   1.14718377   1.15007525
   1.15082051   1.15290514   1.15338502   1.15361892   1.15535221
   1.15850368   1.15887414   1.16054313   1.16189623   1.1620003
   1.16232293   1.1631369    1.16348736   1.16400524   1.1640675
   1.16746734   1.16843114   1.17092945   1.17178464   1.17192004
   1.17439155   1.1761512    1.17635957   1.17677869   1.17684142
   1.17725255   1.17785366   1.17862932   1.18055271   1.18186951
   1.18477787   1.18500436   1.18570889   1.18823998   1.19413359
   1.19701796   1.1986263    1.20540392   1.20901261   1.21652513
   1.21703745   1.22536376   1.22565724   1.22978525   1.23061341
   1.23324735   1.23340725   1.23405017   1.23642158   1.23688846
   1.24413704   1.24667431   1.24967934   1.25723475   1.25822151
   1.25845324   1.26448326   1.26757997   1.27133052   1.27239416
   1.2860525    1.28905672   1.28923059   1.29075431   1.29100112
   1.29809624   1.3018438    1.30794799   1.30877707   1.31321696
   1.31443679   1.31910112   1.32196211   1.33324613   1.3364378
   1.36064639   1.36344694   1.3675706    1.3710327    1.37156535
   1.37449481   1.37516551   1.37675469   1.38141902   1.38488113
   1.38750468   1.38834323   1.38927111   1.39180534   1.39526744
   1.39872955   1.40190276   1.40219166   1.40488735   1.40571603
   1.40586356   1.41330463   1.41604008   1.41950219   1.42000964
   1.4298885    1.43570149   1.43696795   1.43790572   1.43930352
   1.43932914   1.44145677   1.44303091   1.45066114   1.45119946
   1.45412324   1.45437539   1.46104746   1.47254855   1.4938997
   1.51358834   1.52256758   1.52538455   1.52616882   1.52682747
   1.53091025   1.53708069   1.54025163   1.56357681   1.56699283
   1.56764293   1.57351709   1.57729004   1.59045549   1.60261292
   1.63859443   1.65759206   1.66742906   1.71706734   1.71889971
   1.72640688   1.72901455   1.74893362   1.79823478   1.81052412
   1.86884252   1.89488314   1.9248575    1.95117574   1.96275389
   2.03106079   2.10639977   2.27528564   2.27985135   2.38515663
   3.07325719   3.23708093   4.8804758    6.35100041]

  UserWarning,

2022-10-31 11:02:42,480:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-5.10200358 -4.26814219 -3.69281014 -2.48327842 -2.40074346 -2.3870153
 -2.2350873  -2.21002295 -2.13920414 -2.12133852 -2.02564335 -1.99664998
 -1.96829758 -1.60544288 -1.49351958 -1.4291065  -1.328609   -1.23560519
 -1.18417751 -1.1575717  -1.1416605  -1.12475622 -1.08264886 -1.05440975
 -1.02827333 -0.98628171 -0.96600721 -0.80032782 -0.79012411 -0.70076568
 -0.66647754 -0.65137977 -0.61855441 -0.57771591 -0.54391464 -0.50267498
 -0.46564001 -0.40793506 -0.33136495 -0.3270434  -0.3207827  -0.31388241
 -0.31249641 -0.31156086 -0.28577608 -0.27381284 -0.25702741 -0.18942226
 -0.17579604 -0.13867445 -0.08414862 -0.08062515 -0.07813892 -0.04376263
 -0.03990891 -0.03923105  0.02863841  0.04057491  0.04481901  0.0583712
  0.07361377  0.09106553  0.09154163  0.09552292  0.1164942   0.1319999
  0.13432355  0.13987098  0.1418973   0.1682275   0.17376508  0.18174714
  0.18822719  0.1939691   0.20845874  0.23203009  0.23531307  0.24812113
  0.25172928  0.25750948  0.27192044  0.27878085  0.28089166  0.31102229
  0.32189989  0.33502855  0.33927144  0.34589525  0.35023746  0.36046078
  0.3636493   0.36479148  0.3652948   0.36969989  0.36996346  0.37670361
  0.37745095  0.38024646  0.38849048  0.389962    0.39039394  0.39824596
  0.4150139   0.42288418  0.43720367  0.4413516   0.44377702  0.44379175
  0.44993376  0.45109717  0.45212339  0.45871805  0.47333524  0.47472737
  0.47654567  0.49398293  0.49477076  0.49869434  0.49994762  0.50143775
  0.5139551   0.52816061  0.53901391  0.55867772  0.56111623  0.56218437
  0.5622824   0.5647521   0.57029205  0.57390562  0.57600442  0.57655877
  0.57789397  0.58458658  0.58539351  0.58633888  0.58787727  0.59099871
  0.59250863  0.59318834  0.5984192   0.60601922  0.61178747  0.61200026
  0.61779746  0.63576063  0.64742468  0.64880848  0.65484518  0.67416121
  0.67456042  0.67680108  0.67690794  0.68007064  0.69488113  0.69743854
  0.69836016  0.70136856  0.7061001   0.70809986  0.71508114  0.71841965
  0.72565153  0.72809451  0.73657921  0.73698264  0.74067204  0.74293746
  0.74557079  0.75110378  0.75306969  0.75927784  0.75978116  0.76518867
  0.7667836   0.77529715  0.78479788  0.78544953  0.7882024   0.79109503
  0.79294135  0.7965947   0.79780164  0.81005841  0.81573311  0.81635793
  0.81711579  0.81813411  0.82696266  0.8282237   0.82885325  0.83006292
  0.83825555  0.83850348  0.84264707  0.84442012  0.84837748  0.85352399
  0.85553149  0.86225293  0.8662413   0.86766184  0.87099071  0.87257705
  0.87345565  0.87572576  0.87956694  0.88513308  0.88555332  0.88602821
  0.88844759  0.88945583  0.89017149  0.89492946  0.8989964   0.89913333
  0.90101634  0.90429717  0.90542292  0.90972103  0.90994997  0.9124123
  0.91354322  0.91928112  0.9194232   0.92071241  0.92264697  0.925874
  0.92762618  0.93570163  0.94389211  0.94762287  0.95135544  0.97515372
  0.97907294  0.97956843  0.98171591  0.98688505  0.98833708  0.98890974
  0.9924198   0.99870377  1.00026108  1.00313905  1.00778141  1.01075551
  1.01153259  1.01369805  1.01433819  1.01803485  1.02010212  1.02238895
  1.02328843  1.02365221  1.02730774  1.0359944   1.03680632  1.03974128
  1.04191104  1.04991128  1.05025277  1.05282841  1.05318235  1.0566291
  1.05674406  1.05792641  1.05912118  1.06090489  1.06101068  1.06357191
  1.06410156  1.06420739  1.07208962  1.07458992  1.07763672  1.07848701
  1.08681518  1.08789226  1.09242038  1.10779923  1.10811795  1.10974532
  1.11335753  1.11648796  1.12695181  1.13824899  1.13911371  1.1394844
  1.14520004  1.14884636  1.15635836  1.16386687  1.16877643  1.17705935
  1.17769948  1.18191571  1.18220807  1.18226229  1.1830847   1.18538434
  1.18652772  1.1894506   1.19340726  1.19445687  1.20475246  1.20576446
  1.20745272  1.21358304  1.21993392  1.22523365  1.23348532  1.23413285
  1.23621688  1.23660073  1.24850583  1.24945121  1.25556361  1.26169831
  1.26759884  1.26804011  1.27201762  1.27501294  1.27903564  1.28239097
  1.28933451  1.2899113   1.29302671  1.30099007  1.30324843  1.30413748
  1.30724863  1.30969566  1.31881215  1.3212397   1.32404093  1.3331448
  1.33546162  1.34153683  1.35382326  1.36016989  1.37446941  1.37455027
  1.37940305  1.38294848  1.38982616  1.3918876   1.39883199  1.40051012
  1.40226777  1.4098433   1.41858173  1.41939654  1.42010058  1.42089918
  1.42868012  1.43048076  1.43848408  1.4412723   1.44161975  1.44349969
  1.44720018  1.45668769  1.46131927  1.46906867  1.47124522  1.47652656
  1.47732515  1.48046762  1.48179703  1.48473736  1.48476577  1.48722927
  1.48740222  1.49051763  1.50553814  1.51511973  1.51561521  1.51873062
  1.52153185  1.52823742  1.53375113  1.54333272  1.54561901  1.54694361
  1.54876501  1.5509709   1.55650737  1.57019322  1.57346227  1.5757932
  1.57899035  1.58499584  1.58748391  1.60025418  1.60062246  1.60336959
  1.61253715  1.61759151  1.61974722  1.61994975  1.62032306  1.62928973
  1.63148048  1.63307292  1.63763104  1.64269375  1.65510773  1.65618467
  1.65668015  1.65919798  1.65979556  1.66052648  1.66896894  1.67161981
  1.67443222  1.67490571  1.67737281  1.68438983  1.68439766  1.68488702
  1.68489314  1.68793624  1.69823028  1.70164518  1.7127379   1.71594185
  1.71622154  1.71686168  1.73398033  1.73571392  1.73882933  1.74131912
  1.75364005  1.75594369  1.75927659  1.75945504  1.76797598  1.76953211
  1.77264752  1.77947569  1.77986163  1.7814729   1.78366045  1.78686944
  1.78766803  1.79747496  1.8048878   1.80822322  1.81276561  1.81688044
  1.8254626   1.82913673  1.83300161  1.83550375  1.84048312  1.84409401
  1.84683918  1.84989222  1.85367559  1.85477311  1.86643774  1.87566232
  1.87691731  1.88188858  1.88238406  1.88511562  1.89510403  1.90051998
  1.90206669  1.9051303   1.90638056  1.90733588  1.9080698   1.91010157
  1.91059705  1.91371246  1.92753129  1.92873297  1.94261419  1.94322541
  1.96652755  1.96897161  1.96927682  1.97013844  1.97543625  1.97646781
  1.97875516  1.98515895  1.98564238  1.98747577  2.00163024  2.01598568
  2.04158493  2.04510025  2.05678555  2.06899932  2.0793795   2.08299039
  2.09054758  2.09530564  2.09721231  2.09801091  2.10759249  2.10808797
  2.12142511  2.12622389  2.12823587  2.13459243  2.14341093  2.16401847
  2.16433215  2.1709523   2.17704253  2.18339453  2.19430866  2.21703268
  2.22093993  2.23778177  2.25506956  2.26367795  2.26728884  2.27064417
  2.2729117   2.27439344  2.27881901  2.29550183  2.30508341  2.3097075
  2.31928314  2.32371482  2.3275679   2.33194931  2.3519278   2.36486472
  2.39333327  2.39866353  2.41357877  2.43576818  2.47797224  2.49219415
  2.50618523  2.54280818  2.55738208  2.55900031  2.5872133   2.6058447
  2.68998035  2.71869666  2.78831511  2.81255668  2.8411302   2.84474109
  2.87363775  2.94718354  2.97861438  3.07583682  3.11820373  3.26815094
  3.27646829  3.43968407  4.16695055  4.1975614   4.26880641  4.27921617
  4.34094539  4.37070057  5.05546045  5.30992908  5.58393033  6.34405341
  6.71851606  7.20375262 11.29504126 12.37558946]

  UserWarning,

2022-10-31 11:02:42,509:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [ 0.48166673  0.52890319  0.61062807  0.61142614  0.63283718  0.65198407
  0.66373798  0.6678724   0.6821734   0.68516783  0.70239208  0.70429591
  0.71795094  0.72455905  0.74323072  0.76074514  0.76551999  0.77059786
  0.77105194  0.77336395  0.77365785  0.78073812  0.78541474  0.79024181
  0.79136104  0.7939289   0.79504813  0.79700002  0.79761598  0.79803355
  0.79967697  0.80130307  0.80725917  0.80753725  0.80806127  0.80867724
  0.8160514   0.81780043  0.8185985   0.82753022  0.82935004  0.83079974
  0.83446606  0.83490439  0.840721    0.84199555  0.84440809  0.85150418
  0.85178226  0.85246701  0.85546934  0.86284351  0.87021768  0.87063524
  0.87390476  0.87759185  0.88127893  0.88496601  0.88503918  0.8886531
  0.89275775  0.89971435  0.90340144  0.90455121  0.90708852  0.90750608
  0.9107756   0.91119317  0.91346822  0.91437608  0.91446269  0.91488025
  0.919269    0.92037094  0.92808951  0.93076365  0.93289811  0.94068984
  0.94087042  0.94283784  0.94570838  0.94590896  0.94652493  0.94703048
  0.94764645  0.95071757  0.95389909  0.95543818  0.9587077   0.96281235
  0.96325125  0.96373071  0.96496035  0.96856443  0.97529917  0.97588981
  0.97721628  0.97756068  0.97856869  0.98493485  0.98594286  0.98962995
  0.99230902  0.9959961   0.99618757  0.99733203  0.99968319  1.00101465
  1.00724883  1.00806537  1.00838882  1.01024027  1.01040603  1.01543954
  1.01777722  1.02294075  1.02314162  1.02568425  1.02650079  1.02691835
  1.02937133  1.0367455   1.03756204  1.04024111  1.04525966  1.0469972
  1.04842489  1.05438576  1.05518092  1.05632091  1.05755502  1.058868
  1.06010211  1.06391624  1.0657123   1.06624217  1.07116336  1.07361634
  1.07677355  1.07853753  1.08153423  1.0821305   1.08222462  1.08467759
  1.0859117   1.08836468  1.08918122  1.09205176  1.09260004  1.09266188
  1.0928683   1.09908153  1.09942593  1.09988305  1.10190185  1.10311301
  1.10379777  1.10434712  1.1080342   1.10837456  1.11048718  1.11172129
  1.11786135  1.12093247  1.12154844  1.12278254  1.12307955  1.12493445
  1.12646963  1.13015671  1.13260969  1.13629677  1.13688613  1.13753088
  1.13765793  1.13883802  1.13949356  1.13998386  1.14367094  1.14428908
  1.14636356  1.14696603  1.15119999  1.15227922  1.15373772  1.15473219
  1.16261053  1.16517748  1.16526357  1.16895065  1.16965714  1.17217315
  1.17263774  1.17469972  1.17573765  1.17632482  1.1768547   1.18241829
  1.18369899  1.19476024  1.196472    1.19758287  1.19844733  1.1989772
  1.20039724  1.20266429  1.20546688  1.2058215   1.20635137  1.20805381
  1.21220348  1.22019936  1.22457119  1.23413026  1.24108775  1.24142679
  1.24269234  1.24833487  1.2488786   1.26441192  1.27053193  1.28603928
  1.28607289  1.28876745  1.29334038  1.29486657  1.3017068   1.30286719
  1.31316017  1.32284698  1.32748473  1.32917549  1.32945073  1.3317448
  1.33313781  1.33663731  1.34084928  1.34218655  1.35731711  1.36065352
  1.36134217  1.36814184  1.38581696  1.39063253  1.4030574   1.41936953
  1.42072235  1.42999908  1.43283797  1.43463876  1.43867024  1.43949468
  1.44177131  1.44260297  1.44309116  1.4463927   1.46587303  1.46624238
  1.47164301  1.47493234  1.4761314   1.48363943  1.49589961  1.50110658
  1.50140397  1.5089224   1.52840227  1.53017665  1.53317974  1.54796919
  1.54899849  1.56201524  1.58525707  1.58624206  1.5923946   1.59672682
  1.59714186  1.59944092  1.61432441  1.61555752  1.61747605  1.61753321
  1.62104912  1.62138703  1.63280914  1.63653399  1.64253531  1.64573621
  1.64690826  1.64819657  1.64909867  1.6565279   1.65930136  1.66463745
  1.66836279  1.6723553   1.6729212   1.6741875   1.67814179  1.69324485
  1.69623661  1.69736378  1.699247    1.70575345  1.70664812  1.72286228
  1.72376911  1.72520732  1.72829154  1.73507045  1.73741388  1.73796721
  1.73822766  1.74182955  1.74703002  1.74784271  1.77378983  1.77633849
  1.78753609  1.79069314  1.79265472  1.79281466  1.79406417  1.79536997
  1.80127197  1.80429525  1.811233    1.81652709  1.82004712  1.82601848
  1.8264347   1.83225221  1.83651305  1.83789308  1.84115739  1.8463908
  1.84830237  1.84835712  1.8562613   1.85798938  1.85978932  1.86246653
  1.867429    1.86878483  1.86903007  1.87295122  1.87415684  1.88136877
  1.88823432  1.89842874  1.93320395  1.93882854  1.94572381  1.95657593
  1.95817205  1.95961724  1.96119915  1.96460599  1.96942346  1.97502884
  1.97540841  1.98358736  2.00384247  2.00811813  2.01448455  2.01780087
  2.02067174  2.04815232  2.05913689  2.06052231  2.06948573  2.07138179
  2.07606352  2.09209392  2.09317276  2.09461196  2.11996332  2.12455744
  2.12497408  2.12795398  2.12958684  2.13461072  2.16507934  2.16752081
  2.1714295   2.17247824  2.18285608  2.19699764  2.19823217  2.20220137
  2.20681302  2.21285499  2.21877761  2.24401478  2.26498635  2.31981214
  2.32336589  2.3401844   2.34809363  2.34951661  2.35976461  2.36046117
  2.38057314  2.38229661  2.40256448  2.41460443  2.41554351  2.42665233
  2.47824273  2.48166632  2.48190825  2.48754926  2.49336347  2.50582527
  2.5200884   2.54830925  2.55051801  2.56862215  2.57013366  2.58618309
  2.61283138  2.61561674  2.66342064  2.66969001  2.67216661  2.69011524
  2.69118645  2.70048978  2.72078248  2.73524218  2.7435528   2.74465461
  2.75221513  2.76644931  2.7762528   2.77980319  2.78598702  2.79854917
  2.82590995  2.89534725  2.91016682  2.9666467   2.99700317  3.10222901
  3.10523259  3.14595949  3.17324889  3.18397245  3.18748988  3.19513969
  3.20304338  3.20732815  3.23168679  3.25306942  3.25952434  3.29392417
  3.32608543  3.34040379  3.38367298  3.40435848  3.44565203  3.45839617
  3.45928794  3.5735145   3.59413687  3.59679068  3.63364945  3.65600776
  3.69576592  3.7270489   3.78460748  3.8018692   3.94718711  3.96781289
  3.99094944  3.99553551  4.01754959  4.0677185   4.08483264  4.21066178
  4.21748697  4.23075821  4.23176301  4.24318744  4.25268742  4.27926117
  4.32967623  4.35623212  4.40320442  4.40778991  4.40874438  4.44440633
  4.44586746  4.45485663  4.56144208  4.60173366  4.6193721   4.67107756
  4.73492977  4.81148992  4.81439651  4.85103032  4.86168628  4.93515543
  4.97851581  4.98408214  5.02121635  5.2101356   5.24033402  5.27046041
  5.27652126  5.33231286  5.36913085  5.40993316  5.56068012  5.60271221
  5.66545862  5.68709265  5.97301145  6.03080924  6.03442174  6.05867138
  6.07479626  6.17233057  6.18110154  6.41366985  6.5240556   6.69512361
  7.02552453  7.13248657  7.55644495  7.72008991  7.92259984  7.96853647
  8.11753913  8.26243344  8.55503912  8.72289149  8.78450129  8.88994031
  9.14408873  9.1552964   9.19655808  9.61947182  9.79615855  9.91033445
 10.16218191 10.39511856 11.05787558 11.40126321 11.55239073 12.09843946
 12.89310254 13.49026881 13.51546926 13.68191387 14.77408044 14.86458012
 14.86639772 15.72564572 15.98016573 16.54949798 17.76677895 19.72046433
 20.13182818 22.04795744 22.1185273  28.34085293 30.39443383]

  UserWarning,

2022-10-31 11:02:42,556:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.19081510e+01 -9.86240019e+00 -9.10092164e+00 -9.03080899e+00
 -8.32920877e+00 -7.59928361e+00 -7.55170380e+00 -7.40771201e+00
 -6.47002607e+00 -6.38019750e+00 -6.25803071e+00 -6.21580296e+00
 -6.18112689e+00 -5.53632376e+00 -5.43539027e+00 -5.01518760e+00
 -4.86489720e+00 -4.23444555e+00 -4.08795678e+00 -4.00345311e+00
 -3.96624266e+00 -3.80695317e+00 -3.78025118e+00 -3.53296453e+00
 -3.52106941e+00 -3.44604862e+00 -3.41472457e+00 -3.35197864e+00
 -3.17684860e+00 -3.13831459e+00 -3.01436121e+00 -2.71716350e+00
 -2.68096658e+00 -2.56577628e+00 -2.54725715e+00 -2.52928066e+00
 -2.51622420e+00 -2.47805052e+00 -2.34102063e+00 -2.30454267e+00
 -2.14023741e+00 -2.12212304e+00 -2.10603868e+00 -2.05075847e+00
 -1.87125696e+00 -1.79976876e+00 -1.76571928e+00 -1.71035927e+00
 -1.65414713e+00 -1.60640760e+00 -1.58518607e+00 -1.43042939e+00
 -1.42652643e+00 -1.39762096e+00 -1.32242471e+00 -1.26661570e+00
 -1.25958659e+00 -1.22540604e+00 -1.14215880e+00 -1.11687222e+00
 -1.11421772e+00 -1.09279392e+00 -1.01986344e+00 -9.96920613e-01
 -9.10493213e-01 -8.66039733e-01 -8.29886209e-01 -8.02664962e-01
 -7.39569011e-01 -7.26784511e-01 -7.12413674e-01 -6.89550286e-01
 -6.79334417e-01 -6.76055391e-01 -6.74198676e-01 -6.66133153e-01
 -6.16735635e-01 -5.80770251e-01 -5.62459420e-01 -5.61052570e-01
 -5.22519267e-01 -5.21363660e-01 -4.81457748e-01 -4.69633587e-01
 -4.57321343e-01 -4.50950853e-01 -4.42860830e-01 -4.14989489e-01
 -3.80348447e-01 -3.53411975e-01 -3.41862604e-01 -3.41592108e-01
 -3.38864238e-01 -3.32151452e-01 -3.23293002e-01 -3.21689521e-01
 -2.99784622e-01 -2.95709694e-01 -2.87908070e-01 -2.77679171e-01
 -2.76269972e-01 -2.70022948e-01 -2.59615265e-01 -2.46528513e-01
 -2.42449537e-01 -2.38792885e-01 -2.27820861e-01 -2.20951358e-01
 -2.16047593e-01 -2.08857010e-01 -2.05847616e-01 -1.70126787e-01
 -1.46188675e-01 -1.31353057e-01 -1.06276329e-01 -8.43323179e-02
 -8.29804262e-02 -7.76122229e-02 -5.43115030e-02 -3.74812512e-02
 -2.67509462e-02 -2.55305041e-02 -1.58058458e-02 -1.22580162e-02
 -1.07049037e-02  4.20068769e-03  7.08482198e-03  9.13084996e-03
  9.96167720e-03  1.20970553e-02  1.81186249e-02  1.99729058e-02
  2.07281403e-02  2.07322940e-02  2.93806897e-02  3.05337095e-02
  3.52164707e-02  4.12875950e-02  5.52049339e-02  6.47040812e-02
  6.69971211e-02  7.17117738e-02  7.67333377e-02  8.14494337e-02
  8.46449388e-02  8.50527529e-02  9.42680441e-02  9.65011678e-02
  1.01012857e-01  1.03781176e-01  1.06630262e-01  1.08776618e-01
  1.19392528e-01  1.24914159e-01  1.26843052e-01  1.29047078e-01
  1.40157675e-01  1.42092195e-01  1.42256521e-01  1.52380359e-01
  1.55316281e-01  1.55711015e-01  1.60078020e-01  1.62391637e-01
  1.65037740e-01  1.66569114e-01  1.74592354e-01  1.74886185e-01
  1.77237687e-01  1.85885263e-01  2.02267423e-01  2.07829743e-01
  2.18457333e-01  2.21374232e-01  2.33848708e-01  2.40324462e-01
  2.41684318e-01  2.54009959e-01  2.57472786e-01  2.58548481e-01
  2.60002319e-01  2.60379548e-01  2.62017259e-01  2.74317384e-01
  2.80144988e-01  2.82341641e-01  2.84612991e-01  2.97178604e-01
  3.00012485e-01  3.09489711e-01  3.11848906e-01  3.15922614e-01
  3.21081193e-01  3.34737203e-01  3.39157277e-01  3.45824118e-01
  3.48970980e-01  3.53894045e-01  3.58809556e-01  3.58865808e-01
  3.65040056e-01  3.65492871e-01  3.66140588e-01  3.74540609e-01
  3.75410389e-01  3.85916482e-01  3.96929206e-01  3.99893129e-01
  4.01178256e-01  4.03965419e-01  4.07517455e-01  4.13419073e-01
  4.17967088e-01  4.21231688e-01  4.25449841e-01  4.26474764e-01
  4.29918269e-01  4.30895182e-01  4.32260685e-01  4.32987739e-01
  4.35259958e-01  4.44099210e-01  4.44906819e-01  4.47583214e-01
  4.48330341e-01  4.50438563e-01  4.54986782e-01  4.57435689e-01
  4.58818554e-01  4.60075632e-01  4.61116516e-01  4.63002877e-01
  4.66386456e-01  4.67375124e-01  4.74057615e-01  4.74780657e-01
  4.81982661e-01  4.82807191e-01  4.86459195e-01  4.86490559e-01
  4.90839348e-01  4.92356209e-01  4.93337594e-01  4.95745646e-01
  4.97019481e-01  4.98010539e-01  5.17155916e-01  5.22291296e-01
  5.25461715e-01  5.28405575e-01  5.29881857e-01  5.30110642e-01
  5.34587756e-01  5.39946455e-01  5.40825005e-01  5.41054221e-01
  5.43705018e-01  5.45812340e-01  5.52320675e-01  5.53241210e-01
  5.53332679e-01  5.59285031e-01  5.68515044e-01  5.68917542e-01
  5.71963406e-01  5.72363404e-01  5.79973757e-01  5.82632241e-01
  5.89009524e-01  5.89923515e-01  5.92009301e-01  5.97918866e-01
  5.98639158e-01  6.01067470e-01  6.03085747e-01  6.11930059e-01
  6.13606337e-01  6.18442553e-01  6.19879211e-01  6.24852562e-01
  6.25236811e-01  6.26439353e-01  6.31446469e-01  6.34411518e-01
  6.35840994e-01  6.36468872e-01  6.36649725e-01  6.37270749e-01
  6.37633802e-01  6.40751727e-01  6.41725163e-01  6.42706440e-01
  6.46550929e-01  6.47438804e-01  6.48039261e-01  6.48332216e-01
  6.48365291e-01  6.49235812e-01  6.54155950e-01  6.54798841e-01
  6.55695676e-01  6.59814854e-01  6.61708181e-01  6.62345482e-01
  6.62881168e-01  6.63080832e-01  6.64084841e-01  6.67695839e-01
  6.70137220e-01  6.70192192e-01  6.71261298e-01  6.72457849e-01
  6.74263456e-01  6.75219378e-01  6.76925852e-01  6.81540858e-01
  6.83174326e-01  6.83427446e-01  6.86155865e-01  6.87848638e-01
  6.90388751e-01  6.90770871e-01  6.98518802e-01  7.00000885e-01
  7.00147888e-01  7.00358571e-01  7.01283548e-01  7.01567664e-01
  7.02497238e-01  7.03464268e-01  7.03481243e-01  7.04615891e-01
  7.09230898e-01  7.10325924e-01  7.11469723e-01  7.18460911e-01
  7.23222921e-01  7.23899240e-01  7.26411518e-01  7.32305930e-01
  7.32418842e-01  7.33181877e-01  7.33375874e-01  7.33714498e-01
  7.35394469e-01  7.35891713e-01  7.36920937e-01  7.38457963e-01
  7.41127882e-01  7.44621295e-01  7.47154959e-01  7.49486550e-01
  7.50765957e-01  7.50912960e-01  7.52281539e-01  7.53305381e-01
  7.54343901e-01  7.57293245e-01  7.57718036e-01  7.59995970e-01
  7.60763043e-01  7.60871916e-01  7.61018920e-01  7.64610976e-01
  7.66148002e-01  7.67946577e-01  7.69225983e-01  7.69891943e-01
  7.73840989e-01  7.74713852e-01  7.76789434e-01  7.77176590e-01
  7.78455996e-01  7.78743369e-01  7.78785477e-01  7.79716014e-01
  7.81721344e-01  7.83218006e-01  7.85549437e-01  7.86167324e-01
  7.86171376e-01  7.89825058e-01  7.90921842e-01  7.91887988e-01
  7.94440064e-01  7.95636616e-01  7.96916022e-01  7.99782860e-01
  8.00143040e-01  8.00251623e-01  8.04866629e-01  8.05177846e-01
  8.05578026e-01  8.06146035e-01  8.07575702e-01  8.09571105e-01
  8.10761042e-01  8.11765051e-01  8.14096642e-01  8.15376048e-01
  8.15523052e-01  8.18324493e-01  8.19991055e-01  8.24294716e-01
  8.24606061e-01  8.27941662e-01  8.28088666e-01  8.28174777e-01
  8.30159492e-01  8.32703672e-01  8.33836075e-01  8.33983078e-01
  8.38598085e-01  8.39340875e-01  8.40113667e-01  8.41933685e-01
  8.43382531e-01  8.47018542e-01  8.47828098e-01  8.49006803e-01
  8.49123137e-01  8.51016695e-01  8.53488738e-01  8.55691639e-01
  8.57915117e-01  8.60246708e-01  8.60800941e-01  8.61673118e-01
  8.64861714e-01  8.66003004e-01  8.66288124e-01  8.67145130e-01
  8.67810159e-01  8.68760985e-01  8.71760136e-01  8.75279035e-01
  8.76375143e-01  8.77477151e-01  8.77657186e-01  8.77849568e-01
  8.78706734e-01  8.80990149e-01  8.81655179e-01  8.83321740e-01
  8.83792831e-01  8.85605156e-01  8.87936747e-01  8.90518689e-01
  8.97166760e-01  8.97313764e-01  8.98397577e-01  9.01781767e-01
  9.06543777e-01  9.09275240e-01  9.13490214e-01  9.15773790e-01
  9.18037884e-01  9.20241793e-01  9.20388797e-01  9.20830640e-01
  9.21668203e-01  9.27140215e-01  9.27483486e-01  9.29471806e-01
  9.30898216e-01  9.34086813e-01  9.43463830e-01  9.46741895e-01
  9.48467079e-01  9.50215248e-01  9.52693843e-01  9.53640317e-01
  9.60919686e-01  9.64041818e-01  9.67395522e-01  9.75768875e-01
  9.80383882e-01  9.83506199e-01  9.89510422e-01  9.89613895e-01
  9.95928546e-01  9.98843908e-01  1.00168914e+00  1.00257673e+00
  1.00413235e+00  1.00649595e+00  1.01142404e+00  1.02191894e+00
  1.02591720e+00  1.03180737e+00  1.03552617e+00  1.03576396e+00
  1.04350128e+00  1.04413681e+00  1.04932408e+00  1.05341409e+00
  1.05508065e+00  1.05734630e+00  1.06385827e+00  1.06951420e+00
  1.09768481e+00  1.09885300e+00  1.09888136e+00  1.10669240e+00
  1.10746749e+00  1.11272638e+00  1.11734139e+00  1.12170795e+00
  1.12604546e+00  1.13118641e+00  1.13272343e+00  1.13467464e+00
  1.13580142e+00  1.14084579e+00  1.14257608e+00  1.14964644e+00
  1.15242136e+00  1.15382798e+00  1.15426144e+00  1.15622462e+00
  1.15816110e+00  1.16015911e+00  1.16349145e+00  1.16750339e+00
  1.17694646e+00  1.17733647e+00  1.18020331e+00  1.18030139e+00
  1.18656649e+00  1.19245738e+00  1.19904391e+00  1.20502651e+00
  1.20964152e+00  1.21087560e+00  1.22611198e+00  1.23106891e+00
  1.23194777e+00  1.23324350e+00  1.23733156e+00  1.24194657e+00
  1.26055360e+00  1.26516860e+00  1.26963661e+00  1.27439862e+00
  1.27720006e+00  1.28092605e+00  1.29525106e+00  1.29586972e+00
  1.29747365e+00  1.29833065e+00  1.30060259e+00  1.31131867e+00
  1.31826511e+00  1.32288011e+00  1.35980016e+00  1.36106208e+00
  1.36224120e+00  1.49610203e+00  1.53870609e+00  1.56483630e+00
  1.57860723e+00  1.59451164e+00  1.67639928e+00  1.69828384e+00
  1.98062504e+00  2.06888755e+00  2.11369817e+00  2.14640270e+00
  2.21617390e+00  2.70993427e+00  2.90440822e+00  2.98095627e+00
  3.15043535e+00  3.16709144e+00  3.20088076e+00]

  UserWarning,

2022-10-31 11:02:42,581:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.33896520e+01 -6.29108353e+00 -4.70986901e+00 -4.38955757e+00
 -4.35595557e+00 -3.37695929e+00 -3.23553939e+00 -2.82894506e+00
 -2.36178487e+00 -2.32738275e+00 -2.32651319e+00 -2.23566609e+00
 -2.09435073e+00 -2.08198879e+00 -2.03907132e+00 -1.97639984e+00
 -1.96101543e+00 -1.79040183e+00 -1.77889420e+00 -1.76369946e+00
 -1.75435103e+00 -1.72514257e+00 -1.72500828e+00 -1.59935909e+00
 -1.59079614e+00 -1.54018931e+00 -1.52813091e+00 -1.51023832e+00
 -1.49192383e+00 -1.47591587e+00 -1.45089329e+00 -1.44918212e+00
 -1.32588996e+00 -1.25685901e+00 -1.25262908e+00 -1.24129234e+00
 -1.21253951e+00 -1.20543309e+00 -1.20157778e+00 -1.17913488e+00
 -1.17522917e+00 -1.17471724e+00 -1.16899151e+00 -1.13824451e+00
 -1.12842956e+00 -1.11426843e+00 -1.10404316e+00 -1.09099790e+00
 -1.06608220e+00 -1.00977055e+00 -1.00669406e+00 -9.91324206e-01
 -9.88665453e-01 -9.76209125e-01 -9.61856256e-01 -9.60577388e-01
 -9.34687641e-01 -9.09746128e-01 -8.96653852e-01 -8.94520914e-01
 -8.93905301e-01 -8.81632469e-01 -8.69721211e-01 -8.62545422e-01
 -8.52799370e-01 -8.51269762e-01 -8.49774376e-01 -8.40089935e-01
 -8.36261795e-01 -8.32001918e-01 -8.30122460e-01 -8.20461292e-01
 -8.15537705e-01 -7.76511109e-01 -7.66954381e-01 -7.63562717e-01
 -7.62073770e-01 -7.59615307e-01 -7.58655031e-01 -7.54806818e-01
 -7.47896694e-01 -7.45153065e-01 -7.28159233e-01 -7.26847323e-01
 -7.17802222e-01 -7.05339150e-01 -7.04404132e-01 -6.99507316e-01
 -6.95688442e-01 -6.92232551e-01 -6.66858752e-01 -6.61312386e-01
 -6.49848346e-01 -6.45955596e-01 -6.33332331e-01 -6.26410562e-01
 -6.26179344e-01 -6.24846521e-01 -6.17165967e-01 -6.15568912e-01
 -6.14170279e-01 -6.13503490e-01 -6.03072825e-01 -5.97822548e-01
 -5.85962373e-01 -5.83256044e-01 -5.80865446e-01 -5.74320016e-01
 -5.74244355e-01 -5.72237923e-01 -5.51752062e-01 -5.46640754e-01
 -5.39791828e-01 -5.36507951e-01 -5.31009890e-01 -5.26974817e-01
 -5.23210799e-01 -5.21614847e-01 -5.14576149e-01 -5.07619160e-01
 -4.97669861e-01 -4.86383978e-01 -4.73546972e-01 -4.62926298e-01
 -4.62331456e-01 -4.49398626e-01 -4.46503940e-01 -4.43751396e-01
 -4.36965824e-01 -4.36029789e-01 -4.19230818e-01 -4.15612266e-01
 -4.10559656e-01 -4.07719243e-01 -4.06365500e-01 -4.01599607e-01
 -3.98409387e-01 -3.95581599e-01 -3.94058456e-01 -3.91275133e-01
 -3.83982054e-01 -3.83721424e-01 -3.82611672e-01 -3.79344891e-01
 -3.75039541e-01 -3.67271684e-01 -3.56408873e-01 -3.56291956e-01
 -3.56135739e-01 -3.54818820e-01 -3.54560551e-01 -3.51839610e-01
 -3.50211792e-01 -3.43273234e-01 -3.31089431e-01 -3.30821185e-01
 -3.26188454e-01 -3.24710853e-01 -3.24600408e-01 -3.23770641e-01
 -3.05422747e-01 -2.89356290e-01 -2.71217245e-01 -2.70397130e-01
 -2.63950086e-01 -2.59841243e-01 -2.58369345e-01 -2.55481291e-01
 -2.50335913e-01 -2.38332876e-01 -2.30451796e-01 -2.28232773e-01
 -2.13246782e-01 -2.12922433e-01 -2.05474979e-01 -2.04563707e-01
 -2.02746558e-01 -2.00166284e-01 -1.97536553e-01 -1.96089157e-01
 -1.93340650e-01 -1.85914228e-01 -1.84936236e-01 -1.83960066e-01
 -1.81480475e-01 -1.79776876e-01 -1.78920238e-01 -1.70204010e-01
 -1.65030105e-01 -1.60260656e-01 -1.58270677e-01 -1.56686478e-01
 -1.53436865e-01 -1.52892461e-01 -1.46293126e-01 -1.45287509e-01
 -1.42576810e-01 -1.35476218e-01 -1.26668187e-01 -1.25208370e-01
 -1.20863815e-01 -1.12444271e-01 -1.07653504e-01 -9.32227824e-02
 -8.31575550e-02 -7.46379526e-02 -6.99673873e-02 -6.92014447e-02
 -6.63737013e-02 -6.49781257e-02 -6.22911475e-02 -6.01321475e-02
 -4.79521026e-02 -4.68466415e-02 -2.26177638e-02 -2.11586652e-02
 -2.08880052e-02 -1.96877161e-02 -9.77951673e-03 -5.48044007e-03
 -3.99096692e-03 -6.60748867e-04  3.31956138e-03  8.40323714e-03
  1.06844345e-02  1.20099250e-02  1.68833355e-02  2.02653353e-02
  2.24640724e-02  3.12867477e-02  4.08987021e-02  4.25372357e-02
  4.37430817e-02  4.64730847e-02  5.40476731e-02  5.93323444e-02
  6.67038218e-02  7.02148956e-02  7.27425662e-02  7.47224591e-02
  7.52453823e-02  8.58685535e-02  8.96279969e-02  9.59391484e-02
  9.73703754e-02  1.04170304e-01  1.04538779e-01  1.05197605e-01
  1.05444866e-01  1.10324618e-01  1.10663085e-01  1.10998424e-01
  1.13595128e-01  1.13840656e-01  1.19615398e-01  1.26832037e-01
  1.27314820e-01  1.28275096e-01  1.30510760e-01  1.34866341e-01
  1.37800706e-01  1.42105043e-01  1.45206189e-01  1.47177389e-01
  1.49976288e-01  1.57799817e-01  1.65879868e-01  1.75825521e-01
  1.82553233e-01  1.85654378e-01  1.86034443e-01  1.90315440e-01
  1.92788290e-01  1.94737036e-01  1.95116110e-01  2.08951187e-01
  2.11141247e-01  2.12101523e-01  2.23109428e-01  2.28290290e-01
  2.31683305e-01  2.33236480e-01  2.36434826e-01  2.39682604e-01
  2.41309418e-01  2.41433952e-01  2.46180954e-01  2.47812681e-01
  2.48332864e-01  2.49619665e-01  2.53127843e-01  2.60970021e-01
  2.61392449e-01  2.65153211e-01  2.65480584e-01  2.72269911e-01
  2.75633415e-01  2.77626166e-01  2.85190378e-01  2.87952098e-01
  2.91629411e-01  2.92037627e-01  2.92997902e-01  2.96114668e-01
  2.96463948e-01  3.02291637e-01  3.06411252e-01  3.08148152e-01
  3.12579684e-01  3.19073996e-01  3.20348172e-01  3.21817042e-01
  3.25467997e-01  3.26905079e-01  3.28583806e-01  3.29090702e-01
  3.30441316e-01  3.31592868e-01  3.32208348e-01  3.32356371e-01
  3.36232986e-01  3.43440095e-01  3.48683354e-01  3.55877034e-01
  3.62936881e-01  3.64050068e-01  3.65212399e-01  3.68007969e-01
  3.70964234e-01  3.72649046e-01  3.74300094e-01  3.79180823e-01
  3.80526086e-01  3.80617022e-01  3.80917043e-01  3.88354920e-01
  3.90350249e-01  3.96977985e-01  4.05660589e-01  4.13382196e-01
  4.19601368e-01  4.23532431e-01  4.25242371e-01  4.26249291e-01
  4.31546145e-01  4.32816069e-01  4.36774480e-01  4.46495391e-01
  4.48249649e-01  4.54267281e-01  4.54529087e-01  4.59570585e-01
  4.77713100e-01  4.77874364e-01  4.79963796e-01  4.87376650e-01
  4.88697838e-01  4.94278575e-01  4.95238851e-01  4.98030703e-01
  5.01378712e-01  5.06138751e-01  5.10139240e-01  5.10225050e-01
  5.13577227e-01  5.15144983e-01  5.18322554e-01  5.25534209e-01
  5.26405047e-01  5.29641876e-01  5.32375782e-01  5.35525777e-01
  5.35687041e-01  5.38874065e-01  5.46586940e-01  5.49429515e-01
  5.53555502e-01  5.55268823e-01  5.59628514e-01  5.62842521e-01
  5.66339202e-01  5.69510813e-01  5.74359791e-01  5.78462328e-01
  5.84095656e-01  5.87035130e-01  5.87189727e-01  5.92552132e-01
  5.93638197e-01  5.95717013e-01  5.97941691e-01  5.98117837e-01
  5.99218933e-01  6.10042408e-01  6.12693097e-01  6.15404022e-01
  6.15623145e-01  6.16583420e-01  6.27368215e-01  6.27483320e-01
  6.35625788e-01  6.36165202e-01  6.40337766e-01  6.45995507e-01
  6.49148218e-01  6.50490597e-01  6.56071334e-01  6.63174151e-01
  6.64605308e-01  6.67022553e-01  6.76613392e-01  6.78166209e-01
  6.80115313e-01  6.80147262e-01  6.83687073e-01  6.86687724e-01
  6.91141308e-01  6.96519524e-01  6.97479800e-01  7.10563022e-01
  7.14599392e-01  7.17061582e-01  7.17195599e-01  7.31386977e-01
  7.43247152e-01  7.48827889e-01  7.63065839e-01  7.65278260e-01
  7.67320432e-01  7.68773900e-01  7.77415904e-01  7.79123247e-01
  7.80554791e-01  7.87860156e-01  7.97957961e-01  7.98282311e-01
  7.98940589e-01  8.12283356e-01  8.15894321e-01  8.22672583e-01
  8.23420161e-01  8.23884960e-01  8.29724269e-01  8.35947839e-01
  8.48164519e-01  8.55382235e-01  8.57541956e-01  8.59272559e-01
  8.68701509e-01  8.70172458e-01  8.71410623e-01  8.85346035e-01
  8.92065724e-01  8.95610906e-01  8.99246036e-01  9.00592309e-01
  9.09869433e-01  9.11950641e-01  9.14046143e-01  9.20956873e-01
  9.21490282e-01  9.39903541e-01  9.58281648e-01  9.67397813e-01
  9.75036391e-01  9.77078846e-01  9.78763509e-01  9.80617128e-01
  9.89647796e-01  9.93124464e-01  1.00016907e+00  1.00019891e+00
  1.00868500e+00  1.01001499e+00  1.01091249e+00  1.01673597e+00
  1.02106532e+00  1.02997705e+00  1.04064710e+00  1.05628054e+00
  1.06885570e+00  1.07224778e+00  1.09180887e+00  1.10100142e+00
  1.10196170e+00  1.10669244e+00  1.11711116e+00  1.11718092e+00
  1.12017719e+00  1.12154348e+00  1.12596263e+00  1.12984560e+00
  1.14224862e+00  1.14772905e+00  1.15240083e+00  1.15330979e+00
  1.17289263e+00  1.18189780e+00  1.18322779e+00  1.19261138e+00
  1.19375798e+00  1.19963881e+00  1.20594178e+00  1.21243080e+00
  1.21720292e+00  1.21792805e+00  1.22699384e+00  1.22715448e+00
  1.23420617e+00  1.24638997e+00  1.25161681e+00  1.25264135e+00
  1.26279418e+00  1.28366059e+00  1.31463425e+00  1.32035012e+00
  1.33567708e+00  1.35001126e+00  1.35362050e+00  1.35424350e+00
  1.38950036e+00  1.39599893e+00  1.41335815e+00  1.48906772e+00
  1.49508927e+00  1.51643470e+00  1.56304328e+00  1.56762199e+00
  1.60783509e+00  1.66173660e+00  1.68224664e+00  1.68243990e+00
  1.73386117e+00  1.84143515e+00  1.89202375e+00  1.92942634e+00
  1.96959308e+00  2.21055856e+00  2.22541611e+00  2.32408908e+00
  2.34978665e+00  2.43958427e+00  2.44747470e+00  2.54839802e+00
  2.58951154e+00  2.64071910e+00  2.86644175e+00  2.95884930e+00
  3.06734365e+00  3.14293538e+00  3.14343625e+00  3.29780441e+00
  3.71164843e+00  3.82217621e+00  4.32376511e+00  4.40037981e+00
  4.59839529e+00  4.66611149e+00  4.93068069e+00  5.08084113e+00
  5.93213579e+00  6.01400739e+00  6.77981708e+00  7.60407772e+00
  7.85597636e+00  8.44963926e+00  9.04563811e+00  1.11443471e+01
  1.25249005e+01  1.56036453e+01  1.79162084e+01  1.82428379e+01]

  UserWarning,

2022-10-31 11:02:42,612:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.62027726e+01 -1.31138485e+01 -1.17849096e+01 -9.41506453e+00
 -8.36597121e+00 -8.22967679e+00 -8.01895272e+00 -5.24684407e+00
 -5.10490587e+00 -4.73260542e+00 -4.15305637e+00 -4.07644911e+00
 -3.96376620e+00 -3.68165601e+00 -3.56018290e+00 -2.97744717e+00
 -2.76268112e+00 -2.71044274e+00 -2.66491051e+00 -2.44763488e+00
 -2.40206114e+00 -2.31655393e+00 -2.30619807e+00 -2.13107386e+00
 -2.09169457e+00 -2.07795882e+00 -2.04304470e+00 -1.96792499e+00
 -1.95593630e+00 -1.91744124e+00 -1.84250786e+00 -1.81768357e+00
 -1.77211896e+00 -1.75392787e+00 -1.74172512e+00 -1.73898516e+00
 -1.68241379e+00 -1.65572460e+00 -1.63027230e+00 -1.53587351e+00
 -1.51953453e+00 -1.50655823e+00 -1.50123993e+00 -1.47372746e+00
 -1.46000067e+00 -1.45882027e+00 -1.44579650e+00 -1.42509780e+00
 -1.40413988e+00 -1.36966617e+00 -1.31462409e+00 -1.28442147e+00
 -1.28144636e+00 -1.25440430e+00 -1.25073970e+00 -1.24670585e+00
 -1.22903766e+00 -1.21180894e+00 -1.20072310e+00 -1.18166050e+00
 -1.17031992e+00 -1.16030096e+00 -1.15324339e+00 -1.12930566e+00
 -1.12893032e+00 -1.12474058e+00 -1.11693001e+00 -1.10592314e+00
 -1.10102482e+00 -1.07538566e+00 -1.07158773e+00 -1.06574498e+00
 -1.05478898e+00 -1.05284564e+00 -1.04710458e+00 -1.01577775e+00
 -1.01263700e+00 -1.01223047e+00 -1.00896614e+00 -1.00841588e+00
 -1.00144399e+00 -9.78406256e-01 -9.60619092e-01 -9.52164566e-01
 -9.51722184e-01 -9.44434107e-01 -9.18731577e-01 -9.10134642e-01
 -9.04774037e-01 -9.03804962e-01 -8.93036790e-01 -8.85589985e-01
 -8.72072450e-01 -8.47712420e-01 -8.46821537e-01 -8.41131940e-01
 -8.40617340e-01 -8.27670157e-01 -8.22041386e-01 -7.97306280e-01
 -7.94532540e-01 -7.91564423e-01 -7.91483283e-01 -7.86613708e-01
 -7.75999514e-01 -7.70610039e-01 -7.63788467e-01 -7.59075230e-01
 -7.57459579e-01 -7.56382443e-01 -7.47844324e-01 -7.32475812e-01
 -7.31947989e-01 -7.29101514e-01 -6.92787819e-01 -6.83231827e-01
 -6.78644556e-01 -6.62648394e-01 -6.47985870e-01 -6.45887051e-01
 -6.44765405e-01 -6.35345799e-01 -6.34796081e-01 -6.27193453e-01
 -6.24926533e-01 -6.21084775e-01 -6.17143853e-01 -6.16395552e-01
 -6.08683999e-01 -5.99650714e-01 -5.97902949e-01 -5.83098640e-01
 -5.83042071e-01 -5.80588113e-01 -5.69382430e-01 -5.62065159e-01
 -5.53414377e-01 -5.36248401e-01 -5.29428714e-01 -5.28233280e-01
 -5.18252718e-01 -5.13280388e-01 -5.04024068e-01 -4.93001336e-01
 -4.83204218e-01 -4.76864551e-01 -4.74728694e-01 -4.69986626e-01
 -4.68128773e-01 -4.62017125e-01 -4.52903761e-01 -4.36455569e-01
 -4.30673418e-01 -4.30529068e-01 -4.16009597e-01 -4.07343496e-01
 -3.92436023e-01 -3.77078397e-01 -3.68469028e-01 -3.67262600e-01
 -3.61444879e-01 -3.61107159e-01 -3.49406427e-01 -3.41801180e-01
 -3.39696420e-01 -3.30984520e-01 -3.30592924e-01 -3.18268837e-01
 -3.13472101e-01 -3.05912189e-01 -3.03350100e-01 -3.02536212e-01
 -2.84684586e-01 -2.76660784e-01 -2.75996353e-01 -2.73479468e-01
 -2.72130716e-01 -2.68634484e-01 -2.65823082e-01 -2.65793976e-01
 -2.59630591e-01 -2.58177126e-01 -2.38508177e-01 -2.32078412e-01
 -2.29447010e-01 -2.23032303e-01 -2.22212375e-01 -2.11490705e-01
 -1.99616800e-01 -1.99491732e-01 -1.82280306e-01 -1.82055158e-01
 -1.72161002e-01 -1.69595872e-01 -1.66056033e-01 -1.57778892e-01
 -1.51469966e-01 -1.51138977e-01 -1.49866193e-01 -1.49843002e-01
 -1.38164672e-01 -1.36043968e-01 -1.34245043e-01 -1.29925473e-01
 -1.25839178e-01 -1.24945128e-01 -1.18224741e-01 -1.12700636e-01
 -1.11395415e-01 -1.01115796e-01 -9.66266921e-02 -9.06175992e-02
 -8.84895818e-02 -8.36891354e-02 -8.19496969e-02 -8.08486266e-02
 -7.72462829e-02 -6.77799105e-02 -6.15760102e-02 -5.39351477e-02
 -5.20017381e-02 -4.48146671e-02 -4.35712938e-02 -4.27543730e-02
 -3.99913521e-02 -3.82460883e-02 -3.74527981e-02 -2.59501371e-02
 -2.10357600e-02 -1.74349193e-02 -1.48910102e-02 -1.45463828e-02
 -4.50066148e-03 -4.46177964e-03  7.57239241e-04  1.42167019e-03
  4.19219134e-03  5.72987329e-03  1.92408978e-02  2.05830524e-02
  2.15400618e-02  2.20751649e-02  2.90148776e-02  3.34715294e-02
  4.97897491e-02  5.50198764e-02  5.54266410e-02  5.80323752e-02
  7.91696649e-02  7.96909645e-02  8.44336567e-02  8.84733135e-02
  8.97691850e-02  9.31079408e-02  9.38943447e-02  1.01240547e-01
  1.11713572e-01  1.16790393e-01  1.19075441e-01  1.27071296e-01
  1.34049550e-01  1.35465818e-01  1.40130682e-01  1.48959584e-01
  1.54335027e-01  1.59514507e-01  1.70398966e-01  1.73133780e-01
  1.73656802e-01  1.76472864e-01  1.78491020e-01  1.83648878e-01
  1.84603311e-01  1.89727047e-01  1.89939638e-01  1.93728888e-01
  2.04285028e-01  2.06799873e-01  2.08018588e-01  2.24393569e-01
  2.32925809e-01  2.43462166e-01  2.48177316e-01  2.49334963e-01
  2.51665957e-01  2.57784453e-01  2.62871641e-01  2.69806409e-01
  2.70348927e-01  2.71652623e-01  2.76715505e-01  2.78839694e-01
  2.86086692e-01  2.88008139e-01  2.88313035e-01  2.89081207e-01
  2.98224095e-01  3.00492502e-01  3.04966496e-01  3.05348999e-01
  3.12345241e-01  3.13449659e-01  3.17183206e-01  3.17888960e-01
  3.24364748e-01  3.25076031e-01  3.26941066e-01  3.32757180e-01
  3.37139089e-01  3.40496419e-01  3.42895258e-01  3.44138632e-01
  3.55245703e-01  3.63645906e-01  3.63952854e-01  3.70519078e-01
  3.74085944e-01  3.75313175e-01  3.76192772e-01  3.77432586e-01
  3.77880947e-01  3.78244261e-01  3.82265749e-01  3.87805232e-01
  3.89230377e-01  3.90741557e-01  3.99488914e-01  4.04326869e-01
  4.05408710e-01  4.17548705e-01  4.18610173e-01  4.19498661e-01
  4.24458360e-01  4.24910574e-01  4.25787903e-01  4.26625290e-01
  4.35298724e-01  4.35466714e-01  4.36075683e-01  4.36611306e-01
  4.42382657e-01  4.44326937e-01  4.45892275e-01  4.46767939e-01
  4.50107723e-01  4.50335033e-01  4.52686316e-01  4.59743770e-01
  4.63785043e-01  4.65028416e-01  4.67289922e-01  4.67488701e-01
  4.69372083e-01  4.71146912e-01  4.82235712e-01  4.82847643e-01
  4.85589858e-01  4.90749253e-01  5.02089607e-01  5.04108499e-01
  5.10021380e-01  5.11264753e-01  5.13985234e-01  5.17383249e-01
  5.17946452e-01  5.18999128e-01  5.19410490e-01  5.24270563e-01
  5.25308321e-01  5.36835796e-01  5.37774059e-01  5.49665996e-01
  5.56257717e-01  5.56707800e-01  5.62826296e-01  5.63619586e-01
  5.66632085e-01  5.70540417e-01  5.71184504e-01  5.83780132e-01
  5.95902333e-01  5.98492087e-01  6.00810999e-01  6.09855923e-01
  6.16568952e-01  6.21556655e-01  6.23940842e-01  6.27258778e-01
  6.34026100e-01  6.38911801e-01  6.48730392e-01  6.49973765e-01
  6.53773315e-01  6.56253864e-01  6.66870843e-01  6.74842419e-01
  6.90564087e-01  6.94966729e-01  6.96210102e-01  6.97029430e-01
  6.97660694e-01  7.02328598e-01  7.12503106e-01  7.24309447e-01
  7.33401515e-01  7.35935513e-01  7.41203066e-01  7.42446439e-01
  7.42532510e-01  7.49972527e-01  7.85756348e-01  7.88682776e-01
  7.94801272e-01  7.95605118e-01  7.97641781e-01  8.05093307e-01
  8.13594621e-01  8.14217635e-01  8.18106160e-01  8.18971227e-01
  8.36127288e-01  8.36463119e-01  8.40244319e-01  8.41037609e-01
  8.48412695e-01  8.51516837e-01  8.52738341e-01  8.59278321e-01
  8.63405229e-01  8.63944025e-01  8.71317237e-01  8.72110527e-01
  8.81155451e-01  8.88871082e-01  8.99491527e-01  9.15045679e-01
  9.26419186e-01  9.34314129e-01  9.34455990e-01  9.39119009e-01
  9.45211016e-01  9.56673634e-01  9.69794855e-01  9.79746621e-01
  9.82926005e-01  9.89651332e-01  9.92977975e-01  9.95546516e-01
  9.98976342e-01  1.00872693e+00  1.01986446e+00  1.02073538e+00
  1.02598296e+00  1.02758009e+00  1.03314204e+00  1.03494196e+00
  1.04018730e+00  1.04440632e+00  1.05885405e+00  1.06610080e+00
  1.06670274e+00  1.07440394e+00  1.07786240e+00  1.09136673e+00
  1.09344266e+00  1.09594227e+00  1.10091589e+00  1.10206048e+00
  1.11015861e+00  1.11326616e+00  1.11716066e+00  1.13025515e+00
  1.15174933e+00  1.15733010e+00  1.16655449e+00  1.18956672e+00
  1.19202278e+00  1.20480981e+00  1.21050655e+00  1.21451982e+00
  1.22903209e+00  1.24980278e+00  1.25716464e+00  1.26270303e+00
  1.28263320e+00  1.28314959e+00  1.28614496e+00  1.28846512e+00
  1.29553141e+00  1.29603911e+00  1.30084257e+00  1.31618605e+00
  1.31802486e+00  1.32297871e+00  1.33604835e+00  1.33843540e+00
  1.34046253e+00  1.35123445e+00  1.35669486e+00  1.36583999e+00
  1.36600959e+00  1.36690390e+00  1.37651156e+00  1.38196373e+00
  1.38771850e+00  1.38851179e+00  1.38975516e+00  1.39009734e+00
  1.40463927e+00  1.41323830e+00  1.42079453e+00  1.42265189e+00
  1.42658247e+00  1.43221584e+00  1.44131670e+00  1.44811904e+00
  1.45106900e+00  1.51295199e+00  1.51645929e+00  1.52813836e+00
  1.55445068e+00  1.56647518e+00  1.57054235e+00  1.57800367e+00
  1.64831671e+00  1.71873473e+00  1.74662747e+00  1.77258897e+00
  1.77612550e+00  1.78257968e+00  1.81172936e+00  1.85324244e+00
  1.86107754e+00  1.87609800e+00  1.89500610e+00  1.89835487e+00
  1.94223869e+00  1.96927990e+00  1.96967256e+00  2.02172671e+00
  2.02581580e+00  2.06531028e+00  2.08519653e+00  2.08659479e+00
  2.13791610e+00  2.22138823e+00  2.22280978e+00  2.27089762e+00
  2.29279608e+00  2.29808508e+00  2.33274877e+00  2.47294023e+00
  2.49062684e+00  2.50270739e+00  2.63168206e+00  2.67754878e+00
  2.72686089e+00  2.74133313e+00  2.86057184e+00  2.86529582e+00
  2.91446542e+00  3.22435263e+00  3.23377034e+00  3.24812528e+00
  3.39466133e+00  3.66796953e+00  3.76075791e+00  4.13121036e+00
  4.15807269e+00  5.05814722e+00  5.07719690e+00  5.29836491e+00
  5.82284093e+00  5.99491742e+00  6.54866583e+00  8.88813894e+00
  9.75889156e+00]

  UserWarning,

2022-10-31 11:02:42,636:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.02949591e+00 -4.02151209e+00 -3.47315515e+00 -3.02907006e+00
 -2.93371210e+00 -2.92861315e+00 -2.92150684e+00 -2.91809476e+00
 -2.88317942e+00 -2.84567135e+00 -2.80641876e+00 -2.73142095e+00
 -2.69597263e+00 -2.66399057e+00 -2.57300573e+00 -2.54304456e+00
 -2.49209548e+00 -2.49057396e+00 -2.47042249e+00 -2.46169942e+00
 -2.44949416e+00 -2.42466509e+00 -2.41594203e+00 -2.41116674e+00
 -2.33556173e+00 -2.28056354e+00 -2.26915573e+00 -2.22237395e+00
 -2.17849803e+00 -2.14356122e+00 -2.14144522e+00 -2.12454751e+00
 -2.02008280e+00 -1.94127007e+00 -1.88522204e+00 -1.88140711e+00
 -1.87172369e+00 -1.86373986e+00 -1.80429330e+00 -1.78739560e+00
 -1.75010528e+00 -1.71996522e+00 -1.68333503e+00 -1.68293089e+00
 -1.67223280e+00 -1.66943254e+00 -1.65253484e+00 -1.60677744e+00
 -1.57359882e+00 -1.54807013e+00 -1.53934706e+00 -1.53819590e+00
 -1.51767407e+00 -1.47938278e+00 -1.46714139e+00 -1.39706325e+00
 -1.38281331e+00 -1.35741220e+00 -1.34893507e+00 -1.33228063e+00
 -1.32727553e+00 -1.32466814e+00 -1.31538293e+00 -1.30193923e+00
 -1.29954794e+00 -1.27834860e+00 -1.26485025e+00 -1.20219515e+00
 -1.17703997e+00 -1.09269917e+00 -1.08896099e+00 -1.07605745e+00
 -1.07128217e+00 -1.06255910e+00 -1.05746016e+00 -1.05457528e+00
 -1.04586020e+00 -1.04566140e+00 -1.01891771e+00 -9.95128720e-01
 -9.67811935e-01 -9.64967190e-01 -9.59088865e-01 -9.41196689e-01
 -9.32473620e-01 -9.29814339e-01 -9.27698338e-01 -9.19523705e-01
 -9.11863354e-01 -9.03899869e-01 -8.73766307e-01 -8.41125803e-01
 -8.32765706e-01 -8.06335925e-01 -7.97612856e-01 -7.87193772e-01
 -7.78470703e-01 -7.75939872e-01 -7.75811422e-01 -7.63734612e-01
 -7.60104887e-01 -7.51146929e-01 -7.42761253e-01 -7.38905543e-01
 -7.17232559e-01 -7.14810866e-01 -7.14347489e-01 -7.08509489e-01
 -6.95245661e-01 -6.83351805e-01 -6.62752091e-01 -6.60092811e-01
 -6.57369598e-01 -6.49967731e-01 -6.42665117e-01 -6.41079107e-01
 -6.28873848e-01 -6.05630789e-01 -6.04044779e-01 -5.92662429e-01
 -5.90546428e-01 -5.82371795e-01 -5.73648725e-01 -5.69814375e-01
 -5.67349243e-01 -5.29726713e-01 -5.25232047e-01 -5.23116045e-01
 -4.74302713e-01 -4.65369086e-01 -4.60460945e-01 -4.57902765e-01
 -4.57801665e-01 -4.55685663e-01 -4.38787961e-01 -4.17993054e-01
 -4.06033129e-01 -3.94413487e-01 -3.71357579e-01 -3.67875389e-01
 -3.34323250e-01 -3.25600181e-01 -3.23094061e-01 -3.22940900e-01
 -3.03927197e-01 -2.69189672e-01 -2.68478878e-01 -2.53394517e-01
 -2.45971470e-01 -2.45219884e-01 -2.16104199e-01 -2.12611443e-01
 -2.12147130e-01 -2.01308316e-01 -1.98987515e-01 -1.88080136e-01
 -1.85964135e-01 -1.82981190e-01 -1.60359297e-01 -1.33369782e-01
 -1.32032104e-01 -1.29707352e-01 -1.29043106e-01 -1.20653449e-01
 -1.18533753e-01 -1.08000131e-01 -1.01636051e-01 -8.58010666e-02
 -8.29485125e-02 -6.76387965e-02 -5.98264404e-02 -5.58786527e-02
 -5.48053819e-02 -5.32193721e-02 -4.84556055e-02 -3.42056687e-02
 -2.63219860e-02 -2.00751664e-02  5.33485265e-03  1.15517294e-02
  2.45016439e-02  3.32247134e-02  3.84491155e-02  4.90596975e-02
  5.41028587e-02  5.67791755e-02  7.05150313e-02  8.16413920e-02
  8.37573932e-02  9.14695112e-02  9.19095546e-02  1.00655095e-01
  1.18287439e-01  1.30514133e-01  1.37689424e-01  1.49071774e-01
  1.80851143e-01  1.84656183e-01  2.18618157e-01  2.28861107e-01
  2.43942728e-01  2.49237984e-01  2.60480226e-01  2.67980242e-01
  2.71220168e-01  2.80123404e-01  2.83932538e-01  3.02946242e-01
  3.05898869e-01  3.06607972e-01  3.08730749e-01  3.29716409e-01
  3.32715634e-01  3.57649255e-01  3.61653554e-01  3.80862044e-01
  3.98267549e-01  4.06080932e-01  4.07666942e-01  4.07978388e-01
  4.08448656e-01  4.15925855e-01  4.19828528e-01  4.54941083e-01
  4.72773918e-01  4.75097324e-01  5.05237388e-01  5.27928068e-01
  5.36937356e-01  5.80019134e-01  5.84034584e-01  5.95552485e-01
  6.07195172e-01  6.08372078e-01  6.09958088e-01  6.21074327e-01
  6.23200450e-01  6.26938629e-01  6.35379690e-01  6.37361655e-01
  6.46992417e-01  6.51464568e-01  6.67920523e-01  6.73976395e-01
  6.90630832e-01  7.25512318e-01  7.28927762e-01  7.36095783e-01
  7.39307903e-01  7.43232842e-01  7.44818852e-01  7.52437399e-01
  7.53258284e-01  7.64649837e-01  7.65211468e-01  7.74958916e-01
  7.75305213e-01  7.87208116e-01  8.04361286e-01  8.23071162e-01
  8.26214386e-01  8.33666229e-01  8.35552940e-01  8.50853800e-01
  8.52874719e-01  8.62017216e-01  8.69370537e-01  8.71134204e-01
  8.81500562e-01  8.89538320e-01  8.95810024e-01  9.01013393e-01
  9.01096611e-01  9.06197344e-01  9.30801654e-01  9.34974854e-01
  9.42206055e-01  9.58766350e-01  9.64090539e-01  9.72442178e-01
  9.77250062e-01  9.94230603e-01  9.96056668e-01  1.02408073e+00
  1.02619673e+00  1.03163450e+00  1.03243906e+00  1.04710351e+00
  1.04926272e+00  1.05788068e+00  1.05991918e+00  1.06384195e+00
  1.08045804e+00  1.09362711e+00  1.09619419e+00  1.09895130e+00
  1.10886815e+00  1.12766761e+00  1.15619221e+00  1.16759298e+00
  1.19202672e+00  1.19583778e+00  1.19896388e+00  1.21390556e+00
  1.21636765e+00  1.22457111e+00  1.22743607e+00  1.22866911e+00
  1.26497565e+00  1.27056864e+00  1.28206665e+00  1.30323395e+00
  1.30907162e+00  1.31211138e+00  1.31425594e+00  1.32353336e+00
  1.37130926e+00  1.38487641e+00  1.38694972e+00  1.39231778e+00
  1.39532035e+00  1.41772774e+00  1.43480023e+00  1.43761978e+00
  1.47183729e+00  1.47407717e+00  1.48708006e+00  1.49202682e+00
  1.51117955e+00  1.53989725e+00  1.54767486e+00  1.55862682e+00
  1.56262686e+00  1.57526964e+00  1.57705318e+00  1.59659015e+00
  1.61527795e+00  1.62186686e+00  1.62766927e+00  1.63253559e+00
  1.63293253e+00  1.63867300e+00  1.68374712e+00  1.70767483e+00
  1.74710530e+00  1.75050665e+00  1.75638105e+00  1.75789591e+00
  1.78092177e+00  1.79747343e+00  1.82288113e+00  1.86839882e+00
  1.86972719e+00  1.89268518e+00  1.89275757e+00  1.90196168e+00
  1.92077478e+00  1.93304599e+00  1.95434293e+00  1.96340858e+00
  2.00064210e+00  2.01961692e+00  2.02382500e+00  2.02704750e+00
  2.03301342e+00  2.03899535e+00  2.04530910e+00  2.05363441e+00
  2.05515292e+00  2.06034885e+00  2.06867623e+00  2.13004159e+00
  2.13129888e+00  2.13196126e+00  2.14735304e+00  2.16197524e+00
  2.17366072e+00  2.20607881e+00  2.21656775e+00  2.22211831e+00
  2.22323955e+00  2.23221476e+00  2.24446583e+00  2.25036875e+00
  2.25058854e+00  2.26241879e+00  2.27129808e+00  2.28294082e+00
  2.31087145e+00  2.33624927e+00  2.40812157e+00  2.45731853e+00
  2.46012630e+00  2.46239909e+00  2.51185000e+00  2.54768589e+00
  2.56605115e+00  2.56748178e+00  2.57925857e+00  2.59914308e+00
  2.62255705e+00  2.64368433e+00  2.64851444e+00  2.64871223e+00
  2.69202765e+00  2.71619102e+00  2.74015498e+00  2.74031806e+00
  2.74601362e+00  2.76483097e+00  2.76894436e+00  2.79152729e+00
  2.81080297e+00  2.82375742e+00  2.85158486e+00  2.89006415e+00
  2.89201870e+00  2.90628736e+00  2.90918619e+00  2.97441803e+00
  2.98195450e+00  2.99360147e+00  2.99674456e+00  3.00222499e+00
  3.01859093e+00  3.03940722e+00  3.04826718e+00  3.07588559e+00
  3.10118247e+00  3.11894859e+00  3.17052738e+00  3.19510801e+00
  3.21813589e+00  3.29109162e+00  3.32186663e+00  3.35528771e+00
  3.40155215e+00  3.41430760e+00  3.46927731e+00  3.49925741e+00
  3.50786707e+00  3.51639166e+00  3.61622480e+00  3.61736949e+00
  3.67358572e+00  3.76276986e+00  3.77211185e+00  3.88117849e+00
  3.93976036e+00  4.00260889e+00  4.05912410e+00  4.06840583e+00
  4.13071670e+00  4.17986223e+00  4.25419983e+00  4.26640045e+00
  4.31152694e+00  4.35847876e+00  4.41612930e+00  4.55269990e+00
  4.58872069e+00  4.64561044e+00  4.66046619e+00  4.75747283e+00
  4.76430214e+00  4.77217730e+00  4.78171717e+00  4.91652638e+00
  4.96078946e+00  5.12718874e+00  5.14645366e+00  5.25144344e+00
  5.30747248e+00  5.49254472e+00  5.56820564e+00  5.64640918e+00
  5.81061028e+00  5.87176882e+00  6.00134352e+00  6.04409064e+00
  6.29095844e+00  6.52034304e+00  6.52634606e+00  6.53544816e+00
  6.70818294e+00  6.80919393e+00  6.81594862e+00  6.82862614e+00
  7.10980925e+00  7.17451410e+00  7.21840000e+00  7.41513512e+00
  7.47546510e+00  7.50927025e+00  7.57801426e+00  7.64571015e+00
  7.94032953e+00  7.98860523e+00  8.01906953e+00  8.30789565e+00
  8.33996333e+00  9.03140291e+00  9.03162230e+00  9.31763607e+00
  9.47881378e+00  9.89139097e+00  1.05237704e+01  1.10399212e+01
  1.15443761e+01  1.15443790e+01  1.15749028e+01  1.16321327e+01
  1.16618040e+01  1.17376356e+01  1.21508152e+01  1.23855945e+01
  1.27006193e+01  1.27457739e+01  1.30154625e+01  1.30308635e+01
  1.30370326e+01  1.41319687e+01  1.42746320e+01  1.47289039e+01
  1.47739849e+01  1.48884110e+01  1.51002106e+01  1.51710376e+01
  1.55600405e+01  1.55737430e+01  1.61704925e+01  1.65642207e+01
  1.71919070e+01  1.76646587e+01  1.76807927e+01  1.78851516e+01
  1.80121819e+01  1.81326050e+01  1.84930031e+01  1.94543528e+01
  1.98107459e+01  1.99275483e+01  2.00284696e+01  2.08392178e+01
  2.13040017e+01  2.17263334e+01  2.21971700e+01  2.30002532e+01
  2.30054649e+01  2.32971542e+01  2.39433200e+01  2.47136202e+01
  2.48512517e+01  2.48690728e+01  2.50374198e+01  2.51911108e+01
  2.97605393e+01  2.98751229e+01  3.46087220e+01  3.96238765e+01
  4.14811630e+01  4.57195892e+01  4.99969123e+01  5.20027460e+01
  5.36921222e+01  5.81334574e+01  6.15283876e+01  6.57446476e+01
  7.19602913e+01  7.64821248e+01  1.26331783e+02]

  UserWarning,

2022-10-31 11:02:43,459:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-4.88146270e+01 -3.64976326e+01 -3.55351265e+01 -2.90304584e+01
 -2.21047597e+01 -1.62323138e+01 -1.62138232e+01 -1.58699297e+01
 -1.54496855e+01 -1.45922030e+01 -1.43492202e+01 -1.34843683e+01
 -1.32139227e+01 -1.31055609e+01 -1.30821427e+01 -1.30344789e+01
 -1.30178133e+01 -1.27094738e+01 -1.26720717e+01 -1.26150712e+01
 -1.25600685e+01 -1.25379058e+01 -1.20449873e+01 -1.20333760e+01
 -1.19901871e+01 -1.18519297e+01 -1.17404041e+01 -1.15930771e+01
 -1.15380656e+01 -1.14644245e+01 -1.14193158e+01 -1.13885902e+01
 -1.12930276e+01 -1.11503426e+01 -1.11089237e+01 -1.10947469e+01
 -1.10101879e+01 -1.09192233e+01 -1.06675784e+01 -1.06217168e+01
 -1.06026159e+01 -1.05939412e+01 -1.05918752e+01 -1.05635839e+01
 -1.05617678e+01 -1.05012708e+01 -1.04875509e+01 -1.04773069e+01
 -1.01523125e+01 -1.00774799e+01 -9.87244439e+00 -9.86923977e+00
 -9.84089930e+00 -9.62928209e+00 -9.55369158e+00 -9.54355801e+00
 -9.53156198e+00 -9.44019941e+00 -9.43188103e+00 -9.40318992e+00
 -9.37524159e+00 -9.29091940e+00 -9.28628896e+00 -9.18338071e+00
 -9.16548451e+00 -9.14945375e+00 -9.14866302e+00 -9.05230242e+00
 -9.02968853e+00 -9.00541553e+00 -8.95925882e+00 -8.95763029e+00
 -8.93622558e+00 -8.93085761e+00 -8.87952827e+00 -8.81227296e+00
 -8.79011721e+00 -8.78574690e+00 -8.76438267e+00 -8.74147402e+00
 -8.71894778e+00 -8.66162599e+00 -8.65050768e+00 -8.60723747e+00
 -8.50764683e+00 -8.50263271e+00 -8.41218797e+00 -8.34749179e+00
 -8.33946065e+00 -8.32816714e+00 -8.19716648e+00 -8.19389555e+00
 -8.17817505e+00 -8.04651952e+00 -8.02712496e+00 -8.00925642e+00
 -7.99918624e+00 -7.99213099e+00 -7.92494159e+00 -7.90255319e+00
 -7.89664553e+00 -7.83007364e+00 -7.78709550e+00 -7.76609692e+00
 -7.72881141e+00 -7.71247486e+00 -7.59204600e+00 -7.58220688e+00
 -7.55024494e+00 -7.54633124e+00 -7.54618437e+00 -7.53205755e+00
 -7.53098451e+00 -7.44829285e+00 -7.43219178e+00 -7.43158891e+00
 -7.39885861e+00 -7.38715738e+00 -7.38701051e+00 -7.37515080e+00
 -7.36790970e+00 -7.35799824e+00 -7.34114887e+00 -7.34103242e+00
 -7.33231509e+00 -7.20698299e+00 -7.18197502e+00 -7.17556238e+00
 -7.16312595e+00 -7.12942616e+00 -7.11393939e+00 -7.10719363e+00
 -7.07597014e+00 -7.07351105e+00 -7.04780913e+00 -7.03799767e+00
 -7.02134206e+00 -6.97693953e+00 -6.96173619e+00 -6.95683431e+00
 -6.94273291e+00 -6.92158562e+00 -6.91095345e+00 -6.88214082e+00
 -6.87693791e+00 -6.85166613e+00 -6.84277364e+00 -6.78984512e+00
 -6.76191804e+00 -6.76151179e+00 -6.74501459e+00 -6.73959978e+00
 -6.72604240e+00 -6.70780281e+00 -6.70407853e+00 -6.69712265e+00
 -6.69373272e+00 -6.64566593e+00 -6.62061718e+00 -6.59608235e+00
 -6.56686854e+00 -6.55688254e+00 -6.52796279e+00 -6.43099009e+00
 -6.42991678e+00 -6.40598704e+00 -6.38684102e+00 -6.37804311e+00
 -6.36354636e+00 -6.36274751e+00 -6.36183305e+00 -6.35500419e+00
 -6.33901067e+00 -6.31611829e+00 -6.31597141e+00 -6.29843594e+00
 -6.18055024e+00 -6.15679756e+00 -6.15456220e+00 -6.14681156e+00
 -6.14479097e+00 -6.12396602e+00 -6.11093592e+00 -6.06541269e+00
 -6.05305415e+00 -6.04202623e+00 -6.00146804e+00 -5.97677003e+00
 -5.96156509e+00 -5.95190894e+00 -5.95035296e+00 -5.93975548e+00
 -5.91999081e+00 -5.88890424e+00 -5.87140019e+00 -5.87043048e+00
 -5.86774124e+00 -5.82712086e+00 -5.82634373e+00 -5.77794766e+00
 -5.77173454e+00 -5.76756874e+00 -5.75046046e+00 -5.74216714e+00
 -5.73471999e+00 -5.73152324e+00 -5.72051503e+00 -5.70271109e+00
 -5.66327445e+00 -5.61256068e+00 -5.57186253e+00 -5.56669905e+00
 -5.53774094e+00 -5.51948623e+00 -5.50286625e+00 -5.48823968e+00
 -5.44409421e+00 -5.44066730e+00 -5.41626148e+00 -5.40471490e+00
 -5.39048382e+00 -5.37317079e+00 -5.30382405e+00 -5.29094083e+00
 -5.28251560e+00 -5.24773708e+00 -5.20629102e+00 -5.20248970e+00
 -5.18846833e+00 -5.18001454e+00 -5.17017299e+00 -5.15527688e+00
 -5.13142677e+00 -5.11961351e+00 -5.09946011e+00 -5.09271435e+00
 -5.07240069e+00 -5.06850793e+00 -5.05683871e+00 -5.05227683e+00
 -5.04127222e+00 -5.03925164e+00 -5.03176535e+00 -5.02132615e+00
 -5.00700539e+00 -5.00121608e+00 -4.97775593e+00 -4.97098713e+00
 -4.92658460e+00 -4.91659861e+00 -4.91457802e+00 -4.88268288e+00
 -4.86965543e+00 -4.80070947e+00 -4.78777808e+00 -4.76583510e+00
 -4.76306117e+00 -4.74402495e+00 -4.72154911e+00 -4.71156311e+00
 -4.67568747e+00 -4.63291318e+00 -4.59566157e+00 -4.54417621e+00
 -4.54152159e+00 -4.51651362e+00 -4.50450704e+00 -4.49085429e+00
 -4.47079886e+00 -4.45155117e+00 -4.43840053e+00 -4.38234773e+00
 -4.38116678e+00 -4.33663297e+00 -4.33648609e+00 -4.31547848e+00
 -4.31147813e+00 -4.30696164e+00 -4.29757863e+00 -4.27824240e+00
 -4.26561649e+00 -4.22633274e+00 -4.22436077e+00 -4.20387616e+00
 -4.13281356e+00 -4.13145060e+00 -4.10644263e+00 -4.10041031e+00
 -4.09645664e+00 -4.09443605e+00 -4.07664439e+00 -4.07314952e+00
 -4.06169828e+00 -4.06058100e+00 -4.05974939e+00 -4.02008155e+00
 -3.97227675e+00 -3.92721196e+00 -3.92641511e+00 -3.90140714e+00
 -3.88286024e+00 -3.82654472e+00 -3.80253021e+00 -3.77794842e+00
 -3.76724125e+00 -3.76495882e+00 -3.74054724e+00 -3.71884691e+00
 -3.71560240e+00 -3.68786282e+00 -3.68638565e+00 -3.68436507e+00
 -3.66054121e+00 -3.65746590e+00 -3.64136258e+00 -3.60505901e+00
 -3.59510951e+00 -3.55387248e+00 -3.53165483e+00 -3.52705129e+00
 -3.52455690e+00 -3.51634413e+00 -3.50310162e+00 -3.49133616e+00
 -3.48135016e+00 -3.47712304e+00 -3.42808789e+00 -3.42033226e+00
 -3.38093200e+00 -3.37874792e+00 -3.36683284e+00 -3.34540798e+00
 -3.34112820e+00 -3.31130863e+00 -3.29679035e+00 -3.27631467e+00
 -3.27429408e+00 -3.25281499e+00 -3.24739492e+00 -3.23059653e+00
 -3.20512532e+00 -3.19905569e+00 -3.14901273e+00 -3.14330376e+00
 -3.13955981e+00 -3.13681653e+00 -3.11662540e+00 -3.10627314e+00
 -3.10105581e+00 -3.07127918e+00 -3.06858725e+00 -3.06552710e+00
 -3.04912343e+00 -3.04153017e+00 -3.03889297e+00 -3.03540354e+00
 -3.02766314e+00 -3.02423628e+00 -3.00955031e+00 -2.99882462e+00
 -2.96583455e+00 -2.95518853e+00 -2.90123765e+00 -2.89249134e+00
 -2.87622968e+00 -2.87422170e+00 -2.86422310e+00 -2.84922980e+00
 -2.78570418e+00 -2.74206379e+00 -2.74025807e+00 -2.71196088e+00
 -2.69620216e+00 -2.67119419e+00 -2.66739817e+00 -2.66120819e+00
 -2.65918761e+00 -2.65445370e+00 -2.61977066e+00 -2.51148393e+00
 -2.51056123e+00 -2.50384916e+00 -2.45415211e+00 -2.43503616e+00
 -2.42979566e+00 -2.42029706e+00 -2.40146252e+00 -2.35138737e+00
 -2.32608075e+00 -2.32101750e+00 -2.30691222e+00 -2.30030840e+00
 -2.28613117e+00 -2.26931286e+00 -2.21526157e+00 -2.17736808e+00
 -2.14454827e+00 -2.08822462e+00 -2.05559820e+00 -2.04610171e+00
 -2.04408113e+00 -2.01022608e+00 -2.00314386e+00 -1.98551581e+00
 -1.96665155e+00 -1.95425253e+00 -1.94197373e+00 -1.91451313e+00
 -1.84997849e+00 -1.84714044e+00 -1.77148605e+00 -1.77102632e+00
 -1.74831475e+00 -1.72159549e+00 -1.69157613e+00 -1.68862281e+00
 -1.64225182e+00 -1.63462610e+00 -1.57319525e+00 -1.53129034e+00
 -1.49498744e+00 -1.47065215e+00 -1.43706945e+00 -1.39891026e+00
 -1.14960453e+00 -1.11321565e+00 -1.02714534e+00 -1.02699847e+00
 -9.74503723e-01 -9.64834481e-01 -8.88002346e-01 -8.80201040e-01
 -8.67824611e-01 -8.15888760e-01 -7.86969009e-01 -7.78381213e-01
 -7.25918341e-01 -7.20716681e-01 -6.99987218e-01 -6.76402685e-01
 -6.70536332e-01 -6.64129083e-01 -6.62789119e-01 -6.62341444e-01
 -6.08832683e-01 -5.80417940e-01 -5.31075908e-01 -4.95111009e-01
 -4.40811742e-01 -3.80169400e-01 -3.76473759e-01 -3.48724829e-01
 -3.03034337e-01 -2.79075495e-01 -2.64462472e-01 -2.54378997e-01
 -2.52718134e-01 -2.35923123e-01 -2.35776249e-01 -2.06856499e-01
 -1.49544272e-01 -1.29250147e-01 -5.75544917e-02 -3.59042362e-02
 -1.82100633e-03  3.12635540e-02  1.21257831e-01  1.66720152e-01
  2.03214486e-01  2.11162413e-01  2.45017466e-01  2.63792868e-01
  2.72124177e-01  2.93184905e-01  3.40877287e-01  3.79183354e-01
  3.79330227e-01  3.97382309e-01  4.06748991e-01  4.40872803e-01
  4.73567770e-01  4.77901049e-01  6.48279437e-01  7.03753279e-01
  7.12076814e-01  7.34131012e-01  7.43392703e-01  7.43539577e-01
  7.50033282e-01  7.72312454e-01  7.72459327e-01  7.84416186e-01
  8.26415763e-01  8.51008988e-01  8.71526713e-01  8.90972027e-01
  9.13958525e-01  9.16830502e-01  9.41187089e-01  9.83569035e-01
  1.02882512e+00  1.12726011e+00  1.17649713e+00  1.32509162e+00
  1.41131242e+00  1.68868150e+00  1.81659785e+00  1.82455805e+00
  1.82791496e+00  1.96627869e+00  2.11530374e+00  2.12368342e+00
  2.45495367e+00  2.62372628e+00  2.62463727e+00  2.88149484e+00
  3.17227891e+00  3.40806793e+00  3.65813345e+00  4.11234176e+00
  4.21892778e+00  4.28027920e+00  4.48230009e+00  4.58150003e+00
  4.90984034e+00  5.26491520e+00  5.26673350e+00  5.35405631e+00
  5.36086110e+00  5.40985872e+00  5.71523422e+00  6.46208041e+00
  7.62044278e+00  8.12671660e+00  8.22913885e+00  8.73482369e+00
  9.35481703e+00  9.92783967e+00  1.01966118e+01  1.11354442e+01
  1.22314103e+01  1.30305788e+01  1.32808640e+01  1.38474192e+01
  1.53608044e+01  1.66253448e+01  1.68423622e+01  1.73097993e+01
  2.02280915e+01  2.05706354e+01  2.16870121e+01  2.42557264e+01
  2.59289907e+01  2.69831830e+01  3.17220503e+01  3.45124557e+01
  3.75315796e+01  3.87191173e+01  4.24744510e+01  5.00889032e+01
  5.22780870e+01  5.49675696e+01  7.77217712e+01  7.89302134e+01
  8.00919475e+01  1.10505813e+02  1.34761967e+02]

  UserWarning,

2022-10-31 11:02:43,483:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-3.83769746e+00 -3.80257713e+00 -3.32194929e+00 -3.18122204e+00
 -3.13406121e+00 -3.09847766e+00 -3.07529588e+00 -3.03222306e+00
 -3.02493134e+00 -2.97258463e+00 -2.93501084e+00 -2.89976755e+00
 -2.83752710e+00 -2.82956229e+00 -2.81686651e+00 -2.79428359e+00
 -2.75528080e+00 -2.60578399e+00 -2.51282910e+00 -2.44246547e+00
 -2.41534536e+00 -2.38855373e+00 -2.37921410e+00 -2.33685855e+00
 -2.28969772e+00 -2.23137460e+00 -2.14912884e+00 -2.14320751e+00
 -2.05801588e+00 -2.05195764e+00 -2.02028373e+00 -2.01349332e+00
 -2.00597312e+00 -1.92553853e+00 -1.91467681e+00 -1.90275927e+00
 -1.86751598e+00 -1.83138472e+00 -1.82952256e+00 -1.81984847e+00
 -1.80919285e+00 -1.80543335e+00 -1.78207274e+00 -1.77394956e+00
 -1.72678873e+00 -1.71170911e+00 -1.70847115e+00 -1.70358593e+00
 -1.70297441e+00 -1.69531419e+00 -1.67170879e+00 -1.64134549e+00
 -1.59709105e+00 -1.57098187e+00 -1.57019095e+00 -1.56285868e+00
 -1.51569786e+00 -1.51312478e+00 -1.49924597e+00 -1.49249506e+00
 -1.48060973e+00 -1.48057753e+00 -1.47497293e+00 -1.46549791e+00
 -1.44533423e+00 -1.42478684e+00 -1.42213144e+00 -1.41021390e+00
 -1.40656798e+00 -1.38701111e+00 -1.35989099e+00 -1.35851873e+00
 -1.35176781e+00 -1.33537289e+00 -1.32381464e+00 -1.31664748e+00
 -1.30726775e+00 -1.29884402e+00 -1.29218277e+00 -1.28140419e+00
 -1.22848040e+00 -1.21916374e+00 -1.21555491e+00 -1.21341419e+00
 -1.19811210e+00 -1.18273341e+00 -1.16387973e+00 -1.14083530e+00
 -1.14067694e+00 -1.14022378e+00 -1.12875941e+00 -1.12299644e+00
 -1.12170162e+00 -1.11914613e+00 -1.11117957e+00 -1.09351611e+00
 -1.07843650e+00 -1.07296871e+00 -1.05391840e+00 -1.04564666e+00
 -1.03652955e+00 -1.03519299e+00 -1.02315249e+00 -1.01640157e+00
 -1.00807287e+00 -9.99949690e-01 -9.64829363e-01 -9.52788861e-01
 -9.37709247e-01 -9.29586066e-01 -9.17668533e-01 -9.02754940e-01
 -8.94465738e-01 -8.90855748e-01 -8.90708782e-01 -8.82425236e-01
 -8.67345622e-01 -8.50893741e-01 -8.50853640e-01 -8.26634545e-01
 -8.24102114e-01 -8.12061612e-01 -7.96981998e-01 -7.88858817e-01
 -7.77007731e-01 -7.64041599e-01 -7.53738490e-01 -7.26618374e-01
 -7.02931734e-01 -6.83374865e-01 -6.71334363e-01 -6.48131569e-01
 -6.36214036e-01 -6.33340945e-01 -6.28288754e-01 -6.22596490e-01
 -6.17375593e-01 -5.96691384e-01 -5.77767945e-01 -5.52103829e-01
 -5.50936819e-01 -5.48243553e-01 -5.42647617e-01 -5.35605307e-01
 -5.20564797e-01 -5.14212195e-01 -5.11241722e-01 -5.07404320e-01
 -5.03714323e-01 -4.91840861e-01 -4.72283993e-01 -4.60761996e-01
 -4.60243491e-01 -4.46136620e-01 -4.37040696e-01 -4.25123163e-01
 -4.10209570e-01 -4.08147554e-01 -3.94878059e-01 -3.90038232e-01
 -3.89749859e-01 -3.78471952e-01 -3.74800252e-01 -3.54125977e-01
 -3.31556744e-01 -3.19516242e-01 -3.04436628e-01 -3.03121322e-01
 -2.80749988e-01 -2.49152618e-01 -2.34073003e-01 -2.14032290e-01
 -2.11159199e-01 -2.03526981e-01 -2.02004553e-01 -1.97941742e-01
 -1.90829495e-01 -1.88938394e-01 -1.79356112e-01 -1.75041030e-01
 -1.70626534e-01 -1.63709379e-01 -1.55586199e-01 -1.44250356e-01
 -1.42657732e-01 -1.40022739e-01 -1.28589051e-01 -1.26843595e-01
 -1.08583734e-01 -1.00719223e-01 -9.57133362e-02 -9.33457548e-02
 -8.52225743e-02 -8.09432191e-02 -7.31126994e-02 -7.15336371e-02
 -6.96591150e-02 -4.63427459e-02 -2.30192235e-02 -2.29821305e-02
 -2.05085185e-02 -7.41083626e-03  7.04509318e-04  3.08717854e-02
  5.55046743e-02  5.72320191e-02  6.01413138e-02  6.37764135e-02
  6.59921131e-02  6.74222073e-02  7.97256711e-02  8.23652444e-02
  1.02665504e-01  1.05547505e-01  1.06900935e-01  1.16723420e-01
  1.34140038e-01  1.35646433e-01  1.43759564e-01  1.45836925e-01
  1.58084369e-01  1.61066567e-01  1.70366426e-01  2.04503662e-01
  2.05818968e-01  2.06719362e-01  2.08515821e-01  2.20696640e-01
  2.41855513e-01  2.43234387e-01  2.58472367e-01  2.61784456e-01
  2.94772545e-01  3.07455184e-01  3.13037269e-01  3.42610005e-01
  3.45072545e-01  3.45230911e-01  3.52522631e-01  3.61423888e-01
  3.85941988e-01  3.91672331e-01  4.11602444e-01  4.26599580e-01
  4.30771212e-01  4.69404874e-01  4.69563240e-01  4.85958159e-01
  4.94081340e-01  4.98623541e-01  5.06071781e-01  5.11301591e-01
  5.18546056e-01  5.18768382e-01  5.27829357e-01  5.35624081e-01
  5.39926864e-01  5.56321784e-01  5.82590666e-01  5.86633287e-01
  5.93870096e-01  6.10290488e-01  6.11605794e-01  6.18850896e-01
  6.26685408e-01  6.32153189e-01  6.43214355e-01  6.47749485e-01
  6.56676474e-01  6.61805736e-01  6.80654113e-01  6.81969418e-01
  6.96859850e-01  6.96890667e-01  6.97049032e-01  7.05172213e-01
  7.32169360e-01  7.35779757e-01  7.67412657e-01  7.84342271e-01
  8.00000554e-01  8.02532984e-01  8.06301747e-01  8.07743750e-01
  8.25078795e-01  8.37776281e-01  8.45899461e-01  8.51890309e-01
  8.57816994e-01  8.58838120e-01  8.71386042e-01  8.72896609e-01
  8.75572156e-01  8.80786361e-01  8.91744986e-01  9.24332883e-01
  9.26865313e-01  9.27390385e-01  9.30783317e-01  9.37234187e-01
  9.40727802e-01  9.40784764e-01  9.43260233e-01  9.53073015e-01
  9.63265550e-01  9.63423915e-01  9.95493524e-01  1.01848016e+00
  1.03308376e+00  1.08765359e+00  1.08834199e+00  1.11657538e+00
  1.12705926e+00  1.13441024e+00  1.14049856e+00  1.14605285e+00
  1.16235443e+00  1.18290342e+00  1.20058833e+00  1.21328104e+00
  1.22711238e+00  1.23788303e+00  1.26073310e+00  1.27080787e+00
  1.29169749e+00  1.29777898e+00  1.29783052e+00  1.31386971e+00
  1.32353788e+00  1.34595637e+00  1.34864625e+00  1.36774181e+00
  1.37795207e+00  1.38068485e+00  1.38527782e+00  1.38843154e+00
  1.39198487e+00  1.41425770e+00  1.43998248e+00  1.45016866e+00
  1.47426505e+00  1.49835445e+00  1.49836878e+00  1.52201245e+00
  1.54165302e+00  1.55360923e+00  1.56702211e+00  1.58785450e+00
  1.58972056e+00  1.60650941e+00  1.61327505e+00  1.61745088e+00
  1.62870576e+00  1.65271341e+00  1.67694088e+00  1.68024449e+00
  1.69539161e+00  1.71188929e+00  1.73284651e+00  1.73965389e+00
  1.76966029e+00  1.79361939e+00  1.83110466e+00  1.83366829e+00
  1.83455167e+00  1.84647178e+00  1.86582430e+00  1.88796058e+00
  1.89755859e+00  1.89875148e+00  1.90022906e+00  1.92236278e+00
  1.93505899e+00  1.93581256e+00  1.93974840e+00  1.95178710e+00
  1.96189248e+00  1.96532871e+00  1.96718075e+00  1.97870321e+00
  1.99190268e+00  2.00002848e+00  2.00329403e+00  2.00560825e+00
  2.05435829e+00  2.08120711e+00  2.10066019e+00  2.10794444e+00
  2.13144304e+00  2.13191165e+00  2.17001831e+00  2.17269132e+00
  2.19563843e+00  2.23782462e+00  2.27483881e+00  2.28465363e+00
  2.28748093e+00  2.28970112e+00  2.29677270e+00  2.36175473e+00
  2.36193166e+00  2.38530406e+00  2.44953190e+00  2.45687060e+00
  2.47639302e+00  2.52827637e+00  2.53645965e+00  2.54683054e+00
  2.60379744e+00  2.60406121e+00  2.61223061e+00  2.70208026e+00
  2.71383864e+00  2.72324210e+00  2.74912927e+00  2.79404206e+00
  2.79420589e+00  2.81981093e+00  2.84008955e+00  2.90539831e+00
  2.99083802e+00  2.99880203e+00  3.07111084e+00  3.07496746e+00
  3.07930489e+00  3.18869912e+00  3.19137404e+00  3.22779440e+00
  3.23065126e+00  3.25565533e+00  3.27292317e+00  3.33065254e+00
  3.34893215e+00  3.35880501e+00  3.42852715e+00  3.43458682e+00
  3.45559033e+00  3.46395491e+00  3.64248441e+00  3.65295979e+00
  3.66782912e+00  3.89841969e+00  3.89859659e+00  3.90335774e+00
  3.90741748e+00  3.90855374e+00  3.94754154e+00  3.98576740e+00
  4.08084672e+00  4.09173182e+00  4.12528030e+00  4.14348296e+00
  4.19066120e+00  4.20785675e+00  4.37102325e+00  4.48079292e+00
  4.48932896e+00  4.49141683e+00  4.53230500e+00  4.60629852e+00
  4.66718614e+00  4.68153800e+00  4.71135007e+00  4.75536459e+00
  4.75743510e+00  4.87098115e+00  4.88810746e+00  4.90912690e+00
  4.95809601e+00  4.99664804e+00  5.10132216e+00  5.10739159e+00
  5.31584417e+00  5.40879071e+00  5.53224951e+00  5.53489391e+00
  5.63462233e+00  5.65961571e+00  6.06095166e+00  6.10669742e+00
  6.27969250e+00  6.32397605e+00  6.40391972e+00  6.61592899e+00
  6.73279085e+00  6.84524314e+00  6.95585530e+00  7.02262313e+00
  7.05146229e+00  7.31491165e+00  7.34394283e+00  7.57693463e+00
  7.66472242e+00  7.79882285e+00  7.81005929e+00  8.19057562e+00
  8.28309026e+00  8.29530663e+00  8.45728314e+00  8.56530157e+00
  8.60251846e+00  8.62322405e+00  8.75878612e+00  8.77487675e+00
  8.81974920e+00  8.85454016e+00  8.93453591e+00  9.01115887e+00
  9.08895183e+00  9.19887919e+00  9.25631500e+00  9.34497336e+00
  9.38351979e+00  9.52758851e+00  9.63603854e+00  9.97567171e+00
  1.02648875e+01  1.03190222e+01  1.04264126e+01  1.08851220e+01
  1.10378884e+01  1.19156571e+01  1.31783574e+01  1.33808797e+01
  1.36669049e+01  1.44413490e+01  1.44933720e+01  1.48655795e+01
  1.49803060e+01  1.50377175e+01  1.55257944e+01  1.56869779e+01
  1.57205310e+01  1.67076816e+01  1.67389712e+01  1.71707603e+01
  1.73021979e+01  1.75797462e+01  1.80590283e+01  1.81432761e+01
  1.86331241e+01  1.86789700e+01  1.90588332e+01  1.93729777e+01
  1.97796863e+01  1.98037771e+01  2.10635933e+01  2.10870818e+01
  2.25413302e+01  2.29313202e+01  2.29679464e+01  2.31306734e+01
  2.34441226e+01  2.37684057e+01  2.54785390e+01  2.62717703e+01
  2.65852707e+01  2.80297517e+01  2.81343689e+01  2.87778309e+01
  2.91635457e+01  2.97836376e+01  3.02686401e+01  3.03169456e+01
  3.34447517e+01  3.59734894e+01  4.04136791e+01  4.83138231e+01
  5.33055907e+01  5.83707160e+01  7.59863542e+01  7.91274357e+01
  8.41818469e+01]

  UserWarning,

2022-10-31 11:02:43,484:INFO:Calculating mean and std
2022-10-31 11:02:43,485:INFO:Creating metrics dataframe
2022-10-31 11:02:43,489:INFO:Uploading results into container
2022-10-31 11:02:43,489:INFO:Uploading model into container now
2022-10-31 11:02:43,490:INFO:master_model_container: 18
2022-10-31 11:02:43,490:INFO:display_container: 2
2022-10-31 11:02:43,490:INFO:PassiveAggressiveRegressor(random_state=3360)
2022-10-31 11:02:43,490:INFO:create_model() successfully completed......................................
2022-10-31 11:02:43,588:ERROR:create_model() for PassiveAggressiveRegressor(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:02:43,588:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:43,588:INFO:Initializing Huber Regressor
2022-10-31 11:02:43,588:INFO:Total runtime is 1.1766269365946453 minutes
2022-10-31 11:02:43,588:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:43,588:INFO:Initializing create_model()
2022-10-31 11:02:43,588:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:43,588:INFO:Checking exceptions
2022-10-31 11:02:43,603:INFO:Importing libraries
2022-10-31 11:02:43,603:INFO:Copying training dataset
2022-10-31 11:02:43,603:INFO:Defining folds
2022-10-31 11:02:43,603:INFO:Declaring metric variables
2022-10-31 11:02:43,603:INFO:Importing untrained model
2022-10-31 11:02:43,603:INFO:Huber Regressor Imported successfully
2022-10-31 11:02:43,603:INFO:Starting cross validation
2022-10-31 11:02:43,603:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:45,081:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,091:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,170:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,217:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,340:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,387:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,418:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:45,955:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.93234376e-01 -5.49193149e-01 -4.70460058e-01 -4.63099163e-01
 -3.70412121e-01 -3.28409484e-01 -2.30775057e-01 -2.24832252e-01
 -2.06367472e-01 -1.99163461e-01 -1.88203806e-01 -1.67505691e-01
 -1.45918509e-01 -1.16909049e-01 -1.13338321e-01 -1.12892884e-01
 -1.09378730e-01 -1.07551591e-01 -1.03282169e-01 -1.00113985e-01
 -9.68141644e-02 -9.49209519e-02 -8.79601866e-02 -6.97213561e-02
 -2.68574663e-02 -1.12974789e-02 -1.17481050e-03  5.72493760e-03
  8.93970151e-03  1.34180626e-02  2.51098901e-02  2.79836078e-02
  4.23551128e-02  4.63518704e-02  4.72854086e-02  5.04808640e-02
  6.05586696e-02  6.96840506e-02  6.97226412e-02  7.04162829e-02
  7.63528419e-02  7.81804019e-02  7.89965257e-02  8.06334424e-02
  8.44298756e-02  8.65782596e-02  1.00434144e-01  1.00677859e-01
  1.02702299e-01  1.03617746e-01  1.06102490e-01  1.09427032e-01
  1.10414341e-01  1.10923196e-01  1.13689146e-01  1.19118798e-01
  1.20186380e-01  1.20781248e-01  1.22799480e-01  1.23867530e-01
  1.25206453e-01  1.25221109e-01  1.25379194e-01  1.25958198e-01
  1.27001303e-01  1.27803547e-01  1.29367294e-01  1.30863078e-01
  1.31156913e-01  1.35872826e-01  1.40994664e-01  1.42254367e-01
  1.43204147e-01  1.43928146e-01  1.44579154e-01  1.46784959e-01
  1.47171975e-01  1.47443085e-01  1.48786474e-01  1.49170090e-01
  1.52858867e-01  1.54867801e-01  1.58386219e-01  1.59516979e-01
  1.60377498e-01  1.62390203e-01  1.62644385e-01  1.62784376e-01
  1.67164861e-01  1.69011290e-01  1.70974467e-01  1.71116464e-01
  1.72811721e-01  1.73680076e-01  1.73707901e-01  1.74633590e-01
  1.78758212e-01  1.80913408e-01  1.83036813e-01  1.83893402e-01
  1.84018837e-01  1.86560568e-01  1.89610091e-01  1.89873460e-01
  1.90324090e-01  1.91157639e-01  1.93377899e-01  1.93828870e-01
  1.95658398e-01  1.97370498e-01  1.97526169e-01  1.97851330e-01
  2.01683556e-01  2.01805720e-01  2.03231244e-01  2.03537998e-01
  2.04063648e-01  2.04839152e-01  2.05707031e-01  2.05909851e-01
  2.06230919e-01  2.08155912e-01  2.08338873e-01  2.10896170e-01
  2.14274615e-01  2.16812348e-01  2.19600574e-01  2.20196992e-01
  2.20330912e-01  2.22260267e-01  2.22936430e-01  2.24730272e-01
  2.29747936e-01  2.31939987e-01  2.33189577e-01  2.35700730e-01
  2.36637532e-01  2.39938986e-01  2.42033460e-01  2.42562577e-01
  2.43365479e-01  2.44661382e-01  2.44848388e-01  2.45085661e-01
  2.45602728e-01  2.46083373e-01  2.46539135e-01  2.47020658e-01
  2.47370646e-01  2.47825594e-01  2.48120076e-01  2.48469225e-01
  2.49000780e-01  2.49104449e-01  2.49359898e-01  2.50966706e-01
  2.51858816e-01  2.54674390e-01  2.55853575e-01  2.57673727e-01
  2.57878253e-01  2.58037349e-01  2.60514589e-01  2.62215156e-01
  2.63047658e-01  2.63121039e-01  2.63428167e-01  2.68216476e-01
  2.68384515e-01  2.69480773e-01  2.71294897e-01  2.71587826e-01
  2.72546680e-01  2.74137989e-01  2.74380543e-01  2.74478280e-01
  2.74857163e-01  2.74924725e-01  2.79381280e-01  2.79671418e-01
  2.81513354e-01  2.82905545e-01  2.82980584e-01  2.84903279e-01
  2.85080089e-01  2.86385657e-01  2.86708860e-01  2.87022392e-01
  2.87651906e-01  2.87717293e-01  2.88206349e-01  2.88455463e-01
  2.89105514e-01  2.91580983e-01  2.91856173e-01  2.92482170e-01
  2.92763926e-01  2.92770398e-01  2.93733103e-01  2.93843782e-01
  2.93902988e-01  2.95383617e-01  2.98813380e-01  2.99853986e-01
  3.01077949e-01  3.01628200e-01  3.03328227e-01  3.03626442e-01
  3.07604192e-01  3.07890717e-01  3.08884172e-01  3.09120482e-01
  3.09399582e-01  3.10409738e-01  3.14753163e-01  3.15168419e-01
  3.15872033e-01  3.16447150e-01  3.17138133e-01  3.19801398e-01
  3.21726310e-01  3.24451297e-01  3.24481504e-01  3.26645014e-01
  3.26881181e-01  3.27237765e-01  3.28206458e-01  3.28781453e-01
  3.30435852e-01  3.30812355e-01  3.33112578e-01  3.33742578e-01
  3.34032840e-01  3.34592178e-01  3.34758649e-01  3.35377384e-01
  3.35590148e-01  3.36552821e-01  3.37796340e-01  3.38209293e-01
  3.39951315e-01  3.40089519e-01  3.40764585e-01  3.41297382e-01
  3.41530822e-01  3.43468367e-01  3.45340136e-01  3.47050236e-01
  3.47068391e-01  3.48802902e-01  3.48933944e-01  3.50646986e-01
  3.51173613e-01  3.51573242e-01  3.52853401e-01  3.54391029e-01
  3.54879253e-01  3.55342940e-01  3.58372934e-01  3.58963686e-01
  3.59617326e-01  3.61757202e-01  3.63400360e-01  3.63533927e-01
  3.63782496e-01  3.64204969e-01  3.64579562e-01  3.67339715e-01
  3.68751106e-01  3.69282279e-01  3.71297434e-01  3.71518834e-01
  3.71716649e-01  3.71880980e-01  3.73421891e-01  3.73739576e-01
  3.76901094e-01  3.78775423e-01  3.80179674e-01  3.81172685e-01
  3.82099420e-01  3.82565936e-01  3.83971032e-01  3.86211796e-01
  3.94161050e-01  3.94292407e-01  3.95310899e-01  3.97060619e-01
  3.97341859e-01  3.97526683e-01  3.97634068e-01  3.99793330e-01
  4.00505717e-01  4.01116243e-01  4.05544487e-01  4.06343483e-01
  4.08691185e-01  4.12176960e-01  4.12462370e-01  4.12891631e-01
  4.13017381e-01  4.14080502e-01  4.15852558e-01  4.16280126e-01
  4.16630355e-01  4.18373817e-01  4.18677988e-01  4.18812594e-01
  4.19402966e-01  4.20905430e-01  4.22215663e-01  4.23277697e-01
  4.26005393e-01  4.29142364e-01  4.30707955e-01  4.31438937e-01
  4.37666416e-01  4.41414825e-01  4.41691241e-01  4.42498198e-01
  4.43335156e-01  4.47355004e-01  4.48111460e-01  4.48128298e-01
  4.52211067e-01  4.53200149e-01  4.56255873e-01  4.56952790e-01
  4.57872783e-01  4.60657660e-01  4.61607194e-01  4.62906295e-01
  4.65976621e-01  4.66140260e-01  4.68932328e-01  4.69800277e-01
  4.73041426e-01  4.73261201e-01  4.78396438e-01  4.79874303e-01
  4.82804046e-01  4.83689511e-01  4.88256390e-01  4.92482701e-01
  4.93277419e-01  4.95497757e-01  4.96947910e-01  5.01523069e-01
  5.02524563e-01  5.03450888e-01  5.03980450e-01  5.04643970e-01
  5.04794176e-01  5.06490994e-01  5.07287407e-01  5.13205187e-01
  5.13537521e-01  5.13796849e-01  5.14752224e-01  5.18960448e-01
  5.27086250e-01  5.27265567e-01  5.29132950e-01  5.34739406e-01
  5.42672204e-01  5.51663726e-01  5.55479751e-01  5.59606620e-01
  5.59614849e-01  5.62136512e-01  5.62239195e-01  5.71513922e-01
  5.72303601e-01  5.74619620e-01  5.76586815e-01  5.79672308e-01
  5.83157091e-01  5.87882699e-01  5.91150322e-01  5.91380911e-01
  5.94144182e-01  5.96573944e-01  6.00132608e-01  6.00444438e-01
  6.01159110e-01  6.01536387e-01  6.06729420e-01  6.07313943e-01
  6.09233242e-01  6.09320350e-01  6.11140014e-01  6.12677887e-01
  6.19508320e-01  6.24039353e-01  6.33059438e-01  6.37887616e-01
  6.38903207e-01  6.40280450e-01  6.44953959e-01  6.45473483e-01
  6.48560091e-01  6.49818376e-01  6.50666516e-01  6.53466714e-01
  6.57346088e-01  6.57484671e-01  6.58659747e-01  6.61638570e-01
  6.61702625e-01  6.66245614e-01  6.70266147e-01  6.70646803e-01
  6.73524173e-01  6.75388440e-01  6.77746114e-01  6.79431877e-01
  6.81824712e-01  6.84624910e-01  6.86111450e-01  6.87017744e-01
  6.89817943e-01  6.92642565e-01  6.94673351e-01  6.95010976e-01
  6.98342292e-01  6.99888994e-01  7.00204008e-01  7.03372957e-01
  7.05397041e-01  7.07322450e-01  7.10590074e-01  7.13220919e-01
  7.16601476e-01  7.18175940e-01  7.19325536e-01  7.20232428e-01
  7.20976139e-01  7.22791973e-01  7.23368973e-01  7.24912068e-01
  7.26169172e-01  7.28562006e-01  7.29018195e-01  7.31362204e-01
  7.33178038e-01  7.34375917e-01  7.40266374e-01  7.41748270e-01
  7.44141104e-01  7.49334136e-01  7.51312405e-01  7.52572925e-01
  7.54527169e-01  7.58943641e-01  7.59892419e-01  7.61206741e-01
  7.61405838e-01  7.62476989e-01  7.64913235e-01  7.69621794e-01
  7.70106267e-01  7.71109347e-01  7.72820056e-01  7.75299300e-01
  7.76788261e-01  7.79989822e-01  7.85685365e-01  7.88813229e-01
  7.88981936e-01  7.90878398e-01  7.97389733e-01  8.01264463e-01
  8.02582766e-01  8.02787388e-01  8.04064662e-01  8.04885453e-01
  8.08448536e-01  8.11410578e-01  8.13628797e-01  8.14952353e-01
  8.15938788e-01  8.18929088e-01  8.20962062e-01  8.29207895e-01
  8.31707988e-01  8.33740962e-01  8.34400928e-01  8.36043649e-01
  8.37131219e-01  8.38411174e-01  8.44786993e-01  8.44792149e-01
  8.49320060e-01  8.49980026e-01  8.52692280e-01  8.53194790e-01
  8.60366091e-01  8.64899158e-01  8.65793986e-01  8.65957985e-01
  8.68059216e-01  8.68773888e-01  8.70092190e-01  8.70345068e-01
  8.80478256e-01  8.82090891e-01  8.85671288e-01  8.86485014e-01
  8.90864321e-01  8.93193946e-01  8.93768034e-01  8.96057354e-01
  8.98360041e-01  9.00535715e-01  9.01250386e-01  9.03060839e-01
  9.06443419e-01  9.07898867e-01  9.08838472e-01  9.12210915e-01
  9.12296418e-01  9.15264625e-01  9.16829484e-01  9.19758084e-01
  9.22682483e-01  9.26810389e-01  9.33454166e-01  9.34711270e-01
  9.36689538e-01  9.37509353e-01  9.37601615e-01  9.53128613e-01
  9.53180713e-01  9.55158982e-01  9.60173923e-01  9.62654702e-01
  9.64380504e-01  9.65869466e-01  9.67847734e-01  9.68045139e-01
  9.73952844e-01  9.79145876e-01  9.81050584e-01  9.83426832e-01
  9.84338909e-01  9.91119957e-01  9.91834629e-01  9.95770558e-01
  1.00568792e+00  1.00741373e+00  1.00958940e+00  1.01228234e+00
  1.01260676e+00  1.01478243e+00  1.02069014e+00  1.02181368e+00
  1.02299282e+00  1.02448179e+00  1.04328048e+00  1.04376496e+00
  1.04574322e+00  1.04845548e+00  1.05464033e+00  1.05698927e+00
  1.05808695e+00  1.06175725e+00  1.06651536e+00  1.07113139e+00
  1.07690142e+00  1.08209445e+00  1.09248052e+00  1.09767355e+00
  1.10286658e+00  1.11325265e+00  1.12247205e+00  1.13199441e+00
  1.13331011e+00  1.13402478e+00  1.16593045e+00  1.19580460e+00
  1.21706120e+00  1.22744727e+00  1.64118307e+00  1.96872253e+00]

  UserWarning,

2022-10-31 11:02:45,986:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.02394517e+00 -9.91307902e-01 -5.68528071e-01 -5.00034498e-01
 -4.32835153e-01 -4.22473097e-01 -3.98642080e-01 -3.90194549e-01
 -3.63407009e-01 -3.58436791e-01 -3.16262690e-01 -3.13778668e-01
 -2.97821560e-01 -2.86043661e-01 -2.30659742e-01 -2.07251008e-01
 -2.01728004e-01 -1.84714678e-01 -1.74975767e-01 -1.60656261e-01
 -1.57147327e-01 -1.47311396e-01 -1.38677771e-01 -1.21958990e-01
 -1.18505771e-01 -1.02997223e-01 -1.01933007e-01 -9.43605309e-02
 -6.91403075e-02 -6.51388292e-02 -5.96793375e-02 -4.89610179e-02
 -4.84021857e-02 -3.11622320e-02 -1.98792959e-02 -1.62292157e-02
 -1.53950716e-02 -4.46653948e-03 -2.61971363e-04  1.01682597e-02
  1.56474370e-02  2.12128620e-02  2.32883578e-02  3.29234593e-02
  3.34437774e-02  4.11987746e-02  4.13182146e-02  4.13556144e-02
  4.19808927e-02  4.65657105e-02  5.28384008e-02  5.87069492e-02
  5.89732253e-02  6.67158361e-02  6.89234371e-02  6.93007999e-02
  7.93888060e-02  8.23388899e-02  9.50896809e-02  9.51874336e-02
  9.54626333e-02  9.67670287e-02  1.01154900e-01  1.03449211e-01
  1.04389448e-01  1.05000155e-01  1.05817250e-01  1.06677090e-01
  1.07796675e-01  1.09062790e-01  1.12341192e-01  1.14695352e-01
  1.17480759e-01  1.19557398e-01  1.20959274e-01  1.25645771e-01
  1.26123426e-01  1.26474128e-01  1.29891000e-01  1.29902887e-01
  1.29960248e-01  1.33812284e-01  1.35243034e-01  1.38670672e-01
  1.39069835e-01  1.39993626e-01  1.43430982e-01  1.45223303e-01
  1.47372232e-01  1.47487377e-01  1.47889544e-01  1.49011937e-01
  1.53071626e-01  1.53930728e-01  1.54905111e-01  1.55170049e-01
  1.55221836e-01  1.56492925e-01  1.56884083e-01  1.56929518e-01
  1.58110312e-01  1.58977185e-01  1.60299278e-01  1.61666578e-01
  1.62914254e-01  1.63142595e-01  1.64243513e-01  1.66467157e-01
  1.67449989e-01  1.69519791e-01  1.69945213e-01  1.70467773e-01
  1.71009622e-01  1.71940054e-01  1.72098785e-01  1.72358675e-01
  1.73287689e-01  1.75326285e-01  1.75698252e-01  1.78740612e-01
  1.79238775e-01  1.79382558e-01  1.81122234e-01  1.84534758e-01
  1.86111983e-01  1.87949976e-01  1.89499183e-01  1.91304481e-01
  1.92960430e-01  1.94313434e-01  1.95359321e-01  1.95467036e-01
  1.96144984e-01  1.98250771e-01  1.98733419e-01  1.99500775e-01
  2.02214997e-01  2.02344995e-01  2.03551885e-01  2.05726895e-01
  2.06139291e-01  2.07743222e-01  2.07851344e-01  2.08043282e-01
  2.08791385e-01  2.11853826e-01  2.11958377e-01  2.12364025e-01
  2.12721590e-01  2.13218110e-01  2.14590484e-01  2.15832386e-01
  2.16355204e-01  2.18738640e-01  2.19368632e-01  2.19416772e-01
  2.19722913e-01  2.19727353e-01  2.22034574e-01  2.22954608e-01
  2.23504188e-01  2.23726397e-01  2.25004003e-01  2.25181464e-01
  2.25449654e-01  2.28125530e-01  2.29402252e-01  2.29868808e-01
  2.31137924e-01  2.32212647e-01  2.33374971e-01  2.33798420e-01
  2.35397148e-01  2.36400499e-01  2.36673880e-01  2.40041676e-01
  2.40615260e-01  2.41336032e-01  2.41613552e-01  2.45063971e-01
  2.45794684e-01  2.45986505e-01  2.47212673e-01  2.47886589e-01
  2.47954642e-01  2.49607009e-01  2.50796822e-01  2.50893686e-01
  2.51703780e-01  2.53129673e-01  2.53889535e-01  2.54196256e-01
  2.54411531e-01  2.57825459e-01  2.58693895e-01  2.60496929e-01
  2.61969495e-01  2.63346280e-01  2.64677564e-01  2.65060670e-01
  2.65113334e-01  2.66987246e-01  2.67016975e-01  2.67423299e-01
  2.68256289e-01  2.68295027e-01  2.68424334e-01  2.73793039e-01
  2.75738992e-01  2.77193656e-01  2.77719013e-01  2.77729779e-01
  2.78395396e-01  2.79131864e-01  2.79329997e-01  2.80023556e-01
  2.82261126e-01  2.83818213e-01  2.89891603e-01  2.90962300e-01
  2.91288848e-01  2.94979672e-01  2.95529521e-01  2.97817700e-01
  2.99294702e-01  3.01083986e-01  3.05853432e-01  3.06444754e-01
  3.10480781e-01  3.13231787e-01  3.13970611e-01  3.13975135e-01
  3.14247757e-01  3.14638245e-01  3.15094787e-01  3.15508568e-01
  3.17369110e-01  3.17567545e-01  3.19809988e-01  3.19848907e-01
  3.19962012e-01  3.21348831e-01  3.21583675e-01  3.22064101e-01
  3.22116588e-01  3.23362347e-01  3.23518336e-01  3.23536771e-01
  3.24514089e-01  3.27444419e-01  3.31474003e-01  3.33292687e-01
  3.33562127e-01  3.33760486e-01  3.33880671e-01  3.34102159e-01
  3.35144869e-01  3.35259716e-01  3.35461390e-01  3.35704973e-01
  3.36015122e-01  3.37211645e-01  3.41739192e-01  3.42993025e-01
  3.44660231e-01  3.45950519e-01  3.46687442e-01  3.47069758e-01
  3.47088839e-01  3.47368218e-01  3.50281176e-01  3.52027203e-01
  3.58078674e-01  3.63885357e-01  3.64631715e-01  3.64955768e-01
  3.65082906e-01  3.65789217e-01  3.66074370e-01  3.66074503e-01
  3.69075225e-01  3.69330869e-01  3.70566328e-01  3.71931101e-01
  3.75584376e-01  3.76322192e-01  3.76843999e-01  3.79383642e-01
  3.79923177e-01  3.80111358e-01  3.81033322e-01  3.89856606e-01
  3.89883550e-01  3.91944917e-01  3.92432370e-01  3.94021647e-01
  3.94284794e-01  3.95851868e-01  3.98685396e-01  4.00110426e-01
  4.00152905e-01  4.00400237e-01  4.01959741e-01  4.02267904e-01
  4.05357273e-01  4.08035255e-01  4.10861153e-01  4.12947243e-01
  4.19749902e-01  4.28841249e-01  4.28919602e-01  4.29403253e-01
  4.31574716e-01  4.33254782e-01  4.36987751e-01  4.37071349e-01
  4.38824623e-01  4.43069826e-01  4.46454787e-01  4.48257496e-01
  4.49568740e-01  4.52404264e-01  4.59887017e-01  4.61452958e-01
  4.64768963e-01  4.65568666e-01  4.70216360e-01  4.73339675e-01
  4.73640568e-01  4.73806060e-01  4.83359439e-01  4.84156492e-01
  4.84168750e-01  4.86602096e-01  5.04080337e-01  5.06983797e-01
  5.10917522e-01  5.11731357e-01  5.13984861e-01  5.16347626e-01
  5.22554641e-01  5.26185620e-01  5.29806359e-01  5.31705631e-01
  5.35605217e-01  5.37410513e-01  5.44045240e-01  5.44698565e-01
  5.46643051e-01  5.51903619e-01  5.53373799e-01  5.57354512e-01
  5.57894048e-01  5.63163641e-01  5.69012066e-01  5.73107660e-01
  5.78730488e-01  5.84710951e-01  5.86204298e-01  5.88982177e-01
  5.95477525e-01  6.00093794e-01  6.00995052e-01  6.02038280e-01
  6.06654549e-01  6.12785116e-01  6.17867773e-01  6.18558870e-01
  6.23175139e-01  6.24274757e-01  6.27791508e-01  6.30463291e-01
  6.31868142e-01  6.32407677e-01  6.37023946e-01  6.42403610e-01
  6.46201617e-01  6.46256484e-01  6.49968967e-01  6.52817240e-01
  6.60105292e-01  6.60641427e-01  6.64721561e-01  6.68688285e-01
  6.72009712e-01  6.73412597e-01  6.74232673e-01  6.76899480e-01
  6.79557400e-01  6.80514854e-01  6.83186637e-01  6.85606032e-01
  6.85739283e-01  6.91317029e-01  6.93770251e-01  6.93798285e-01
  6.97035444e-01  7.01651713e-01  7.02135705e-01  7.03596200e-01
  7.04292086e-01  7.05629939e-01  7.05704500e-01  7.06267982e-01
  7.07752869e-01  7.12828738e-01  7.15500521e-01  7.17445007e-01
  7.20116790e-01  7.24678057e-01  7.25965215e-01  7.26677545e-01
  7.27285704e-01  7.28947461e-01  7.29349328e-01  7.31116596e-01
  7.31293814e-01  7.31901973e-01  7.33965597e-01  7.35718539e-01
  7.35910083e-01  7.36637480e-01  7.38040364e-01  7.38581866e-01
  7.39065858e-01  7.40312346e-01  7.43170363e-01  7.43198135e-01
  7.43682127e-01  7.43889770e-01  7.44430291e-01  7.47814404e-01
  7.49109345e-01  7.50863194e-01  7.51258020e-01  7.52430673e-01
  7.56779840e-01  7.58613741e-01  7.61663211e-01  7.66346138e-01
  7.70895750e-01  7.71085630e-01  7.72300701e-01  7.73448395e-01
  7.74862473e-01  7.74918708e-01  7.84203055e-01  7.89360826e-01
  7.93199059e-01  7.93977095e-01  7.96529740e-01  7.97153737e-01
  8.01769948e-01  8.02189743e-01  8.05479649e-01  8.05762279e-01
  8.07067360e-01  8.10378548e-01  8.11422281e-01  8.12442171e-01
  8.13022460e-01  8.14195114e-01  8.16872410e-01  8.21111227e-01
  8.21555572e-01  8.21674709e-01  8.23663872e-01  8.24711346e-01
  8.25641433e-01  8.26290978e-01  8.27394376e-01  8.28043921e-01
  8.28843624e-01  8.30343765e-01  8.31986230e-01  8.32387983e-01
  8.38076162e-01  8.41892728e-01  8.42692431e-01  8.46051967e-01
  8.47308700e-01  8.51924969e-01  8.56474114e-01  8.57584972e-01
  8.59211521e-01  8.59794322e-01  8.62201241e-01  8.64481104e-01
  8.66774672e-01  8.71971321e-01  8.73393259e-01  8.75006315e-01
  8.76050048e-01  8.79622584e-01  8.83439150e-01  8.87951336e-01
  8.89567097e-01  8.92671688e-01  8.92907909e-01  8.98087660e-01
  9.01364691e-01  9.02703929e-01  9.04456872e-01  9.07820760e-01
  9.10241524e-01  9.11136764e-01  9.16552736e-01  9.20932808e-01
  9.21169005e-01  9.26829008e-01  9.31897973e-01  9.43186713e-01
  9.45294084e-01  9.47047027e-01  9.47303558e-01  9.51123760e-01
  9.54526622e-01  9.57535676e-01  9.65512103e-01  9.73700908e-01
  9.77607968e-01  9.78774490e-01  9.79552454e-01  9.86840506e-01
  9.89512389e-01  9.90413042e-01  9.96073044e-01  9.99645580e-01
  1.00530558e+00  1.00705852e+00  1.01167479e+00  1.01453812e+00
  1.01629106e+00  1.01915439e+00  1.02090733e+00  1.02303761e+00
  1.02377066e+00  1.04060456e+00  1.04358681e+00  1.04398868e+00
  1.04860495e+00  1.04990521e+00  1.05146827e+00  1.05452148e+00
  1.05608454e+00  1.05783748e+00  1.05832148e+00  1.05964951e+00
  1.06011439e+00  1.06531708e+00  1.06707002e+00  1.07028485e+00
  1.07166335e+00  1.07366891e+00  1.07630256e+00  1.08091883e+00
  1.08117536e+00  1.08766538e+00  1.09015137e+00  1.09938391e+00
  1.10400018e+00  1.13113431e+00  1.14093033e+00  1.14554660e+00
  1.14928216e+00  1.15016287e+00  1.15270223e+00  1.15939540e+00
  1.16301209e+00  1.18777266e+00  1.19424865e+00  1.19645526e+00
  1.24539148e+00  2.00251804e+00]

  UserWarning,

2022-10-31 11:02:46,002:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.41134465 -0.80852636 -0.66366494 -0.60065338 -0.39064185 -0.30419143
 -0.26125861 -0.23040176 -0.22674382 -0.16873175 -0.15120994 -0.12292245
 -0.11669695 -0.10626173 -0.07413818 -0.07027442 -0.06766873 -0.06729524
 -0.06597223 -0.06100698 -0.05442673  0.00933431  0.01444316  0.01563048
  0.01944043  0.0212226   0.03734339  0.04576386  0.0519172   0.0623445
  0.06600065  0.06743309  0.06959989  0.0729199   0.07341824  0.0767626
  0.07917716  0.08322686  0.08809022  0.08920149  0.0928652   0.09334931
  0.09520222  0.0968378   0.09734057  0.09869916  0.10040341  0.10185741
  0.10511934  0.10711434  0.12019528  0.12051921  0.12169661  0.12343804
  0.1236601   0.12610354  0.12933668  0.13110094  0.13134794  0.13347434
  0.13370648  0.13381481  0.13911597  0.14157013  0.14483124  0.14611641
  0.1461404   0.14649932  0.15000925  0.15543427  0.15653833  0.15900767
  0.16231778  0.16470593  0.1655876   0.16779794  0.16981434  0.17102463
  0.17140731  0.17174398  0.17508409  0.18191427  0.18236688  0.18441663
  0.18562893  0.18827769  0.18986457  0.19130016  0.1926813   0.19284522
  0.19304602  0.19834205  0.20029387  0.20312752  0.20356718  0.20420851
  0.20517624  0.20594501  0.20624019  0.21034152  0.21165025  0.2118972
  0.21213545  0.21230995  0.21294083  0.21344698  0.21365406  0.21435295
  0.21467343  0.21550356  0.21610516  0.21646007  0.21751672  0.22017815
  0.22024122  0.22162547  0.22227875  0.22236921  0.22301022  0.22441068
  0.22449071  0.22484925  0.22586217  0.22646718  0.23041174  0.23082283
  0.2315473   0.2317854   0.23230743  0.23408819  0.23492989  0.23496734
  0.23604136  0.23639211  0.23847604  0.23878083  0.23898872  0.23922005
  0.23976984  0.24057496  0.24196685  0.2447092   0.2461901   0.24634883
  0.24716503  0.2495234   0.24988791  0.25041141  0.2506053   0.25116202
  0.25214766  0.25228183  0.25269326  0.25608629  0.25669524  0.25718345
  0.25737822  0.25782313  0.25846545  0.25969666  0.26013127  0.2624523
  0.26311958  0.26382937  0.26471164  0.26592974  0.2667434   0.26971431
  0.26997083  0.27001425  0.27249329  0.27437721  0.27709831  0.27804939
  0.27811315  0.27843396  0.27859088  0.27867111  0.27909484  0.28042389
  0.28064225  0.28074086  0.2813433   0.28197742  0.2822024   0.28268048
  0.28458731  0.28459287  0.28534344  0.28642662  0.2872544   0.28962381
  0.28984115  0.29020157  0.29062024  0.29067666  0.29170977  0.29225253
  0.29425926  0.29428185  0.29440836  0.29478903  0.29487771  0.29853305
  0.29900945  0.3007099   0.30108406  0.30514109  0.30518419  0.30787394
  0.30820423  0.30850543  0.30950773  0.31002133  0.31218959  0.313162
  0.31600576  0.31604086  0.31751383  0.31980399  0.32006295  0.32137529
  0.32166113  0.32473865  0.32489287  0.3268147   0.32734418  0.33019376
  0.33027454  0.33046889  0.33248571  0.33256828  0.33335638  0.33371742
  0.33620781  0.33640638  0.33651241  0.33654668  0.33674786  0.33856501
  0.33865883  0.33986649  0.3418004   0.34224628  0.34233948  0.34433592
  0.34439783  0.3445603   0.34492359  0.34505785  0.34654515  0.34694525
  0.34781761  0.34927193  0.34949472  0.35038534  0.35122925  0.35179842
  0.35200681  0.35208425  0.35219896  0.35250673  0.35357123  0.35483092
  0.35646599  0.35741721  0.36096402  0.36420546  0.36435437  0.36523769
  0.36603323  0.36668843  0.36761043  0.36822135  0.36843486  0.36931361
  0.3705649   0.37182004  0.37510956  0.37624511  0.37809344  0.37844362
  0.37858146  0.38151963  0.3817152   0.38663604  0.38894427  0.38899121
  0.39044432  0.39558522  0.39561602  0.39691382  0.39964198  0.39985665
  0.40204516  0.40276615  0.40380312  0.40383828  0.40498683  0.40696864
  0.40853839  0.40909783  0.40985542  0.41061123  0.4106736   0.41118627
  0.41617678  0.41974412  0.4230703   0.42469136  0.42867399  0.42906131
  0.43259263  0.43494066  0.43865261  0.44354694  0.44728156  0.44803877
  0.44916203  0.45528581  0.45657056  0.46466927  0.46493734  0.46724465
  0.46823298  0.46881022  0.47184527  0.47295904  0.47657058  0.48660831
  0.49125201  0.49274345  0.49685524  0.50226629  0.50565643  0.50795814
  0.50802157  0.508102    0.50892885  0.51037622  0.51290347  0.51546981
  0.52130554  0.52462209  0.53105417  0.53256891  0.53913121  0.54933361
  0.55065655  0.55201345  0.56153395  0.56766101  0.57029271  0.58444085
  0.59235881  0.59285397  0.5934733   0.60307625  0.60420028  0.60493137
  0.60651834  0.60787772  0.61336613  0.61370348  0.61400911  0.61447188
  0.61730684  0.61748066  0.61866878  0.62361206  0.62841353  0.63188508
  0.63465319  0.63668655  0.63893654  0.64511641  0.64523297  0.64704436
  0.64770447  0.652086    0.66306329  0.66424251  0.67029686  0.67162678
  0.67208954  0.67543755  0.67925682  0.6794261   0.68055381  0.68122972
  0.68470127  0.68501333  0.68637222  0.68638523  0.69083267  0.69430422
  0.69563414  0.69910569  0.70390716  0.70504973  0.70523708  0.70574816
  0.70700685  0.70870863  0.71351011  0.71369388  0.71484003  0.715415
  0.71739819  0.71831158  0.71930661  0.7196415   0.72062612  0.72247007
  0.72247872  0.72311305  0.72610547  0.72678855  0.72924444  0.73208167
  0.73404592  0.73751747  0.73917997  0.74126241  0.74149866  0.74483261
  0.74818556  0.74963408  0.75192188  0.7547741   0.75566683  0.75649492
  0.75672335  0.75755993  0.75875171  0.7612964   0.76152483  0.76568332
  0.76615041  0.7663263   0.76854344  0.76937029  0.77048479  0.77592924
  0.77814638  0.77842975  0.78073071  0.78686211  0.78969068  0.79033366
  0.79132869  0.79410001  0.79646505  0.79764881  0.7999366   0.80409509
  0.80789113  0.81086947  0.81117145  0.81214287  0.81434102  0.81605942
  0.81849951  0.82330098  0.82487806  0.82721954  0.82803545  0.828517
  0.83017951  0.83095052  0.83354691  0.83811995  0.83834838  0.83904681
  0.84250687  0.84278611  0.84292142  0.84707991  0.84730834  0.85210981
  0.85235697  0.85496993  0.85732584  0.86422643  0.8662858   0.86651423
  0.86804085  0.8693438   0.87068137  0.8713157   0.87611717  0.8786501
  0.88069021  0.88091864  0.88572012  0.887874    0.88919599  0.89052159
  0.89532306  0.90012453  0.90053909  0.90360041  0.90817345  0.90840188
  0.90972748  0.91297492  0.91365958  0.91738907  0.9177764   0.91800483
  0.91929125  0.91933042  0.92061684  0.92259719  0.9228063   0.92413189
  0.9244688   0.92893769  0.92941202  0.93240924  0.93459584  0.93503303
  0.93733632  0.93854846  0.94201219  0.94310935  0.94369182  0.94391708
  0.94439863  0.94860634  0.94883768  0.9513867   0.95490628  0.96121807
  0.96236065  0.96395325  0.96601955  0.97082102  0.97562249  0.9757481
  0.98019553  0.98042396  0.98174956  0.98454921  0.99002691  0.99077365
  0.99459995  0.9972434   1.00993123  1.01266061  1.01380584  1.02363721
  1.02520147  1.02821025  1.03393859  1.04724875  1.05221761  1.0584038
  1.05918436  1.06426613  1.07039319  1.07102768  1.07622497  1.08395043
  1.08582792  1.08804506  1.09062939  1.09543086  1.1026779   1.10432042
  1.11240523  1.16275888]

  UserWarning,

2022-10-31 11:02:46,067:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.58110627e+00 -1.02345555e+00 -9.43224607e-01 -5.65232950e-01
 -5.20461346e-01 -3.69003339e-01 -2.46686786e-01 -2.31302144e-01
 -2.15550103e-01 -2.13665920e-01 -1.93670843e-01 -1.49895481e-01
 -1.41310068e-01 -1.32269587e-01 -1.21490718e-01 -1.17662453e-01
 -1.03055678e-01 -9.59736837e-02 -8.63118632e-02 -8.50569258e-02
 -8.30220280e-02 -6.75843709e-02 -6.59236936e-02 -5.58018819e-02
 -5.34004193e-02 -3.36130986e-02 -1.03815938e-02 -9.77107319e-03
 -9.24633439e-03 -8.08953309e-03 -4.68401761e-03 -1.19481132e-03
 -4.50298138e-04  2.05737489e-02  3.21860448e-02  3.89449142e-02
  4.11615590e-02  5.15623568e-02  5.20382932e-02  5.33762670e-02
  6.23099140e-02  6.96331909e-02  6.97770543e-02  7.01186109e-02
  7.04491165e-02  7.90050672e-02  8.10478509e-02  8.31236698e-02
  8.33481188e-02  8.34754600e-02  8.48647837e-02  8.66821063e-02
  9.22475848e-02  1.04772462e-01  1.05532179e-01  1.05984693e-01
  1.07511631e-01  1.12910603e-01  1.15038964e-01  1.15139587e-01
  1.17781829e-01  1.21986729e-01  1.22514348e-01  1.25384727e-01
  1.31220093e-01  1.31964635e-01  1.34864054e-01  1.35461293e-01
  1.35903758e-01  1.38016189e-01  1.38424308e-01  1.42421010e-01
  1.44270545e-01  1.47331739e-01  1.47583700e-01  1.47943640e-01
  1.53301606e-01  1.53309996e-01  1.54224078e-01  1.55877179e-01
  1.57158277e-01  1.59980220e-01  1.62746041e-01  1.63605479e-01
  1.65752993e-01  1.65958414e-01  1.66882199e-01  1.68703263e-01
  1.71024807e-01  1.71173007e-01  1.71372998e-01  1.71425096e-01
  1.73400843e-01  1.73796319e-01  1.74990216e-01  1.82405313e-01
  1.83299714e-01  1.83917522e-01  1.84793535e-01  1.86145469e-01
  1.86741441e-01  1.93388918e-01  1.94473091e-01  1.96302381e-01
  1.96626188e-01  1.98672700e-01  2.00945727e-01  2.01538596e-01
  2.02404084e-01  2.03779528e-01  2.04152711e-01  2.08381907e-01
  2.08939331e-01  2.09402355e-01  2.09409762e-01  2.09450689e-01
  2.10384280e-01  2.15314231e-01  2.15464616e-01  2.15631249e-01
  2.17019386e-01  2.17396398e-01  2.18293183e-01  2.21615555e-01
  2.24145855e-01  2.24740760e-01  2.27448866e-01  2.27809852e-01
  2.28039218e-01  2.30059143e-01  2.30455272e-01  2.31559435e-01
  2.31828140e-01  2.32162253e-01  2.35539567e-01  2.35635481e-01
  2.37884760e-01  2.39821407e-01  2.40257946e-01  2.43051168e-01
  2.43866291e-01  2.45098889e-01  2.47851156e-01  2.48259763e-01
  2.48522926e-01  2.50933959e-01  2.51894100e-01  2.52202519e-01
  2.52862356e-01  2.54478987e-01  2.55568624e-01  2.56215022e-01
  2.56822680e-01  2.57377207e-01  2.58467746e-01  2.58879769e-01
  2.59889765e-01  2.60206731e-01  2.61104799e-01  2.63147423e-01
  2.64184681e-01  2.64252293e-01  2.65206178e-01  2.67922844e-01
  2.68873002e-01  2.69620749e-01  2.70418516e-01  2.70657472e-01
  2.71200679e-01  2.74025321e-01  2.74043516e-01  2.74222661e-01
  2.74881946e-01  2.77106668e-01  2.77664203e-01  2.77670289e-01
  2.77825352e-01  2.78643418e-01  2.78891699e-01  2.79649309e-01
  2.80915128e-01  2.80957807e-01  2.81150880e-01  2.81700341e-01
  2.83252782e-01  2.83334399e-01  2.83432438e-01  2.83823659e-01
  2.85086678e-01  2.85551637e-01  2.85640677e-01  2.87273949e-01
  2.87411484e-01  2.87808172e-01  2.89270079e-01  2.91127055e-01
  2.92789947e-01  2.94344628e-01  2.96693747e-01  2.97559726e-01
  2.99527627e-01  2.99732940e-01  3.00023610e-01  3.00182192e-01
  3.01528407e-01  3.01993289e-01  3.02635771e-01  3.03252721e-01
  3.03384476e-01  3.04302008e-01  3.05060347e-01  3.07169050e-01
  3.07433897e-01  3.10835461e-01  3.12712975e-01  3.12815384e-01
  3.13621263e-01  3.15233042e-01  3.16816098e-01  3.17088869e-01
  3.18071130e-01  3.18408688e-01  3.18889917e-01  3.20568222e-01
  3.21787820e-01  3.23058825e-01  3.23793057e-01  3.23975287e-01
  3.24534168e-01  3.25259996e-01  3.26560309e-01  3.27435950e-01
  3.28416983e-01  3.32911930e-01  3.34195452e-01  3.34640345e-01
  3.36111578e-01  3.37581593e-01  3.37748238e-01  3.38163156e-01
  3.38378919e-01  3.38474538e-01  3.38904502e-01  3.40379494e-01
  3.41084322e-01  3.43767738e-01  3.45448314e-01  3.46716585e-01
  3.46813300e-01  3.52261954e-01  3.55134648e-01  3.55279771e-01
  3.55867674e-01  3.57255480e-01  3.57917816e-01  3.59327301e-01
  3.60547760e-01  3.61701331e-01  3.62674511e-01  3.65790295e-01
  3.66496766e-01  3.67212621e-01  3.67955505e-01  3.68031032e-01
  3.68435115e-01  3.68856483e-01  3.68886876e-01  3.68906079e-01
  3.69612550e-01  3.70326148e-01  3.70429406e-01  3.71961109e-01
  3.72021864e-01  3.74576638e-01  3.75526404e-01  3.76859914e-01
  3.77252061e-01  3.77782467e-01  3.78253432e-01  3.79202747e-01
  3.79338414e-01  3.80236047e-01  3.81381586e-01  3.81810677e-01
  3.82074017e-01  3.82418695e-01  3.83022265e-01  3.83609039e-01
  3.84485000e-01  3.84758397e-01  3.87291950e-01  3.87778692e-01
  3.88346086e-01  3.96948137e-01  3.98025272e-01  3.98098204e-01
  3.98727729e-01  3.99112235e-01  4.00370387e-01  4.03150949e-01
  4.12568986e-01  4.13505570e-01  4.16966298e-01  4.17700181e-01
  4.20122418e-01  4.21260825e-01  4.21320669e-01  4.21626076e-01
  4.21678923e-01  4.24245681e-01  4.25727365e-01  4.26802175e-01
  4.26883114e-01  4.29512236e-01  4.31822669e-01  4.32336391e-01
  4.33647876e-01  4.38190501e-01  4.38962643e-01  4.40835321e-01
  4.41306286e-01  4.46827747e-01  4.47537854e-01  4.47568176e-01
  4.49750607e-01  4.52803289e-01  4.55255579e-01  4.56885206e-01
  4.58376332e-01  4.58996858e-01  4.59784604e-01  4.61119861e-01
  4.67881653e-01  4.70982443e-01  4.71116049e-01  4.71934903e-01
  4.77214011e-01  4.77705555e-01  4.88921989e-01  4.92479000e-01
  4.92992947e-01  4.93149028e-01  4.93862886e-01  4.95606818e-01
  4.95610885e-01  4.98548113e-01  5.03534636e-01  5.08371852e-01
  5.17719205e-01  5.20582688e-01  5.21086248e-01  5.23479535e-01
  5.23624237e-01  5.24629604e-01  5.31391931e-01  5.37333015e-01
  5.39529694e-01  5.40904938e-01  5.45508961e-01  5.48980897e-01
  5.51740529e-01  5.53460214e-01  5.54494101e-01  5.56628703e-01
  5.60267688e-01  5.60497419e-01  5.64362789e-01  5.65430554e-01
  5.79038263e-01  5.85378968e-01  5.91610536e-01  6.05305303e-01
  6.06414169e-01  6.07004450e-01  6.07189457e-01  6.16351803e-01
  6.16739208e-01  6.23906498e-01  6.36328338e-01  6.46953834e-01
  6.47400421e-01  6.53691616e-01  6.54178214e-01  6.60451711e-01
  6.62147789e-01  6.68849168e-01  6.70026604e-01  6.72872919e-01
  6.79246138e-01  6.82220271e-01  6.85022124e-01  6.86456418e-01
  6.91130622e-01  6.93486349e-01  6.94246406e-01  6.94725337e-01
  6.97362190e-01  6.98505663e-01  6.99633390e-01  7.00477974e-01
  7.00914976e-01  7.04030760e-01  7.06709543e-01  7.08881413e-01
  7.10262329e-01  7.10840982e-01  7.13378113e-01  7.16056895e-01
  7.19172679e-01  7.19609681e-01  7.22767394e-01  7.25404247e-01
  7.25841249e-01  7.27001868e-01  7.28957033e-01  7.32072818e-01
  7.34437668e-01  7.35188602e-01  7.38304386e-01  7.40528628e-01
  7.41420170e-01  7.42825892e-01  7.44030287e-01  7.46133575e-01
  7.47237794e-01  7.50767522e-01  7.53883306e-01  7.56562089e-01
  7.56999091e-01  7.60114875e-01  7.60538114e-01  7.62486146e-01
  7.63130646e-01  7.63230659e-01  7.63967830e-01  7.65672934e-01
  7.68574740e-01  7.70168698e-01  7.70199398e-01  7.70910239e-01
  7.71833498e-01  7.73001251e-01  7.73315182e-01  7.75625131e-01
  7.77191404e-01  7.78272565e-01  7.78441465e-01  7.78709567e-01
  7.78809579e-01  7.84024632e-01  7.87412419e-01  7.88894103e-01
  7.90528203e-01  7.90801751e-01  7.90995884e-01  7.91272716e-01
  7.93236004e-01  7.93917535e-01  7.95125671e-01  7.96759771e-01
  7.97404272e-01  7.97546213e-01  8.00620068e-01  8.02087452e-01
  8.04473024e-01  8.08898265e-01  8.12338692e-01  8.13820376e-01
  8.15454476e-01  8.20002348e-01  8.21686044e-01  8.27917612e-01
  8.37264965e-01  8.40380749e-01  8.40502952e-01  8.43496533e-01
  8.46469343e-01  8.46654246e-01  8.48209938e-01  8.49728101e-01
  8.50354723e-01  8.52843885e-01  8.53422538e-01  8.57441354e-01
  8.59075454e-01  8.60557138e-01  8.65307022e-01  8.67951841e-01
  8.68422806e-01  8.69043388e-01  8.69261791e-01  8.77517857e-01
  8.77770159e-01  8.80885943e-01  8.84001727e-01  8.87117511e-01
  8.90607616e-01  8.96212562e-01  8.96464863e-01  9.05341251e-01
  9.08675699e-01  9.11791483e-01  9.12528654e-01  9.14907267e-01
  9.15159568e-01  9.22128307e-01  9.24586255e-01  9.27664633e-01
  9.33854273e-01  9.36161946e-01  9.36717756e-01  9.36970057e-01
  9.40085841e-01  9.49180892e-01  9.52555496e-01  9.55412461e-01
  9.55596097e-01  9.56149632e-01  9.61008843e-01  9.67875597e-01
  9.68502219e-01  9.70210389e-01  9.70949453e-01  9.74107165e-01
  9.74373372e-01  9.77264878e-01  9.80338734e-01  9.81806117e-01
  9.83454518e-01  9.83635879e-01  9.84191689e-01  9.86570302e-01
  9.90675558e-01  9.93539041e-01  9.93791342e-01  9.98187880e-01
  9.99770610e-01  1.00208056e+00  1.00288639e+00  1.00361661e+00
  1.00600218e+00  1.01465429e+00  1.02084393e+00  1.02158110e+00
  1.02469688e+00  1.02522137e+00  1.02590216e+00  1.02781267e+00
  1.03241958e+00  1.03716002e+00  1.04027580e+00  1.04339159e+00
  1.04932706e+00  1.04962316e+00  1.05097370e+00  1.05538376e+00
  1.05580513e+00  1.05807897e+00  1.06056813e+00  1.06208629e+00
  1.07362001e+00  1.07913260e+00  1.09439273e+00  1.09473526e+00
  1.10502804e+00  1.11105135e+00  1.11474579e+00  1.16625898e+00
  1.26419023e+00]

  UserWarning,

2022-10-31 11:02:46,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.18658625e+00 -1.01205108e+00 -9.30236092e-01 -8.94532314e-01
 -8.10770293e-01 -7.24909254e-01 -6.86767923e-01 -5.78800079e-01
 -5.25376384e-01 -5.17070043e-01 -5.10845409e-01 -4.63348478e-01
 -4.46780312e-01 -3.77239564e-01 -3.68917880e-01 -3.56656585e-01
 -3.26342795e-01 -3.26250012e-01 -3.14302704e-01 -2.88486518e-01
 -2.62340453e-01 -2.49015238e-01 -2.45340784e-01 -2.34217059e-01
 -2.18459324e-01 -2.08960171e-01 -1.96643782e-01 -1.87298196e-01
 -1.49411621e-01 -1.40403601e-01 -1.32453282e-01 -9.84652389e-02
 -7.64847147e-02 -7.52311053e-02 -6.21926345e-02 -3.87995989e-02
 -3.85329525e-02 -3.61056653e-02 -3.54848436e-02 -2.30950995e-02
  1.25959453e-03  6.32938167e-03  7.97873199e-03  2.12301775e-02
  2.23417854e-02  3.90857048e-02  4.84855186e-02  4.99782247e-02
  5.71929193e-02  5.72735971e-02  6.61066711e-02  6.80842451e-02
  7.28704937e-02  7.32195772e-02  7.55837774e-02  7.85005991e-02
  7.86468452e-02  8.27052540e-02  8.43250923e-02  8.51386206e-02
  8.63204088e-02  8.66208515e-02  8.67801290e-02  8.78882338e-02
  8.84251601e-02  9.53017140e-02  9.54687243e-02  9.63416597e-02
  1.01132418e-01  1.01494902e-01  1.05848751e-01  1.07890259e-01
  1.07948386e-01  1.16714338e-01  1.17932229e-01  1.20368270e-01
  1.27608284e-01  1.32027713e-01  1.33550738e-01  1.33818301e-01
  1.33982247e-01  1.34448767e-01  1.37075055e-01  1.44419624e-01
  1.44671755e-01  1.45267340e-01  1.46237419e-01  1.46255486e-01
  1.47147321e-01  1.48167749e-01  1.50884732e-01  1.50916991e-01
  1.52455440e-01  1.52763677e-01  1.53721261e-01  1.57128481e-01
  1.58300356e-01  1.61172488e-01  1.62014851e-01  1.63163061e-01
  1.65146661e-01  1.66843430e-01  1.71135541e-01  1.78373172e-01
  1.80341612e-01  1.80810774e-01  1.80888403e-01  1.81859173e-01
  1.82230169e-01  1.83061442e-01  1.83745403e-01  1.84974414e-01
  1.87362019e-01  1.87560803e-01  1.87822269e-01  1.88872243e-01
  1.91033622e-01  1.93059803e-01  1.93473734e-01  1.94291439e-01
  1.94369978e-01  1.95674165e-01  1.96182416e-01  1.96236697e-01
  1.97120337e-01  1.97304402e-01  1.97341207e-01  1.97489199e-01
  1.98219016e-01  1.98257387e-01  1.98679059e-01  1.99799656e-01
  2.00978797e-01  2.01341500e-01  2.01347087e-01  2.02419749e-01
  2.02454128e-01  2.02793911e-01  2.04600735e-01  2.07460403e-01
  2.07671733e-01  2.08403762e-01  2.08834269e-01  2.09253855e-01
  2.13095957e-01  2.14075962e-01  2.14236008e-01  2.15671212e-01
  2.16630692e-01  2.17303210e-01  2.17664614e-01  2.19137980e-01
  2.21073989e-01  2.24055935e-01  2.24749925e-01  2.28788981e-01
  2.29002276e-01  2.31302299e-01  2.32363623e-01  2.33327889e-01
  2.36756192e-01  2.37261830e-01  2.37806178e-01  2.41430556e-01
  2.42643254e-01  2.43230993e-01  2.43694886e-01  2.44792195e-01
  2.45303209e-01  2.46672547e-01  2.46733599e-01  2.46772420e-01
  2.46962065e-01  2.48025991e-01  2.48108072e-01  2.49164200e-01
  2.50692899e-01  2.51659299e-01  2.53889926e-01  2.54398880e-01
  2.58341551e-01  2.58430525e-01  2.59172720e-01  2.59701405e-01
  2.60931284e-01  2.61102043e-01  2.63799490e-01  2.64579434e-01
  2.65227175e-01  2.66390686e-01  2.67306797e-01  2.67619827e-01
  2.68533573e-01  2.68580185e-01  2.69565292e-01  2.69917278e-01
  2.71274114e-01  2.71438531e-01  2.71912732e-01  2.72894520e-01
  2.75282425e-01  2.76772423e-01  2.77915745e-01  2.79313348e-01
  2.79907161e-01  2.80840436e-01  2.81024700e-01  2.81980098e-01
  2.83120594e-01  2.83403966e-01  2.84500666e-01  2.85900853e-01
  2.86967383e-01  2.87239570e-01  2.88686778e-01  2.88874062e-01
  2.90431413e-01  2.90693185e-01  2.90908456e-01  2.91713872e-01
  2.93538510e-01  2.95396357e-01  2.96361830e-01  2.97317743e-01
  2.97761307e-01  2.99030370e-01  2.99262559e-01  3.02213092e-01
  3.02890938e-01  3.03034990e-01  3.04221561e-01  3.05266940e-01
  3.05439956e-01  3.05675642e-01  3.05740107e-01  3.06040405e-01
  3.06506315e-01  3.06871008e-01  3.07342344e-01  3.07377537e-01
  3.10333935e-01  3.12966503e-01  3.13604384e-01  3.15591983e-01
  3.17460972e-01  3.17774133e-01  3.18269157e-01  3.18414758e-01
  3.18572853e-01  3.19281131e-01  3.20253105e-01  3.20723190e-01
  3.24513050e-01  3.25754714e-01  3.28180046e-01  3.29494930e-01
  3.31059544e-01  3.31062539e-01  3.31199209e-01  3.31241694e-01
  3.32593625e-01  3.33188302e-01  3.35107817e-01  3.35444723e-01
  3.35663565e-01  3.36963572e-01  3.37255053e-01  3.37346469e-01
  3.39552745e-01  3.39964801e-01  3.40958434e-01  3.41334610e-01
  3.41354159e-01  3.43468404e-01  3.45073867e-01  3.47253226e-01
  3.48246105e-01  3.48274560e-01  3.51242857e-01  3.55022752e-01
  3.55986550e-01  3.57654314e-01  3.58149394e-01  3.60708270e-01
  3.61515634e-01  3.63172966e-01  3.63543462e-01  3.64850442e-01
  3.65573305e-01  3.66451047e-01  3.69624851e-01  3.70262110e-01
  3.70517485e-01  3.71508145e-01  3.72965620e-01  3.74912279e-01
  3.76185347e-01  3.76337466e-01  3.76576326e-01  3.77721180e-01
  3.79892118e-01  3.80122795e-01  3.80852960e-01  3.86108114e-01
  3.86315094e-01  3.88653752e-01  3.89341355e-01  3.93595072e-01
  4.03045806e-01  4.05116496e-01  4.05821202e-01  4.11929886e-01
  4.12093635e-01  4.19771594e-01  4.20570254e-01  4.24378164e-01
  4.24778838e-01  4.26133357e-01  4.28302817e-01  4.29545933e-01
  4.31296298e-01  4.31574990e-01  4.34824784e-01  4.39842380e-01
  4.40896839e-01  4.45129731e-01  4.46087818e-01  4.47333893e-01
  4.49765544e-01  4.53755965e-01  4.63867496e-01  4.69305877e-01
  4.70332039e-01  4.70556084e-01  4.71891266e-01  4.72752434e-01
  4.74775949e-01  4.80389846e-01  4.86658157e-01  4.86884829e-01
  4.92577474e-01  4.93989792e-01  5.01219663e-01  5.01310115e-01
  5.01409489e-01  5.04288711e-01  5.07290570e-01  5.17482436e-01
  5.18360073e-01  5.24319761e-01  5.25213690e-01  5.31334617e-01
  5.33912534e-01  5.36834434e-01  5.39577580e-01  5.43347080e-01
  5.51188752e-01  5.60983362e-01  5.63340071e-01  5.64603989e-01
  5.70578438e-01  5.72865357e-01  5.78500851e-01  5.80239541e-01
  5.81651326e-01  5.86833997e-01  5.97024299e-01  6.04964811e-01
  6.08457911e-01  6.10492276e-01  6.16618218e-01  6.16972925e-01
  6.21391787e-01  6.21681099e-01  6.27895176e-01  6.29917125e-01
  6.30013293e-01  6.31008082e-01  6.32167307e-01  6.38227021e-01
  6.44463196e-01  6.44538101e-01  6.55959610e-01  6.60671021e-01
  6.64378940e-01  6.64379176e-01  6.65578641e-01  6.66588740e-01
  6.67185384e-01  6.70696318e-01  6.71457514e-01  6.72109707e-01
  6.72798270e-01  6.73169918e-01  6.73949007e-01  6.75604714e-01
  6.77958753e-01  6.78221138e-01  6.78411157e-01  6.81217600e-01
  6.81822692e-01  6.85037327e-01  6.85072142e-01  6.85247996e-01
  6.86830487e-01  6.87610491e-01  6.89636931e-01  6.91311841e-01
  6.92443374e-01  6.93015132e-01  6.94691172e-01  6.98056261e-01
  6.99521948e-01  7.01910802e-01  7.03290542e-01  7.05134835e-01
  7.05327266e-01  7.06475591e-01  7.07870464e-01  7.07941278e-01
  7.09282034e-01  7.10747722e-01  7.11099467e-01  7.12241364e-01
  7.13554165e-01  7.15943019e-01  7.16360608e-01  7.17701364e-01
  7.18510629e-01  7.23314251e-01  7.24468706e-01  7.26120695e-01
  7.27938127e-01  7.28746023e-01  7.28927138e-01  7.29080024e-01
  7.29707978e-01  7.31733581e-01  7.32781679e-01  7.33646096e-01
  7.34540025e-01  7.39896493e-01  7.41618599e-01  7.44160339e-01
  7.45319790e-01  7.45765798e-01  7.48035781e-01  7.50381915e-01
  7.51378685e-01  7.52426783e-01  7.53452851e-01  7.54185128e-01
  7.55650816e-01  7.56545049e-01  7.56991572e-01  7.57593661e-01
  7.58904086e-01  7.59798015e-01  7.62604459e-01  7.67239003e-01
  7.68217345e-01  7.70130096e-01  7.70725554e-01  7.72071887e-01
  7.78465613e-01  7.79794015e-01  7.83297660e-01  7.84476614e-01
  7.84874891e-01  7.86104103e-01  7.86913368e-01  7.89936615e-01
  7.91716990e-01  7.95918743e-01  7.98355945e-01  8.00136320e-01
  8.02942764e-01  8.03289975e-01  8.03516428e-01  8.17713966e-01
  8.23613936e-01  8.26850013e-01  8.32033266e-01  8.32402507e-01
  8.33232967e-01  8.34184852e-01  8.34839709e-01  8.35390872e-01
  8.36620084e-01  8.42232971e-01  8.43259039e-01  8.45076472e-01
  8.46065483e-01  8.54484813e-01  8.57291256e-01  8.60877703e-01
  8.61167963e-01  8.61563387e-01  8.62904143e-01  8.71323473e-01
  8.76490351e-01  8.76936360e-01  8.88162134e-01  8.90968577e-01
  8.91338054e-01  8.93377102e-01  8.95892901e-01  8.97346690e-01
  9.00987167e-01  9.05867777e-01  9.07807237e-01  9.10613681e-01
  9.12067470e-01  9.26099687e-01  9.27452341e-01  9.32171299e-01
  9.36919769e-01  9.37325460e-01  9.39023176e-01  9.39726212e-01
  9.42938347e-01  9.45339099e-01  9.46916330e-01  9.47419700e-01
  9.49722773e-01  9.54164121e-01  9.56970564e-01  9.59371316e-01
  9.59777007e-01  9.60976708e-01  9.62137442e-01  9.63783151e-01
  9.65389894e-01  9.66540631e-01  9.67790646e-01  9.72174320e-01
  9.73809224e-01  9.87841441e-01  9.88889539e-01  9.91695982e-01
  9.93454328e-01  9.96260771e-01  9.97308869e-01  9.98173286e-01
  9.99288573e-01  1.00468010e+00  1.00659262e+00  1.00748654e+00
  1.01029299e+00  1.01572476e+00  1.01590588e+00  1.01612889e+00
  1.01871232e+00  1.02937945e+00  1.02993809e+00  1.03098619e+00
  1.03555098e+00  1.03659908e+00  1.03835742e+00  1.04221196e+00
  1.04501841e+00  1.05327868e+00  1.05905062e+00  1.06466351e+00
  1.07493623e+00  1.07869573e+00  1.08150217e+00  1.08711506e+00
  1.09272794e+00  1.10676016e+00  1.11517949e+00  1.12013605e+00
  1.14867570e+00  1.26711939e+00  1.79238476e+00]

  UserWarning,

2022-10-31 11:02:46,139:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-9.10082179e-01 -8.76442440e-01 -6.51369554e-01 -4.90775106e-01
 -4.79369754e-01 -4.45968109e-01 -3.06507584e-01 -2.58435323e-01
 -2.57004185e-01 -2.09219816e-01 -1.27774678e-01 -1.25479223e-01
 -8.39942577e-02 -6.98246027e-02 -5.73604296e-02 -5.71414462e-02
 -5.71268388e-02 -5.62974241e-02 -4.88736950e-02 -4.81000726e-02
 -3.70698829e-02 -6.41915072e-03 -1.45109443e-03  1.04805353e-02
  1.53734815e-02  1.96319808e-02  2.40042139e-02  2.98284200e-02
  3.30029091e-02  3.78912291e-02  4.32433602e-02  4.94702187e-02
  5.30788177e-02  5.64074482e-02  6.54687187e-02  7.44448897e-02
  7.44691529e-02  7.50332373e-02  7.81052496e-02  8.84642920e-02
  8.84997878e-02  9.00874549e-02  9.51059380e-02  9.61369829e-02
  1.04895385e-01  1.05634312e-01  1.10472625e-01  1.12867561e-01
  1.14671553e-01  1.15773107e-01  1.16673620e-01  1.16856565e-01
  1.16934592e-01  1.22102841e-01  1.23962118e-01  1.26808635e-01
  1.26953660e-01  1.27061672e-01  1.29673178e-01  1.31388443e-01
  1.32517629e-01  1.34121819e-01  1.34399149e-01  1.41279175e-01
  1.45414363e-01  1.51085922e-01  1.55149084e-01  1.56179816e-01
  1.57270787e-01  1.59267562e-01  1.60744311e-01  1.63172309e-01
  1.63336905e-01  1.68464699e-01  1.70934300e-01  1.74295135e-01
  1.76049419e-01  1.76257856e-01  1.76549268e-01  1.76718098e-01
  1.77169396e-01  1.78915401e-01  1.81948277e-01  1.84593209e-01
  1.84884597e-01  1.85486775e-01  1.85539561e-01  1.85871786e-01
  1.86010073e-01  1.86292942e-01  1.87720815e-01  1.91192695e-01
  1.91792458e-01  1.94455565e-01  1.97451682e-01  1.99367840e-01
  2.00744171e-01  2.04072098e-01  2.04176084e-01  2.07124835e-01
  2.08171342e-01  2.08607485e-01  2.10578272e-01  2.11590698e-01
  2.14704516e-01  2.16148003e-01  2.16722727e-01  2.16792470e-01
  2.18097259e-01  2.18160792e-01  2.18175545e-01  2.19766300e-01
  2.20240707e-01  2.20408271e-01  2.21063447e-01  2.24311243e-01
  2.25285944e-01  2.26907754e-01  2.27965259e-01  2.28099471e-01
  2.29043367e-01  2.30094379e-01  2.30564269e-01  2.31459744e-01
  2.32951068e-01  2.34258276e-01  2.35346289e-01  2.35882191e-01
  2.37270388e-01  2.37512499e-01  2.37853024e-01  2.39374829e-01
  2.39853790e-01  2.39879231e-01  2.40275497e-01  2.41255599e-01
  2.44225751e-01  2.44267018e-01  2.45887616e-01  2.46442796e-01
  2.47200979e-01  2.47372906e-01  2.47953258e-01  2.48136477e-01
  2.48630261e-01  2.50792110e-01  2.50998315e-01  2.51003904e-01
  2.51870930e-01  2.52227932e-01  2.52993016e-01  2.53964011e-01
  2.54119273e-01  2.55973007e-01  2.56315863e-01  2.56852921e-01
  2.58170772e-01  2.59796595e-01  2.60509853e-01  2.60559043e-01
  2.60840408e-01  2.62923638e-01  2.64103150e-01  2.64639774e-01
  2.64788389e-01  2.67505226e-01  2.67615868e-01  2.67938329e-01
  2.69349728e-01  2.70294401e-01  2.70977682e-01  2.73312633e-01
  2.73724578e-01  2.73954133e-01  2.74538126e-01  2.75983218e-01
  2.76693474e-01  2.78883743e-01  2.80854991e-01  2.81287381e-01
  2.81679738e-01  2.82458846e-01  2.83850330e-01  2.86244548e-01
  2.86675395e-01  2.91169757e-01  2.91316036e-01  2.93840632e-01
  2.93847759e-01  2.93880819e-01  2.93927255e-01  2.95304432e-01
  2.97792696e-01  3.00018640e-01  3.00383726e-01  3.01899722e-01
  3.03877865e-01  3.04423491e-01  3.04896552e-01  3.05600291e-01
  3.06218992e-01  3.09109131e-01  3.09116492e-01  3.09432319e-01
  3.10816547e-01  3.10952955e-01  3.11444071e-01  3.12328633e-01
  3.12417150e-01  3.13901707e-01  3.15930979e-01  3.19487909e-01
  3.19974368e-01  3.24049106e-01  3.25459022e-01  3.25495671e-01
  3.25952029e-01  3.26131096e-01  3.27043285e-01  3.27591021e-01
  3.27695847e-01  3.28150473e-01  3.28221522e-01  3.29335349e-01
  3.29702472e-01  3.30642112e-01  3.31105261e-01  3.32284587e-01
  3.32740407e-01  3.32986824e-01  3.33849893e-01  3.34014254e-01
  3.34346672e-01  3.39647677e-01  3.40555013e-01  3.41279859e-01
  3.42328138e-01  3.45509071e-01  3.45710239e-01  3.47163081e-01
  3.47436065e-01  3.48315768e-01  3.48522513e-01  3.48749652e-01
  3.52517614e-01  3.53863718e-01  3.54913692e-01  3.55414182e-01
  3.57041748e-01  3.58914045e-01  3.61705984e-01  3.62343220e-01
  3.64624522e-01  3.64806357e-01  3.65220435e-01  3.66363573e-01
  3.66554198e-01  3.71437909e-01  3.74653692e-01  3.74739360e-01
  3.75263511e-01  3.76826667e-01  3.77980602e-01  3.78436757e-01
  3.79301157e-01  3.82491400e-01  3.83898095e-01  3.84576903e-01
  3.86144281e-01  3.87916285e-01  3.89469855e-01  3.90896075e-01
  3.95244663e-01  3.96446188e-01  3.98338453e-01  3.99127171e-01
  3.99164984e-01  4.01975245e-01  4.02710070e-01  4.04382964e-01
  4.08520327e-01  4.09352780e-01  4.09789315e-01  4.14443717e-01
  4.14664076e-01  4.15565120e-01  4.15647314e-01  4.18415332e-01
  4.23793138e-01  4.24625591e-01  4.25768970e-01  4.32195332e-01
  4.32199811e-01  4.32419255e-01  4.32496300e-01  4.32681749e-01
  4.33975012e-01  4.34807465e-01  4.37525416e-01  4.41871868e-01
  4.47670460e-01  4.49245521e-01  4.49836014e-01  4.51481912e-01
  4.51532866e-01  4.52711929e-01  4.52834924e-01  4.53268338e-01
  4.60438240e-01  4.61199884e-01  4.64225239e-01  4.66438565e-01
  4.69357595e-01  4.70960543e-01  4.74664675e-01  4.76361477e-01
  4.84948341e-01  4.85211803e-01  4.87029894e-01  4.93852895e-01
  4.98381992e-01  5.01498430e-01  5.02642969e-01  5.08054651e-01
  5.09373342e-01  5.10268766e-01  5.10578427e-01  5.11504888e-01
  5.14223984e-01  5.15260612e-01  5.20744480e-01  5.20854309e-01
  5.29556918e-01  5.36942133e-01  5.38563489e-01  5.39117089e-01
  5.43880213e-01  5.45812497e-01  5.48194742e-01  5.55694960e-01
  5.58966670e-01  5.58978318e-01  5.59293029e-01  5.60990566e-01
  5.61216127e-01  5.70284414e-01  5.75692916e-01  5.77907802e-01
  5.80832079e-01  5.81008293e-01  5.85323375e-01  5.85557225e-01
  5.89186405e-01  5.90648162e-01  5.96104890e-01  6.01472036e-01
  6.03140586e-01  6.03728279e-01  6.05920973e-01  6.07557101e-01
  6.08061333e-01  6.11220598e-01  6.23677067e-01  6.25870302e-01
  6.31375658e-01  6.32089151e-01  6.37857847e-01  6.39093747e-01
  6.52094166e-01  6.56830343e-01  6.58215998e-01  6.59110205e-01
  6.61921280e-01  6.63312532e-01  6.65587578e-01  6.66202706e-01
  6.66597798e-01  6.67012217e-01  6.70603343e-01  6.70609773e-01
  6.70870942e-01  6.72468945e-01  6.73494406e-01  6.73889874e-01
  6.75929457e-01  6.77194091e-01  6.82618397e-01  6.84096036e-01
  6.84659591e-01  6.87375965e-01  6.91234690e-01  6.92466902e-01
  6.92800271e-01  6.92832693e-01  6.96668326e-01  6.97923630e-01
  7.00842390e-01  7.03014567e-01  7.04157947e-01  7.04832169e-01
  7.08105504e-01  7.10222996e-01  7.12830650e-01  7.14601915e-01
  7.17370180e-01  7.17507168e-01  7.18287378e-01  7.19366803e-01
  7.19409059e-01  7.22971418e-01  7.23829406e-01  7.27689042e-01
  7.28469252e-01  7.33194398e-01  7.33877755e-01  7.34148421e-01
  7.36478949e-01  7.37818673e-01  7.38285335e-01  7.38651126e-01
  7.40302633e-01  7.40817786e-01  7.42543819e-01  7.43376272e-01
  7.45859555e-01  7.48333996e-01  7.48467209e-01  7.48833000e-01
  7.49334537e-01  7.53923937e-01  7.56306400e-01  7.58649083e-01
  7.63740020e-01  7.64105811e-01  7.64983590e-01  7.65760576e-01
  7.66223303e-01  7.68830957e-01  7.70031551e-01  7.76666011e-01
  7.77206445e-01  7.78170897e-01  7.80910371e-01  7.84103768e-01
  7.86587051e-01  7.89194705e-01  7.91263569e-01  7.91677988e-01
  7.94285642e-01  7.96064227e-01  7.99376579e-01  7.99709948e-01
  8.01859862e-01  8.01909030e-01  8.14126064e-01  8.14649390e-01
  8.14935900e-01  8.14982759e-01  8.17132673e-01  8.19273665e-01
  8.21539701e-01  8.25164633e-01  8.27314547e-01  8.29922201e-01
  8.31573708e-01  8.32405484e-01  8.41008929e-01  8.42266733e-01
  8.42518454e-01  8.47678295e-01  8.48358948e-01  8.50285949e-01
  8.50619318e-01  8.52769232e-01  8.53086797e-01  8.53344630e-01
  8.57860169e-01  8.60904358e-01  8.64396322e-01  8.65558760e-01
  8.70302650e-01  8.70983066e-01  8.74476000e-01  8.77775172e-01
  8.83314854e-01  8.87281390e-01  8.88405791e-01  8.93496728e-01
  8.94147662e-01  8.94525953e-01  8.96437751e-01  8.98561474e-01
  9.01442210e-01  9.08769539e-01  9.13860476e-01  9.16405581e-01
  9.17270456e-01  9.18488687e-01  9.18951413e-01  9.21581183e-01
  9.21995602e-01  9.24375719e-01  9.26150920e-01  9.27086539e-01
  9.32074310e-01  9.32510845e-01  9.34198033e-01  9.34224224e-01
  9.37165247e-01  9.37601782e-01  9.38070907e-01  9.41892688e-01
  9.44406098e-01  9.52541224e-01  9.52874593e-01  9.53946698e-01
  9.68147404e-01  9.69711575e-01  9.72904972e-01  9.74951720e-01
  9.77021445e-01  9.78329278e-01  9.80042657e-01  9.81488843e-01
  9.83086846e-01  9.84532063e-01  9.93268720e-01  9.93602089e-01
  9.99573952e-01  1.00073977e+00  1.00378396e+00  1.00466489e+00
  1.00854153e+00  1.00968491e+00  1.00975583e+00  1.01401902e+00
  1.01439602e+00  1.01554986e+00  1.01872341e+00  1.02414771e+00
  1.02424304e+00  1.03908715e+00  1.03930132e+00  1.03942052e+00
  1.04290176e+00  1.04417809e+00  1.04960240e+00  1.05274192e+00
  1.05469333e+00  1.05978427e+00  1.06122949e+00  1.06147146e+00
  1.06487521e+00  1.07165333e+00  1.07422463e+00  1.07505708e+00
  1.07879102e+00  1.08509965e+00  1.08875146e+00  1.08951474e+00
  1.09542083e+00  1.09967931e+00  1.10762326e+00  1.11069364e+00
  1.11823668e+00  1.13105739e+00  1.14588539e+00  1.15456066e+00
  1.16871450e+00  1.48164603e+00  1.81039475e+00]

  UserWarning,

2022-10-31 11:02:46,186:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.93687547 -0.57463752 -0.51922338 -0.47493392 -0.46309081 -0.45315214
 -0.39291297 -0.3665535  -0.25490687 -0.20215651 -0.18373131 -0.18300332
 -0.16366255 -0.14410256 -0.14219383 -0.12923542 -0.12406035 -0.08613412
 -0.08206558 -0.07645082 -0.06993387 -0.06588381 -0.05704008 -0.05139737
 -0.04031512 -0.03228783 -0.01690983 -0.01427324 -0.01115054 -0.00866479
  0.00614432  0.02008152  0.02384665  0.02624864  0.02651848  0.03772956
  0.03808797  0.03909089  0.04073963  0.05627787  0.06475016  0.06614454
  0.07970702  0.08134484  0.0892537   0.09518123  0.09892181  0.09900897
  0.10173716  0.10299344  0.11278196  0.11420738  0.11566032  0.11591456
  0.12299947  0.12588834  0.12839433  0.13120949  0.13185554  0.132422
  0.13673626  0.13872544  0.14003073  0.14388185  0.14662132  0.14946086
  0.14963948  0.15032692  0.15081089  0.15464864  0.1549788   0.15771652
  0.15908046  0.16289163  0.16395212  0.1674637   0.16774583  0.1679354
  0.17061887  0.17391724  0.17427189  0.17502902  0.17591669  0.17922098
  0.17972273  0.18292037  0.18373878  0.18522646  0.1859498   0.18705743
  0.1880066   0.18838916  0.18907514  0.19143859  0.19173465  0.19193416
  0.19243999  0.19345728  0.19480008  0.19483103  0.1951397   0.19696977
  0.19908814  0.19933784  0.19945833  0.20104082  0.2027409   0.20320811
  0.20329915  0.20352445  0.20421378  0.20516236  0.20844476  0.20874853
  0.20876623  0.20878069  0.20899532  0.20998347  0.21011786  0.21256466
  0.21464747  0.21612684  0.21768626  0.22080825  0.22082103  0.22158174
  0.22177659  0.22217168  0.22278359  0.22402137  0.22442342  0.22588032
  0.22675452  0.22789419  0.22913772  0.23027481  0.2310233   0.23107341
  0.23111661  0.23287359  0.23395225  0.23410146  0.23575416  0.23617825
  0.23986351  0.23997412  0.24250332  0.24433212  0.24484054  0.2450555
  0.24688572  0.24703375  0.25083392  0.25106455  0.25206723  0.2526744
  0.25330145  0.25501504  0.25645289  0.25646188  0.2571863   0.25761386
  0.25853405  0.25875356  0.25923657  0.25999286  0.26049908  0.26090392
  0.26210316  0.26214047  0.26247688  0.26258974  0.26412765  0.26431299
  0.26715167  0.26768432  0.26888504  0.26909586  0.26912027  0.26997618
  0.27116566  0.27191656  0.27424201  0.27465964  0.27495939  0.27663723
  0.277516    0.27801936  0.27828027  0.28063221  0.28097016  0.28167238
  0.2823524   0.28249386  0.28257745  0.2835769   0.2838643   0.28501786
  0.28556227  0.2860138   0.28650234  0.29070613  0.29132512  0.29300914
  0.29303938  0.29394808  0.29486822  0.29676584  0.29752251  0.29929434
  0.29958333  0.30143966  0.30201225  0.30498091  0.30604252  0.30712537
  0.30858909  0.3120862   0.31221678  0.31231721  0.31340638  0.31397753
  0.31445252  0.31667542  0.31844591  0.31935141  0.3210057   0.32137931
  0.32285485  0.32374877  0.32449623  0.32714471  0.32748265  0.32760694
  0.3297994   0.33069573  0.33375074  0.33539831  0.33898414  0.33900259
  0.34095021  0.34351837  0.34393799  0.34440658  0.34558947  0.34756179
  0.34808886  0.34943064  0.34996657  0.35168927  0.35196523  0.35440411
  0.35586536  0.35792045  0.35949367  0.35970538  0.36027201  0.36091307
  0.36661294  0.36743811  0.36787334  0.37038256  0.37057053  0.37087044
  0.37118734  0.37155929  0.37181556  0.3731549   0.37370004  0.37419945
  0.37454797  0.37516718  0.37529537  0.37796574  0.37814089  0.37843656
  0.38118543  0.38157566  0.3821653   0.38229305  0.38230091  0.38335254
  0.38433648  0.38448664  0.38683933  0.38752607  0.38954571  0.38989535
  0.39029107  0.39336437  0.39356667  0.39504228  0.39560507  0.3956627
  0.39898952  0.39934895  0.40476731  0.40610873  0.40702195  0.412848
  0.41328719  0.41727273  0.42356376  0.42500864  0.42527907  0.42529769
  0.42587063  0.42634239  0.42720869  0.42733996  0.42799052  0.42964978
  0.4304499   0.4378283   0.43847003  0.43951242  0.44076365  0.442902
  0.44782862  0.44806946  0.45239758  0.45581683  0.45625545  0.46014096
  0.46440271  0.47045392  0.4726847   0.47314824  0.47377739  0.4740219
  0.47487536  0.47625905  0.47661633  0.48017126  0.48271706  0.48779376
  0.48865595  0.49456889  0.49586117  0.49962344  0.50022056  0.5007041
  0.50506462  0.50885816  0.5101982   0.51310885  0.51479563  0.51679188
  0.51690435  0.51755734  0.53626878  0.5408941   0.54151518  0.54866911
  0.56043101  0.56463211  0.5655064   0.56602623  0.5667762   0.57499635
  0.58010525  0.58236872  0.58303851  0.58541502  0.58611099  0.59385555
  0.61321238  0.61680902  0.61739956  0.62131438  0.62330489  0.63300927
  0.63459245  0.63463335  0.63954402  0.64114303  0.64728507  0.64738186
  0.65424444  0.65456255  0.65670719  0.6597867   0.66294289  0.66540978
  0.66783612  0.66945984  0.66965947  0.67066539  0.6735099   0.67393698
  0.67509308  0.67582149  0.68199057  0.68238349  0.68700699  0.68906065
  0.68971015  0.68981001  0.69125736  0.69196014  0.69534339  0.69781028
  0.69837533  0.70209404  0.7050374   0.70689526  0.70768101  0.71154365
  0.71742033  0.71806059  0.71853024  0.72107712  0.72211065  0.72372002
  0.72412091  0.7277439   0.73021078  0.73040284  0.73075321  0.73426084
  0.73584402  0.73691979  0.73732624  0.73821137  0.73908437  0.74278805
  0.74641103  0.74697608  0.74801575  0.74872262  0.75046109  0.75123822
  0.75210734  0.75391283  0.75856122  0.76167116  0.76261128  0.76666135
  0.77071141  0.7720273   0.77855146  0.77855501  0.77881153  0.78235487
  0.7828616   0.78533207  0.78613007  0.78691166  0.78849484  0.78855791
  0.79173519  0.79380853  0.79578525  0.79757895  0.80191172  0.80376912
  0.80388537  0.80772702  0.81279979  0.81603556  0.81690835  0.82008563
  0.82095841  0.82132238  0.82336222  0.82353737  0.82413569  0.83146235
  0.83177088  0.8320274   0.83223581  0.83457229  0.83551241  0.83597542
  0.83685092  0.83913989  0.84012752  0.84301422  0.84361254  0.844386
  0.85273537  0.854139    0.85820617  0.85887267  0.85998793  0.86037784
  0.86463632  0.86750293  0.86868638  0.86989193  0.86995151  0.87273644
  0.87344869  0.87618818  0.8767865   0.88083657  0.88185927  0.88317328
  0.88407262  0.88590933  0.88609218  0.88893669  0.88950174  0.89703682
  0.89993834  0.90033595  0.90048856  0.90108688  0.90387562  0.90930363
  0.90986868  0.91198649  0.91323707  0.91415798  0.91728713  0.91994608
  0.92102674  0.9250768   0.93317693  0.93476011  0.93607211  0.93753745
  0.9418421   0.94545168  0.94620262  0.94631924  0.95313938  0.95835281
  0.95850001  0.96395008  0.97061962  0.97323708  0.97709795  0.97735255
  0.97743975  0.97772762  0.9784938   0.98177768  0.98522942  0.99108336
  0.99392787  0.99449292  0.99797793  1.002028    1.00263523  1.00323355
  1.00601344  1.00607806  1.01417819  1.01474323  1.02470465  1.0262951
  1.02632837  1.03037844  1.03499355  1.04252863  1.04464088  1.05872888
  1.06577033  1.06739405  1.07270924  1.07549417  1.0835943   1.08764436
  1.08919157  1.09444719  1.09480437  1.09979455  1.10327957  1.10635564
  1.12004487  1.12126134  1.12814499  1.13685235  1.14272152  1.14708204
  1.21562265]

  UserWarning,

2022-10-31 11:02:46,217:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.64268827 -0.99660573 -0.37418449 -0.35382033 -0.34653466 -0.33863599
 -0.33595239 -0.20650815 -0.18412706 -0.1839415  -0.18038059 -0.16388032
 -0.15688743 -0.14634985 -0.13842736 -0.10861606 -0.09143959 -0.08487879
 -0.08029366 -0.07052987 -0.05951303 -0.04808789 -0.03746261 -0.00784441
 -0.00341475  0.01018095  0.01149039  0.01292629  0.01615731  0.0229153
  0.02982826  0.03278635  0.0471334   0.05095456  0.05726271  0.05806863
  0.06216715  0.06242279  0.06565881  0.06650544  0.07104469  0.07259984
  0.07835931  0.08522284  0.08636039  0.09098011  0.09155626  0.0921569
  0.09364884  0.09589319  0.09992509  0.10006206  0.10279672  0.10563484
  0.10868046  0.10969085  0.11026788  0.11027234  0.11276478  0.12123106
  0.12237177  0.1257494   0.13057344  0.13190587  0.13231517  0.13359399
  0.13578725  0.14025139  0.14086721  0.14128372  0.14183909  0.1432671
  0.14340858  0.14658544  0.14735128  0.15074637  0.15130812  0.1531732
  0.15368653  0.15587814  0.15750649  0.15875537  0.15916553  0.16641298
  0.16711654  0.16845114  0.16879311  0.16885085  0.16922277  0.17018961
  0.17218243  0.17223853  0.17326673  0.17345973  0.1737407   0.17483492
  0.17832573  0.17892415  0.17965511  0.17979707  0.18084931  0.18215897
  0.18403179  0.1851698   0.18536633  0.18860544  0.18881668  0.18934409
  0.1895704   0.18973514  0.19450452  0.19543712  0.19724993  0.1982141
  0.19874251  0.19899273  0.19926871  0.19930897  0.19933693  0.19997089
  0.20147546  0.20314826  0.20362435  0.20400156  0.20407049  0.20435485
  0.20501166  0.20599422  0.20617404  0.20818475  0.20917812  0.20921391
  0.21014175  0.21249576  0.21329529  0.21344625  0.21419091  0.21433616
  0.21465026  0.21550668  0.21614975  0.21769869  0.21850674  0.21906566
  0.21931166  0.22384613  0.2247167   0.2255513   0.22805964  0.22858415
  0.23021651  0.23345796  0.23613891  0.23685532  0.23814197  0.23831332
  0.23967179  0.24003001  0.24027181  0.24042091  0.2411621   0.24409878
  0.24446539  0.24480985  0.24499584  0.24788623  0.24919843  0.24928686
  0.24951792  0.25156432  0.2516548   0.25370631  0.25376421  0.26041363
  0.26343468  0.26410148  0.26468318  0.26505606  0.26539328  0.26613418
  0.26671615  0.26798206  0.26872285  0.27069137  0.27120465  0.27212904
  0.27285657  0.27391569  0.27439489  0.27523302  0.27594006  0.27602029
  0.27759682  0.27874716  0.27962204  0.28012252  0.2805552   0.28065414
  0.28151907  0.28207002  0.28234514  0.28387559  0.28708339  0.28741704
  0.28776993  0.28919851  0.29090577  0.29199691  0.29208529  0.29287357
  0.29303806  0.29345724  0.29435646  0.29469348  0.29562281  0.2959253
  0.2978836   0.29838145  0.2987252   0.29918153  0.29931913  0.30011221
  0.30138686  0.30164303  0.30220324  0.30532614  0.30571588  0.30625727
  0.30636015  0.30894257  0.30902219  0.30919243  0.31089291  0.31119037
  0.31162057  0.31286652  0.31296897  0.31318769  0.32207364  0.32219205
  0.32264695  0.32283333  0.32338318  0.32773013  0.32954971  0.33000444
  0.33042978  0.33232236  0.33307123  0.33340693  0.33575127  0.33582213
  0.33626416  0.33824833  0.34152512  0.3416588   0.34514489  0.34720432
  0.34905184  0.34957304  0.35077629  0.35091268  0.35330285  0.35347433
  0.35355262  0.35399636  0.35610949  0.35717186  0.36051236  0.36103376
  0.36231302  0.36381533  0.36430033  0.36499245  0.36501622  0.36597971
  0.36672059  0.36763758  0.36786968  0.36973117  0.37158977  0.37217093
  0.37409585  0.37474287  0.37494699  0.37894945  0.37954654  0.3797344
  0.38029733  0.38197432  0.38268926  0.38394733  0.38429409  0.38613621
  0.38648298  0.38868605  0.39221897  0.39362703  0.39376326  0.39664027
  0.39816349  0.39832159  0.39949677  0.40079737  0.40118807  0.40325766
  0.40764093  0.40874932  0.40933657  0.4094791   0.4111438   0.4119912
  0.41449895  0.41469997  0.41678622  0.41735672  0.42049718  0.42536018
  0.42821728  0.43533668  0.43560817  0.4466834   0.44899285  0.44913248
  0.45143867  0.45679018  0.45856241  0.45874301  0.46291     0.46414553
  0.46534341  0.46894201  0.47083801  0.47119947  0.47798516  0.47877428
  0.47964937  0.48169213  0.48213385  0.48313621  0.4854332   0.49030428
  0.4966217   0.50071355  0.50847935  0.51244645  0.51509427  0.52062463
  0.52164855  0.52172006  0.52204792  0.52402979  0.52762382  0.53616908
  0.54064842  0.54257149  0.54388409  0.54398474  0.54713701  0.54725056
  0.54838121  0.55131742  0.55557698  0.55746859  0.56187086  0.5646443
  0.56812704  0.56903575  0.56938302  0.57187222  0.57804795  0.5921946
  0.5939659   0.59596096  0.59800876  0.60000632  0.60454872  0.60748996
  0.61765951  0.63661439  0.64409947  0.65347568  0.65556927  0.65589651
  0.65826063  0.66030799  0.66365495  0.66381108  0.66626802  0.6716075
  0.6767414   0.68371037  0.68400159  0.68664672  0.68874031  0.6922434
  0.69347903  0.69557929  0.70160376  0.7056016   0.70795981  0.7125168
  0.713691    0.71507904  0.7156103   0.71805993  0.72002158  0.72034902
  0.72455648  0.72475803  0.72494828  0.72545851  0.73138879  0.73877265
  0.74086623  0.74301388  0.746452    0.74697634  0.74779765  0.74825009
  0.74910804  0.75062201  0.75298881  0.75374324  0.75508239  0.75536073
  0.75777375  0.76009945  0.76246625  0.76285805  0.76455983  0.77025562
  0.77055564  0.77194369  0.77284571  0.778776    0.78333398  0.78351472
  0.78379305  0.78627053  0.78643818  0.78825344  0.78853177  0.78921318
  0.79481423  0.79602909  0.79773088  0.79898797  0.80056543  0.80076781
  0.80123396  0.80457     0.80481245  0.80647174  0.80748665  0.80829112
  0.815166    0.81651666  0.81822118  0.82263297  0.82328788  0.82644153
  0.82911644  0.83016535  0.83118786  0.83354705  0.83564064  0.83828577
  0.84511808  0.84539641  0.84585896  0.84815501  0.84945442  0.85013513
  0.85459552  0.85791078  0.85859767  0.86059984  0.8616346   0.86617337
  0.87105061  0.8725931   0.87856745  0.87945475  0.88132605  0.8880449
  0.89278362  0.89628309  0.89752234  0.89786961  0.90226106  0.90362091
  0.90847438  0.91647722  0.91682449  0.91873333  0.9197667   0.92506629
  0.92595466  0.92599964  0.9300908   0.93063827  0.93069338  0.93216798
  0.93407758  0.93485148  0.93517679  0.93617297  0.9381907   0.94150535
  0.95268519  0.95606383  0.95662591  0.95742391  0.96358487  0.96386442
  0.96860314  0.96895041  0.97334186  0.97842785  0.98958214  0.99229674
  1.0014719   1.00686017  1.00774747  1.00896922  1.01159889  1.01633761
  1.01847854  1.02077405  1.02107633  1.0228984   1.02581505  1.03010907
  1.03209406  1.03529249  1.04223546  1.04432523  1.04780687  1.05728431
  1.0585414   1.05898609  1.06372481  1.07320226  1.08097791  1.0852642
  1.08571663  1.08624865  1.09519407  1.10082009  1.10467151  1.11066732
  1.11070964  1.12362639  1.1250259   1.12796273  1.13579519  1.13784255
  1.14258127  1.14484504  1.15205871  1.156345    1.16597259  1.17451668
  1.18391967  1.18482737  1.20208471  1.22283723  1.23231467  1.32795596
  1.46032158  3.07740705]

  UserWarning,

2022-10-31 11:02:46,740:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:46,740:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:47,085:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.64014118 -0.36876024 -0.35803047 -0.32756465 -0.29310233 -0.2832101
 -0.27004623 -0.24221667 -0.23116187 -0.21133078 -0.20898645 -0.19003144
 -0.17295199 -0.16415602 -0.12811308 -0.12213404 -0.10560491 -0.07636008
 -0.07578691 -0.05809929 -0.04760263 -0.04055405 -0.03154922 -0.03142498
 -0.0263632  -0.02449662 -0.01547774 -0.01204083 -0.01107757 -0.0102332
 -0.0077892   0.00627323  0.00861486  0.00997257  0.01434047  0.02325376
  0.02872651  0.03507737  0.05022481  0.05070224  0.05557179  0.05860642
  0.05947838  0.0621791   0.06226546  0.07273602  0.07353324  0.07794524
  0.08112785  0.08569672  0.09177711  0.09349709  0.09376251  0.09616806
  0.09636823  0.09643875  0.09736742  0.10086329  0.10238432  0.10608444
  0.11314896  0.11452871  0.11479156  0.11619132  0.11703518  0.11721816
  0.11808974  0.12238853  0.12375111  0.12391745  0.1288741   0.13016737
  0.13034572  0.13411175  0.13580306  0.13785956  0.13842318  0.13890027
  0.13909509  0.13963623  0.14048787  0.1411126   0.14131727  0.14225198
  0.14295473  0.14316799  0.14428865  0.14823777  0.15014493  0.15071714
  0.15084461  0.15408397  0.15413078  0.15458095  0.15463138  0.15587204
  0.15594025  0.15745996  0.15998464  0.16027075  0.16245027  0.16449597
  0.16661019  0.17286483  0.17474409  0.17706508  0.18031069  0.18151894
  0.18204969  0.1833091   0.18700583  0.18832139  0.18849844  0.19088117
  0.19200624  0.19289943  0.19333434  0.19359753  0.1958921   0.19610274
  0.19620904  0.19625731  0.19944809  0.20159147  0.20338311  0.20350458
  0.2036199   0.20399136  0.20413792  0.20442348  0.20465545  0.20645976
  0.20980866  0.2099405   0.21016552  0.21034925  0.21119633  0.21137359
  0.21174173  0.21195604  0.21206415  0.21259812  0.21286728  0.21307874
  0.2136321   0.21389152  0.2139773   0.21415366  0.21418988  0.21423697
  0.21491199  0.21517178  0.21611843  0.21683153  0.21819511  0.21830473
  0.218681    0.22004232  0.22010168  0.22050979  0.22060266  0.22170289
  0.22392158  0.22421365  0.22468643  0.22513857  0.22567992  0.22727347
  0.22756002  0.22806399  0.23006441  0.23057156  0.23196603  0.23274739
  0.23363463  0.235747    0.2358274   0.23596777  0.24005912  0.24059217
  0.24065036  0.24087718  0.24130887  0.24150358  0.24387915  0.24523751
  0.245521    0.24615135  0.24786879  0.24980821  0.25043642  0.2513165
  0.25157926  0.25382436  0.254507    0.25842344  0.25990254  0.26059308
  0.26145186  0.26227032  0.26266774  0.26501612  0.26539197  0.26647899
  0.2666396   0.26690676  0.26878468  0.26879058  0.26991844  0.27187798
  0.27239393  0.27604717  0.27620344  0.27622671  0.27633202  0.27680659
  0.27765741  0.27876964  0.27939954  0.28020543  0.28065526  0.28283282
  0.28284908  0.28410353  0.28516901  0.28715759  0.28804301  0.28807114
  0.29053456  0.2908448   0.2931742   0.29324354  0.29331571  0.2936626
  0.29405613  0.29549802  0.2976909   0.30212265  0.3046885   0.30834668
  0.31274573  0.31453912  0.31520612  0.3163365   0.31686414  0.31867459
  0.32055125  0.32083154  0.3220544   0.32214607  0.32250107  0.32270164
  0.3237012   0.32423836  0.32531381  0.32678729  0.32771733  0.32863526
  0.32878073  0.32901502  0.32922701  0.32934198  0.32952029  0.33087654
  0.3309439   0.33153928  0.33161257  0.3316482   0.33291083  0.33428336
  0.33473251  0.33898679  0.3411482   0.3426739   0.34400607  0.34461499
  0.34496786  0.34603599  0.34636101  0.34715022  0.34912944  0.34997482
  0.35004811  0.35102364  0.3530014   0.35373522  0.3543877   0.35586834
  0.3561025   0.35742233  0.35815641  0.35860192  0.35972663  0.36010472
  0.36033291  0.36705221  0.36892401  0.37050488  0.37284467  0.37587239
  0.37907639  0.3804239   0.39075862  0.39459331  0.39995061  0.40076387
  0.40451787  0.40618869  0.4090917   0.41047634  0.41107336  0.41181687
  0.41182519  0.41421578  0.41827827  0.41936189  0.42114598  0.42357735
  0.42462793  0.42657362  0.42831504  0.43038401  0.43200214  0.43394784
  0.43568925  0.43937636  0.43963494  0.44173698  0.44417613  0.45523745
  0.45898657  0.46360002  0.46743605  0.46962924  0.48077211  0.48127325
  0.49739133  0.49801431  0.50316986  0.50553678  0.50685697  0.50899545
  0.51230952  0.51958898  0.52153211  0.5216054   0.52366927  0.52377707
  0.52614177  0.53282318  0.53773696  0.54544256  0.54757161  0.55454055
  0.56316989  0.5639836   0.57079891  0.57513721  0.58593468  0.59199781
  0.5924981   0.59294705  0.59380808  0.59600314  0.59651509  0.60028551
  0.60106354  0.60457851  0.61222957  0.61715835  0.61855321  0.61863774
  0.62692456  0.628688    0.63135153  0.63974932  0.64712354  0.64843188
  0.64878709  0.6512883   0.65182172  0.65550883  0.65866252  0.65919593
  0.66288304  0.66830781  0.67394437  0.67498443  0.67830788  0.68131858
  0.68565817  0.6886928   0.68943757  0.69237991  0.69415218  0.69584815
  0.69606702  0.69671949  0.69975413  0.70170443  0.70196747  0.70344123
  0.7043408   0.70712834  0.70737191  0.70778082  0.70788552  0.71515504
  0.71560103  0.71574073  0.71603386  0.71884214  0.72097301  0.72252925
  0.72556388  0.72925099  0.7329381   0.73662521  0.73857551  0.74031232
  0.74096479  0.74154588  0.74218574  0.74258428  0.74399942  0.74483707
  0.74720389  0.74919759  0.75068213  0.75874786  0.76100754  0.76144293
  0.76331635  0.76973589  0.76980918  0.77346265  0.77349629  0.77620736
  0.7771834   0.77997135  0.78087051  0.78282081  0.78302968  0.78348043
  0.79012173  0.79193183  0.7946269   0.79492743  0.79861454  0.80435369
  0.80494346  0.81231768  0.8132635   0.81489213  0.823379    0.82504969
  0.82612378  0.82706611  0.82789915  0.83262664  0.83527337  0.83936561
  0.84063777  0.84181454  0.84550165  0.84673983  0.84715411  0.84749375
  0.84831971  0.84897831  0.84918876  0.84993353  0.85287587  0.85656298
  0.86025008  0.86038566  0.86040654  0.86205973  0.86477023  0.86558965
  0.8676243   0.86778076  0.87019875  0.87296387  0.87868563  0.87952327
  0.88056857  0.88075684  0.88229944  0.88252919  0.88605984  0.88614301
  0.88730583  0.88974695  0.8926639   0.89704787  0.89837779  0.90164592
  0.90533303  0.90539661  0.90691153  0.90803562  0.90818249  0.91202606
  0.91217384  0.91548342  0.92008147  0.93114279  0.93414871  0.9363351
  0.94129317  0.94152293  0.94173179  0.94505358  0.94521003  0.94589122
  0.95258425  0.95627136  0.95695255  0.95710901  0.95995847  0.96364557
  0.96448322  0.96609451  0.96733268  0.96801387  0.97061926  0.97463361
  0.9747069   0.97923166  0.98208112  0.98276231  0.98291876  0.98576822
  0.98806969  0.98941877  0.99314244  0.99398009  0.99711747  1.00051666
  1.00420377  1.00504141  1.00669387  1.00789087  1.01157798  1.01610274
  1.01886323  1.02632642  1.03001352  1.03231499  1.03738774  1.03822539
  1.0419125   1.05175526  1.05666093  1.06034804  1.06229373  1.06403515
  1.06772225  1.06937471  1.07140936  1.07509647  1.08247069  1.0861578
  1.09149736  1.09353201  1.10090623  1.10737668  1.11104094  1.1214092
  1.1239078   1.12581222  1.48957267]

  UserWarning,

2022-10-31 11:02:47,087:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.09698041 -0.93489423 -0.67909634 -0.42387626 -0.39510235 -0.32232198
 -0.32052321 -0.31343848 -0.30723898 -0.29863338 -0.29844982 -0.28825512
 -0.26525356 -0.2547263  -0.25049992 -0.21067178 -0.20567348 -0.17996869
 -0.16749471 -0.15594796 -0.14035947 -0.1401346  -0.13194009 -0.09097088
 -0.08324782 -0.07484575 -0.05976917 -0.04816768 -0.02738447 -0.02496643
 -0.02026285 -0.01913742 -0.0142563  -0.00838906  0.01499846  0.01607746
  0.0202345   0.02696049  0.03195615  0.05752003  0.06148725  0.0638901
  0.0643944   0.07215163  0.07328931  0.07336343  0.08275033  0.08344441
  0.08485467  0.09130012  0.10051099  0.10184953  0.10594226  0.10615584
  0.10876492  0.11028439  0.1111815   0.1130144   0.11422519  0.11491497
  0.11687198  0.11751424  0.11768288  0.11812445  0.11831637  0.1226174
  0.12467633  0.12515473  0.12549954  0.12659312  0.12750793  0.12825516
  0.12861861  0.1294524   0.13177067  0.13417375  0.13548645  0.14078839
  0.14450078  0.14473415  0.14612131  0.14706283  0.14725901  0.14760948
  0.14866793  0.15032408  0.15131493  0.15334943  0.15548928  0.15699821
  0.16100481  0.16141921  0.16176893  0.16203958  0.16275774  0.16506976
  0.16761582  0.16930762  0.16949316  0.17055914  0.17099432  0.17163866
  0.17218171  0.17570929  0.1773926   0.17790797  0.17811287  0.17813146
  0.1790638   0.1796334   0.17982133  0.1813103   0.18177585  0.18189765
  0.18337321  0.1837039   0.18475971  0.18527168  0.1863104   0.18642948
  0.18699076  0.18699432  0.18744196  0.18751691  0.18854392  0.18978309
  0.18983044  0.19013202  0.19081167  0.19263177  0.19381546  0.19511597
  0.19540364  0.19705999  0.19780319  0.19870463  0.1998932   0.20154652
  0.20412233  0.2047783   0.20543744  0.20549172  0.2060103   0.20602323
  0.20613605  0.20714065  0.2081876   0.20962398  0.21360719  0.21450951
  0.21513768  0.21556146  0.21821398  0.2184736   0.21865054  0.21909448
  0.21916846  0.2193558   0.21974734  0.22186307  0.2225252   0.22509509
  0.22669453  0.22675357  0.22760598  0.22781627  0.22842715  0.23116979
  0.23339175  0.23382659  0.23629838  0.23697657  0.23805995  0.23807891
  0.2382192   0.24414175  0.24592146  0.24801951  0.2486207   0.252533
  0.25254272  0.25348342  0.25384768  0.25415437  0.25593558  0.25650364
  0.25695171  0.25973528  0.26059681  0.26114593  0.26182845  0.26225311
  0.26275442  0.26348551  0.26449586  0.26480375  0.26483515  0.26488766
  0.26518465  0.26537244  0.26599289  0.26811371  0.26816134  0.26935638
  0.2697778   0.27082433  0.27457814  0.27509456  0.27639758  0.27818368
  0.28003139  0.28111382  0.28113638  0.28132355  0.28276128  0.28465949
  0.28878948  0.29102102  0.29193934  0.29300462  0.29363105  0.29405941
  0.2947156   0.29495334  0.29609868  0.29656239  0.29700237  0.29864126
  0.29943582  0.2998293   0.3002518   0.30192614  0.30363756  0.30602662
  0.30675355  0.30676373  0.30684305  0.30791045  0.30957994  0.30962361
  0.31147214  0.31162814  0.31259754  0.31269889  0.31335885  0.31553536
  0.31581693  0.31599733  0.31708171  0.31743371  0.31980057  0.32082761
  0.32086786  0.32195303  0.32205393  0.32271603  0.32284379  0.32415229
  0.32554082  0.32631643  0.32905341  0.32911543  0.33129189  0.33452791
  0.33591034  0.33686001  0.33709492  0.33738557  0.33757721  0.33764641
  0.33790547  0.33848506  0.34076491  0.34076624  0.34203181  0.34835234
  0.35017834  0.35043704  0.35129162  0.35480486  0.35741871  0.35888212
  0.36230464  0.36671154  0.37246439  0.37288069  0.37390069  0.37968766
  0.38110823  0.38359859  0.38458604  0.38633067  0.39152058  0.39444002
  0.39482102  0.39777956  0.39793951  0.40045757  0.40061453  0.40064265
  0.40250018  0.40307144  0.40417651  0.40468126  0.40561712  0.40804128
  0.41063436  0.41502731  0.41519609  0.4163432   0.41966809  0.41976899
  0.42156036  0.42354095  0.42987075  0.43300018  0.43533093  0.43683755
  0.44295581  0.44660699  0.45170023  0.45530823  0.45793722  0.45959354
  0.46045224  0.46119348  0.46326679  0.46406464  0.46864646  0.47069458
  0.47510243  0.47842858  0.48302029  0.4853531   0.4858726   0.48925995
  0.49140181  0.49179274  0.49412169  0.49535918  0.50489326  0.51038777
  0.5126102   0.51718866  0.51960485  0.52055564  0.52361504  0.52679264
  0.55174061  0.57147832  0.5752527   0.58126139  0.58371876  0.58705361
  0.59323789  0.60139944  0.60182858  0.60266328  0.60577324  0.60614627
  0.60890028  0.60969667  0.61201877  0.61716993  0.61874289  0.62202476
  0.62449276  0.63942632  0.64008524  0.64059497  0.64315958  0.64569431
  0.65255923  0.65704365  0.6582382   0.6602214   0.66135669  0.66447519
  0.66466     0.66503322  0.67127021  0.6719207   0.67325168  0.67383068
  0.67438871  0.67454877  0.67694918  0.6775072   0.67822529  0.68226773
  0.68281296  0.68318617  0.68680942  0.68830957  0.68942316  0.69309969
  0.69566016  0.69611728  0.69621818  0.698169    0.69933668  0.70070526
  0.70189715  0.70245518  0.70722346  0.70813414  0.70869217  0.71087106
  0.71125264  0.71181067  0.71437114  0.71492916  0.71714078  0.71745542
  0.71804766  0.71932951  0.72060813  0.72116616  0.72532632  0.72545306
  0.72709143  0.72740315  0.72844482  0.73139412  0.74152692  0.74243761
  0.7471558   0.75339279  0.75411088  0.75481069  0.75546962  0.75707174
  0.75848721  0.75858811  0.75886759  0.75935211  0.76083435  0.76168061
  0.76426708  0.76586678  0.7665372   0.7678427   0.7679436   0.77012249
  0.77210377  0.77522227  0.77850784  0.79081475  0.79779802  0.80091651
  0.80604584  0.80715351  0.810272    0.8133905   0.81558913  0.81588946
  0.8164081   0.81683122  0.81812986  0.82417861  0.82619888  0.82807611
  0.82898298  0.83119757  0.83210148  0.83515008  0.83535774  0.8379659
  0.83833847  0.84144842  0.84311353  0.84348963  0.84372829  0.84382919
  0.84678709  0.84976809  0.85299135  0.85417944  0.85704945  0.85886638
  0.86016795  0.86041644  0.86291323  0.86952344  0.87289042  0.87576043
  0.87887893  0.88199743  0.88511592  0.89135292  0.8916014   0.90070841
  0.90351736  0.90406684  0.91024248  0.91941938  0.92270305  0.92996297
  0.93213331  0.93502979  0.93827795  0.93837885  0.94149735  0.94452712
  0.95004603  0.95085284  0.95211975  0.95466459  0.95698893  0.95708983
  0.95792854  0.96020833  0.96125     0.97258141  0.9820378   0.9850554
  0.9882748   0.9889253   0.98954171  0.99139329  0.99555346  0.99756039
  0.99851432  1.00698578  1.00786981  1.00802745  1.0091205   1.01104504
  1.01114594  1.01426444  1.01586716  1.01634127  1.01713445  1.01738294
  1.01945976  1.02009531  1.02257826  1.02346229  1.02361993  1.02455954
  1.02673843  1.02985692  1.03287452  1.0419577   1.0485679   1.0500203
  1.0516864   1.05464726  1.0548049   1.05678082  1.05792339  1.06104189
  1.06416039  1.07039738  1.0731247   1.07975287  1.08287137  1.08438473
  1.09206922  1.09739934  1.10441173  1.10870337  1.16483631  1.30637236
  1.33721137  1.43694916  1.54531314]

  UserWarning,

2022-10-31 11:02:47,088:INFO:Calculating mean and std
2022-10-31 11:02:47,089:INFO:Creating metrics dataframe
2022-10-31 11:02:47,093:INFO:Uploading results into container
2022-10-31 11:02:47,094:INFO:Uploading model into container now
2022-10-31 11:02:47,094:INFO:master_model_container: 19
2022-10-31 11:02:47,094:INFO:display_container: 2
2022-10-31 11:02:47,094:INFO:HuberRegressor()
2022-10-31 11:02:47,094:INFO:create_model() successfully completed......................................
2022-10-31 11:02:47,202:WARNING:create_model() for HuberRegressor() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:47,202:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:47,202:INFO:Initializing create_model()
2022-10-31 11:02:47,202:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:47,202:INFO:Checking exceptions
2022-10-31 11:02:47,202:INFO:Importing libraries
2022-10-31 11:02:47,202:INFO:Copying training dataset
2022-10-31 11:02:47,202:INFO:Defining folds
2022-10-31 11:02:47,202:INFO:Declaring metric variables
2022-10-31 11:02:47,202:INFO:Importing untrained model
2022-10-31 11:02:47,202:INFO:Huber Regressor Imported successfully
2022-10-31 11:02:47,202:INFO:Starting cross validation
2022-10-31 11:02:47,202:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:48,707:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,707:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,738:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,848:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,884:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,906:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,984:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:48,999:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:49,600:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.02394517e+00 -9.91307902e-01 -5.68528071e-01 -5.00034498e-01
 -4.32835153e-01 -4.22473097e-01 -3.98642080e-01 -3.90194549e-01
 -3.63407009e-01 -3.58436791e-01 -3.16262690e-01 -3.13778668e-01
 -2.97821560e-01 -2.86043661e-01 -2.30659742e-01 -2.07251008e-01
 -2.01728004e-01 -1.84714678e-01 -1.74975767e-01 -1.60656261e-01
 -1.57147327e-01 -1.47311396e-01 -1.38677771e-01 -1.21958990e-01
 -1.18505771e-01 -1.02997223e-01 -1.01933007e-01 -9.43605309e-02
 -6.91403075e-02 -6.51388292e-02 -5.96793375e-02 -4.89610179e-02
 -4.84021857e-02 -3.11622320e-02 -1.98792959e-02 -1.62292157e-02
 -1.53950716e-02 -4.46653948e-03 -2.61971363e-04  1.01682597e-02
  1.56474370e-02  2.12128620e-02  2.32883578e-02  3.29234593e-02
  3.34437774e-02  4.11987746e-02  4.13182146e-02  4.13556144e-02
  4.19808927e-02  4.65657105e-02  5.28384008e-02  5.87069492e-02
  5.89732253e-02  6.67158361e-02  6.89234371e-02  6.93007999e-02
  7.93888060e-02  8.23388899e-02  9.50896809e-02  9.51874336e-02
  9.54626333e-02  9.67670287e-02  1.01154900e-01  1.03449211e-01
  1.04389448e-01  1.05000155e-01  1.05817250e-01  1.06677090e-01
  1.07796675e-01  1.09062790e-01  1.12341192e-01  1.14695352e-01
  1.17480759e-01  1.19557398e-01  1.20959274e-01  1.25645771e-01
  1.26123426e-01  1.26474128e-01  1.29891000e-01  1.29902887e-01
  1.29960248e-01  1.33812284e-01  1.35243034e-01  1.38670672e-01
  1.39069835e-01  1.39993626e-01  1.43430982e-01  1.45223303e-01
  1.47372232e-01  1.47487377e-01  1.47889544e-01  1.49011937e-01
  1.53071626e-01  1.53930728e-01  1.54905111e-01  1.55170049e-01
  1.55221836e-01  1.56492925e-01  1.56884083e-01  1.56929518e-01
  1.58110312e-01  1.58977185e-01  1.60299278e-01  1.61666578e-01
  1.62914254e-01  1.63142595e-01  1.64243513e-01  1.66467157e-01
  1.67449989e-01  1.69519791e-01  1.69945213e-01  1.70467773e-01
  1.71009622e-01  1.71940054e-01  1.72098785e-01  1.72358675e-01
  1.73287689e-01  1.75326285e-01  1.75698252e-01  1.78740612e-01
  1.79238775e-01  1.79382558e-01  1.81122234e-01  1.84534758e-01
  1.86111983e-01  1.87949976e-01  1.89499183e-01  1.91304481e-01
  1.92960430e-01  1.94313434e-01  1.95359321e-01  1.95467036e-01
  1.96144984e-01  1.98250771e-01  1.98733419e-01  1.99500775e-01
  2.02214997e-01  2.02344995e-01  2.03551885e-01  2.05726895e-01
  2.06139291e-01  2.07743222e-01  2.07851344e-01  2.08043282e-01
  2.08791385e-01  2.11853826e-01  2.11958377e-01  2.12364025e-01
  2.12721590e-01  2.13218110e-01  2.14590484e-01  2.15832386e-01
  2.16355204e-01  2.18738640e-01  2.19368632e-01  2.19416772e-01
  2.19722913e-01  2.19727353e-01  2.22034574e-01  2.22954608e-01
  2.23504188e-01  2.23726397e-01  2.25004003e-01  2.25181464e-01
  2.25449654e-01  2.28125530e-01  2.29402252e-01  2.29868808e-01
  2.31137924e-01  2.32212647e-01  2.33374971e-01  2.33798420e-01
  2.35397148e-01  2.36400499e-01  2.36673880e-01  2.40041676e-01
  2.40615260e-01  2.41336032e-01  2.41613552e-01  2.45063971e-01
  2.45794684e-01  2.45986505e-01  2.47212673e-01  2.47886589e-01
  2.47954642e-01  2.49607009e-01  2.50796822e-01  2.50893686e-01
  2.51703780e-01  2.53129673e-01  2.53889535e-01  2.54196256e-01
  2.54411531e-01  2.57825459e-01  2.58693895e-01  2.60496929e-01
  2.61969495e-01  2.63346280e-01  2.64677564e-01  2.65060670e-01
  2.65113334e-01  2.66987246e-01  2.67016975e-01  2.67423299e-01
  2.68256289e-01  2.68295027e-01  2.68424334e-01  2.73793039e-01
  2.75738992e-01  2.77193656e-01  2.77719013e-01  2.77729779e-01
  2.78395396e-01  2.79131864e-01  2.79329997e-01  2.80023556e-01
  2.82261126e-01  2.83818213e-01  2.89891603e-01  2.90962300e-01
  2.91288848e-01  2.94979672e-01  2.95529521e-01  2.97817700e-01
  2.99294702e-01  3.01083986e-01  3.05853432e-01  3.06444754e-01
  3.10480781e-01  3.13231787e-01  3.13970611e-01  3.13975135e-01
  3.14247757e-01  3.14638245e-01  3.15094787e-01  3.15508568e-01
  3.17369110e-01  3.17567545e-01  3.19809988e-01  3.19848907e-01
  3.19962012e-01  3.21348831e-01  3.21583675e-01  3.22064101e-01
  3.22116588e-01  3.23362347e-01  3.23518336e-01  3.23536771e-01
  3.24514089e-01  3.27444419e-01  3.31474003e-01  3.33292687e-01
  3.33562127e-01  3.33760486e-01  3.33880671e-01  3.34102159e-01
  3.35144869e-01  3.35259716e-01  3.35461390e-01  3.35704973e-01
  3.36015122e-01  3.37211645e-01  3.41739192e-01  3.42993025e-01
  3.44660231e-01  3.45950519e-01  3.46687442e-01  3.47069758e-01
  3.47088839e-01  3.47368218e-01  3.50281176e-01  3.52027203e-01
  3.58078674e-01  3.63885357e-01  3.64631715e-01  3.64955768e-01
  3.65082906e-01  3.65789217e-01  3.66074370e-01  3.66074503e-01
  3.69075225e-01  3.69330869e-01  3.70566328e-01  3.71931101e-01
  3.75584376e-01  3.76322192e-01  3.76843999e-01  3.79383642e-01
  3.79923177e-01  3.80111358e-01  3.81033322e-01  3.89856606e-01
  3.89883550e-01  3.91944917e-01  3.92432370e-01  3.94021647e-01
  3.94284794e-01  3.95851868e-01  3.98685396e-01  4.00110426e-01
  4.00152905e-01  4.00400237e-01  4.01959741e-01  4.02267904e-01
  4.05357273e-01  4.08035255e-01  4.10861153e-01  4.12947243e-01
  4.19749902e-01  4.28841249e-01  4.28919602e-01  4.29403253e-01
  4.31574716e-01  4.33254782e-01  4.36987751e-01  4.37071349e-01
  4.38824623e-01  4.43069826e-01  4.46454787e-01  4.48257496e-01
  4.49568740e-01  4.52404264e-01  4.59887017e-01  4.61452958e-01
  4.64768963e-01  4.65568666e-01  4.70216360e-01  4.73339675e-01
  4.73640568e-01  4.73806060e-01  4.83359439e-01  4.84156492e-01
  4.84168750e-01  4.86602096e-01  5.04080337e-01  5.06983797e-01
  5.10917522e-01  5.11731357e-01  5.13984861e-01  5.16347626e-01
  5.22554641e-01  5.26185620e-01  5.29806359e-01  5.31705631e-01
  5.35605217e-01  5.37410513e-01  5.44045240e-01  5.44698565e-01
  5.46643051e-01  5.51903619e-01  5.53373799e-01  5.57354512e-01
  5.57894048e-01  5.63163641e-01  5.69012066e-01  5.73107660e-01
  5.78730488e-01  5.84710951e-01  5.86204298e-01  5.88982177e-01
  5.95477525e-01  6.00093794e-01  6.00995052e-01  6.02038280e-01
  6.06654549e-01  6.12785116e-01  6.17867773e-01  6.18558870e-01
  6.23175139e-01  6.24274757e-01  6.27791508e-01  6.30463291e-01
  6.31868142e-01  6.32407677e-01  6.37023946e-01  6.42403610e-01
  6.46201617e-01  6.46256484e-01  6.49968967e-01  6.52817240e-01
  6.60105292e-01  6.60641427e-01  6.64721561e-01  6.68688285e-01
  6.72009712e-01  6.73412597e-01  6.74232673e-01  6.76899480e-01
  6.79557400e-01  6.80514854e-01  6.83186637e-01  6.85606032e-01
  6.85739283e-01  6.91317029e-01  6.93770251e-01  6.93798285e-01
  6.97035444e-01  7.01651713e-01  7.02135705e-01  7.03596200e-01
  7.04292086e-01  7.05629939e-01  7.05704500e-01  7.06267982e-01
  7.07752869e-01  7.12828738e-01  7.15500521e-01  7.17445007e-01
  7.20116790e-01  7.24678057e-01  7.25965215e-01  7.26677545e-01
  7.27285704e-01  7.28947461e-01  7.29349328e-01  7.31116596e-01
  7.31293814e-01  7.31901973e-01  7.33965597e-01  7.35718539e-01
  7.35910083e-01  7.36637480e-01  7.38040364e-01  7.38581866e-01
  7.39065858e-01  7.40312346e-01  7.43170363e-01  7.43198135e-01
  7.43682127e-01  7.43889770e-01  7.44430291e-01  7.47814404e-01
  7.49109345e-01  7.50863194e-01  7.51258020e-01  7.52430673e-01
  7.56779840e-01  7.58613741e-01  7.61663211e-01  7.66346138e-01
  7.70895750e-01  7.71085630e-01  7.72300701e-01  7.73448395e-01
  7.74862473e-01  7.74918708e-01  7.84203055e-01  7.89360826e-01
  7.93199059e-01  7.93977095e-01  7.96529740e-01  7.97153737e-01
  8.01769948e-01  8.02189743e-01  8.05479649e-01  8.05762279e-01
  8.07067360e-01  8.10378548e-01  8.11422281e-01  8.12442171e-01
  8.13022460e-01  8.14195114e-01  8.16872410e-01  8.21111227e-01
  8.21555572e-01  8.21674709e-01  8.23663872e-01  8.24711346e-01
  8.25641433e-01  8.26290978e-01  8.27394376e-01  8.28043921e-01
  8.28843624e-01  8.30343765e-01  8.31986230e-01  8.32387983e-01
  8.38076162e-01  8.41892728e-01  8.42692431e-01  8.46051967e-01
  8.47308700e-01  8.51924969e-01  8.56474114e-01  8.57584972e-01
  8.59211521e-01  8.59794322e-01  8.62201241e-01  8.64481104e-01
  8.66774672e-01  8.71971321e-01  8.73393259e-01  8.75006315e-01
  8.76050048e-01  8.79622584e-01  8.83439150e-01  8.87951336e-01
  8.89567097e-01  8.92671688e-01  8.92907909e-01  8.98087660e-01
  9.01364691e-01  9.02703929e-01  9.04456872e-01  9.07820760e-01
  9.10241524e-01  9.11136764e-01  9.16552736e-01  9.20932808e-01
  9.21169005e-01  9.26829008e-01  9.31897973e-01  9.43186713e-01
  9.45294084e-01  9.47047027e-01  9.47303558e-01  9.51123760e-01
  9.54526622e-01  9.57535676e-01  9.65512103e-01  9.73700908e-01
  9.77607968e-01  9.78774490e-01  9.79552454e-01  9.86840506e-01
  9.89512389e-01  9.90413042e-01  9.96073044e-01  9.99645580e-01
  1.00530558e+00  1.00705852e+00  1.01167479e+00  1.01453812e+00
  1.01629106e+00  1.01915439e+00  1.02090733e+00  1.02303761e+00
  1.02377066e+00  1.04060456e+00  1.04358681e+00  1.04398868e+00
  1.04860495e+00  1.04990521e+00  1.05146827e+00  1.05452148e+00
  1.05608454e+00  1.05783748e+00  1.05832148e+00  1.05964951e+00
  1.06011439e+00  1.06531708e+00  1.06707002e+00  1.07028485e+00
  1.07166335e+00  1.07366891e+00  1.07630256e+00  1.08091883e+00
  1.08117536e+00  1.08766538e+00  1.09015137e+00  1.09938391e+00
  1.10400018e+00  1.13113431e+00  1.14093033e+00  1.14554660e+00
  1.14928216e+00  1.15016287e+00  1.15270223e+00  1.15939540e+00
  1.16301209e+00  1.18777266e+00  1.19424865e+00  1.19645526e+00
  1.24539148e+00  2.00251804e+00]

  UserWarning,

2022-10-31 11:02:49,600:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.93234376e-01 -5.49193149e-01 -4.70460058e-01 -4.63099163e-01
 -3.70412121e-01 -3.28409484e-01 -2.30775057e-01 -2.24832252e-01
 -2.06367472e-01 -1.99163461e-01 -1.88203806e-01 -1.67505691e-01
 -1.45918509e-01 -1.16909049e-01 -1.13338321e-01 -1.12892884e-01
 -1.09378730e-01 -1.07551591e-01 -1.03282169e-01 -1.00113985e-01
 -9.68141644e-02 -9.49209519e-02 -8.79601866e-02 -6.97213561e-02
 -2.68574663e-02 -1.12974789e-02 -1.17481050e-03  5.72493760e-03
  8.93970151e-03  1.34180626e-02  2.51098901e-02  2.79836078e-02
  4.23551128e-02  4.63518704e-02  4.72854086e-02  5.04808640e-02
  6.05586696e-02  6.96840506e-02  6.97226412e-02  7.04162829e-02
  7.63528419e-02  7.81804019e-02  7.89965257e-02  8.06334424e-02
  8.44298756e-02  8.65782596e-02  1.00434144e-01  1.00677859e-01
  1.02702299e-01  1.03617746e-01  1.06102490e-01  1.09427032e-01
  1.10414341e-01  1.10923196e-01  1.13689146e-01  1.19118798e-01
  1.20186380e-01  1.20781248e-01  1.22799480e-01  1.23867530e-01
  1.25206453e-01  1.25221109e-01  1.25379194e-01  1.25958198e-01
  1.27001303e-01  1.27803547e-01  1.29367294e-01  1.30863078e-01
  1.31156913e-01  1.35872826e-01  1.40994664e-01  1.42254367e-01
  1.43204147e-01  1.43928146e-01  1.44579154e-01  1.46784959e-01
  1.47171975e-01  1.47443085e-01  1.48786474e-01  1.49170090e-01
  1.52858867e-01  1.54867801e-01  1.58386219e-01  1.59516979e-01
  1.60377498e-01  1.62390203e-01  1.62644385e-01  1.62784376e-01
  1.67164861e-01  1.69011290e-01  1.70974467e-01  1.71116464e-01
  1.72811721e-01  1.73680076e-01  1.73707901e-01  1.74633590e-01
  1.78758212e-01  1.80913408e-01  1.83036813e-01  1.83893402e-01
  1.84018837e-01  1.86560568e-01  1.89610091e-01  1.89873460e-01
  1.90324090e-01  1.91157639e-01  1.93377899e-01  1.93828870e-01
  1.95658398e-01  1.97370498e-01  1.97526169e-01  1.97851330e-01
  2.01683556e-01  2.01805720e-01  2.03231244e-01  2.03537998e-01
  2.04063648e-01  2.04839152e-01  2.05707031e-01  2.05909851e-01
  2.06230919e-01  2.08155912e-01  2.08338873e-01  2.10896170e-01
  2.14274615e-01  2.16812348e-01  2.19600574e-01  2.20196992e-01
  2.20330912e-01  2.22260267e-01  2.22936430e-01  2.24730272e-01
  2.29747936e-01  2.31939987e-01  2.33189577e-01  2.35700730e-01
  2.36637532e-01  2.39938986e-01  2.42033460e-01  2.42562577e-01
  2.43365479e-01  2.44661382e-01  2.44848388e-01  2.45085661e-01
  2.45602728e-01  2.46083373e-01  2.46539135e-01  2.47020658e-01
  2.47370646e-01  2.47825594e-01  2.48120076e-01  2.48469225e-01
  2.49000780e-01  2.49104449e-01  2.49359898e-01  2.50966706e-01
  2.51858816e-01  2.54674390e-01  2.55853575e-01  2.57673727e-01
  2.57878253e-01  2.58037349e-01  2.60514589e-01  2.62215156e-01
  2.63047658e-01  2.63121039e-01  2.63428167e-01  2.68216476e-01
  2.68384515e-01  2.69480773e-01  2.71294897e-01  2.71587826e-01
  2.72546680e-01  2.74137989e-01  2.74380543e-01  2.74478280e-01
  2.74857163e-01  2.74924725e-01  2.79381280e-01  2.79671418e-01
  2.81513354e-01  2.82905545e-01  2.82980584e-01  2.84903279e-01
  2.85080089e-01  2.86385657e-01  2.86708860e-01  2.87022392e-01
  2.87651906e-01  2.87717293e-01  2.88206349e-01  2.88455463e-01
  2.89105514e-01  2.91580983e-01  2.91856173e-01  2.92482170e-01
  2.92763926e-01  2.92770398e-01  2.93733103e-01  2.93843782e-01
  2.93902988e-01  2.95383617e-01  2.98813380e-01  2.99853986e-01
  3.01077949e-01  3.01628200e-01  3.03328227e-01  3.03626442e-01
  3.07604192e-01  3.07890717e-01  3.08884172e-01  3.09120482e-01
  3.09399582e-01  3.10409738e-01  3.14753163e-01  3.15168419e-01
  3.15872033e-01  3.16447150e-01  3.17138133e-01  3.19801398e-01
  3.21726310e-01  3.24451297e-01  3.24481504e-01  3.26645014e-01
  3.26881181e-01  3.27237765e-01  3.28206458e-01  3.28781453e-01
  3.30435852e-01  3.30812355e-01  3.33112578e-01  3.33742578e-01
  3.34032840e-01  3.34592178e-01  3.34758649e-01  3.35377384e-01
  3.35590148e-01  3.36552821e-01  3.37796340e-01  3.38209293e-01
  3.39951315e-01  3.40089519e-01  3.40764585e-01  3.41297382e-01
  3.41530822e-01  3.43468367e-01  3.45340136e-01  3.47050236e-01
  3.47068391e-01  3.48802902e-01  3.48933944e-01  3.50646986e-01
  3.51173613e-01  3.51573242e-01  3.52853401e-01  3.54391029e-01
  3.54879253e-01  3.55342940e-01  3.58372934e-01  3.58963686e-01
  3.59617326e-01  3.61757202e-01  3.63400360e-01  3.63533927e-01
  3.63782496e-01  3.64204969e-01  3.64579562e-01  3.67339715e-01
  3.68751106e-01  3.69282279e-01  3.71297434e-01  3.71518834e-01
  3.71716649e-01  3.71880980e-01  3.73421891e-01  3.73739576e-01
  3.76901094e-01  3.78775423e-01  3.80179674e-01  3.81172685e-01
  3.82099420e-01  3.82565936e-01  3.83971032e-01  3.86211796e-01
  3.94161050e-01  3.94292407e-01  3.95310899e-01  3.97060619e-01
  3.97341859e-01  3.97526683e-01  3.97634068e-01  3.99793330e-01
  4.00505717e-01  4.01116243e-01  4.05544487e-01  4.06343483e-01
  4.08691185e-01  4.12176960e-01  4.12462370e-01  4.12891631e-01
  4.13017381e-01  4.14080502e-01  4.15852558e-01  4.16280126e-01
  4.16630355e-01  4.18373817e-01  4.18677988e-01  4.18812594e-01
  4.19402966e-01  4.20905430e-01  4.22215663e-01  4.23277697e-01
  4.26005393e-01  4.29142364e-01  4.30707955e-01  4.31438937e-01
  4.37666416e-01  4.41414825e-01  4.41691241e-01  4.42498198e-01
  4.43335156e-01  4.47355004e-01  4.48111460e-01  4.48128298e-01
  4.52211067e-01  4.53200149e-01  4.56255873e-01  4.56952790e-01
  4.57872783e-01  4.60657660e-01  4.61607194e-01  4.62906295e-01
  4.65976621e-01  4.66140260e-01  4.68932328e-01  4.69800277e-01
  4.73041426e-01  4.73261201e-01  4.78396438e-01  4.79874303e-01
  4.82804046e-01  4.83689511e-01  4.88256390e-01  4.92482701e-01
  4.93277419e-01  4.95497757e-01  4.96947910e-01  5.01523069e-01
  5.02524563e-01  5.03450888e-01  5.03980450e-01  5.04643970e-01
  5.04794176e-01  5.06490994e-01  5.07287407e-01  5.13205187e-01
  5.13537521e-01  5.13796849e-01  5.14752224e-01  5.18960448e-01
  5.27086250e-01  5.27265567e-01  5.29132950e-01  5.34739406e-01
  5.42672204e-01  5.51663726e-01  5.55479751e-01  5.59606620e-01
  5.59614849e-01  5.62136512e-01  5.62239195e-01  5.71513922e-01
  5.72303601e-01  5.74619620e-01  5.76586815e-01  5.79672308e-01
  5.83157091e-01  5.87882699e-01  5.91150322e-01  5.91380911e-01
  5.94144182e-01  5.96573944e-01  6.00132608e-01  6.00444438e-01
  6.01159110e-01  6.01536387e-01  6.06729420e-01  6.07313943e-01
  6.09233242e-01  6.09320350e-01  6.11140014e-01  6.12677887e-01
  6.19508320e-01  6.24039353e-01  6.33059438e-01  6.37887616e-01
  6.38903207e-01  6.40280450e-01  6.44953959e-01  6.45473483e-01
  6.48560091e-01  6.49818376e-01  6.50666516e-01  6.53466714e-01
  6.57346088e-01  6.57484671e-01  6.58659747e-01  6.61638570e-01
  6.61702625e-01  6.66245614e-01  6.70266147e-01  6.70646803e-01
  6.73524173e-01  6.75388440e-01  6.77746114e-01  6.79431877e-01
  6.81824712e-01  6.84624910e-01  6.86111450e-01  6.87017744e-01
  6.89817943e-01  6.92642565e-01  6.94673351e-01  6.95010976e-01
  6.98342292e-01  6.99888994e-01  7.00204008e-01  7.03372957e-01
  7.05397041e-01  7.07322450e-01  7.10590074e-01  7.13220919e-01
  7.16601476e-01  7.18175940e-01  7.19325536e-01  7.20232428e-01
  7.20976139e-01  7.22791973e-01  7.23368973e-01  7.24912068e-01
  7.26169172e-01  7.28562006e-01  7.29018195e-01  7.31362204e-01
  7.33178038e-01  7.34375917e-01  7.40266374e-01  7.41748270e-01
  7.44141104e-01  7.49334136e-01  7.51312405e-01  7.52572925e-01
  7.54527169e-01  7.58943641e-01  7.59892419e-01  7.61206741e-01
  7.61405838e-01  7.62476989e-01  7.64913235e-01  7.69621794e-01
  7.70106267e-01  7.71109347e-01  7.72820056e-01  7.75299300e-01
  7.76788261e-01  7.79989822e-01  7.85685365e-01  7.88813229e-01
  7.88981936e-01  7.90878398e-01  7.97389733e-01  8.01264463e-01
  8.02582766e-01  8.02787388e-01  8.04064662e-01  8.04885453e-01
  8.08448536e-01  8.11410578e-01  8.13628797e-01  8.14952353e-01
  8.15938788e-01  8.18929088e-01  8.20962062e-01  8.29207895e-01
  8.31707988e-01  8.33740962e-01  8.34400928e-01  8.36043649e-01
  8.37131219e-01  8.38411174e-01  8.44786993e-01  8.44792149e-01
  8.49320060e-01  8.49980026e-01  8.52692280e-01  8.53194790e-01
  8.60366091e-01  8.64899158e-01  8.65793986e-01  8.65957985e-01
  8.68059216e-01  8.68773888e-01  8.70092190e-01  8.70345068e-01
  8.80478256e-01  8.82090891e-01  8.85671288e-01  8.86485014e-01
  8.90864321e-01  8.93193946e-01  8.93768034e-01  8.96057354e-01
  8.98360041e-01  9.00535715e-01  9.01250386e-01  9.03060839e-01
  9.06443419e-01  9.07898867e-01  9.08838472e-01  9.12210915e-01
  9.12296418e-01  9.15264625e-01  9.16829484e-01  9.19758084e-01
  9.22682483e-01  9.26810389e-01  9.33454166e-01  9.34711270e-01
  9.36689538e-01  9.37509353e-01  9.37601615e-01  9.53128613e-01
  9.53180713e-01  9.55158982e-01  9.60173923e-01  9.62654702e-01
  9.64380504e-01  9.65869466e-01  9.67847734e-01  9.68045139e-01
  9.73952844e-01  9.79145876e-01  9.81050584e-01  9.83426832e-01
  9.84338909e-01  9.91119957e-01  9.91834629e-01  9.95770558e-01
  1.00568792e+00  1.00741373e+00  1.00958940e+00  1.01228234e+00
  1.01260676e+00  1.01478243e+00  1.02069014e+00  1.02181368e+00
  1.02299282e+00  1.02448179e+00  1.04328048e+00  1.04376496e+00
  1.04574322e+00  1.04845548e+00  1.05464033e+00  1.05698927e+00
  1.05808695e+00  1.06175725e+00  1.06651536e+00  1.07113139e+00
  1.07690142e+00  1.08209445e+00  1.09248052e+00  1.09767355e+00
  1.10286658e+00  1.11325265e+00  1.12247205e+00  1.13199441e+00
  1.13331011e+00  1.13402478e+00  1.16593045e+00  1.19580460e+00
  1.21706120e+00  1.22744727e+00  1.64118307e+00  1.96872253e+00]

  UserWarning,

2022-10-31 11:02:49,648:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.41134465 -0.80852636 -0.66366494 -0.60065338 -0.39064185 -0.30419143
 -0.26125861 -0.23040176 -0.22674382 -0.16873175 -0.15120994 -0.12292245
 -0.11669695 -0.10626173 -0.07413818 -0.07027442 -0.06766873 -0.06729524
 -0.06597223 -0.06100698 -0.05442673  0.00933431  0.01444316  0.01563048
  0.01944043  0.0212226   0.03734339  0.04576386  0.0519172   0.0623445
  0.06600065  0.06743309  0.06959989  0.0729199   0.07341824  0.0767626
  0.07917716  0.08322686  0.08809022  0.08920149  0.0928652   0.09334931
  0.09520222  0.0968378   0.09734057  0.09869916  0.10040341  0.10185741
  0.10511934  0.10711434  0.12019528  0.12051921  0.12169661  0.12343804
  0.1236601   0.12610354  0.12933668  0.13110094  0.13134794  0.13347434
  0.13370648  0.13381481  0.13911597  0.14157013  0.14483124  0.14611641
  0.1461404   0.14649932  0.15000925  0.15543427  0.15653833  0.15900767
  0.16231778  0.16470593  0.1655876   0.16779794  0.16981434  0.17102463
  0.17140731  0.17174398  0.17508409  0.18191427  0.18236688  0.18441663
  0.18562893  0.18827769  0.18986457  0.19130016  0.1926813   0.19284522
  0.19304602  0.19834205  0.20029387  0.20312752  0.20356718  0.20420851
  0.20517624  0.20594501  0.20624019  0.21034152  0.21165025  0.2118972
  0.21213545  0.21230995  0.21294083  0.21344698  0.21365406  0.21435295
  0.21467343  0.21550356  0.21610516  0.21646007  0.21751672  0.22017815
  0.22024122  0.22162547  0.22227875  0.22236921  0.22301022  0.22441068
  0.22449071  0.22484925  0.22586217  0.22646718  0.23041174  0.23082283
  0.2315473   0.2317854   0.23230743  0.23408819  0.23492989  0.23496734
  0.23604136  0.23639211  0.23847604  0.23878083  0.23898872  0.23922005
  0.23976984  0.24057496  0.24196685  0.2447092   0.2461901   0.24634883
  0.24716503  0.2495234   0.24988791  0.25041141  0.2506053   0.25116202
  0.25214766  0.25228183  0.25269326  0.25608629  0.25669524  0.25718345
  0.25737822  0.25782313  0.25846545  0.25969666  0.26013127  0.2624523
  0.26311958  0.26382937  0.26471164  0.26592974  0.2667434   0.26971431
  0.26997083  0.27001425  0.27249329  0.27437721  0.27709831  0.27804939
  0.27811315  0.27843396  0.27859088  0.27867111  0.27909484  0.28042389
  0.28064225  0.28074086  0.2813433   0.28197742  0.2822024   0.28268048
  0.28458731  0.28459287  0.28534344  0.28642662  0.2872544   0.28962381
  0.28984115  0.29020157  0.29062024  0.29067666  0.29170977  0.29225253
  0.29425926  0.29428185  0.29440836  0.29478903  0.29487771  0.29853305
  0.29900945  0.3007099   0.30108406  0.30514109  0.30518419  0.30787394
  0.30820423  0.30850543  0.30950773  0.31002133  0.31218959  0.313162
  0.31600576  0.31604086  0.31751383  0.31980399  0.32006295  0.32137529
  0.32166113  0.32473865  0.32489287  0.3268147   0.32734418  0.33019376
  0.33027454  0.33046889  0.33248571  0.33256828  0.33335638  0.33371742
  0.33620781  0.33640638  0.33651241  0.33654668  0.33674786  0.33856501
  0.33865883  0.33986649  0.3418004   0.34224628  0.34233948  0.34433592
  0.34439783  0.3445603   0.34492359  0.34505785  0.34654515  0.34694525
  0.34781761  0.34927193  0.34949472  0.35038534  0.35122925  0.35179842
  0.35200681  0.35208425  0.35219896  0.35250673  0.35357123  0.35483092
  0.35646599  0.35741721  0.36096402  0.36420546  0.36435437  0.36523769
  0.36603323  0.36668843  0.36761043  0.36822135  0.36843486  0.36931361
  0.3705649   0.37182004  0.37510956  0.37624511  0.37809344  0.37844362
  0.37858146  0.38151963  0.3817152   0.38663604  0.38894427  0.38899121
  0.39044432  0.39558522  0.39561602  0.39691382  0.39964198  0.39985665
  0.40204516  0.40276615  0.40380312  0.40383828  0.40498683  0.40696864
  0.40853839  0.40909783  0.40985542  0.41061123  0.4106736   0.41118627
  0.41617678  0.41974412  0.4230703   0.42469136  0.42867399  0.42906131
  0.43259263  0.43494066  0.43865261  0.44354694  0.44728156  0.44803877
  0.44916203  0.45528581  0.45657056  0.46466927  0.46493734  0.46724465
  0.46823298  0.46881022  0.47184527  0.47295904  0.47657058  0.48660831
  0.49125201  0.49274345  0.49685524  0.50226629  0.50565643  0.50795814
  0.50802157  0.508102    0.50892885  0.51037622  0.51290347  0.51546981
  0.52130554  0.52462209  0.53105417  0.53256891  0.53913121  0.54933361
  0.55065655  0.55201345  0.56153395  0.56766101  0.57029271  0.58444085
  0.59235881  0.59285397  0.5934733   0.60307625  0.60420028  0.60493137
  0.60651834  0.60787772  0.61336613  0.61370348  0.61400911  0.61447188
  0.61730684  0.61748066  0.61866878  0.62361206  0.62841353  0.63188508
  0.63465319  0.63668655  0.63893654  0.64511641  0.64523297  0.64704436
  0.64770447  0.652086    0.66306329  0.66424251  0.67029686  0.67162678
  0.67208954  0.67543755  0.67925682  0.6794261   0.68055381  0.68122972
  0.68470127  0.68501333  0.68637222  0.68638523  0.69083267  0.69430422
  0.69563414  0.69910569  0.70390716  0.70504973  0.70523708  0.70574816
  0.70700685  0.70870863  0.71351011  0.71369388  0.71484003  0.715415
  0.71739819  0.71831158  0.71930661  0.7196415   0.72062612  0.72247007
  0.72247872  0.72311305  0.72610547  0.72678855  0.72924444  0.73208167
  0.73404592  0.73751747  0.73917997  0.74126241  0.74149866  0.74483261
  0.74818556  0.74963408  0.75192188  0.7547741   0.75566683  0.75649492
  0.75672335  0.75755993  0.75875171  0.7612964   0.76152483  0.76568332
  0.76615041  0.7663263   0.76854344  0.76937029  0.77048479  0.77592924
  0.77814638  0.77842975  0.78073071  0.78686211  0.78969068  0.79033366
  0.79132869  0.79410001  0.79646505  0.79764881  0.7999366   0.80409509
  0.80789113  0.81086947  0.81117145  0.81214287  0.81434102  0.81605942
  0.81849951  0.82330098  0.82487806  0.82721954  0.82803545  0.828517
  0.83017951  0.83095052  0.83354691  0.83811995  0.83834838  0.83904681
  0.84250687  0.84278611  0.84292142  0.84707991  0.84730834  0.85210981
  0.85235697  0.85496993  0.85732584  0.86422643  0.8662858   0.86651423
  0.86804085  0.8693438   0.87068137  0.8713157   0.87611717  0.8786501
  0.88069021  0.88091864  0.88572012  0.887874    0.88919599  0.89052159
  0.89532306  0.90012453  0.90053909  0.90360041  0.90817345  0.90840188
  0.90972748  0.91297492  0.91365958  0.91738907  0.9177764   0.91800483
  0.91929125  0.91933042  0.92061684  0.92259719  0.9228063   0.92413189
  0.9244688   0.92893769  0.92941202  0.93240924  0.93459584  0.93503303
  0.93733632  0.93854846  0.94201219  0.94310935  0.94369182  0.94391708
  0.94439863  0.94860634  0.94883768  0.9513867   0.95490628  0.96121807
  0.96236065  0.96395325  0.96601955  0.97082102  0.97562249  0.9757481
  0.98019553  0.98042396  0.98174956  0.98454921  0.99002691  0.99077365
  0.99459995  0.9972434   1.00993123  1.01266061  1.01380584  1.02363721
  1.02520147  1.02821025  1.03393859  1.04724875  1.05221761  1.0584038
  1.05918436  1.06426613  1.07039319  1.07102768  1.07622497  1.08395043
  1.08582792  1.08804506  1.09062939  1.09543086  1.1026779   1.10432042
  1.11240523  1.16275888]

  UserWarning,

2022-10-31 11:02:49,721:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.58110627e+00 -1.02345555e+00 -9.43224607e-01 -5.65232950e-01
 -5.20461346e-01 -3.69003339e-01 -2.46686786e-01 -2.31302144e-01
 -2.15550103e-01 -2.13665920e-01 -1.93670843e-01 -1.49895481e-01
 -1.41310068e-01 -1.32269587e-01 -1.21490718e-01 -1.17662453e-01
 -1.03055678e-01 -9.59736837e-02 -8.63118632e-02 -8.50569258e-02
 -8.30220280e-02 -6.75843709e-02 -6.59236936e-02 -5.58018819e-02
 -5.34004193e-02 -3.36130986e-02 -1.03815938e-02 -9.77107319e-03
 -9.24633439e-03 -8.08953309e-03 -4.68401761e-03 -1.19481132e-03
 -4.50298138e-04  2.05737489e-02  3.21860448e-02  3.89449142e-02
  4.11615590e-02  5.15623568e-02  5.20382932e-02  5.33762670e-02
  6.23099140e-02  6.96331909e-02  6.97770543e-02  7.01186109e-02
  7.04491165e-02  7.90050672e-02  8.10478509e-02  8.31236698e-02
  8.33481188e-02  8.34754600e-02  8.48647837e-02  8.66821063e-02
  9.22475848e-02  1.04772462e-01  1.05532179e-01  1.05984693e-01
  1.07511631e-01  1.12910603e-01  1.15038964e-01  1.15139587e-01
  1.17781829e-01  1.21986729e-01  1.22514348e-01  1.25384727e-01
  1.31220093e-01  1.31964635e-01  1.34864054e-01  1.35461293e-01
  1.35903758e-01  1.38016189e-01  1.38424308e-01  1.42421010e-01
  1.44270545e-01  1.47331739e-01  1.47583700e-01  1.47943640e-01
  1.53301606e-01  1.53309996e-01  1.54224078e-01  1.55877179e-01
  1.57158277e-01  1.59980220e-01  1.62746041e-01  1.63605479e-01
  1.65752993e-01  1.65958414e-01  1.66882199e-01  1.68703263e-01
  1.71024807e-01  1.71173007e-01  1.71372998e-01  1.71425096e-01
  1.73400843e-01  1.73796319e-01  1.74990216e-01  1.82405313e-01
  1.83299714e-01  1.83917522e-01  1.84793535e-01  1.86145469e-01
  1.86741441e-01  1.93388918e-01  1.94473091e-01  1.96302381e-01
  1.96626188e-01  1.98672700e-01  2.00945727e-01  2.01538596e-01
  2.02404084e-01  2.03779528e-01  2.04152711e-01  2.08381907e-01
  2.08939331e-01  2.09402355e-01  2.09409762e-01  2.09450689e-01
  2.10384280e-01  2.15314231e-01  2.15464616e-01  2.15631249e-01
  2.17019386e-01  2.17396398e-01  2.18293183e-01  2.21615555e-01
  2.24145855e-01  2.24740760e-01  2.27448866e-01  2.27809852e-01
  2.28039218e-01  2.30059143e-01  2.30455272e-01  2.31559435e-01
  2.31828140e-01  2.32162253e-01  2.35539567e-01  2.35635481e-01
  2.37884760e-01  2.39821407e-01  2.40257946e-01  2.43051168e-01
  2.43866291e-01  2.45098889e-01  2.47851156e-01  2.48259763e-01
  2.48522926e-01  2.50933959e-01  2.51894100e-01  2.52202519e-01
  2.52862356e-01  2.54478987e-01  2.55568624e-01  2.56215022e-01
  2.56822680e-01  2.57377207e-01  2.58467746e-01  2.58879769e-01
  2.59889765e-01  2.60206731e-01  2.61104799e-01  2.63147423e-01
  2.64184681e-01  2.64252293e-01  2.65206178e-01  2.67922844e-01
  2.68873002e-01  2.69620749e-01  2.70418516e-01  2.70657472e-01
  2.71200679e-01  2.74025321e-01  2.74043516e-01  2.74222661e-01
  2.74881946e-01  2.77106668e-01  2.77664203e-01  2.77670289e-01
  2.77825352e-01  2.78643418e-01  2.78891699e-01  2.79649309e-01
  2.80915128e-01  2.80957807e-01  2.81150880e-01  2.81700341e-01
  2.83252782e-01  2.83334399e-01  2.83432438e-01  2.83823659e-01
  2.85086678e-01  2.85551637e-01  2.85640677e-01  2.87273949e-01
  2.87411484e-01  2.87808172e-01  2.89270079e-01  2.91127055e-01
  2.92789947e-01  2.94344628e-01  2.96693747e-01  2.97559726e-01
  2.99527627e-01  2.99732940e-01  3.00023610e-01  3.00182192e-01
  3.01528407e-01  3.01993289e-01  3.02635771e-01  3.03252721e-01
  3.03384476e-01  3.04302008e-01  3.05060347e-01  3.07169050e-01
  3.07433897e-01  3.10835461e-01  3.12712975e-01  3.12815384e-01
  3.13621263e-01  3.15233042e-01  3.16816098e-01  3.17088869e-01
  3.18071130e-01  3.18408688e-01  3.18889917e-01  3.20568222e-01
  3.21787820e-01  3.23058825e-01  3.23793057e-01  3.23975287e-01
  3.24534168e-01  3.25259996e-01  3.26560309e-01  3.27435950e-01
  3.28416983e-01  3.32911930e-01  3.34195452e-01  3.34640345e-01
  3.36111578e-01  3.37581593e-01  3.37748238e-01  3.38163156e-01
  3.38378919e-01  3.38474538e-01  3.38904502e-01  3.40379494e-01
  3.41084322e-01  3.43767738e-01  3.45448314e-01  3.46716585e-01
  3.46813300e-01  3.52261954e-01  3.55134648e-01  3.55279771e-01
  3.55867674e-01  3.57255480e-01  3.57917816e-01  3.59327301e-01
  3.60547760e-01  3.61701331e-01  3.62674511e-01  3.65790295e-01
  3.66496766e-01  3.67212621e-01  3.67955505e-01  3.68031032e-01
  3.68435115e-01  3.68856483e-01  3.68886876e-01  3.68906079e-01
  3.69612550e-01  3.70326148e-01  3.70429406e-01  3.71961109e-01
  3.72021864e-01  3.74576638e-01  3.75526404e-01  3.76859914e-01
  3.77252061e-01  3.77782467e-01  3.78253432e-01  3.79202747e-01
  3.79338414e-01  3.80236047e-01  3.81381586e-01  3.81810677e-01
  3.82074017e-01  3.82418695e-01  3.83022265e-01  3.83609039e-01
  3.84485000e-01  3.84758397e-01  3.87291950e-01  3.87778692e-01
  3.88346086e-01  3.96948137e-01  3.98025272e-01  3.98098204e-01
  3.98727729e-01  3.99112235e-01  4.00370387e-01  4.03150949e-01
  4.12568986e-01  4.13505570e-01  4.16966298e-01  4.17700181e-01
  4.20122418e-01  4.21260825e-01  4.21320669e-01  4.21626076e-01
  4.21678923e-01  4.24245681e-01  4.25727365e-01  4.26802175e-01
  4.26883114e-01  4.29512236e-01  4.31822669e-01  4.32336391e-01
  4.33647876e-01  4.38190501e-01  4.38962643e-01  4.40835321e-01
  4.41306286e-01  4.46827747e-01  4.47537854e-01  4.47568176e-01
  4.49750607e-01  4.52803289e-01  4.55255579e-01  4.56885206e-01
  4.58376332e-01  4.58996858e-01  4.59784604e-01  4.61119861e-01
  4.67881653e-01  4.70982443e-01  4.71116049e-01  4.71934903e-01
  4.77214011e-01  4.77705555e-01  4.88921989e-01  4.92479000e-01
  4.92992947e-01  4.93149028e-01  4.93862886e-01  4.95606818e-01
  4.95610885e-01  4.98548113e-01  5.03534636e-01  5.08371852e-01
  5.17719205e-01  5.20582688e-01  5.21086248e-01  5.23479535e-01
  5.23624237e-01  5.24629604e-01  5.31391931e-01  5.37333015e-01
  5.39529694e-01  5.40904938e-01  5.45508961e-01  5.48980897e-01
  5.51740529e-01  5.53460214e-01  5.54494101e-01  5.56628703e-01
  5.60267688e-01  5.60497419e-01  5.64362789e-01  5.65430554e-01
  5.79038263e-01  5.85378968e-01  5.91610536e-01  6.05305303e-01
  6.06414169e-01  6.07004450e-01  6.07189457e-01  6.16351803e-01
  6.16739208e-01  6.23906498e-01  6.36328338e-01  6.46953834e-01
  6.47400421e-01  6.53691616e-01  6.54178214e-01  6.60451711e-01
  6.62147789e-01  6.68849168e-01  6.70026604e-01  6.72872919e-01
  6.79246138e-01  6.82220271e-01  6.85022124e-01  6.86456418e-01
  6.91130622e-01  6.93486349e-01  6.94246406e-01  6.94725337e-01
  6.97362190e-01  6.98505663e-01  6.99633390e-01  7.00477974e-01
  7.00914976e-01  7.04030760e-01  7.06709543e-01  7.08881413e-01
  7.10262329e-01  7.10840982e-01  7.13378113e-01  7.16056895e-01
  7.19172679e-01  7.19609681e-01  7.22767394e-01  7.25404247e-01
  7.25841249e-01  7.27001868e-01  7.28957033e-01  7.32072818e-01
  7.34437668e-01  7.35188602e-01  7.38304386e-01  7.40528628e-01
  7.41420170e-01  7.42825892e-01  7.44030287e-01  7.46133575e-01
  7.47237794e-01  7.50767522e-01  7.53883306e-01  7.56562089e-01
  7.56999091e-01  7.60114875e-01  7.60538114e-01  7.62486146e-01
  7.63130646e-01  7.63230659e-01  7.63967830e-01  7.65672934e-01
  7.68574740e-01  7.70168698e-01  7.70199398e-01  7.70910239e-01
  7.71833498e-01  7.73001251e-01  7.73315182e-01  7.75625131e-01
  7.77191404e-01  7.78272565e-01  7.78441465e-01  7.78709567e-01
  7.78809579e-01  7.84024632e-01  7.87412419e-01  7.88894103e-01
  7.90528203e-01  7.90801751e-01  7.90995884e-01  7.91272716e-01
  7.93236004e-01  7.93917535e-01  7.95125671e-01  7.96759771e-01
  7.97404272e-01  7.97546213e-01  8.00620068e-01  8.02087452e-01
  8.04473024e-01  8.08898265e-01  8.12338692e-01  8.13820376e-01
  8.15454476e-01  8.20002348e-01  8.21686044e-01  8.27917612e-01
  8.37264965e-01  8.40380749e-01  8.40502952e-01  8.43496533e-01
  8.46469343e-01  8.46654246e-01  8.48209938e-01  8.49728101e-01
  8.50354723e-01  8.52843885e-01  8.53422538e-01  8.57441354e-01
  8.59075454e-01  8.60557138e-01  8.65307022e-01  8.67951841e-01
  8.68422806e-01  8.69043388e-01  8.69261791e-01  8.77517857e-01
  8.77770159e-01  8.80885943e-01  8.84001727e-01  8.87117511e-01
  8.90607616e-01  8.96212562e-01  8.96464863e-01  9.05341251e-01
  9.08675699e-01  9.11791483e-01  9.12528654e-01  9.14907267e-01
  9.15159568e-01  9.22128307e-01  9.24586255e-01  9.27664633e-01
  9.33854273e-01  9.36161946e-01  9.36717756e-01  9.36970057e-01
  9.40085841e-01  9.49180892e-01  9.52555496e-01  9.55412461e-01
  9.55596097e-01  9.56149632e-01  9.61008843e-01  9.67875597e-01
  9.68502219e-01  9.70210389e-01  9.70949453e-01  9.74107165e-01
  9.74373372e-01  9.77264878e-01  9.80338734e-01  9.81806117e-01
  9.83454518e-01  9.83635879e-01  9.84191689e-01  9.86570302e-01
  9.90675558e-01  9.93539041e-01  9.93791342e-01  9.98187880e-01
  9.99770610e-01  1.00208056e+00  1.00288639e+00  1.00361661e+00
  1.00600218e+00  1.01465429e+00  1.02084393e+00  1.02158110e+00
  1.02469688e+00  1.02522137e+00  1.02590216e+00  1.02781267e+00
  1.03241958e+00  1.03716002e+00  1.04027580e+00  1.04339159e+00
  1.04932706e+00  1.04962316e+00  1.05097370e+00  1.05538376e+00
  1.05580513e+00  1.05807897e+00  1.06056813e+00  1.06208629e+00
  1.07362001e+00  1.07913260e+00  1.09439273e+00  1.09473526e+00
  1.10502804e+00  1.11105135e+00  1.11474579e+00  1.16625898e+00
  1.26419023e+00]

  UserWarning,

2022-10-31 11:02:49,768:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.18658625e+00 -1.01205108e+00 -9.30236092e-01 -8.94532314e-01
 -8.10770293e-01 -7.24909254e-01 -6.86767923e-01 -5.78800079e-01
 -5.25376384e-01 -5.17070043e-01 -5.10845409e-01 -4.63348478e-01
 -4.46780312e-01 -3.77239564e-01 -3.68917880e-01 -3.56656585e-01
 -3.26342795e-01 -3.26250012e-01 -3.14302704e-01 -2.88486518e-01
 -2.62340453e-01 -2.49015238e-01 -2.45340784e-01 -2.34217059e-01
 -2.18459324e-01 -2.08960171e-01 -1.96643782e-01 -1.87298196e-01
 -1.49411621e-01 -1.40403601e-01 -1.32453282e-01 -9.84652389e-02
 -7.64847147e-02 -7.52311053e-02 -6.21926345e-02 -3.87995989e-02
 -3.85329525e-02 -3.61056653e-02 -3.54848436e-02 -2.30950995e-02
  1.25959453e-03  6.32938167e-03  7.97873199e-03  2.12301775e-02
  2.23417854e-02  3.90857048e-02  4.84855186e-02  4.99782247e-02
  5.71929193e-02  5.72735971e-02  6.61066711e-02  6.80842451e-02
  7.28704937e-02  7.32195772e-02  7.55837774e-02  7.85005991e-02
  7.86468452e-02  8.27052540e-02  8.43250923e-02  8.51386206e-02
  8.63204088e-02  8.66208515e-02  8.67801290e-02  8.78882338e-02
  8.84251601e-02  9.53017140e-02  9.54687243e-02  9.63416597e-02
  1.01132418e-01  1.01494902e-01  1.05848751e-01  1.07890259e-01
  1.07948386e-01  1.16714338e-01  1.17932229e-01  1.20368270e-01
  1.27608284e-01  1.32027713e-01  1.33550738e-01  1.33818301e-01
  1.33982247e-01  1.34448767e-01  1.37075055e-01  1.44419624e-01
  1.44671755e-01  1.45267340e-01  1.46237419e-01  1.46255486e-01
  1.47147321e-01  1.48167749e-01  1.50884732e-01  1.50916991e-01
  1.52455440e-01  1.52763677e-01  1.53721261e-01  1.57128481e-01
  1.58300356e-01  1.61172488e-01  1.62014851e-01  1.63163061e-01
  1.65146661e-01  1.66843430e-01  1.71135541e-01  1.78373172e-01
  1.80341612e-01  1.80810774e-01  1.80888403e-01  1.81859173e-01
  1.82230169e-01  1.83061442e-01  1.83745403e-01  1.84974414e-01
  1.87362019e-01  1.87560803e-01  1.87822269e-01  1.88872243e-01
  1.91033622e-01  1.93059803e-01  1.93473734e-01  1.94291439e-01
  1.94369978e-01  1.95674165e-01  1.96182416e-01  1.96236697e-01
  1.97120337e-01  1.97304402e-01  1.97341207e-01  1.97489199e-01
  1.98219016e-01  1.98257387e-01  1.98679059e-01  1.99799656e-01
  2.00978797e-01  2.01341500e-01  2.01347087e-01  2.02419749e-01
  2.02454128e-01  2.02793911e-01  2.04600735e-01  2.07460403e-01
  2.07671733e-01  2.08403762e-01  2.08834269e-01  2.09253855e-01
  2.13095957e-01  2.14075962e-01  2.14236008e-01  2.15671212e-01
  2.16630692e-01  2.17303210e-01  2.17664614e-01  2.19137980e-01
  2.21073989e-01  2.24055935e-01  2.24749925e-01  2.28788981e-01
  2.29002276e-01  2.31302299e-01  2.32363623e-01  2.33327889e-01
  2.36756192e-01  2.37261830e-01  2.37806178e-01  2.41430556e-01
  2.42643254e-01  2.43230993e-01  2.43694886e-01  2.44792195e-01
  2.45303209e-01  2.46672547e-01  2.46733599e-01  2.46772420e-01
  2.46962065e-01  2.48025991e-01  2.48108072e-01  2.49164200e-01
  2.50692899e-01  2.51659299e-01  2.53889926e-01  2.54398880e-01
  2.58341551e-01  2.58430525e-01  2.59172720e-01  2.59701405e-01
  2.60931284e-01  2.61102043e-01  2.63799490e-01  2.64579434e-01
  2.65227175e-01  2.66390686e-01  2.67306797e-01  2.67619827e-01
  2.68533573e-01  2.68580185e-01  2.69565292e-01  2.69917278e-01
  2.71274114e-01  2.71438531e-01  2.71912732e-01  2.72894520e-01
  2.75282425e-01  2.76772423e-01  2.77915745e-01  2.79313348e-01
  2.79907161e-01  2.80840436e-01  2.81024700e-01  2.81980098e-01
  2.83120594e-01  2.83403966e-01  2.84500666e-01  2.85900853e-01
  2.86967383e-01  2.87239570e-01  2.88686778e-01  2.88874062e-01
  2.90431413e-01  2.90693185e-01  2.90908456e-01  2.91713872e-01
  2.93538510e-01  2.95396357e-01  2.96361830e-01  2.97317743e-01
  2.97761307e-01  2.99030370e-01  2.99262559e-01  3.02213092e-01
  3.02890938e-01  3.03034990e-01  3.04221561e-01  3.05266940e-01
  3.05439956e-01  3.05675642e-01  3.05740107e-01  3.06040405e-01
  3.06506315e-01  3.06871008e-01  3.07342344e-01  3.07377537e-01
  3.10333935e-01  3.12966503e-01  3.13604384e-01  3.15591983e-01
  3.17460972e-01  3.17774133e-01  3.18269157e-01  3.18414758e-01
  3.18572853e-01  3.19281131e-01  3.20253105e-01  3.20723190e-01
  3.24513050e-01  3.25754714e-01  3.28180046e-01  3.29494930e-01
  3.31059544e-01  3.31062539e-01  3.31199209e-01  3.31241694e-01
  3.32593625e-01  3.33188302e-01  3.35107817e-01  3.35444723e-01
  3.35663565e-01  3.36963572e-01  3.37255053e-01  3.37346469e-01
  3.39552745e-01  3.39964801e-01  3.40958434e-01  3.41334610e-01
  3.41354159e-01  3.43468404e-01  3.45073867e-01  3.47253226e-01
  3.48246105e-01  3.48274560e-01  3.51242857e-01  3.55022752e-01
  3.55986550e-01  3.57654314e-01  3.58149394e-01  3.60708270e-01
  3.61515634e-01  3.63172966e-01  3.63543462e-01  3.64850442e-01
  3.65573305e-01  3.66451047e-01  3.69624851e-01  3.70262110e-01
  3.70517485e-01  3.71508145e-01  3.72965620e-01  3.74912279e-01
  3.76185347e-01  3.76337466e-01  3.76576326e-01  3.77721180e-01
  3.79892118e-01  3.80122795e-01  3.80852960e-01  3.86108114e-01
  3.86315094e-01  3.88653752e-01  3.89341355e-01  3.93595072e-01
  4.03045806e-01  4.05116496e-01  4.05821202e-01  4.11929886e-01
  4.12093635e-01  4.19771594e-01  4.20570254e-01  4.24378164e-01
  4.24778838e-01  4.26133357e-01  4.28302817e-01  4.29545933e-01
  4.31296298e-01  4.31574990e-01  4.34824784e-01  4.39842380e-01
  4.40896839e-01  4.45129731e-01  4.46087818e-01  4.47333893e-01
  4.49765544e-01  4.53755965e-01  4.63867496e-01  4.69305877e-01
  4.70332039e-01  4.70556084e-01  4.71891266e-01  4.72752434e-01
  4.74775949e-01  4.80389846e-01  4.86658157e-01  4.86884829e-01
  4.92577474e-01  4.93989792e-01  5.01219663e-01  5.01310115e-01
  5.01409489e-01  5.04288711e-01  5.07290570e-01  5.17482436e-01
  5.18360073e-01  5.24319761e-01  5.25213690e-01  5.31334617e-01
  5.33912534e-01  5.36834434e-01  5.39577580e-01  5.43347080e-01
  5.51188752e-01  5.60983362e-01  5.63340071e-01  5.64603989e-01
  5.70578438e-01  5.72865357e-01  5.78500851e-01  5.80239541e-01
  5.81651326e-01  5.86833997e-01  5.97024299e-01  6.04964811e-01
  6.08457911e-01  6.10492276e-01  6.16618218e-01  6.16972925e-01
  6.21391787e-01  6.21681099e-01  6.27895176e-01  6.29917125e-01
  6.30013293e-01  6.31008082e-01  6.32167307e-01  6.38227021e-01
  6.44463196e-01  6.44538101e-01  6.55959610e-01  6.60671021e-01
  6.64378940e-01  6.64379176e-01  6.65578641e-01  6.66588740e-01
  6.67185384e-01  6.70696318e-01  6.71457514e-01  6.72109707e-01
  6.72798270e-01  6.73169918e-01  6.73949007e-01  6.75604714e-01
  6.77958753e-01  6.78221138e-01  6.78411157e-01  6.81217600e-01
  6.81822692e-01  6.85037327e-01  6.85072142e-01  6.85247996e-01
  6.86830487e-01  6.87610491e-01  6.89636931e-01  6.91311841e-01
  6.92443374e-01  6.93015132e-01  6.94691172e-01  6.98056261e-01
  6.99521948e-01  7.01910802e-01  7.03290542e-01  7.05134835e-01
  7.05327266e-01  7.06475591e-01  7.07870464e-01  7.07941278e-01
  7.09282034e-01  7.10747722e-01  7.11099467e-01  7.12241364e-01
  7.13554165e-01  7.15943019e-01  7.16360608e-01  7.17701364e-01
  7.18510629e-01  7.23314251e-01  7.24468706e-01  7.26120695e-01
  7.27938127e-01  7.28746023e-01  7.28927138e-01  7.29080024e-01
  7.29707978e-01  7.31733581e-01  7.32781679e-01  7.33646096e-01
  7.34540025e-01  7.39896493e-01  7.41618599e-01  7.44160339e-01
  7.45319790e-01  7.45765798e-01  7.48035781e-01  7.50381915e-01
  7.51378685e-01  7.52426783e-01  7.53452851e-01  7.54185128e-01
  7.55650816e-01  7.56545049e-01  7.56991572e-01  7.57593661e-01
  7.58904086e-01  7.59798015e-01  7.62604459e-01  7.67239003e-01
  7.68217345e-01  7.70130096e-01  7.70725554e-01  7.72071887e-01
  7.78465613e-01  7.79794015e-01  7.83297660e-01  7.84476614e-01
  7.84874891e-01  7.86104103e-01  7.86913368e-01  7.89936615e-01
  7.91716990e-01  7.95918743e-01  7.98355945e-01  8.00136320e-01
  8.02942764e-01  8.03289975e-01  8.03516428e-01  8.17713966e-01
  8.23613936e-01  8.26850013e-01  8.32033266e-01  8.32402507e-01
  8.33232967e-01  8.34184852e-01  8.34839709e-01  8.35390872e-01
  8.36620084e-01  8.42232971e-01  8.43259039e-01  8.45076472e-01
  8.46065483e-01  8.54484813e-01  8.57291256e-01  8.60877703e-01
  8.61167963e-01  8.61563387e-01  8.62904143e-01  8.71323473e-01
  8.76490351e-01  8.76936360e-01  8.88162134e-01  8.90968577e-01
  8.91338054e-01  8.93377102e-01  8.95892901e-01  8.97346690e-01
  9.00987167e-01  9.05867777e-01  9.07807237e-01  9.10613681e-01
  9.12067470e-01  9.26099687e-01  9.27452341e-01  9.32171299e-01
  9.36919769e-01  9.37325460e-01  9.39023176e-01  9.39726212e-01
  9.42938347e-01  9.45339099e-01  9.46916330e-01  9.47419700e-01
  9.49722773e-01  9.54164121e-01  9.56970564e-01  9.59371316e-01
  9.59777007e-01  9.60976708e-01  9.62137442e-01  9.63783151e-01
  9.65389894e-01  9.66540631e-01  9.67790646e-01  9.72174320e-01
  9.73809224e-01  9.87841441e-01  9.88889539e-01  9.91695982e-01
  9.93454328e-01  9.96260771e-01  9.97308869e-01  9.98173286e-01
  9.99288573e-01  1.00468010e+00  1.00659262e+00  1.00748654e+00
  1.01029299e+00  1.01572476e+00  1.01590588e+00  1.01612889e+00
  1.01871232e+00  1.02937945e+00  1.02993809e+00  1.03098619e+00
  1.03555098e+00  1.03659908e+00  1.03835742e+00  1.04221196e+00
  1.04501841e+00  1.05327868e+00  1.05905062e+00  1.06466351e+00
  1.07493623e+00  1.07869573e+00  1.08150217e+00  1.08711506e+00
  1.09272794e+00  1.10676016e+00  1.11517949e+00  1.12013605e+00
  1.14867570e+00  1.26711939e+00  1.79238476e+00]

  UserWarning,

2022-10-31 11:02:49,783:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.93687547 -0.57463752 -0.51922338 -0.47493392 -0.46309081 -0.45315214
 -0.39291297 -0.3665535  -0.25490687 -0.20215651 -0.18373131 -0.18300332
 -0.16366255 -0.14410256 -0.14219383 -0.12923542 -0.12406035 -0.08613412
 -0.08206558 -0.07645082 -0.06993387 -0.06588381 -0.05704008 -0.05139737
 -0.04031512 -0.03228783 -0.01690983 -0.01427324 -0.01115054 -0.00866479
  0.00614432  0.02008152  0.02384665  0.02624864  0.02651848  0.03772956
  0.03808797  0.03909089  0.04073963  0.05627787  0.06475016  0.06614454
  0.07970702  0.08134484  0.0892537   0.09518123  0.09892181  0.09900897
  0.10173716  0.10299344  0.11278196  0.11420738  0.11566032  0.11591456
  0.12299947  0.12588834  0.12839433  0.13120949  0.13185554  0.132422
  0.13673626  0.13872544  0.14003073  0.14388185  0.14662132  0.14946086
  0.14963948  0.15032692  0.15081089  0.15464864  0.1549788   0.15771652
  0.15908046  0.16289163  0.16395212  0.1674637   0.16774583  0.1679354
  0.17061887  0.17391724  0.17427189  0.17502902  0.17591669  0.17922098
  0.17972273  0.18292037  0.18373878  0.18522646  0.1859498   0.18705743
  0.1880066   0.18838916  0.18907514  0.19143859  0.19173465  0.19193416
  0.19243999  0.19345728  0.19480008  0.19483103  0.1951397   0.19696977
  0.19908814  0.19933784  0.19945833  0.20104082  0.2027409   0.20320811
  0.20329915  0.20352445  0.20421378  0.20516236  0.20844476  0.20874853
  0.20876623  0.20878069  0.20899532  0.20998347  0.21011786  0.21256466
  0.21464747  0.21612684  0.21768626  0.22080825  0.22082103  0.22158174
  0.22177659  0.22217168  0.22278359  0.22402137  0.22442342  0.22588032
  0.22675452  0.22789419  0.22913772  0.23027481  0.2310233   0.23107341
  0.23111661  0.23287359  0.23395225  0.23410146  0.23575416  0.23617825
  0.23986351  0.23997412  0.24250332  0.24433212  0.24484054  0.2450555
  0.24688572  0.24703375  0.25083392  0.25106455  0.25206723  0.2526744
  0.25330145  0.25501504  0.25645289  0.25646188  0.2571863   0.25761386
  0.25853405  0.25875356  0.25923657  0.25999286  0.26049908  0.26090392
  0.26210316  0.26214047  0.26247688  0.26258974  0.26412765  0.26431299
  0.26715167  0.26768432  0.26888504  0.26909586  0.26912027  0.26997618
  0.27116566  0.27191656  0.27424201  0.27465964  0.27495939  0.27663723
  0.277516    0.27801936  0.27828027  0.28063221  0.28097016  0.28167238
  0.2823524   0.28249386  0.28257745  0.2835769   0.2838643   0.28501786
  0.28556227  0.2860138   0.28650234  0.29070613  0.29132512  0.29300914
  0.29303938  0.29394808  0.29486822  0.29676584  0.29752251  0.29929434
  0.29958333  0.30143966  0.30201225  0.30498091  0.30604252  0.30712537
  0.30858909  0.3120862   0.31221678  0.31231721  0.31340638  0.31397753
  0.31445252  0.31667542  0.31844591  0.31935141  0.3210057   0.32137931
  0.32285485  0.32374877  0.32449623  0.32714471  0.32748265  0.32760694
  0.3297994   0.33069573  0.33375074  0.33539831  0.33898414  0.33900259
  0.34095021  0.34351837  0.34393799  0.34440658  0.34558947  0.34756179
  0.34808886  0.34943064  0.34996657  0.35168927  0.35196523  0.35440411
  0.35586536  0.35792045  0.35949367  0.35970538  0.36027201  0.36091307
  0.36661294  0.36743811  0.36787334  0.37038256  0.37057053  0.37087044
  0.37118734  0.37155929  0.37181556  0.3731549   0.37370004  0.37419945
  0.37454797  0.37516718  0.37529537  0.37796574  0.37814089  0.37843656
  0.38118543  0.38157566  0.3821653   0.38229305  0.38230091  0.38335254
  0.38433648  0.38448664  0.38683933  0.38752607  0.38954571  0.38989535
  0.39029107  0.39336437  0.39356667  0.39504228  0.39560507  0.3956627
  0.39898952  0.39934895  0.40476731  0.40610873  0.40702195  0.412848
  0.41328719  0.41727273  0.42356376  0.42500864  0.42527907  0.42529769
  0.42587063  0.42634239  0.42720869  0.42733996  0.42799052  0.42964978
  0.4304499   0.4378283   0.43847003  0.43951242  0.44076365  0.442902
  0.44782862  0.44806946  0.45239758  0.45581683  0.45625545  0.46014096
  0.46440271  0.47045392  0.4726847   0.47314824  0.47377739  0.4740219
  0.47487536  0.47625905  0.47661633  0.48017126  0.48271706  0.48779376
  0.48865595  0.49456889  0.49586117  0.49962344  0.50022056  0.5007041
  0.50506462  0.50885816  0.5101982   0.51310885  0.51479563  0.51679188
  0.51690435  0.51755734  0.53626878  0.5408941   0.54151518  0.54866911
  0.56043101  0.56463211  0.5655064   0.56602623  0.5667762   0.57499635
  0.58010525  0.58236872  0.58303851  0.58541502  0.58611099  0.59385555
  0.61321238  0.61680902  0.61739956  0.62131438  0.62330489  0.63300927
  0.63459245  0.63463335  0.63954402  0.64114303  0.64728507  0.64738186
  0.65424444  0.65456255  0.65670719  0.6597867   0.66294289  0.66540978
  0.66783612  0.66945984  0.66965947  0.67066539  0.6735099   0.67393698
  0.67509308  0.67582149  0.68199057  0.68238349  0.68700699  0.68906065
  0.68971015  0.68981001  0.69125736  0.69196014  0.69534339  0.69781028
  0.69837533  0.70209404  0.7050374   0.70689526  0.70768101  0.71154365
  0.71742033  0.71806059  0.71853024  0.72107712  0.72211065  0.72372002
  0.72412091  0.7277439   0.73021078  0.73040284  0.73075321  0.73426084
  0.73584402  0.73691979  0.73732624  0.73821137  0.73908437  0.74278805
  0.74641103  0.74697608  0.74801575  0.74872262  0.75046109  0.75123822
  0.75210734  0.75391283  0.75856122  0.76167116  0.76261128  0.76666135
  0.77071141  0.7720273   0.77855146  0.77855501  0.77881153  0.78235487
  0.7828616   0.78533207  0.78613007  0.78691166  0.78849484  0.78855791
  0.79173519  0.79380853  0.79578525  0.79757895  0.80191172  0.80376912
  0.80388537  0.80772702  0.81279979  0.81603556  0.81690835  0.82008563
  0.82095841  0.82132238  0.82336222  0.82353737  0.82413569  0.83146235
  0.83177088  0.8320274   0.83223581  0.83457229  0.83551241  0.83597542
  0.83685092  0.83913989  0.84012752  0.84301422  0.84361254  0.844386
  0.85273537  0.854139    0.85820617  0.85887267  0.85998793  0.86037784
  0.86463632  0.86750293  0.86868638  0.86989193  0.86995151  0.87273644
  0.87344869  0.87618818  0.8767865   0.88083657  0.88185927  0.88317328
  0.88407262  0.88590933  0.88609218  0.88893669  0.88950174  0.89703682
  0.89993834  0.90033595  0.90048856  0.90108688  0.90387562  0.90930363
  0.90986868  0.91198649  0.91323707  0.91415798  0.91728713  0.91994608
  0.92102674  0.9250768   0.93317693  0.93476011  0.93607211  0.93753745
  0.9418421   0.94545168  0.94620262  0.94631924  0.95313938  0.95835281
  0.95850001  0.96395008  0.97061962  0.97323708  0.97709795  0.97735255
  0.97743975  0.97772762  0.9784938   0.98177768  0.98522942  0.99108336
  0.99392787  0.99449292  0.99797793  1.002028    1.00263523  1.00323355
  1.00601344  1.00607806  1.01417819  1.01474323  1.02470465  1.0262951
  1.02632837  1.03037844  1.03499355  1.04252863  1.04464088  1.05872888
  1.06577033  1.06739405  1.07270924  1.07549417  1.0835943   1.08764436
  1.08919157  1.09444719  1.09480437  1.09979455  1.10327957  1.10635564
  1.12004487  1.12126134  1.12814499  1.13685235  1.14272152  1.14708204
  1.21562265]

  UserWarning,

2022-10-31 11:02:49,830:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-9.10082179e-01 -8.76442440e-01 -6.51369554e-01 -4.90775106e-01
 -4.79369754e-01 -4.45968109e-01 -3.06507584e-01 -2.58435323e-01
 -2.57004185e-01 -2.09219816e-01 -1.27774678e-01 -1.25479223e-01
 -8.39942577e-02 -6.98246027e-02 -5.73604296e-02 -5.71414462e-02
 -5.71268388e-02 -5.62974241e-02 -4.88736950e-02 -4.81000726e-02
 -3.70698829e-02 -6.41915072e-03 -1.45109443e-03  1.04805353e-02
  1.53734815e-02  1.96319808e-02  2.40042139e-02  2.98284200e-02
  3.30029091e-02  3.78912291e-02  4.32433602e-02  4.94702187e-02
  5.30788177e-02  5.64074482e-02  6.54687187e-02  7.44448897e-02
  7.44691529e-02  7.50332373e-02  7.81052496e-02  8.84642920e-02
  8.84997878e-02  9.00874549e-02  9.51059380e-02  9.61369829e-02
  1.04895385e-01  1.05634312e-01  1.10472625e-01  1.12867561e-01
  1.14671553e-01  1.15773107e-01  1.16673620e-01  1.16856565e-01
  1.16934592e-01  1.22102841e-01  1.23962118e-01  1.26808635e-01
  1.26953660e-01  1.27061672e-01  1.29673178e-01  1.31388443e-01
  1.32517629e-01  1.34121819e-01  1.34399149e-01  1.41279175e-01
  1.45414363e-01  1.51085922e-01  1.55149084e-01  1.56179816e-01
  1.57270787e-01  1.59267562e-01  1.60744311e-01  1.63172309e-01
  1.63336905e-01  1.68464699e-01  1.70934300e-01  1.74295135e-01
  1.76049419e-01  1.76257856e-01  1.76549268e-01  1.76718098e-01
  1.77169396e-01  1.78915401e-01  1.81948277e-01  1.84593209e-01
  1.84884597e-01  1.85486775e-01  1.85539561e-01  1.85871786e-01
  1.86010073e-01  1.86292942e-01  1.87720815e-01  1.91192695e-01
  1.91792458e-01  1.94455565e-01  1.97451682e-01  1.99367840e-01
  2.00744171e-01  2.04072098e-01  2.04176084e-01  2.07124835e-01
  2.08171342e-01  2.08607485e-01  2.10578272e-01  2.11590698e-01
  2.14704516e-01  2.16148003e-01  2.16722727e-01  2.16792470e-01
  2.18097259e-01  2.18160792e-01  2.18175545e-01  2.19766300e-01
  2.20240707e-01  2.20408271e-01  2.21063447e-01  2.24311243e-01
  2.25285944e-01  2.26907754e-01  2.27965259e-01  2.28099471e-01
  2.29043367e-01  2.30094379e-01  2.30564269e-01  2.31459744e-01
  2.32951068e-01  2.34258276e-01  2.35346289e-01  2.35882191e-01
  2.37270388e-01  2.37512499e-01  2.37853024e-01  2.39374829e-01
  2.39853790e-01  2.39879231e-01  2.40275497e-01  2.41255599e-01
  2.44225751e-01  2.44267018e-01  2.45887616e-01  2.46442796e-01
  2.47200979e-01  2.47372906e-01  2.47953258e-01  2.48136477e-01
  2.48630261e-01  2.50792110e-01  2.50998315e-01  2.51003904e-01
  2.51870930e-01  2.52227932e-01  2.52993016e-01  2.53964011e-01
  2.54119273e-01  2.55973007e-01  2.56315863e-01  2.56852921e-01
  2.58170772e-01  2.59796595e-01  2.60509853e-01  2.60559043e-01
  2.60840408e-01  2.62923638e-01  2.64103150e-01  2.64639774e-01
  2.64788389e-01  2.67505226e-01  2.67615868e-01  2.67938329e-01
  2.69349728e-01  2.70294401e-01  2.70977682e-01  2.73312633e-01
  2.73724578e-01  2.73954133e-01  2.74538126e-01  2.75983218e-01
  2.76693474e-01  2.78883743e-01  2.80854991e-01  2.81287381e-01
  2.81679738e-01  2.82458846e-01  2.83850330e-01  2.86244548e-01
  2.86675395e-01  2.91169757e-01  2.91316036e-01  2.93840632e-01
  2.93847759e-01  2.93880819e-01  2.93927255e-01  2.95304432e-01
  2.97792696e-01  3.00018640e-01  3.00383726e-01  3.01899722e-01
  3.03877865e-01  3.04423491e-01  3.04896552e-01  3.05600291e-01
  3.06218992e-01  3.09109131e-01  3.09116492e-01  3.09432319e-01
  3.10816547e-01  3.10952955e-01  3.11444071e-01  3.12328633e-01
  3.12417150e-01  3.13901707e-01  3.15930979e-01  3.19487909e-01
  3.19974368e-01  3.24049106e-01  3.25459022e-01  3.25495671e-01
  3.25952029e-01  3.26131096e-01  3.27043285e-01  3.27591021e-01
  3.27695847e-01  3.28150473e-01  3.28221522e-01  3.29335349e-01
  3.29702472e-01  3.30642112e-01  3.31105261e-01  3.32284587e-01
  3.32740407e-01  3.32986824e-01  3.33849893e-01  3.34014254e-01
  3.34346672e-01  3.39647677e-01  3.40555013e-01  3.41279859e-01
  3.42328138e-01  3.45509071e-01  3.45710239e-01  3.47163081e-01
  3.47436065e-01  3.48315768e-01  3.48522513e-01  3.48749652e-01
  3.52517614e-01  3.53863718e-01  3.54913692e-01  3.55414182e-01
  3.57041748e-01  3.58914045e-01  3.61705984e-01  3.62343220e-01
  3.64624522e-01  3.64806357e-01  3.65220435e-01  3.66363573e-01
  3.66554198e-01  3.71437909e-01  3.74653692e-01  3.74739360e-01
  3.75263511e-01  3.76826667e-01  3.77980602e-01  3.78436757e-01
  3.79301157e-01  3.82491400e-01  3.83898095e-01  3.84576903e-01
  3.86144281e-01  3.87916285e-01  3.89469855e-01  3.90896075e-01
  3.95244663e-01  3.96446188e-01  3.98338453e-01  3.99127171e-01
  3.99164984e-01  4.01975245e-01  4.02710070e-01  4.04382964e-01
  4.08520327e-01  4.09352780e-01  4.09789315e-01  4.14443717e-01
  4.14664076e-01  4.15565120e-01  4.15647314e-01  4.18415332e-01
  4.23793138e-01  4.24625591e-01  4.25768970e-01  4.32195332e-01
  4.32199811e-01  4.32419255e-01  4.32496300e-01  4.32681749e-01
  4.33975012e-01  4.34807465e-01  4.37525416e-01  4.41871868e-01
  4.47670460e-01  4.49245521e-01  4.49836014e-01  4.51481912e-01
  4.51532866e-01  4.52711929e-01  4.52834924e-01  4.53268338e-01
  4.60438240e-01  4.61199884e-01  4.64225239e-01  4.66438565e-01
  4.69357595e-01  4.70960543e-01  4.74664675e-01  4.76361477e-01
  4.84948341e-01  4.85211803e-01  4.87029894e-01  4.93852895e-01
  4.98381992e-01  5.01498430e-01  5.02642969e-01  5.08054651e-01
  5.09373342e-01  5.10268766e-01  5.10578427e-01  5.11504888e-01
  5.14223984e-01  5.15260612e-01  5.20744480e-01  5.20854309e-01
  5.29556918e-01  5.36942133e-01  5.38563489e-01  5.39117089e-01
  5.43880213e-01  5.45812497e-01  5.48194742e-01  5.55694960e-01
  5.58966670e-01  5.58978318e-01  5.59293029e-01  5.60990566e-01
  5.61216127e-01  5.70284414e-01  5.75692916e-01  5.77907802e-01
  5.80832079e-01  5.81008293e-01  5.85323375e-01  5.85557225e-01
  5.89186405e-01  5.90648162e-01  5.96104890e-01  6.01472036e-01
  6.03140586e-01  6.03728279e-01  6.05920973e-01  6.07557101e-01
  6.08061333e-01  6.11220598e-01  6.23677067e-01  6.25870302e-01
  6.31375658e-01  6.32089151e-01  6.37857847e-01  6.39093747e-01
  6.52094166e-01  6.56830343e-01  6.58215998e-01  6.59110205e-01
  6.61921280e-01  6.63312532e-01  6.65587578e-01  6.66202706e-01
  6.66597798e-01  6.67012217e-01  6.70603343e-01  6.70609773e-01
  6.70870942e-01  6.72468945e-01  6.73494406e-01  6.73889874e-01
  6.75929457e-01  6.77194091e-01  6.82618397e-01  6.84096036e-01
  6.84659591e-01  6.87375965e-01  6.91234690e-01  6.92466902e-01
  6.92800271e-01  6.92832693e-01  6.96668326e-01  6.97923630e-01
  7.00842390e-01  7.03014567e-01  7.04157947e-01  7.04832169e-01
  7.08105504e-01  7.10222996e-01  7.12830650e-01  7.14601915e-01
  7.17370180e-01  7.17507168e-01  7.18287378e-01  7.19366803e-01
  7.19409059e-01  7.22971418e-01  7.23829406e-01  7.27689042e-01
  7.28469252e-01  7.33194398e-01  7.33877755e-01  7.34148421e-01
  7.36478949e-01  7.37818673e-01  7.38285335e-01  7.38651126e-01
  7.40302633e-01  7.40817786e-01  7.42543819e-01  7.43376272e-01
  7.45859555e-01  7.48333996e-01  7.48467209e-01  7.48833000e-01
  7.49334537e-01  7.53923937e-01  7.56306400e-01  7.58649083e-01
  7.63740020e-01  7.64105811e-01  7.64983590e-01  7.65760576e-01
  7.66223303e-01  7.68830957e-01  7.70031551e-01  7.76666011e-01
  7.77206445e-01  7.78170897e-01  7.80910371e-01  7.84103768e-01
  7.86587051e-01  7.89194705e-01  7.91263569e-01  7.91677988e-01
  7.94285642e-01  7.96064227e-01  7.99376579e-01  7.99709948e-01
  8.01859862e-01  8.01909030e-01  8.14126064e-01  8.14649390e-01
  8.14935900e-01  8.14982759e-01  8.17132673e-01  8.19273665e-01
  8.21539701e-01  8.25164633e-01  8.27314547e-01  8.29922201e-01
  8.31573708e-01  8.32405484e-01  8.41008929e-01  8.42266733e-01
  8.42518454e-01  8.47678295e-01  8.48358948e-01  8.50285949e-01
  8.50619318e-01  8.52769232e-01  8.53086797e-01  8.53344630e-01
  8.57860169e-01  8.60904358e-01  8.64396322e-01  8.65558760e-01
  8.70302650e-01  8.70983066e-01  8.74476000e-01  8.77775172e-01
  8.83314854e-01  8.87281390e-01  8.88405791e-01  8.93496728e-01
  8.94147662e-01  8.94525953e-01  8.96437751e-01  8.98561474e-01
  9.01442210e-01  9.08769539e-01  9.13860476e-01  9.16405581e-01
  9.17270456e-01  9.18488687e-01  9.18951413e-01  9.21581183e-01
  9.21995602e-01  9.24375719e-01  9.26150920e-01  9.27086539e-01
  9.32074310e-01  9.32510845e-01  9.34198033e-01  9.34224224e-01
  9.37165247e-01  9.37601782e-01  9.38070907e-01  9.41892688e-01
  9.44406098e-01  9.52541224e-01  9.52874593e-01  9.53946698e-01
  9.68147404e-01  9.69711575e-01  9.72904972e-01  9.74951720e-01
  9.77021445e-01  9.78329278e-01  9.80042657e-01  9.81488843e-01
  9.83086846e-01  9.84532063e-01  9.93268720e-01  9.93602089e-01
  9.99573952e-01  1.00073977e+00  1.00378396e+00  1.00466489e+00
  1.00854153e+00  1.00968491e+00  1.00975583e+00  1.01401902e+00
  1.01439602e+00  1.01554986e+00  1.01872341e+00  1.02414771e+00
  1.02424304e+00  1.03908715e+00  1.03930132e+00  1.03942052e+00
  1.04290176e+00  1.04417809e+00  1.04960240e+00  1.05274192e+00
  1.05469333e+00  1.05978427e+00  1.06122949e+00  1.06147146e+00
  1.06487521e+00  1.07165333e+00  1.07422463e+00  1.07505708e+00
  1.07879102e+00  1.08509965e+00  1.08875146e+00  1.08951474e+00
  1.09542083e+00  1.09967931e+00  1.10762326e+00  1.11069364e+00
  1.11823668e+00  1.13105739e+00  1.14588539e+00  1.15456066e+00
  1.16871450e+00  1.48164603e+00  1.81039475e+00]

  UserWarning,

2022-10-31 11:02:49,830:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.64268827 -0.99660573 -0.37418449 -0.35382033 -0.34653466 -0.33863599
 -0.33595239 -0.20650815 -0.18412706 -0.1839415  -0.18038059 -0.16388032
 -0.15688743 -0.14634985 -0.13842736 -0.10861606 -0.09143959 -0.08487879
 -0.08029366 -0.07052987 -0.05951303 -0.04808789 -0.03746261 -0.00784441
 -0.00341475  0.01018095  0.01149039  0.01292629  0.01615731  0.0229153
  0.02982826  0.03278635  0.0471334   0.05095456  0.05726271  0.05806863
  0.06216715  0.06242279  0.06565881  0.06650544  0.07104469  0.07259984
  0.07835931  0.08522284  0.08636039  0.09098011  0.09155626  0.0921569
  0.09364884  0.09589319  0.09992509  0.10006206  0.10279672  0.10563484
  0.10868046  0.10969085  0.11026788  0.11027234  0.11276478  0.12123106
  0.12237177  0.1257494   0.13057344  0.13190587  0.13231517  0.13359399
  0.13578725  0.14025139  0.14086721  0.14128372  0.14183909  0.1432671
  0.14340858  0.14658544  0.14735128  0.15074637  0.15130812  0.1531732
  0.15368653  0.15587814  0.15750649  0.15875537  0.15916553  0.16641298
  0.16711654  0.16845114  0.16879311  0.16885085  0.16922277  0.17018961
  0.17218243  0.17223853  0.17326673  0.17345973  0.1737407   0.17483492
  0.17832573  0.17892415  0.17965511  0.17979707  0.18084931  0.18215897
  0.18403179  0.1851698   0.18536633  0.18860544  0.18881668  0.18934409
  0.1895704   0.18973514  0.19450452  0.19543712  0.19724993  0.1982141
  0.19874251  0.19899273  0.19926871  0.19930897  0.19933693  0.19997089
  0.20147546  0.20314826  0.20362435  0.20400156  0.20407049  0.20435485
  0.20501166  0.20599422  0.20617404  0.20818475  0.20917812  0.20921391
  0.21014175  0.21249576  0.21329529  0.21344625  0.21419091  0.21433616
  0.21465026  0.21550668  0.21614975  0.21769869  0.21850674  0.21906566
  0.21931166  0.22384613  0.2247167   0.2255513   0.22805964  0.22858415
  0.23021651  0.23345796  0.23613891  0.23685532  0.23814197  0.23831332
  0.23967179  0.24003001  0.24027181  0.24042091  0.2411621   0.24409878
  0.24446539  0.24480985  0.24499584  0.24788623  0.24919843  0.24928686
  0.24951792  0.25156432  0.2516548   0.25370631  0.25376421  0.26041363
  0.26343468  0.26410148  0.26468318  0.26505606  0.26539328  0.26613418
  0.26671615  0.26798206  0.26872285  0.27069137  0.27120465  0.27212904
  0.27285657  0.27391569  0.27439489  0.27523302  0.27594006  0.27602029
  0.27759682  0.27874716  0.27962204  0.28012252  0.2805552   0.28065414
  0.28151907  0.28207002  0.28234514  0.28387559  0.28708339  0.28741704
  0.28776993  0.28919851  0.29090577  0.29199691  0.29208529  0.29287357
  0.29303806  0.29345724  0.29435646  0.29469348  0.29562281  0.2959253
  0.2978836   0.29838145  0.2987252   0.29918153  0.29931913  0.30011221
  0.30138686  0.30164303  0.30220324  0.30532614  0.30571588  0.30625727
  0.30636015  0.30894257  0.30902219  0.30919243  0.31089291  0.31119037
  0.31162057  0.31286652  0.31296897  0.31318769  0.32207364  0.32219205
  0.32264695  0.32283333  0.32338318  0.32773013  0.32954971  0.33000444
  0.33042978  0.33232236  0.33307123  0.33340693  0.33575127  0.33582213
  0.33626416  0.33824833  0.34152512  0.3416588   0.34514489  0.34720432
  0.34905184  0.34957304  0.35077629  0.35091268  0.35330285  0.35347433
  0.35355262  0.35399636  0.35610949  0.35717186  0.36051236  0.36103376
  0.36231302  0.36381533  0.36430033  0.36499245  0.36501622  0.36597971
  0.36672059  0.36763758  0.36786968  0.36973117  0.37158977  0.37217093
  0.37409585  0.37474287  0.37494699  0.37894945  0.37954654  0.3797344
  0.38029733  0.38197432  0.38268926  0.38394733  0.38429409  0.38613621
  0.38648298  0.38868605  0.39221897  0.39362703  0.39376326  0.39664027
  0.39816349  0.39832159  0.39949677  0.40079737  0.40118807  0.40325766
  0.40764093  0.40874932  0.40933657  0.4094791   0.4111438   0.4119912
  0.41449895  0.41469997  0.41678622  0.41735672  0.42049718  0.42536018
  0.42821728  0.43533668  0.43560817  0.4466834   0.44899285  0.44913248
  0.45143867  0.45679018  0.45856241  0.45874301  0.46291     0.46414553
  0.46534341  0.46894201  0.47083801  0.47119947  0.47798516  0.47877428
  0.47964937  0.48169213  0.48213385  0.48313621  0.4854332   0.49030428
  0.4966217   0.50071355  0.50847935  0.51244645  0.51509427  0.52062463
  0.52164855  0.52172006  0.52204792  0.52402979  0.52762382  0.53616908
  0.54064842  0.54257149  0.54388409  0.54398474  0.54713701  0.54725056
  0.54838121  0.55131742  0.55557698  0.55746859  0.56187086  0.5646443
  0.56812704  0.56903575  0.56938302  0.57187222  0.57804795  0.5921946
  0.5939659   0.59596096  0.59800876  0.60000632  0.60454872  0.60748996
  0.61765951  0.63661439  0.64409947  0.65347568  0.65556927  0.65589651
  0.65826063  0.66030799  0.66365495  0.66381108  0.66626802  0.6716075
  0.6767414   0.68371037  0.68400159  0.68664672  0.68874031  0.6922434
  0.69347903  0.69557929  0.70160376  0.7056016   0.70795981  0.7125168
  0.713691    0.71507904  0.7156103   0.71805993  0.72002158  0.72034902
  0.72455648  0.72475803  0.72494828  0.72545851  0.73138879  0.73877265
  0.74086623  0.74301388  0.746452    0.74697634  0.74779765  0.74825009
  0.74910804  0.75062201  0.75298881  0.75374324  0.75508239  0.75536073
  0.75777375  0.76009945  0.76246625  0.76285805  0.76455983  0.77025562
  0.77055564  0.77194369  0.77284571  0.778776    0.78333398  0.78351472
  0.78379305  0.78627053  0.78643818  0.78825344  0.78853177  0.78921318
  0.79481423  0.79602909  0.79773088  0.79898797  0.80056543  0.80076781
  0.80123396  0.80457     0.80481245  0.80647174  0.80748665  0.80829112
  0.815166    0.81651666  0.81822118  0.82263297  0.82328788  0.82644153
  0.82911644  0.83016535  0.83118786  0.83354705  0.83564064  0.83828577
  0.84511808  0.84539641  0.84585896  0.84815501  0.84945442  0.85013513
  0.85459552  0.85791078  0.85859767  0.86059984  0.8616346   0.86617337
  0.87105061  0.8725931   0.87856745  0.87945475  0.88132605  0.8880449
  0.89278362  0.89628309  0.89752234  0.89786961  0.90226106  0.90362091
  0.90847438  0.91647722  0.91682449  0.91873333  0.9197667   0.92506629
  0.92595466  0.92599964  0.9300908   0.93063827  0.93069338  0.93216798
  0.93407758  0.93485148  0.93517679  0.93617297  0.9381907   0.94150535
  0.95268519  0.95606383  0.95662591  0.95742391  0.96358487  0.96386442
  0.96860314  0.96895041  0.97334186  0.97842785  0.98958214  0.99229674
  1.0014719   1.00686017  1.00774747  1.00896922  1.01159889  1.01633761
  1.01847854  1.02077405  1.02107633  1.0228984   1.02581505  1.03010907
  1.03209406  1.03529249  1.04223546  1.04432523  1.04780687  1.05728431
  1.0585414   1.05898609  1.06372481  1.07320226  1.08097791  1.0852642
  1.08571663  1.08624865  1.09519407  1.10082009  1.10467151  1.11066732
  1.11070964  1.12362639  1.1250259   1.12796273  1.13579519  1.13784255
  1.14258127  1.14484504  1.15205871  1.156345    1.16597259  1.17451668
  1.18391967  1.18482737  1.20208471  1.22283723  1.23231467  1.32795596
  1.46032158  3.07740705]

  UserWarning,

2022-10-31 11:02:50,368:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:50,384:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:02:50,706:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.64014118 -0.36876024 -0.35803047 -0.32756465 -0.29310233 -0.2832101
 -0.27004623 -0.24221667 -0.23116187 -0.21133078 -0.20898645 -0.19003144
 -0.17295199 -0.16415602 -0.12811308 -0.12213404 -0.10560491 -0.07636008
 -0.07578691 -0.05809929 -0.04760263 -0.04055405 -0.03154922 -0.03142498
 -0.0263632  -0.02449662 -0.01547774 -0.01204083 -0.01107757 -0.0102332
 -0.0077892   0.00627323  0.00861486  0.00997257  0.01434047  0.02325376
  0.02872651  0.03507737  0.05022481  0.05070224  0.05557179  0.05860642
  0.05947838  0.0621791   0.06226546  0.07273602  0.07353324  0.07794524
  0.08112785  0.08569672  0.09177711  0.09349709  0.09376251  0.09616806
  0.09636823  0.09643875  0.09736742  0.10086329  0.10238432  0.10608444
  0.11314896  0.11452871  0.11479156  0.11619132  0.11703518  0.11721816
  0.11808974  0.12238853  0.12375111  0.12391745  0.1288741   0.13016737
  0.13034572  0.13411175  0.13580306  0.13785956  0.13842318  0.13890027
  0.13909509  0.13963623  0.14048787  0.1411126   0.14131727  0.14225198
  0.14295473  0.14316799  0.14428865  0.14823777  0.15014493  0.15071714
  0.15084461  0.15408397  0.15413078  0.15458095  0.15463138  0.15587204
  0.15594025  0.15745996  0.15998464  0.16027075  0.16245027  0.16449597
  0.16661019  0.17286483  0.17474409  0.17706508  0.18031069  0.18151894
  0.18204969  0.1833091   0.18700583  0.18832139  0.18849844  0.19088117
  0.19200624  0.19289943  0.19333434  0.19359753  0.1958921   0.19610274
  0.19620904  0.19625731  0.19944809  0.20159147  0.20338311  0.20350458
  0.2036199   0.20399136  0.20413792  0.20442348  0.20465545  0.20645976
  0.20980866  0.2099405   0.21016552  0.21034925  0.21119633  0.21137359
  0.21174173  0.21195604  0.21206415  0.21259812  0.21286728  0.21307874
  0.2136321   0.21389152  0.2139773   0.21415366  0.21418988  0.21423697
  0.21491199  0.21517178  0.21611843  0.21683153  0.21819511  0.21830473
  0.218681    0.22004232  0.22010168  0.22050979  0.22060266  0.22170289
  0.22392158  0.22421365  0.22468643  0.22513857  0.22567992  0.22727347
  0.22756002  0.22806399  0.23006441  0.23057156  0.23196603  0.23274739
  0.23363463  0.235747    0.2358274   0.23596777  0.24005912  0.24059217
  0.24065036  0.24087718  0.24130887  0.24150358  0.24387915  0.24523751
  0.245521    0.24615135  0.24786879  0.24980821  0.25043642  0.2513165
  0.25157926  0.25382436  0.254507    0.25842344  0.25990254  0.26059308
  0.26145186  0.26227032  0.26266774  0.26501612  0.26539197  0.26647899
  0.2666396   0.26690676  0.26878468  0.26879058  0.26991844  0.27187798
  0.27239393  0.27604717  0.27620344  0.27622671  0.27633202  0.27680659
  0.27765741  0.27876964  0.27939954  0.28020543  0.28065526  0.28283282
  0.28284908  0.28410353  0.28516901  0.28715759  0.28804301  0.28807114
  0.29053456  0.2908448   0.2931742   0.29324354  0.29331571  0.2936626
  0.29405613  0.29549802  0.2976909   0.30212265  0.3046885   0.30834668
  0.31274573  0.31453912  0.31520612  0.3163365   0.31686414  0.31867459
  0.32055125  0.32083154  0.3220544   0.32214607  0.32250107  0.32270164
  0.3237012   0.32423836  0.32531381  0.32678729  0.32771733  0.32863526
  0.32878073  0.32901502  0.32922701  0.32934198  0.32952029  0.33087654
  0.3309439   0.33153928  0.33161257  0.3316482   0.33291083  0.33428336
  0.33473251  0.33898679  0.3411482   0.3426739   0.34400607  0.34461499
  0.34496786  0.34603599  0.34636101  0.34715022  0.34912944  0.34997482
  0.35004811  0.35102364  0.3530014   0.35373522  0.3543877   0.35586834
  0.3561025   0.35742233  0.35815641  0.35860192  0.35972663  0.36010472
  0.36033291  0.36705221  0.36892401  0.37050488  0.37284467  0.37587239
  0.37907639  0.3804239   0.39075862  0.39459331  0.39995061  0.40076387
  0.40451787  0.40618869  0.4090917   0.41047634  0.41107336  0.41181687
  0.41182519  0.41421578  0.41827827  0.41936189  0.42114598  0.42357735
  0.42462793  0.42657362  0.42831504  0.43038401  0.43200214  0.43394784
  0.43568925  0.43937636  0.43963494  0.44173698  0.44417613  0.45523745
  0.45898657  0.46360002  0.46743605  0.46962924  0.48077211  0.48127325
  0.49739133  0.49801431  0.50316986  0.50553678  0.50685697  0.50899545
  0.51230952  0.51958898  0.52153211  0.5216054   0.52366927  0.52377707
  0.52614177  0.53282318  0.53773696  0.54544256  0.54757161  0.55454055
  0.56316989  0.5639836   0.57079891  0.57513721  0.58593468  0.59199781
  0.5924981   0.59294705  0.59380808  0.59600314  0.59651509  0.60028551
  0.60106354  0.60457851  0.61222957  0.61715835  0.61855321  0.61863774
  0.62692456  0.628688    0.63135153  0.63974932  0.64712354  0.64843188
  0.64878709  0.6512883   0.65182172  0.65550883  0.65866252  0.65919593
  0.66288304  0.66830781  0.67394437  0.67498443  0.67830788  0.68131858
  0.68565817  0.6886928   0.68943757  0.69237991  0.69415218  0.69584815
  0.69606702  0.69671949  0.69975413  0.70170443  0.70196747  0.70344123
  0.7043408   0.70712834  0.70737191  0.70778082  0.70788552  0.71515504
  0.71560103  0.71574073  0.71603386  0.71884214  0.72097301  0.72252925
  0.72556388  0.72925099  0.7329381   0.73662521  0.73857551  0.74031232
  0.74096479  0.74154588  0.74218574  0.74258428  0.74399942  0.74483707
  0.74720389  0.74919759  0.75068213  0.75874786  0.76100754  0.76144293
  0.76331635  0.76973589  0.76980918  0.77346265  0.77349629  0.77620736
  0.7771834   0.77997135  0.78087051  0.78282081  0.78302968  0.78348043
  0.79012173  0.79193183  0.7946269   0.79492743  0.79861454  0.80435369
  0.80494346  0.81231768  0.8132635   0.81489213  0.823379    0.82504969
  0.82612378  0.82706611  0.82789915  0.83262664  0.83527337  0.83936561
  0.84063777  0.84181454  0.84550165  0.84673983  0.84715411  0.84749375
  0.84831971  0.84897831  0.84918876  0.84993353  0.85287587  0.85656298
  0.86025008  0.86038566  0.86040654  0.86205973  0.86477023  0.86558965
  0.8676243   0.86778076  0.87019875  0.87296387  0.87868563  0.87952327
  0.88056857  0.88075684  0.88229944  0.88252919  0.88605984  0.88614301
  0.88730583  0.88974695  0.8926639   0.89704787  0.89837779  0.90164592
  0.90533303  0.90539661  0.90691153  0.90803562  0.90818249  0.91202606
  0.91217384  0.91548342  0.92008147  0.93114279  0.93414871  0.9363351
  0.94129317  0.94152293  0.94173179  0.94505358  0.94521003  0.94589122
  0.95258425  0.95627136  0.95695255  0.95710901  0.95995847  0.96364557
  0.96448322  0.96609451  0.96733268  0.96801387  0.97061926  0.97463361
  0.9747069   0.97923166  0.98208112  0.98276231  0.98291876  0.98576822
  0.98806969  0.98941877  0.99314244  0.99398009  0.99711747  1.00051666
  1.00420377  1.00504141  1.00669387  1.00789087  1.01157798  1.01610274
  1.01886323  1.02632642  1.03001352  1.03231499  1.03738774  1.03822539
  1.0419125   1.05175526  1.05666093  1.06034804  1.06229373  1.06403515
  1.06772225  1.06937471  1.07140936  1.07509647  1.08247069  1.0861578
  1.09149736  1.09353201  1.10090623  1.10737668  1.11104094  1.1214092
  1.1239078   1.12581222  1.48957267]

  UserWarning,

2022-10-31 11:02:50,721:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.09698041 -0.93489423 -0.67909634 -0.42387626 -0.39510235 -0.32232198
 -0.32052321 -0.31343848 -0.30723898 -0.29863338 -0.29844982 -0.28825512
 -0.26525356 -0.2547263  -0.25049992 -0.21067178 -0.20567348 -0.17996869
 -0.16749471 -0.15594796 -0.14035947 -0.1401346  -0.13194009 -0.09097088
 -0.08324782 -0.07484575 -0.05976917 -0.04816768 -0.02738447 -0.02496643
 -0.02026285 -0.01913742 -0.0142563  -0.00838906  0.01499846  0.01607746
  0.0202345   0.02696049  0.03195615  0.05752003  0.06148725  0.0638901
  0.0643944   0.07215163  0.07328931  0.07336343  0.08275033  0.08344441
  0.08485467  0.09130012  0.10051099  0.10184953  0.10594226  0.10615584
  0.10876492  0.11028439  0.1111815   0.1130144   0.11422519  0.11491497
  0.11687198  0.11751424  0.11768288  0.11812445  0.11831637  0.1226174
  0.12467633  0.12515473  0.12549954  0.12659312  0.12750793  0.12825516
  0.12861861  0.1294524   0.13177067  0.13417375  0.13548645  0.14078839
  0.14450078  0.14473415  0.14612131  0.14706283  0.14725901  0.14760948
  0.14866793  0.15032408  0.15131493  0.15334943  0.15548928  0.15699821
  0.16100481  0.16141921  0.16176893  0.16203958  0.16275774  0.16506976
  0.16761582  0.16930762  0.16949316  0.17055914  0.17099432  0.17163866
  0.17218171  0.17570929  0.1773926   0.17790797  0.17811287  0.17813146
  0.1790638   0.1796334   0.17982133  0.1813103   0.18177585  0.18189765
  0.18337321  0.1837039   0.18475971  0.18527168  0.1863104   0.18642948
  0.18699076  0.18699432  0.18744196  0.18751691  0.18854392  0.18978309
  0.18983044  0.19013202  0.19081167  0.19263177  0.19381546  0.19511597
  0.19540364  0.19705999  0.19780319  0.19870463  0.1998932   0.20154652
  0.20412233  0.2047783   0.20543744  0.20549172  0.2060103   0.20602323
  0.20613605  0.20714065  0.2081876   0.20962398  0.21360719  0.21450951
  0.21513768  0.21556146  0.21821398  0.2184736   0.21865054  0.21909448
  0.21916846  0.2193558   0.21974734  0.22186307  0.2225252   0.22509509
  0.22669453  0.22675357  0.22760598  0.22781627  0.22842715  0.23116979
  0.23339175  0.23382659  0.23629838  0.23697657  0.23805995  0.23807891
  0.2382192   0.24414175  0.24592146  0.24801951  0.2486207   0.252533
  0.25254272  0.25348342  0.25384768  0.25415437  0.25593558  0.25650364
  0.25695171  0.25973528  0.26059681  0.26114593  0.26182845  0.26225311
  0.26275442  0.26348551  0.26449586  0.26480375  0.26483515  0.26488766
  0.26518465  0.26537244  0.26599289  0.26811371  0.26816134  0.26935638
  0.2697778   0.27082433  0.27457814  0.27509456  0.27639758  0.27818368
  0.28003139  0.28111382  0.28113638  0.28132355  0.28276128  0.28465949
  0.28878948  0.29102102  0.29193934  0.29300462  0.29363105  0.29405941
  0.2947156   0.29495334  0.29609868  0.29656239  0.29700237  0.29864126
  0.29943582  0.2998293   0.3002518   0.30192614  0.30363756  0.30602662
  0.30675355  0.30676373  0.30684305  0.30791045  0.30957994  0.30962361
  0.31147214  0.31162814  0.31259754  0.31269889  0.31335885  0.31553536
  0.31581693  0.31599733  0.31708171  0.31743371  0.31980057  0.32082761
  0.32086786  0.32195303  0.32205393  0.32271603  0.32284379  0.32415229
  0.32554082  0.32631643  0.32905341  0.32911543  0.33129189  0.33452791
  0.33591034  0.33686001  0.33709492  0.33738557  0.33757721  0.33764641
  0.33790547  0.33848506  0.34076491  0.34076624  0.34203181  0.34835234
  0.35017834  0.35043704  0.35129162  0.35480486  0.35741871  0.35888212
  0.36230464  0.36671154  0.37246439  0.37288069  0.37390069  0.37968766
  0.38110823  0.38359859  0.38458604  0.38633067  0.39152058  0.39444002
  0.39482102  0.39777956  0.39793951  0.40045757  0.40061453  0.40064265
  0.40250018  0.40307144  0.40417651  0.40468126  0.40561712  0.40804128
  0.41063436  0.41502731  0.41519609  0.4163432   0.41966809  0.41976899
  0.42156036  0.42354095  0.42987075  0.43300018  0.43533093  0.43683755
  0.44295581  0.44660699  0.45170023  0.45530823  0.45793722  0.45959354
  0.46045224  0.46119348  0.46326679  0.46406464  0.46864646  0.47069458
  0.47510243  0.47842858  0.48302029  0.4853531   0.4858726   0.48925995
  0.49140181  0.49179274  0.49412169  0.49535918  0.50489326  0.51038777
  0.5126102   0.51718866  0.51960485  0.52055564  0.52361504  0.52679264
  0.55174061  0.57147832  0.5752527   0.58126139  0.58371876  0.58705361
  0.59323789  0.60139944  0.60182858  0.60266328  0.60577324  0.60614627
  0.60890028  0.60969667  0.61201877  0.61716993  0.61874289  0.62202476
  0.62449276  0.63942632  0.64008524  0.64059497  0.64315958  0.64569431
  0.65255923  0.65704365  0.6582382   0.6602214   0.66135669  0.66447519
  0.66466     0.66503322  0.67127021  0.6719207   0.67325168  0.67383068
  0.67438871  0.67454877  0.67694918  0.6775072   0.67822529  0.68226773
  0.68281296  0.68318617  0.68680942  0.68830957  0.68942316  0.69309969
  0.69566016  0.69611728  0.69621818  0.698169    0.69933668  0.70070526
  0.70189715  0.70245518  0.70722346  0.70813414  0.70869217  0.71087106
  0.71125264  0.71181067  0.71437114  0.71492916  0.71714078  0.71745542
  0.71804766  0.71932951  0.72060813  0.72116616  0.72532632  0.72545306
  0.72709143  0.72740315  0.72844482  0.73139412  0.74152692  0.74243761
  0.7471558   0.75339279  0.75411088  0.75481069  0.75546962  0.75707174
  0.75848721  0.75858811  0.75886759  0.75935211  0.76083435  0.76168061
  0.76426708  0.76586678  0.7665372   0.7678427   0.7679436   0.77012249
  0.77210377  0.77522227  0.77850784  0.79081475  0.79779802  0.80091651
  0.80604584  0.80715351  0.810272    0.8133905   0.81558913  0.81588946
  0.8164081   0.81683122  0.81812986  0.82417861  0.82619888  0.82807611
  0.82898298  0.83119757  0.83210148  0.83515008  0.83535774  0.8379659
  0.83833847  0.84144842  0.84311353  0.84348963  0.84372829  0.84382919
  0.84678709  0.84976809  0.85299135  0.85417944  0.85704945  0.85886638
  0.86016795  0.86041644  0.86291323  0.86952344  0.87289042  0.87576043
  0.87887893  0.88199743  0.88511592  0.89135292  0.8916014   0.90070841
  0.90351736  0.90406684  0.91024248  0.91941938  0.92270305  0.92996297
  0.93213331  0.93502979  0.93827795  0.93837885  0.94149735  0.94452712
  0.95004603  0.95085284  0.95211975  0.95466459  0.95698893  0.95708983
  0.95792854  0.96020833  0.96125     0.97258141  0.9820378   0.9850554
  0.9882748   0.9889253   0.98954171  0.99139329  0.99555346  0.99756039
  0.99851432  1.00698578  1.00786981  1.00802745  1.0091205   1.01104504
  1.01114594  1.01426444  1.01586716  1.01634127  1.01713445  1.01738294
  1.01945976  1.02009531  1.02257826  1.02346229  1.02361993  1.02455954
  1.02673843  1.02985692  1.03287452  1.0419577   1.0485679   1.0500203
  1.0516864   1.05464726  1.0548049   1.05678082  1.05792339  1.06104189
  1.06416039  1.07039738  1.0731247   1.07975287  1.08287137  1.08438473
  1.09206922  1.09739934  1.10441173  1.10870337  1.16483631  1.30637236
  1.33721137  1.43694916  1.54531314]

  UserWarning,

2022-10-31 11:02:50,721:INFO:Calculating mean and std
2022-10-31 11:02:50,721:INFO:Creating metrics dataframe
2022-10-31 11:02:50,721:INFO:Uploading results into container
2022-10-31 11:02:50,721:INFO:Uploading model into container now
2022-10-31 11:02:50,721:INFO:master_model_container: 20
2022-10-31 11:02:50,721:INFO:display_container: 2
2022-10-31 11:02:50,721:INFO:HuberRegressor()
2022-10-31 11:02:50,721:INFO:create_model() successfully completed......................................
2022-10-31 11:02:50,831:ERROR:create_model() for HuberRegressor() raised an exception or returned all 0.0:
2022-10-31 11:02:50,846:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:50,846:INFO:Initializing K Neighbors Regressor
2022-10-31 11:02:50,846:INFO:Total runtime is 1.2976085980733236 minutes
2022-10-31 11:02:50,847:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:50,848:INFO:Initializing create_model()
2022-10-31 11:02:50,848:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:50,848:INFO:Checking exceptions
2022-10-31 11:02:50,848:INFO:Importing libraries
2022-10-31 11:02:50,848:INFO:Copying training dataset
2022-10-31 11:02:50,856:INFO:Defining folds
2022-10-31 11:02:50,856:INFO:Declaring metric variables
2022-10-31 11:02:50,856:INFO:Importing untrained model
2022-10-31 11:02:50,857:INFO:K Neighbors Regressor Imported successfully
2022-10-31 11:02:50,858:INFO:Starting cross validation
2022-10-31 11:02:50,859:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:53,213:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,319:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,397:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,397:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,454:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,487:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,520:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:53,551:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:54,519:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:54,535:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:54,535:INFO:Calculating mean and std
2022-10-31 11:02:54,535:INFO:Creating metrics dataframe
2022-10-31 11:02:54,535:INFO:Uploading results into container
2022-10-31 11:02:54,551:INFO:Uploading model into container now
2022-10-31 11:02:54,551:INFO:master_model_container: 21
2022-10-31 11:02:54,551:INFO:display_container: 2
2022-10-31 11:02:54,551:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-31 11:02:54,551:INFO:create_model() successfully completed......................................
2022-10-31 11:02:54,654:WARNING:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:02:54,654:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:54,654:INFO:Initializing create_model()
2022-10-31 11:02:54,654:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:54,654:INFO:Checking exceptions
2022-10-31 11:02:54,654:INFO:Importing libraries
2022-10-31 11:02:54,654:INFO:Copying training dataset
2022-10-31 11:02:54,672:INFO:Defining folds
2022-10-31 11:02:54,672:INFO:Declaring metric variables
2022-10-31 11:02:54,672:INFO:Importing untrained model
2022-10-31 11:02:54,673:INFO:K Neighbors Regressor Imported successfully
2022-10-31 11:02:54,673:INFO:Starting cross validation
2022-10-31 11:02:54,675:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:02:57,042:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,042:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,102:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,227:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,282:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,287:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,333:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:57,380:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:58,348:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:58,363:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2 0.4 0.6 0.8]

  UserWarning,

2022-10-31 11:02:58,363:INFO:Calculating mean and std
2022-10-31 11:02:58,363:INFO:Creating metrics dataframe
2022-10-31 11:02:58,379:INFO:Uploading results into container
2022-10-31 11:02:58,379:INFO:Uploading model into container now
2022-10-31 11:02:58,379:INFO:master_model_container: 22
2022-10-31 11:02:58,379:INFO:display_container: 2
2022-10-31 11:02:58,379:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-31 11:02:58,379:INFO:create_model() successfully completed......................................
2022-10-31 11:02:58,502:ERROR:create_model() for KNeighborsRegressor(n_jobs=-1) raised an exception or returned all 0.0:
2022-10-31 11:02:58,502:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:02:58,502:INFO:Initializing Decision Tree Regressor
2022-10-31 11:02:58,502:INFO:Total runtime is 1.4252038121223451 minutes
2022-10-31 11:02:58,502:INFO:SubProcess create_model() called ==================================
2022-10-31 11:02:58,502:INFO:Initializing create_model()
2022-10-31 11:02:58,502:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:02:58,502:INFO:Checking exceptions
2022-10-31 11:02:58,502:INFO:Importing libraries
2022-10-31 11:02:58,502:INFO:Copying training dataset
2022-10-31 11:02:58,502:INFO:Defining folds
2022-10-31 11:02:58,502:INFO:Declaring metric variables
2022-10-31 11:02:58,502:INFO:Importing untrained model
2022-10-31 11:02:58,502:INFO:Decision Tree Regressor Imported successfully
2022-10-31 11:02:58,502:INFO:Starting cross validation
2022-10-31 11:02:58,518:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:00,731:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.42857143 0.45454545 0.5
 0.55555556 0.58333333 0.6        0.61538462 0.63636364 0.66666667
 0.7        0.71428571 0.75       0.8        0.83333333 0.875
 0.88888889 0.9       ]

  UserWarning,

2022-10-31 11:03:00,793:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.375      0.41666667 0.44444444
 0.5        0.57142857 0.58333333 0.6        0.625      0.66666667
 0.7        0.71428571 0.75       0.8        0.83333333 0.85714286
 0.875     ]

  UserWarning,

2022-10-31 11:03:00,793:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.28571429 0.33333333 0.42857143 0.5        0.52941176
 0.57142857 0.58333333 0.6        0.625      0.66666667 0.69230769
 0.71428571 0.75       0.76923077 0.77777778 0.8        0.8125
 0.83333333 0.85714286 0.88888889 0.9       ]

  UserWarning,

2022-10-31 11:03:00,793:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.33333333 0.375      0.4        0.42857143 0.44444444 0.5
 0.54545455 0.57142857 0.58333333 0.6        0.66666667 0.69230769
 0.7        0.71428571 0.75       0.8        0.83333333 0.84615385
 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:00,824:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.28571429 0.33333333 0.4        0.41666667 0.45454545 0.5
 0.58333333 0.6        0.625      0.64285714 0.66666667 0.69230769
 0.75       0.8        0.83333333 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:00,858:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.33333333 0.4        0.42857143 0.5        0.55555556
 0.58333333 0.66666667 0.71428571 0.75       0.8        0.81818182
 0.875     ]

  UserWarning,

2022-10-31 11:03:00,865:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.33333333 0.4        0.5        0.58333333 0.6
 0.63636364 0.66666667 0.69230769 0.75       0.76923077 0.77777778
 0.8        0.83333333 0.85714286]

  UserWarning,

2022-10-31 11:03:00,872:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.33333333 0.4        0.42857143 0.5        0.57142857
 0.58333333 0.6        0.66666667 0.7        0.71428571 0.72727273
 0.75       0.78571429 0.8        0.81818182 0.83333333 0.88888889]

  UserWarning,

2022-10-31 11:03:01,882:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.33333333 0.375      0.5        0.54545455 0.57142857
 0.6        0.625      0.63636364 0.66666667 0.7        0.71428571
 0.75       0.77777778 0.8        0.81818182 0.85714286 0.88888889]

  UserWarning,

2022-10-31 11:03:01,884:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.375      0.4        0.44444444
 0.5        0.54545455 0.57142857 0.61538462 0.625      0.66666667
 0.7        0.75       0.78571429 0.8        0.81818182 0.83333333
 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:01,884:INFO:Calculating mean and std
2022-10-31 11:03:01,884:INFO:Creating metrics dataframe
2022-10-31 11:03:01,884:INFO:Uploading results into container
2022-10-31 11:03:01,884:INFO:Uploading model into container now
2022-10-31 11:03:01,884:INFO:master_model_container: 23
2022-10-31 11:03:01,884:INFO:display_container: 2
2022-10-31 11:03:01,884:INFO:DecisionTreeRegressor(random_state=3360)
2022-10-31 11:03:01,884:INFO:create_model() successfully completed......................................
2022-10-31 11:03:01,996:WARNING:create_model() for DecisionTreeRegressor(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:01,996:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:01,996:INFO:Initializing create_model()
2022-10-31 11:03:01,996:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:01,996:INFO:Checking exceptions
2022-10-31 11:03:02,014:INFO:Importing libraries
2022-10-31 11:03:02,014:INFO:Copying training dataset
2022-10-31 11:03:02,020:INFO:Defining folds
2022-10-31 11:03:02,020:INFO:Declaring metric variables
2022-10-31 11:03:02,020:INFO:Importing untrained model
2022-10-31 11:03:02,021:INFO:Decision Tree Regressor Imported successfully
2022-10-31 11:03:02,021:INFO:Starting cross validation
2022-10-31 11:03:02,023:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:04,411:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.42857143 0.45454545 0.5
 0.55555556 0.58333333 0.6        0.61538462 0.63636364 0.66666667
 0.7        0.71428571 0.75       0.8        0.83333333 0.875
 0.88888889 0.9       ]

  UserWarning,

2022-10-31 11:03:04,496:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.375      0.41666667 0.44444444
 0.5        0.57142857 0.58333333 0.6        0.625      0.66666667
 0.7        0.71428571 0.75       0.8        0.83333333 0.85714286
 0.875     ]

  UserWarning,

2022-10-31 11:03:04,499:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.33333333 0.375      0.4        0.42857143 0.44444444 0.5
 0.54545455 0.57142857 0.58333333 0.6        0.66666667 0.69230769
 0.7        0.71428571 0.75       0.8        0.83333333 0.84615385
 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:04,515:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.28571429 0.33333333 0.42857143 0.5        0.52941176
 0.57142857 0.58333333 0.6        0.625      0.66666667 0.69230769
 0.71428571 0.75       0.76923077 0.77777778 0.8        0.8125
 0.83333333 0.85714286 0.88888889 0.9       ]

  UserWarning,

2022-10-31 11:03:04,584:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.33333333 0.4        0.5        0.58333333 0.6
 0.63636364 0.66666667 0.69230769 0.75       0.76923077 0.77777778
 0.8        0.83333333 0.85714286]

  UserWarning,

2022-10-31 11:03:04,601:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.28571429 0.33333333 0.4        0.41666667 0.45454545 0.5
 0.58333333 0.6        0.625      0.64285714 0.66666667 0.69230769
 0.75       0.8        0.83333333 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:04,633:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.33333333 0.4        0.42857143 0.5        0.57142857
 0.58333333 0.6        0.66666667 0.7        0.71428571 0.72727273
 0.75       0.78571429 0.8        0.81818182 0.83333333 0.88888889]

  UserWarning,

2022-10-31 11:03:04,641:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.33333333 0.4        0.42857143 0.5        0.55555556
 0.58333333 0.66666667 0.71428571 0.75       0.8        0.81818182
 0.875     ]

  UserWarning,

2022-10-31 11:03:05,641:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.25       0.33333333 0.375      0.5        0.54545455 0.57142857
 0.6        0.625      0.63636364 0.66666667 0.7        0.71428571
 0.75       0.77777778 0.8        0.81818182 0.85714286 0.88888889]

  UserWarning,

2022-10-31 11:03:05,648:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.2        0.25       0.33333333 0.375      0.4        0.44444444
 0.5        0.54545455 0.57142857 0.61538462 0.625      0.66666667
 0.7        0.75       0.78571429 0.8        0.81818182 0.83333333
 0.85714286 0.875     ]

  UserWarning,

2022-10-31 11:03:05,648:INFO:Calculating mean and std
2022-10-31 11:03:05,648:INFO:Creating metrics dataframe
2022-10-31 11:03:05,664:INFO:Uploading results into container
2022-10-31 11:03:05,665:INFO:Uploading model into container now
2022-10-31 11:03:05,665:INFO:master_model_container: 24
2022-10-31 11:03:05,665:INFO:display_container: 2
2022-10-31 11:03:05,665:INFO:DecisionTreeRegressor(random_state=3360)
2022-10-31 11:03:05,665:INFO:create_model() successfully completed......................................
2022-10-31 11:03:05,776:ERROR:create_model() for DecisionTreeRegressor(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:05,776:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:05,776:INFO:Initializing Random Forest Regressor
2022-10-31 11:03:05,776:INFO:Total runtime is 1.5464325030644737 minutes
2022-10-31 11:03:05,777:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:05,777:INFO:Initializing create_model()
2022-10-31 11:03:05,777:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:05,778:INFO:Checking exceptions
2022-10-31 11:03:05,781:INFO:Importing libraries
2022-10-31 11:03:05,781:INFO:Copying training dataset
2022-10-31 11:03:05,787:INFO:Defining folds
2022-10-31 11:03:05,787:INFO:Declaring metric variables
2022-10-31 11:03:05,787:INFO:Importing untrained model
2022-10-31 11:03:05,788:INFO:Random Forest Regressor Imported successfully
2022-10-31 11:03:05,788:INFO:Starting cross validation
2022-10-31 11:03:05,790:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:10,819:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04671429 0.05
 0.055      0.06       0.07       0.08       0.08466667 0.09
 0.1        0.11       0.12       0.12488095 0.1249343  0.13
 0.14       0.15       0.16       0.165      0.17       0.17333333
 0.17371281 0.17485859 0.18       0.18359921 0.19       0.19666667
 0.2        0.20833333 0.21       0.22       0.23       0.24
 0.25       0.26       0.26666667 0.26677381 0.27       0.27795238
 0.27939683 0.28       0.28734812 0.29       0.3        0.31
 0.32       0.33       0.33333333 0.34       0.35       0.35304207
 0.36833333 0.36900397 0.37       0.375      0.38       0.39
 0.39525552 0.4        0.41       0.42       0.42268072 0.42319048
 0.43       0.43188095 0.44       0.44146181 0.445      0.44892857
 0.45       0.46       0.46195455 0.47       0.48       0.48614444
 0.49       0.49262386 0.495      0.50208333 0.50391777 0.51
 0.51580491 0.52       0.52183558 0.52825397 0.53       0.53292857
 0.54       0.55       0.55904762 0.56       0.56466974 0.57
 0.57093146 0.57363822 0.575      0.5776756  0.58       0.58333333
 0.58691651 0.59       0.6        0.6012518  0.61       0.6135
 0.61555952 0.61833333 0.62902778 0.63036605 0.64       0.65
 0.65210101 0.65363201 0.65488563 0.66294048 0.66333333 0.67120928
 0.68       0.68527778 0.68942821 0.69       0.69190476 0.7
 0.70074735 0.706      0.71       0.71716667 0.7197619  0.72
 0.73       0.73236802 0.735      0.74       0.7475     0.74866409
 0.74879762 0.75       0.75156999 0.76       0.77       0.7714127
 0.78       0.785      0.8        0.80035714 0.80645595 0.81
 0.81583333 0.82056327 0.82333333 0.83       0.83333333 0.83631593
 0.83926335 0.84       0.84403448 0.84475397 0.84519949 0.84537851
 0.85111111 0.86       0.87038095 0.87662322 0.87819916 0.88
 0.88111252 0.88853968 0.89666667 0.91       0.91568182 0.91971212
 0.92       0.926      0.94       0.94233333 0.9425     0.94526071
 0.95       0.96       0.96233333 0.966      0.96703968 0.968
 0.96864286 0.97       0.9777298  0.97857143 0.98       0.9825
 0.98355556 0.985      0.99       0.99355556 0.995      0.9955
 0.99666667]

  UserWarning,

2022-10-31 11:03:10,887:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.03333333 0.04       0.04666667
 0.04933333 0.05       0.06       0.06674026 0.07       0.08
 0.09       0.1        0.11       0.12       0.121      0.13
 0.14       0.15       0.1525     0.16       0.17       0.17316667
 0.18       0.19       0.1925     0.2        0.2068355  0.21
 0.21033333 0.22       0.22333333 0.23       0.23666667 0.24
 0.25       0.26       0.26713095 0.2675     0.27       0.28
 0.28946747 0.29       0.3        0.31       0.31407143 0.32
 0.325      0.32666667 0.32890476 0.33       0.34       0.35
 0.36       0.36297661 0.37       0.37069048 0.38       0.39
 0.39785642 0.4        0.40016667 0.41       0.41436121 0.41717857
 0.41966667 0.42       0.43       0.44       0.44582143 0.455
 0.45792929 0.458      0.46       0.46666667 0.46830305 0.47
 0.48       0.48157143 0.48168692 0.485      0.48918322 0.49
 0.49008586 0.49219266 0.5        0.50566667 0.50616667 0.50632468
 0.50716667 0.50966667 0.51       0.51381349 0.51570238 0.52
 0.524      0.53       0.53582143 0.54146326 0.55       0.5525
 0.55993594 0.56       0.57       0.58       0.5866748  0.59
 0.59890419 0.60049672 0.61       0.61078571 0.61228571 0.61266667
 0.615      0.62       0.63       0.64597878 0.65       0.66
 0.66016667 0.66985142 0.67       0.67416667 0.67983333 0.68364286
 0.68857143 0.69       0.69804762 0.7        0.70083333 0.70422926
 0.70511805 0.71       0.71012031 0.71397619 0.71516667 0.72
 0.72214175 0.7275     0.73       0.74       0.75       0.75437698
 0.76       0.76458339 0.77       0.77797222 0.78       0.79
 0.79100541 0.79925144 0.8        0.80452507 0.80583333 0.80979365
 0.81       0.81053419 0.81279762 0.81444372 0.81678175 0.82333333
 0.82979512 0.83       0.84       0.84125064 0.84666667 0.84687363
 0.85583333 0.86       0.8652707  0.8692619  0.87       0.88
 0.88070192 0.89       0.89378436 0.8975     0.90570418 0.90928732
 0.91       0.92       0.93       0.93146825 0.94       0.9425
 0.945      0.95       0.95283333 0.95483333 0.95625    0.95793254
 0.96       0.96033333 0.97       0.97166667 0.98       0.985
 0.98503596 0.98571429 0.98833333 0.99       0.9911044  0.99175
 0.99333333 0.99466667 0.995     ]

  UserWarning,

2022-10-31 11:03:11,156:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04333333 0.05
 0.06       0.06916667 0.07       0.08       0.09       0.1
 0.11       0.12       0.13       0.13666667 0.14       0.15
 0.15045455 0.15768254 0.16       0.17       0.1704241  0.17233333
 0.18       0.19       0.2        0.21       0.22       0.23
 0.24       0.25       0.255      0.26       0.27       0.28
 0.29       0.29357143 0.295      0.297      0.3        0.30638095
 0.31       0.31266667 0.32       0.33       0.335      0.34
 0.35       0.36       0.37       0.3777856  0.37895238 0.37989219
 0.38       0.38309524 0.38680952 0.39       0.39528175 0.39976873
 0.4        0.40204762 0.41       0.42       0.42333333 0.4251274
 0.43       0.43642857 0.44       0.44133333 0.44201773 0.44495643
 0.45       0.46       0.46666667 0.47       0.47355263 0.47665237
 0.48       0.49       0.49830805 0.5        0.52       0.5215
 0.52738095 0.52828571 0.53433333 0.54       0.5455792  0.55
 0.552      0.56       0.57776334 0.58       0.58080473 0.59
 0.6        0.61       0.62       0.63       0.64       0.64621032
 0.65       0.65063012 0.65132143 0.65762157 0.66       0.66065557
 0.67       0.68       0.68469048 0.69566667 0.7        0.70607073
 0.71       0.715      0.72       0.72457143 0.7285     0.72939904
 0.73246429 0.74       0.74066667 0.74784436 0.75       0.75594395
 0.75690394 0.76       0.7675873  0.775      0.79165404 0.79872908
 0.8        0.8175     0.81961272 0.82       0.824      0.82916667
 0.82962987 0.83416522 0.84335714 0.8435     0.85       0.86
 0.86217929 0.86402203 0.8658678  0.86786219 0.86802203 0.87
 0.8725     0.87366234 0.87963889 0.88607373 0.8875994  0.8897619
 0.89       0.8935994  0.9        0.90416667 0.90766667 0.91
 0.91066667 0.91166667 0.915      0.92       0.93       0.93161724
 0.94       0.94290476 0.94316667 0.945      0.94869048 0.95
 0.95666667 0.96       0.96083333 0.962      0.96541667 0.97
 0.97333333 0.98       0.98286472 0.98290476 0.98717424 0.99
 0.99416667 0.99666667 0.998     ]

  UserWarning,

2022-10-31 11:03:11,203:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.077      0.08       0.09       0.1        0.11
 0.1175     0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.185      0.18871703 0.19       0.19090873
 0.2        0.21       0.21430051 0.22       0.23       0.23498274
 0.24       0.25       0.26       0.2675     0.27       0.28
 0.29       0.291      0.3        0.31       0.32       0.32333333
 0.33       0.34       0.34320238 0.35       0.3536398  0.36
 0.36216667 0.37       0.375      0.37742316 0.38       0.39
 0.4        0.41       0.41667494 0.41697619 0.42       0.42009524
 0.42333333 0.43196032 0.43764286 0.44       0.45       0.45966667
 0.46       0.46307677 0.468      0.46933333 0.47       0.47169048
 0.47804601 0.48       0.49       0.49583189 0.5        0.50383333
 0.50902368 0.51       0.51098413 0.51466667 0.51609091 0.51808333
 0.52       0.52268254 0.524      0.52971429 0.53       0.534
 0.54       0.54333333 0.5455     0.55       0.56       0.5605
 0.57       0.57916667 0.58       0.58114104 0.58201315 0.5834743
 0.58616667 0.59778571 0.6        0.61       0.63       0.63216667
 0.6321746  0.635      0.64       0.64497619 0.6475     0.6486651
 0.65       0.66       0.66096825 0.67166667 0.67294657 0.673
 0.67616667 0.68       0.69236222 0.69416667 0.69843362 0.7
 0.70216487 0.70258333 0.70416667 0.70729523 0.71       0.72
 0.72048752 0.72224204 0.7246922  0.72733333 0.73       0.73083097
 0.74       0.745      0.74685021 0.7472619  0.75       0.75048771
 0.75269048 0.75542857 0.76       0.77       0.77333333 0.78
 0.7875     0.79       0.79181611 0.792      0.808      0.81
 0.81153965 0.815      0.82       0.82018895 0.82132001 0.83325577
 0.84       0.84456349 0.857      0.8662619  0.87       0.8775
 0.88       0.88161673 0.88923077 0.89083333 0.8975     0.90666667
 0.91       0.9125121  0.92       0.92066667 0.9345     0.935
 0.94       0.95       0.95583333 0.95758297 0.96       0.96633333
 0.97       0.97166667 0.98       0.98243506 0.98625    0.98666667
 0.98891667 0.98928571 0.98964286 0.99       0.992      0.998     ]

  UserWarning,

2022-10-31 11:03:11,269:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.02733333 0.03       0.04       0.05
 0.05733333 0.06       0.07       0.07666667 0.08       0.09
 0.1        0.11       0.11461985 0.12       0.13       0.14
 0.15       0.15766667 0.16       0.17       0.174      0.17666667
 0.18       0.18166667 0.19       0.19386832 0.2        0.21
 0.22       0.23       0.24       0.25       0.26       0.26666667
 0.27       0.27224242 0.28       0.28213453 0.28270222 0.28666667
 0.29       0.3        0.31       0.32       0.325      0.33
 0.34       0.35       0.35333333 0.35792857 0.36       0.36151479
 0.37       0.37451876 0.375      0.38       0.38833333 0.39
 0.392      0.39821429 0.4        0.41       0.42       0.42774098
 0.43       0.43215115 0.44       0.44516667 0.44607995 0.45
 0.45150794 0.46       0.46733081 0.47       0.47771429 0.48
 0.49       0.4909599  0.49216667 0.49304762 0.5        0.50623016
 0.50858333 0.51       0.51294048 0.52339683 0.53       0.53083333
 0.53266667 0.53372222 0.54       0.55       0.57       0.57472291
 0.57858183 0.58666667 0.59       0.59158726 0.59434127 0.6
 0.61       0.61659096 0.61833333 0.61912522 0.62066667 0.62717427
 0.63       0.64       0.6425     0.64628571 0.65       0.65433333
 0.655      0.65830952 0.66       0.66725518 0.67       0.67130159
 0.6762381  0.68       0.69       0.69313889 0.69570583 0.7
 0.71       0.71833333 0.72       0.73       0.73028571 0.74
 0.74880952 0.75       0.75044386 0.75492857 0.76       0.76888889
 0.77       0.77142063 0.78345423 0.78939683 0.79       0.79022958
 0.79266667 0.79666667 0.8        0.80170013 0.8073355  0.80756291
 0.80833333 0.80883333 0.81       0.81483059 0.81928571 0.82468066
 0.82807143 0.83       0.83572222 0.83733405 0.84       0.84421082
 0.84482902 0.84662013 0.84875    0.85       0.85466667 0.86
 0.8617381  0.87       0.87137354 0.88349988 0.89       0.89127228
 0.89166667 0.9        0.9005     0.90063889 0.90568895 0.91
 0.91169048 0.91556663 0.92       0.92269048 0.94       0.94166667
 0.94433333 0.945      0.95       0.95945238 0.96       0.96361752
 0.96714286 0.97       0.97716667 0.97916667 0.98       0.98004762
 0.98083333 0.98338095 0.98666667 0.98833333 0.99       0.992
 0.99214286 0.995      0.99666667]

  UserWarning,

2022-10-31 11:03:11,294:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.066      0.07       0.08       0.09       0.09344048 0.094
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.16883333 0.17       0.18       0.18833333 0.19
 0.2        0.21       0.2125     0.22       0.23       0.23766667
 0.24       0.25       0.25666667 0.26       0.265      0.27
 0.27047619 0.28       0.28166667 0.28230556 0.29       0.2985
 0.3        0.30586833 0.306      0.31       0.32       0.324
 0.32583333 0.3288373  0.33       0.3375     0.34       0.34833333
 0.35       0.35833333 0.35966667 0.36495055 0.36558333 0.37
 0.38       0.38125    0.39406746 0.396      0.3985947  0.4
 0.40894048 0.41       0.42333333 0.42845238 0.43       0.44
 0.45       0.46       0.4612619  0.46200808 0.47       0.48212698
 0.49022823 0.5        0.51       0.51397367 0.51972619 0.52
 0.53       0.54       0.54987745 0.55       0.55283333 0.56
 0.56397619 0.56621434 0.57       0.57731919 0.58       0.58059486
 0.58579762 0.58819471 0.58896698 0.59       0.59372222 0.6
 0.60262981 0.60333333 0.61       0.62110065 0.62308333 0.63
 0.64       0.64266053 0.65       0.65433333 0.66325    0.665
 0.66685143 0.67       0.67072872 0.67916667 0.68       0.68286649
 0.68742857 0.68808719 0.68980952 0.69       0.7        0.70361797
 0.70381349 0.71       0.71575938 0.72       0.7287381  0.73264683
 0.74       0.74191667 0.75       0.7545     0.75838736 0.76
 0.76338492 0.76833333 0.77       0.77041667 0.77064286 0.78
 0.78582359 0.78833333 0.79       0.8        0.80249603 0.81
 0.81392857 0.8165     0.82666667 0.82945635 0.82958333 0.83
 0.83017745 0.84       0.84127797 0.84904762 0.85       0.85316667
 0.854      0.85476467 0.85716991 0.86       0.86       0.86143759
 0.86266819 0.86355123 0.86742857 0.8695     0.87       0.8710469
 0.87538095 0.88       0.88578205 0.89003339 0.9        0.90771825
 0.91       0.91792063 0.92       0.92655952 0.92666667 0.93
 0.935      0.94       0.945      0.95       0.95466667 0.956
 0.96333333 0.965      0.96818831 0.98       0.98625    0.98695818
 0.9875     0.9895     0.99       0.99083333 0.99625    0.99666667
 0.99714286 0.9975     0.998     ]

  UserWarning,

2022-10-31 11:03:11,325:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.11333333
 0.12       0.12092063 0.13       0.14       0.15       0.15571429
 0.16       0.17       0.18       0.18857143 0.19       0.1905
 0.2        0.2005     0.21       0.22       0.23       0.23583333
 0.24       0.24322032 0.24709871 0.25       0.25333333 0.26
 0.26081349 0.27       0.27084921 0.2735     0.2761456  0.27624206
 0.278      0.28       0.29       0.2925     0.3        0.30133333
 0.31       0.31207736 0.3130873  0.32       0.32078519 0.33
 0.33116667 0.33244048 0.34       0.35       0.35958333 0.36
 0.37       0.37534144 0.38       0.38797619 0.39       0.39285237
 0.4        0.41       0.41674223 0.42       0.43       0.44
 0.44567857 0.46       0.46422397 0.47       0.47680952 0.48
 0.481      0.48185065 0.48390079 0.49       0.5        0.5068676
 0.51893604 0.52       0.52483412 0.53       0.53841089 0.54
 0.54033333 0.55       0.56       0.56266745 0.56975799 0.57
 0.57716667 0.58131734 0.58299387 0.58966919 0.59       0.59230952
 0.5985     0.59884776 0.6        0.601      0.60137109 0.61
 0.61294841 0.61666667 0.62       0.63       0.64       0.64238297
 0.64408711 0.645      0.65174123 0.65632943 0.65951468 0.66
 0.66666667 0.67033333 0.67127381 0.68       0.68165584 0.69
 0.7        0.705      0.70966667 0.71       0.71028571 0.71192874
 0.7128254  0.71744444 0.71983333 0.72       0.72066667 0.72208397
 0.72854618 0.73       0.73309776 0.74       0.74119048 0.745
 0.74830456 0.75       0.75144794 0.75415873 0.76       0.76027381
 0.76144444 0.77       0.77328023 0.77716667 0.77933333 0.77982211
 0.78       0.78385066 0.7845     0.78783333 0.79       0.79933333
 0.80155303 0.80352533 0.81       0.8102381  0.82       0.82183514
 0.83       0.83200587 0.84       0.84664463 0.85       0.86
 0.86836003 0.87       0.87479365 0.875      0.88       0.88238823
 0.89       0.9        0.90109096 0.90928571 0.91       0.91666667
 0.92       0.92625    0.92916667 0.93014286 0.94       0.94011111
 0.94031746 0.94066667 0.9435     0.94383874 0.944      0.95
 0.95244444 0.959      0.96       0.96616667 0.97       0.97766667
 0.98       0.98045238 0.98083333 0.984      0.98647619 0.9875
 0.99       0.99233333 0.994      0.996      0.99666667]

  UserWarning,

2022-10-31 11:03:11,325:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.01583333 0.02       0.02666667 0.03       0.04
 0.05       0.06       0.065      0.07       0.07416667 0.08
 0.08423091 0.09       0.1        0.11       0.12       0.13
 0.13166667 0.13190079 0.14       0.15       0.16       0.17
 0.17853644 0.18       0.18666667 0.19       0.19795763 0.2
 0.20155294 0.21       0.22       0.225      0.23       0.24
 0.25       0.26       0.26498413 0.26736111 0.27       0.28
 0.29       0.2925     0.3        0.308      0.31       0.31236111
 0.32       0.32263167 0.32331349 0.33       0.33911905 0.34
 0.35       0.35028145 0.35716667 0.36       0.3625     0.37
 0.38       0.3845     0.39       0.4        0.40695707 0.40833333
 0.41       0.41714935 0.42       0.43       0.44       0.45
 0.46       0.4657684  0.47       0.47527717 0.48       0.48262302
 0.48416667 0.48740593 0.49       0.5        0.502      0.50633333
 0.509      0.50981031 0.52       0.53       0.54       0.54272264
 0.55       0.55308333 0.56       0.56292857 0.57       0.57152381
 0.58       0.58025117 0.59       0.59181349 0.6        0.60483333
 0.60545238 0.61       0.62       0.62616667 0.63       0.6305448
 0.64       0.64016667 0.65       0.67       0.67033333 0.68
 0.68592172 0.68853608 0.69       0.69169182 0.69581946 0.69645689
 0.7        0.70428571 0.7075     0.71       0.72       0.72683333
 0.73       0.73571429 0.73747619 0.74       0.74417316 0.75
 0.75034609 0.7507687  0.75232515 0.75249847 0.75511508 0.75666667
 0.75783579 0.76       0.76095238 0.7632381  0.7755     0.78475397
 0.78955952 0.79       0.8        0.80066667 0.80519697 0.80529582
 0.80803457 0.81       0.81956663 0.82       0.8235     0.82725
 0.82969481 0.83       0.84683333 0.85       0.85032784 0.85563131
 0.85641667 0.85716667 0.85868279 0.86       0.8629296  0.86437027
 0.87       0.87451218 0.88       0.88188095 0.88980556 0.89400072
 0.9        0.90833333 0.91       0.92       0.93       0.95
 0.96       0.96666667 0.97       0.97256818 0.975      0.97516667
 0.98       0.98083333 0.98107792 0.98333333 0.98430952 0.98857143
 0.99       0.99083333 0.99333333 0.99370833 0.99416667 0.99777778]

  UserWarning,

2022-10-31 11:03:12,859:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.00875    0.01       0.01166667 0.02       0.03       0.04
 0.05       0.05833333 0.06       0.07       0.08       0.08283333
 0.09       0.1        0.1025     0.11       0.12       0.13
 0.14       0.15       0.16       0.17       0.17083333 0.18
 0.18433333 0.19       0.2        0.21       0.21416667 0.21687166
 0.22       0.23       0.23016667 0.24       0.24929762 0.25
 0.25891126 0.259      0.26       0.27       0.28       0.29
 0.29392857 0.3        0.31       0.31471429 0.31766667 0.32
 0.33       0.34       0.34868131 0.35       0.35145238 0.36
 0.36364302 0.37       0.37294048 0.37700583 0.38       0.382
 0.38315668 0.39       0.4        0.41       0.41266667 0.42
 0.421      0.43       0.43014286 0.44       0.45       0.46
 0.47535714 0.4770423  0.48       0.48816667 0.49       0.49028152
 0.49333333 0.49502381 0.50216667 0.51564286 0.52907143 0.53
 0.53919323 0.54       0.54061905 0.54583333 0.55       0.55651803
 0.55676976 0.56       0.56179121 0.57       0.57316667 0.57988636
 0.586      0.58805403 0.59       0.59116667 0.6        0.60811472
 0.60890476 0.61       0.61191667 0.61490873 0.62166667 0.63
 0.63805665 0.64       0.64230952 0.6485     0.64869411 0.65447175
 0.66       0.67       0.67135714 0.68       0.68702381 0.69
 0.69528571 0.7        0.70714286 0.7090119  0.71       0.71496725
 0.71812771 0.72       0.72675    0.72754762 0.72959449 0.73
 0.73055556 0.73314286 0.73955411 0.74       0.75       0.75111905
 0.76       0.76678571 0.76784524 0.76918651 0.77       0.77990873
 0.78       0.78086274 0.79143937 0.79940873 0.8        0.80312779
 0.80422957 0.81       0.81090781 0.81161905 0.81166667 0.82
 0.83       0.83824495 0.84       0.84067491 0.84375397 0.84441595
 0.84864466 0.8554633  0.85595238 0.85980952 0.868      0.87
 0.8705     0.87425    0.875      0.87972944 0.88       0.88081421
 0.89       0.89333333 0.89991667 0.9        0.90345238 0.908
 0.9095     0.91       0.91063889 0.92       0.92814845 0.93
 0.93166667 0.94151046 0.94694048 0.94744012 0.95       0.96
 0.96584127 0.96833333 0.97       0.97833333 0.98       0.985
 0.99       0.99333333 0.99571429 0.99714286 0.99833333]

  UserWarning,

2022-10-31 11:03:12,910:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.0075     0.01       0.02       0.02261364 0.03       0.04
 0.05       0.05833333 0.06       0.065      0.0683254  0.07
 0.08       0.0865     0.09       0.0935     0.1        0.11
 0.12       0.12333333 0.13       0.14       0.14178571 0.15
 0.16       0.16361414 0.16966667 0.17       0.18       0.18333333
 0.19       0.2        0.21       0.21300433 0.22       0.23
 0.24       0.25       0.26       0.2625     0.27       0.28
 0.285      0.28933333 0.29       0.29333333 0.3        0.3082381
 0.31       0.32       0.3261978  0.33       0.34       0.35
 0.35222849 0.36       0.37       0.37066667 0.3725     0.375
 0.37809524 0.38       0.39       0.39107295 0.4        0.408
 0.41       0.42       0.42416667 0.43       0.43333333 0.43846429
 0.44       0.44152607 0.45       0.45666667 0.46       0.46228968
 0.47       0.47883333 0.48       0.49       0.4911533  0.49759524
 0.499      0.49983333 0.5        0.50069733 0.50258333 0.51
 0.51683067 0.52       0.52485317 0.53       0.53563611 0.53876046
 0.54       0.545      0.54626335 0.5495     0.55       0.57
 0.58       0.59       0.593      0.6        0.61       0.61652381
 0.62       0.62121429 0.62494646 0.62856643 0.63       0.64
 0.65       0.65294517 0.6592381  0.66       0.6627326  0.67
 0.67428175 0.68       0.68224173 0.69       0.7        0.7012619
 0.70235759 0.70333333 0.70333333 0.71       0.7152619  0.71721429
 0.72       0.73       0.73435317 0.74       0.7415     0.74268254
 0.75       0.75063292 0.75138095 0.75591289 0.75895058 0.76
 0.77       0.77383333 0.77414494 0.78       0.78066667 0.7882619
 0.78983531 0.79064286 0.79178571 0.79283333 0.795      0.79916667
 0.8        0.80319841 0.80466667 0.81       0.8151363  0.8164246
 0.82       0.83       0.83577778 0.83580556 0.83787662 0.83825472
 0.842      0.85133333 0.85157531 0.87       0.8762212  0.87894367
 0.88       0.88305556 0.884      0.8849881  0.89       0.89011905
 0.9        0.90583333 0.90733333 0.90733333 0.91       0.91080952
 0.91260606 0.91802237 0.92333333 0.9325     0.934      0.93828247
 0.94       0.94083333 0.94333333 0.95591667 0.96       0.96166667
 0.97       0.97666667 0.9825     0.98892857 0.99       0.9925
 0.995      0.996      0.9975     0.99875   ]

  UserWarning,

2022-10-31 11:03:12,910:INFO:Calculating mean and std
2022-10-31 11:03:12,910:INFO:Creating metrics dataframe
2022-10-31 11:03:12,910:INFO:Uploading results into container
2022-10-31 11:03:12,910:INFO:Uploading model into container now
2022-10-31 11:03:12,910:INFO:master_model_container: 25
2022-10-31 11:03:12,910:INFO:display_container: 2
2022-10-31 11:03:12,910:INFO:RandomForestRegressor(n_jobs=-1, random_state=3360)
2022-10-31 11:03:12,910:INFO:create_model() successfully completed......................................
2022-10-31 11:03:13,046:WARNING:create_model() for RandomForestRegressor(n_jobs=-1, random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:13,046:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:13,047:INFO:Initializing create_model()
2022-10-31 11:03:13,047:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:13,047:INFO:Checking exceptions
2022-10-31 11:03:13,050:INFO:Importing libraries
2022-10-31 11:03:13,050:INFO:Copying training dataset
2022-10-31 11:03:13,055:INFO:Defining folds
2022-10-31 11:03:13,055:INFO:Declaring metric variables
2022-10-31 11:03:13,055:INFO:Importing untrained model
2022-10-31 11:03:13,056:INFO:Random Forest Regressor Imported successfully
2022-10-31 11:03:13,057:INFO:Starting cross validation
2022-10-31 11:03:13,058:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:18,001:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.03333333 0.04       0.04666667
 0.04933333 0.05       0.06       0.06674026 0.07       0.08
 0.09       0.1        0.11       0.12       0.121      0.13
 0.14       0.15       0.1525     0.16       0.17       0.17316667
 0.18       0.19       0.1925     0.2        0.2068355  0.21
 0.21033333 0.22       0.22333333 0.23       0.23666667 0.24
 0.25       0.26       0.26713095 0.2675     0.27       0.28
 0.28946747 0.29       0.3        0.31       0.31407143 0.32
 0.325      0.32666667 0.32890476 0.33       0.34       0.35
 0.36       0.36297661 0.37       0.37069048 0.38       0.39
 0.39785642 0.4        0.40016667 0.41       0.41436121 0.41717857
 0.41966667 0.42       0.43       0.44       0.44582143 0.455
 0.45792929 0.458      0.46       0.46666667 0.46830305 0.47
 0.48       0.48157143 0.48168692 0.485      0.48918322 0.49
 0.49008586 0.49219266 0.5        0.50566667 0.50616667 0.50632468
 0.50716667 0.50966667 0.51       0.51381349 0.51570238 0.52
 0.524      0.53       0.53582143 0.54146326 0.55       0.5525
 0.55993594 0.56       0.57       0.58       0.5866748  0.59
 0.59890419 0.60049672 0.61       0.61078571 0.61228571 0.61266667
 0.615      0.62       0.63       0.64597878 0.65       0.66
 0.66016667 0.66985142 0.67       0.67416667 0.67983333 0.68364286
 0.68857143 0.69       0.69804762 0.7        0.70083333 0.70422926
 0.70511805 0.71       0.71012031 0.71397619 0.71516667 0.72
 0.72214175 0.7275     0.73       0.74       0.75       0.75437698
 0.76       0.76458339 0.77       0.77797222 0.78       0.79
 0.79100541 0.79925144 0.8        0.80452507 0.80583333 0.80979365
 0.81       0.81053419 0.81279762 0.81444372 0.81678175 0.82333333
 0.82979512 0.83       0.84       0.84125064 0.84666667 0.84687363
 0.85583333 0.86       0.8652707  0.8692619  0.87       0.88
 0.88070192 0.89       0.89378436 0.8975     0.90570418 0.90928732
 0.91       0.92       0.93       0.93146825 0.94       0.9425
 0.945      0.95       0.95283333 0.95483333 0.95625    0.95793254
 0.96       0.96033333 0.97       0.97166667 0.98       0.985
 0.98503596 0.98571429 0.98833333 0.99       0.9911044  0.99175
 0.99333333 0.99466667 0.995     ]

  UserWarning,

2022-10-31 11:03:18,049:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04671429 0.05
 0.055      0.06       0.07       0.08       0.08466667 0.09
 0.1        0.11       0.12       0.12488095 0.1249343  0.13
 0.14       0.15       0.16       0.165      0.17       0.17333333
 0.17371281 0.17485859 0.18       0.18359921 0.19       0.19666667
 0.2        0.20833333 0.21       0.22       0.23       0.24
 0.25       0.26       0.26666667 0.26677381 0.27       0.27795238
 0.27939683 0.28       0.28734812 0.29       0.3        0.31
 0.32       0.33       0.33333333 0.34       0.35       0.35304207
 0.36833333 0.36900397 0.37       0.375      0.38       0.39
 0.39525552 0.4        0.41       0.42       0.42268072 0.42319048
 0.43       0.43188095 0.44       0.44146181 0.445      0.44892857
 0.45       0.46       0.46195455 0.47       0.48       0.48614444
 0.49       0.49262386 0.495      0.50208333 0.50391777 0.51
 0.51580491 0.52       0.52183558 0.52825397 0.53       0.53292857
 0.54       0.55       0.55904762 0.56       0.56466974 0.57
 0.57093146 0.57363822 0.575      0.5776756  0.58       0.58333333
 0.58691651 0.59       0.6        0.6012518  0.61       0.6135
 0.61555952 0.61833333 0.62902778 0.63036605 0.64       0.65
 0.65210101 0.65363201 0.65488563 0.66294048 0.66333333 0.67120928
 0.68       0.68527778 0.68942821 0.69       0.69190476 0.7
 0.70074735 0.706      0.71       0.71716667 0.7197619  0.72
 0.73       0.73236802 0.735      0.74       0.7475     0.74866409
 0.74879762 0.75       0.75156999 0.76       0.77       0.7714127
 0.78       0.785      0.8        0.80035714 0.80645595 0.81
 0.81583333 0.82056327 0.82333333 0.83       0.83333333 0.83631593
 0.83926335 0.84       0.84403448 0.84475397 0.84519949 0.84537851
 0.85111111 0.86       0.87038095 0.87662322 0.87819916 0.88
 0.88111252 0.88853968 0.89666667 0.91       0.91568182 0.91971212
 0.92       0.926      0.94       0.94233333 0.9425     0.94526071
 0.95       0.96       0.96233333 0.966      0.96703968 0.968
 0.96864286 0.97       0.9777298  0.97857143 0.98       0.9825
 0.98355556 0.985      0.99       0.99355556 0.995      0.9955
 0.99666667]

  UserWarning,

2022-10-31 11:03:18,309:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.01583333 0.02       0.02666667 0.03       0.04
 0.05       0.06       0.065      0.07       0.07416667 0.08
 0.08423091 0.09       0.1        0.11       0.12       0.13
 0.13166667 0.13190079 0.14       0.15       0.16       0.17
 0.17853644 0.18       0.18666667 0.19       0.19795763 0.2
 0.20155294 0.21       0.22       0.225      0.23       0.24
 0.25       0.26       0.26498413 0.26736111 0.27       0.28
 0.29       0.2925     0.3        0.308      0.31       0.31236111
 0.32       0.32263167 0.32331349 0.33       0.33911905 0.34
 0.35       0.35028145 0.35716667 0.36       0.3625     0.37
 0.38       0.3845     0.39       0.4        0.40695707 0.40833333
 0.41       0.41714935 0.42       0.43       0.44       0.45
 0.46       0.4657684  0.47       0.47527717 0.48       0.48262302
 0.48416667 0.48740593 0.49       0.5        0.502      0.50633333
 0.509      0.50981031 0.52       0.53       0.54       0.54272264
 0.55       0.55308333 0.56       0.56292857 0.57       0.57152381
 0.58       0.58025117 0.59       0.59181349 0.6        0.60483333
 0.60545238 0.61       0.62       0.62616667 0.63       0.6305448
 0.64       0.64016667 0.65       0.67       0.67033333 0.68
 0.68592172 0.68853608 0.69       0.69169182 0.69581946 0.69645689
 0.7        0.70428571 0.7075     0.71       0.72       0.72683333
 0.73       0.73571429 0.73747619 0.74       0.74417316 0.75
 0.75034609 0.7507687  0.75232515 0.75249847 0.75511508 0.75666667
 0.75783579 0.76       0.76095238 0.7632381  0.7755     0.78475397
 0.78955952 0.79       0.8        0.80066667 0.80519697 0.80529582
 0.80803457 0.81       0.81956663 0.82       0.8235     0.82725
 0.82969481 0.83       0.84683333 0.85       0.85032784 0.85563131
 0.85641667 0.85716667 0.85868279 0.86       0.8629296  0.86437027
 0.87       0.87451218 0.88       0.88188095 0.88980556 0.89400072
 0.9        0.90833333 0.91       0.92       0.93       0.95
 0.96       0.96666667 0.97       0.97256818 0.975      0.97516667
 0.98       0.98083333 0.98107792 0.98333333 0.98430952 0.98857143
 0.99       0.99083333 0.99333333 0.99370833 0.99416667 0.99777778]

  UserWarning,

2022-10-31 11:03:18,324:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.02733333 0.03       0.04       0.05
 0.05733333 0.06       0.07       0.07666667 0.08       0.09
 0.1        0.11       0.11461985 0.12       0.13       0.14
 0.15       0.15766667 0.16       0.17       0.174      0.17666667
 0.18       0.18166667 0.19       0.19386832 0.2        0.21
 0.22       0.23       0.24       0.25       0.26       0.26666667
 0.27       0.27224242 0.28       0.28213453 0.28270222 0.28666667
 0.29       0.3        0.31       0.32       0.325      0.33
 0.34       0.35       0.35333333 0.35792857 0.36       0.36151479
 0.37       0.37451876 0.375      0.38       0.38833333 0.39
 0.392      0.39821429 0.4        0.41       0.42       0.42774098
 0.43       0.43215115 0.44       0.44516667 0.44607995 0.45
 0.45150794 0.46       0.46733081 0.47       0.47771429 0.48
 0.49       0.4909599  0.49216667 0.49304762 0.5        0.50623016
 0.50858333 0.51       0.51294048 0.52339683 0.53       0.53083333
 0.53266667 0.53372222 0.54       0.55       0.57       0.57472291
 0.57858183 0.58666667 0.59       0.59158726 0.59434127 0.6
 0.61       0.61659096 0.61833333 0.61912522 0.62066667 0.62717427
 0.63       0.64       0.6425     0.64628571 0.65       0.65433333
 0.655      0.65830952 0.66       0.66725518 0.67       0.67130159
 0.6762381  0.68       0.69       0.69313889 0.69570583 0.7
 0.71       0.71833333 0.72       0.73       0.73028571 0.74
 0.74880952 0.75       0.75044386 0.75492857 0.76       0.76888889
 0.77       0.77142063 0.78345423 0.78939683 0.79       0.79022958
 0.79266667 0.79666667 0.8        0.80170013 0.8073355  0.80756291
 0.80833333 0.80883333 0.81       0.81483059 0.81928571 0.82468066
 0.82807143 0.83       0.83572222 0.83733405 0.84       0.84421082
 0.84482902 0.84662013 0.84875    0.85       0.85466667 0.86
 0.8617381  0.87       0.87137354 0.88349988 0.89       0.89127228
 0.89166667 0.9        0.9005     0.90063889 0.90568895 0.91
 0.91169048 0.91556663 0.92       0.92269048 0.94       0.94166667
 0.94433333 0.945      0.95       0.95945238 0.96       0.96361752
 0.96714286 0.97       0.97716667 0.97916667 0.98       0.98004762
 0.98083333 0.98338095 0.98666667 0.98833333 0.99       0.992
 0.99214286 0.995      0.99666667]

  UserWarning,

2022-10-31 11:03:18,356:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.066      0.07       0.08       0.09       0.09344048 0.094
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.16883333 0.17       0.18       0.18833333 0.19
 0.2        0.21       0.2125     0.22       0.23       0.23766667
 0.24       0.25       0.25666667 0.26       0.265      0.27
 0.27047619 0.28       0.28166667 0.28230556 0.29       0.2985
 0.3        0.30586833 0.306      0.31       0.32       0.324
 0.32583333 0.3288373  0.33       0.3375     0.34       0.34833333
 0.35       0.35833333 0.35966667 0.36495055 0.36558333 0.37
 0.38       0.38125    0.39406746 0.396      0.3985947  0.4
 0.40894048 0.41       0.42333333 0.42845238 0.43       0.44
 0.45       0.46       0.4612619  0.46200808 0.47       0.48212698
 0.49022823 0.5        0.51       0.51397367 0.51972619 0.52
 0.53       0.54       0.54987745 0.55       0.55283333 0.56
 0.56397619 0.56621434 0.57       0.57731919 0.58       0.58059486
 0.58579762 0.58819471 0.58896698 0.59       0.59372222 0.6
 0.60262981 0.60333333 0.61       0.62110065 0.62308333 0.63
 0.64       0.64266053 0.65       0.65433333 0.66325    0.665
 0.66685143 0.67       0.67072872 0.67916667 0.68       0.68286649
 0.68742857 0.68808719 0.68980952 0.69       0.7        0.70361797
 0.70381349 0.71       0.71575938 0.72       0.7287381  0.73264683
 0.74       0.74191667 0.75       0.7545     0.75838736 0.76
 0.76338492 0.76833333 0.77       0.77041667 0.77064286 0.78
 0.78582359 0.78833333 0.79       0.8        0.80249603 0.81
 0.81392857 0.8165     0.82666667 0.82945635 0.82958333 0.83
 0.83017745 0.84       0.84127797 0.84904762 0.85       0.85316667
 0.854      0.85476467 0.85716991 0.86       0.86       0.86143759
 0.86266819 0.86355123 0.86742857 0.8695     0.87       0.8710469
 0.87538095 0.88       0.88578205 0.89003339 0.9        0.90771825
 0.91       0.91792063 0.92       0.92655952 0.92666667 0.93
 0.935      0.94       0.945      0.95       0.95466667 0.956
 0.96333333 0.965      0.96818831 0.98       0.98625    0.98695818
 0.9875     0.9895     0.99       0.99083333 0.99625    0.99666667
 0.99714286 0.9975     0.998     ]

  UserWarning,

2022-10-31 11:03:18,387:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.11333333
 0.12       0.12092063 0.13       0.14       0.15       0.15571429
 0.16       0.17       0.18       0.18857143 0.19       0.1905
 0.2        0.2005     0.21       0.22       0.23       0.23583333
 0.24       0.24322032 0.24709871 0.25       0.25333333 0.26
 0.26081349 0.27       0.27084921 0.2735     0.2761456  0.27624206
 0.278      0.28       0.29       0.2925     0.3        0.30133333
 0.31       0.31207736 0.3130873  0.32       0.32078519 0.33
 0.33116667 0.33244048 0.34       0.35       0.35958333 0.36
 0.37       0.37534144 0.38       0.38797619 0.39       0.39285237
 0.4        0.41       0.41674223 0.42       0.43       0.44
 0.44567857 0.46       0.46422397 0.47       0.47680952 0.48
 0.481      0.48185065 0.48390079 0.49       0.5        0.5068676
 0.51893604 0.52       0.52483412 0.53       0.53841089 0.54
 0.54033333 0.55       0.56       0.56266745 0.56975799 0.57
 0.57716667 0.58131734 0.58299387 0.58966919 0.59       0.59230952
 0.5985     0.59884776 0.6        0.601      0.60137109 0.61
 0.61294841 0.61666667 0.62       0.63       0.64       0.64238297
 0.64408711 0.645      0.65174123 0.65632943 0.65951468 0.66
 0.66666667 0.67033333 0.67127381 0.68       0.68165584 0.69
 0.7        0.705      0.70966667 0.71       0.71028571 0.71192874
 0.7128254  0.71744444 0.71983333 0.72       0.72066667 0.72208397
 0.72854618 0.73       0.73309776 0.74       0.74119048 0.745
 0.74830456 0.75       0.75144794 0.75415873 0.76       0.76027381
 0.76144444 0.77       0.77328023 0.77716667 0.77933333 0.77982211
 0.78       0.78385066 0.7845     0.78783333 0.79       0.79933333
 0.80155303 0.80352533 0.81       0.8102381  0.82       0.82183514
 0.83       0.83200587 0.84       0.84664463 0.85       0.86
 0.86836003 0.87       0.87479365 0.875      0.88       0.88238823
 0.89       0.9        0.90109096 0.90928571 0.91       0.91666667
 0.92       0.92625    0.92916667 0.93014286 0.94       0.94011111
 0.94031746 0.94066667 0.9435     0.94383874 0.944      0.95
 0.95244444 0.959      0.96       0.96616667 0.97       0.97766667
 0.98       0.98045238 0.98083333 0.984      0.98647619 0.9875
 0.99       0.99233333 0.994      0.996      0.99666667]

  UserWarning,

2022-10-31 11:03:18,434:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04333333 0.05
 0.06       0.06916667 0.07       0.08       0.09       0.1
 0.11       0.12       0.13       0.13666667 0.14       0.15
 0.15045455 0.15768254 0.16       0.17       0.1704241  0.17233333
 0.18       0.19       0.2        0.21       0.22       0.23
 0.24       0.25       0.255      0.26       0.27       0.28
 0.29       0.29357143 0.295      0.297      0.3        0.30638095
 0.31       0.31266667 0.32       0.33       0.335      0.34
 0.35       0.36       0.37       0.3777856  0.37895238 0.37989219
 0.38       0.38309524 0.38680952 0.39       0.39528175 0.39976873
 0.4        0.40204762 0.41       0.42       0.42333333 0.4251274
 0.43       0.43642857 0.44       0.44133333 0.44201773 0.44495643
 0.45       0.46       0.46666667 0.47       0.47355263 0.47665237
 0.48       0.49       0.49830805 0.5        0.52       0.5215
 0.52738095 0.52828571 0.53433333 0.54       0.5455792  0.55
 0.552      0.56       0.57776334 0.58       0.58080473 0.59
 0.6        0.61       0.62       0.63       0.64       0.64621032
 0.65       0.65063012 0.65132143 0.65762157 0.66       0.66065557
 0.67       0.68       0.68469048 0.69566667 0.7        0.70607073
 0.71       0.715      0.72       0.72457143 0.7285     0.72939904
 0.73246429 0.74       0.74066667 0.74784436 0.75       0.75594395
 0.75690394 0.76       0.7675873  0.775      0.79165404 0.79872908
 0.8        0.8175     0.81961272 0.82       0.824      0.82916667
 0.82962987 0.83416522 0.84335714 0.8435     0.85       0.86
 0.86217929 0.86402203 0.8658678  0.86786219 0.86802203 0.87
 0.8725     0.87366234 0.87963889 0.88607373 0.8875994  0.8897619
 0.89       0.8935994  0.9        0.90416667 0.90766667 0.91
 0.91066667 0.91166667 0.915      0.92       0.93       0.93161724
 0.94       0.94290476 0.94316667 0.945      0.94869048 0.95
 0.95666667 0.96       0.96083333 0.962      0.96541667 0.97
 0.97333333 0.98       0.98286472 0.98290476 0.98717424 0.99
 0.99416667 0.99666667 0.998     ]

  UserWarning,

2022-10-31 11:03:18,600:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.077      0.08       0.09       0.1        0.11
 0.1175     0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.185      0.18871703 0.19       0.19090873
 0.2        0.21       0.21430051 0.22       0.23       0.23498274
 0.24       0.25       0.26       0.2675     0.27       0.28
 0.29       0.291      0.3        0.31       0.32       0.32333333
 0.33       0.34       0.34320238 0.35       0.3536398  0.36
 0.36216667 0.37       0.375      0.37742316 0.38       0.39
 0.4        0.41       0.41667494 0.41697619 0.42       0.42009524
 0.42333333 0.43196032 0.43764286 0.44       0.45       0.45966667
 0.46       0.46307677 0.468      0.46933333 0.47       0.47169048
 0.47804601 0.48       0.49       0.49583189 0.5        0.50383333
 0.50902368 0.51       0.51098413 0.51466667 0.51609091 0.51808333
 0.52       0.52268254 0.524      0.52971429 0.53       0.534
 0.54       0.54333333 0.5455     0.55       0.56       0.5605
 0.57       0.57916667 0.58       0.58114104 0.58201315 0.5834743
 0.58616667 0.59778571 0.6        0.61       0.63       0.63216667
 0.6321746  0.635      0.64       0.64497619 0.6475     0.6486651
 0.65       0.66       0.66096825 0.67166667 0.67294657 0.673
 0.67616667 0.68       0.69236222 0.69416667 0.69843362 0.7
 0.70216487 0.70258333 0.70416667 0.70729523 0.71       0.72
 0.72048752 0.72224204 0.7246922  0.72733333 0.73       0.73083097
 0.74       0.745      0.74685021 0.7472619  0.75       0.75048771
 0.75269048 0.75542857 0.76       0.77       0.77333333 0.78
 0.7875     0.79       0.79181611 0.792      0.808      0.81
 0.81153965 0.815      0.82       0.82018895 0.82132001 0.83325577
 0.84       0.84456349 0.857      0.8662619  0.87       0.8775
 0.88       0.88161673 0.88923077 0.89083333 0.8975     0.90666667
 0.91       0.9125121  0.92       0.92066667 0.9345     0.935
 0.94       0.95       0.95583333 0.95758297 0.96       0.96633333
 0.97       0.97166667 0.98       0.98243506 0.98625    0.98666667
 0.98891667 0.98928571 0.98964286 0.99       0.992      0.998     ]

  UserWarning,

2022-10-31 11:03:20,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.00875    0.01       0.01166667 0.02       0.03       0.04
 0.05       0.05833333 0.06       0.07       0.08       0.08283333
 0.09       0.1        0.1025     0.11       0.12       0.13
 0.14       0.15       0.16       0.17       0.17083333 0.18
 0.18433333 0.19       0.2        0.21       0.21416667 0.21687166
 0.22       0.23       0.23016667 0.24       0.24929762 0.25
 0.25891126 0.259      0.26       0.27       0.28       0.29
 0.29392857 0.3        0.31       0.31471429 0.31766667 0.32
 0.33       0.34       0.34868131 0.35       0.35145238 0.36
 0.36364302 0.37       0.37294048 0.37700583 0.38       0.382
 0.38315668 0.39       0.4        0.41       0.41266667 0.42
 0.421      0.43       0.43014286 0.44       0.45       0.46
 0.47535714 0.4770423  0.48       0.48816667 0.49       0.49028152
 0.49333333 0.49502381 0.50216667 0.51564286 0.52907143 0.53
 0.53919323 0.54       0.54061905 0.54583333 0.55       0.55651803
 0.55676976 0.56       0.56179121 0.57       0.57316667 0.57988636
 0.586      0.58805403 0.59       0.59116667 0.6        0.60811472
 0.60890476 0.61       0.61191667 0.61490873 0.62166667 0.63
 0.63805665 0.64       0.64230952 0.6485     0.64869411 0.65447175
 0.66       0.67       0.67135714 0.68       0.68702381 0.69
 0.69528571 0.7        0.70714286 0.7090119  0.71       0.71496725
 0.71812771 0.72       0.72675    0.72754762 0.72959449 0.73
 0.73055556 0.73314286 0.73955411 0.74       0.75       0.75111905
 0.76       0.76678571 0.76784524 0.76918651 0.77       0.77990873
 0.78       0.78086274 0.79143937 0.79940873 0.8        0.80312779
 0.80422957 0.81       0.81090781 0.81161905 0.81166667 0.82
 0.83       0.83824495 0.84       0.84067491 0.84375397 0.84441595
 0.84864466 0.8554633  0.85595238 0.85980952 0.868      0.87
 0.8705     0.87425    0.875      0.87972944 0.88       0.88081421
 0.89       0.89333333 0.89991667 0.9        0.90345238 0.908
 0.9095     0.91       0.91063889 0.92       0.92814845 0.93
 0.93166667 0.94151046 0.94694048 0.94744012 0.95       0.96
 0.96584127 0.96833333 0.97       0.97833333 0.98       0.985
 0.99       0.99333333 0.99571429 0.99714286 0.99833333]

  UserWarning,

2022-10-31 11:03:20,108:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.0075     0.01       0.02       0.02261364 0.03       0.04
 0.05       0.05833333 0.06       0.065      0.0683254  0.07
 0.08       0.0865     0.09       0.0935     0.1        0.11
 0.12       0.12333333 0.13       0.14       0.14178571 0.15
 0.16       0.16361414 0.16966667 0.17       0.18       0.18333333
 0.19       0.2        0.21       0.21300433 0.22       0.23
 0.24       0.25       0.26       0.2625     0.27       0.28
 0.285      0.28933333 0.29       0.29333333 0.3        0.3082381
 0.31       0.32       0.3261978  0.33       0.34       0.35
 0.35222849 0.36       0.37       0.37066667 0.3725     0.375
 0.37809524 0.38       0.39       0.39107295 0.4        0.408
 0.41       0.42       0.42416667 0.43       0.43333333 0.43846429
 0.44       0.44152607 0.45       0.45666667 0.46       0.46228968
 0.47       0.47883333 0.48       0.49       0.4911533  0.49759524
 0.499      0.49983333 0.5        0.50069733 0.50258333 0.51
 0.51683067 0.52       0.52485317 0.53       0.53563611 0.53876046
 0.54       0.545      0.54626335 0.5495     0.55       0.57
 0.58       0.59       0.593      0.6        0.61       0.61652381
 0.62       0.62121429 0.62494646 0.62856643 0.63       0.64
 0.65       0.65294517 0.6592381  0.66       0.6627326  0.67
 0.67428175 0.68       0.68224173 0.69       0.7        0.7012619
 0.70235759 0.70333333 0.70333333 0.71       0.7152619  0.71721429
 0.72       0.73       0.73435317 0.74       0.7415     0.74268254
 0.75       0.75063292 0.75138095 0.75591289 0.75895058 0.76
 0.77       0.77383333 0.77414494 0.78       0.78066667 0.7882619
 0.78983531 0.79064286 0.79178571 0.79283333 0.795      0.79916667
 0.8        0.80319841 0.80466667 0.81       0.8151363  0.8164246
 0.82       0.83       0.83577778 0.83580556 0.83787662 0.83825472
 0.842      0.85133333 0.85157531 0.87       0.8762212  0.87894367
 0.88       0.88305556 0.884      0.8849881  0.89       0.89011905
 0.9        0.90583333 0.90733333 0.90733333 0.91       0.91080952
 0.91260606 0.91802237 0.92333333 0.9325     0.934      0.93828247
 0.94       0.94083333 0.94333333 0.95591667 0.96       0.96166667
 0.97       0.97666667 0.9825     0.98892857 0.99       0.9925
 0.995      0.996      0.9975     0.99875   ]

  UserWarning,

2022-10-31 11:03:20,108:INFO:Calculating mean and std
2022-10-31 11:03:20,108:INFO:Creating metrics dataframe
2022-10-31 11:03:20,108:INFO:Uploading results into container
2022-10-31 11:03:20,124:INFO:Uploading model into container now
2022-10-31 11:03:20,124:INFO:master_model_container: 26
2022-10-31 11:03:20,124:INFO:display_container: 2
2022-10-31 11:03:20,124:INFO:RandomForestRegressor(n_jobs=-1, random_state=3360)
2022-10-31 11:03:20,124:INFO:create_model() successfully completed......................................
2022-10-31 11:03:20,233:ERROR:create_model() for RandomForestRegressor(n_jobs=-1, random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:20,233:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:20,233:INFO:Initializing Extra Trees Regressor
2022-10-31 11:03:20,233:INFO:Total runtime is 1.7873881657918296 minutes
2022-10-31 11:03:20,233:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:20,233:INFO:Initializing create_model()
2022-10-31 11:03:20,233:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:20,233:INFO:Checking exceptions
2022-10-31 11:03:20,250:INFO:Importing libraries
2022-10-31 11:03:20,250:INFO:Copying training dataset
2022-10-31 11:03:20,250:INFO:Defining folds
2022-10-31 11:03:20,250:INFO:Declaring metric variables
2022-10-31 11:03:20,250:INFO:Importing untrained model
2022-10-31 11:03:20,256:INFO:Extra Trees Regressor Imported successfully
2022-10-31 11:03:20,257:INFO:Starting cross validation
2022-10-31 11:03:20,258:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:24,480:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04666667 0.05
 0.06       0.065      0.07       0.08       0.09       0.095
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.165      0.17       0.18       0.19       0.2
 0.2        0.21       0.22       0.23       0.23846154 0.24
 0.25       0.26       0.265      0.27       0.275      0.28
 0.29       0.3        0.31       0.31       0.33       0.33333333
 0.335      0.34       0.35       0.36       0.37       0.38
 0.39       0.4        0.4075     0.41       0.42       0.42857143
 0.44       0.45       0.45454545 0.455      0.46       0.47
 0.48       0.48666667 0.48666667 0.49       0.5        0.51
 0.52       0.52272727 0.53       0.54       0.55       0.55333333
 0.55555556 0.56       0.57       0.58       0.58333333 0.59
 0.59166667 0.595      0.6        0.6        0.61       0.61538462
 0.62       0.63       0.63636364 0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.71428571 0.71615385 0.72       0.73       0.74
 0.75       0.76       0.77       0.79       0.795      0.8
 0.8        0.82       0.83333333 0.84       0.84666667 0.85
 0.86       0.865      0.87       0.875      0.88888889 0.9
 0.9        0.91       0.9175     0.92       0.93       0.94
 0.944      0.95       0.96       0.97       0.98       0.99      ]

  UserWarning,

2022-10-31 11:03:24,645:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.006      0.01       0.02       0.03       0.04       0.05
 0.06       0.07       0.08       0.09       0.09666667 0.1
 0.11       0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.18428571 0.19       0.2        0.2
 0.21       0.22       0.225      0.23       0.24       0.25
 0.26       0.27       0.28       0.29       0.3        0.301
 0.31       0.32       0.33       0.33333333 0.34       0.34857143
 0.35       0.355      0.36       0.365      0.37       0.375
 0.38       0.39       0.4        0.41       0.41666667 0.42
 0.43       0.44       0.44444444 0.445      0.45       0.46
 0.47       0.48       0.49       0.5        0.51       0.52
 0.54       0.55       0.555      0.56       0.57142857 0.58
 0.582      0.58333333 0.59       0.6        0.6        0.61
 0.62       0.625      0.63       0.638      0.64       0.65
 0.65666667 0.66       0.66666667 0.67       0.69       0.6975
 0.7        0.7        0.71       0.71428571 0.72       0.73
 0.74       0.75       0.77       0.78       0.8        0.8
 0.81       0.81583333 0.82       0.83       0.83333333 0.84
 0.845      0.855      0.85714286 0.86       0.87       0.875
 0.89       0.9        0.902      0.92       0.93       0.94
 0.95       0.96       0.97       0.975      0.98       0.99
 0.9925    ]

  UserWarning,

2022-10-31 11:03:24,704:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.075      0.08       0.09       0.1        0.11
 0.12       0.13       0.14       0.15       0.15       0.16
 0.1625     0.17       0.18       0.19       0.2        0.20666667
 0.21       0.22       0.23       0.235      0.24       0.25
 0.26       0.27       0.275      0.28       0.28571429 0.3
 0.31       0.32       0.33       0.33333333 0.34       0.345
 0.35       0.36       0.37       0.38       0.39       0.4075
 0.41       0.42       0.42857143 0.43       0.44       0.45
 0.45428571 0.46       0.47       0.48       0.49       0.5
 0.50071429 0.51       0.52       0.52941176 0.53       0.53285714
 0.54       0.55       0.56       0.57       0.57142857 0.58
 0.58333333 0.59       0.6        0.6        0.61       0.61666667
 0.62       0.625      0.63       0.64       0.655      0.66
 0.66666667 0.67       0.68       0.684      0.685      0.69
 0.69666667 0.71       0.71428571 0.72       0.73       0.74
 0.74       0.74666667 0.75       0.76       0.76923077 0.77
 0.77777778 0.78       0.78923077 0.79       0.8        0.8
 0.82       0.83       0.83333333 0.84       0.85       0.85714286
 0.86       0.87       0.88       0.884375   0.88888889 0.89
 0.9        0.91       0.92       0.93       0.93666667 0.94
 0.95       0.96       0.97       0.975      0.98       0.98461538
 0.99       0.99142857]

  UserWarning,

2022-10-31 11:03:24,735:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.025      0.03       0.035      0.04
 0.05       0.06       0.07       0.075      0.08       0.08425
 0.09       0.1        0.1        0.11       0.12       0.13
 0.14       0.15       0.16       0.17       0.18       0.19
 0.2        0.20525    0.21       0.22       0.23       0.24
 0.25       0.26       0.27       0.28       0.29       0.3
 0.31       0.32       0.33       0.33333333 0.34       0.35
 0.36       0.365      0.37       0.375      0.38       0.39
 0.4        0.4        0.41       0.41       0.42       0.42857143
 0.43       0.44       0.44444444 0.45       0.46       0.47
 0.48       0.49       0.5        0.51       0.51666667 0.52
 0.53       0.54       0.54545455 0.55       0.56       0.57142857
 0.58       0.58333333 0.59       0.6        0.6        0.61
 0.62       0.62       0.62333333 0.64       0.65       0.655
 0.66       0.66666667 0.67       0.68       0.69       0.69230769
 0.7        0.705      0.71       0.71428571 0.72       0.73
 0.74       0.75       0.76       0.77       0.78       0.8
 0.8        0.82       0.83333333 0.84       0.84142857 0.84615385
 0.85       0.85714286 0.87       0.875      0.88       0.9
 0.91       0.91692308 0.92       0.92857143 0.93       0.94
 0.945      0.95       0.96       0.965      0.97       0.97666667
 0.98       0.99       0.995     ]

  UserWarning,

2022-10-31 11:03:24,935:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.11545455
 0.12       0.13       0.14       0.15       0.16       0.17
 0.18       0.19       0.2        0.2        0.21       0.22
 0.23       0.23833333 0.24       0.25       0.26       0.27
 0.27666667 0.28       0.29       0.3        0.31       0.32
 0.33       0.33333333 0.34       0.35       0.36       0.37
 0.38       0.39       0.4        0.4        0.41       0.42
 0.42857143 0.43       0.44       0.44       0.45       0.46
 0.47       0.49       0.5        0.51       0.52       0.53
 0.55       0.56       0.57       0.57142857 0.58       0.58333333
 0.59       0.6        0.6        0.61       0.61333333 0.61818182
 0.63       0.64       0.65       0.66       0.66666667 0.67
 0.68       0.69       0.7        0.7        0.71       0.71428571
 0.72       0.72727273 0.73       0.75       0.76       0.77
 0.78571429 0.8        0.81       0.81818182 0.83       0.83166667
 0.83333333 0.84       0.85       0.86       0.87       0.88888889
 0.89       0.89666667 0.9        0.91       0.92       0.93
 0.94       0.94333333 0.95       0.952      0.96       0.9675
 0.97       0.97666667 0.98       0.982      0.985      0.98666667
 0.99      ]

  UserWarning,

2022-10-31 11:03:24,966:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.015      0.02       0.03       0.04       0.05
 0.06       0.07       0.08       0.09       0.1        0.11
 0.12       0.13       0.14       0.1475     0.15       0.16
 0.17       0.18       0.19       0.195      0.2        0.2
 0.21       0.22       0.23       0.24       0.25       0.255
 0.26       0.27       0.272      0.28       0.29       0.29285714
 0.295      0.3        0.31       0.32       0.33       0.33333333
 0.335      0.34       0.35       0.36       0.37       0.38
 0.39       0.4        0.4        0.41       0.42       0.42857143
 0.43       0.44       0.445      0.45       0.46       0.47
 0.49       0.5        0.51       0.52       0.52666667 0.53
 0.54       0.546      0.55       0.55555556 0.56       0.565
 0.57       0.58       0.58333333 0.59       0.6        0.62
 0.63       0.64       0.65       0.66       0.66333333 0.66666667
 0.67       0.69       0.7        0.71       0.71428571 0.73
 0.73       0.74       0.75       0.77       0.78       0.79
 0.795      0.79666667 0.8        0.8        0.81       0.81818182
 0.82       0.82333333 0.83       0.84       0.85       0.86
 0.87       0.875      0.885      0.89       0.9        0.9025
 0.91       0.92       0.94       0.95       0.96       0.97
 0.9725     0.97375    0.97571429 0.98       0.99      ]

  UserWarning,

2022-10-31 11:03:25,013:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.12
 0.125      0.13       0.135      0.14       0.1475     0.15
 0.16       0.17       0.18       0.19       0.2        0.21
 0.22       0.23       0.235      0.24       0.25       0.26
 0.265      0.27       0.28       0.29       0.3        0.305
 0.31       0.32       0.33       0.33333333 0.34       0.35
 0.37       0.378      0.38       0.39       0.395      0.4
 0.4        0.41       0.42       0.43       0.44       0.45
 0.46       0.47       0.475      0.47857143 0.48       0.49
 0.495      0.5        0.51       0.52       0.53       0.54
 0.55       0.56       0.562      0.57       0.58       0.58333333
 0.59       0.6        0.6        0.61       0.62       0.63
 0.63636364 0.64       0.65       0.66       0.66666667 0.67
 0.68       0.69       0.69230769 0.7        0.71       0.72
 0.735      0.74       0.75       0.76       0.76923077 0.77
 0.77777778 0.785      0.79       0.8        0.81       0.81076923
 0.815      0.83       0.83333333 0.84       0.85       0.85714286
 0.87       0.88       0.8875     0.89       0.9        0.91
 0.92       0.92666667 0.93       0.94       0.95       0.9525
 0.96       0.97       0.975      0.9775     0.98       0.99
 0.9925    ]

  UserWarning,

2022-10-31 11:03:25,028:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.052
 0.06       0.07       0.08       0.08333333 0.09       0.1
 0.11       0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.19       0.2        0.21       0.22
 0.23       0.24       0.25       0.26       0.27       0.28
 0.28571429 0.29       0.3        0.31       0.32       0.33
 0.33333333 0.34       0.35       0.36       0.36875    0.37
 0.375      0.38       0.39       0.4        0.4        0.41
 0.41666667 0.42       0.43       0.431      0.45454545 0.455
 0.46       0.46333333 0.47       0.48       0.49       0.5
 0.51       0.5175     0.52       0.53       0.54       0.56
 0.57       0.58       0.58333333 0.59       0.6        0.61
 0.62       0.625      0.63       0.63       0.64       0.64285714
 0.65       0.66       0.66666667 0.68       0.69       0.69230769
 0.7        0.71       0.725      0.73       0.74       0.75
 0.75       0.76       0.77       0.785      0.79       0.8
 0.8        0.802      0.805      0.82       0.825      0.83
 0.83333333 0.84666667 0.85       0.855      0.85714286 0.86
 0.87       0.875      0.885      0.91       0.912      0.92
 0.93       0.94       0.94375    0.95       0.95125    0.955
 0.96       0.962      0.9675     0.97       0.97285714 0.98
 0.985      0.99      ]

  UserWarning,

2022-10-31 11:03:26,334:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.055
 0.06       0.06       0.06666667 0.07       0.08       0.08333333
 0.09       0.1        0.11       0.12       0.13       0.14
 0.15       0.16       0.17       0.18       0.19       0.2
 0.205      0.21       0.22       0.23       0.24       0.25
 0.26       0.265      0.27       0.2725     0.2775     0.28
 0.29       0.3        0.31       0.315      0.32       0.33
 0.33333333 0.34       0.35       0.36       0.37       0.375
 0.38       0.38333333 0.39       0.4        0.42       0.43
 0.43       0.44       0.44857143 0.45       0.46       0.47
 0.4725     0.49       0.5        0.505      0.51       0.54
 0.54545455 0.55       0.56       0.57       0.57142857 0.58
 0.59       0.6        0.6        0.61       0.62       0.625
 0.62666667 0.63       0.63636364 0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.71428571 0.73       0.75       0.75333333 0.76
 0.77       0.77777778 0.78       0.79857143 0.8        0.805
 0.81       0.81818182 0.82       0.83       0.834      0.85
 0.85714286 0.86       0.87       0.88       0.885      0.9
 0.90444444 0.91       0.92       0.93       0.94       0.95
 0.96       0.97       0.97142857 0.98       0.985      0.99
 0.99714286]

  UserWarning,

2022-10-31 11:03:26,504:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.075      0.08       0.086      0.09       0.095
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.17       0.18       0.19       0.2        0.21
 0.22       0.23       0.24       0.25       0.26       0.265
 0.27       0.28       0.29       0.3        0.31       0.32
 0.33       0.33333333 0.34       0.34666667 0.35       0.36
 0.36666667 0.37       0.375      0.38       0.39       0.4
 0.4        0.41       0.41333333 0.42       0.43       0.44
 0.44444444 0.45       0.455      0.46       0.47       0.47
 0.48       0.49       0.5        0.505      0.52       0.525
 0.54       0.54545455 0.55       0.56       0.57       0.57142857
 0.57833333 0.58       0.59       0.6        0.61       0.61538462
 0.62       0.625      0.63       0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.72       0.73       0.74       0.75       0.76
 0.77       0.775      0.78       0.78333333 0.78571429 0.79
 0.8        0.8        0.81071429 0.81818182 0.82       0.83333333
 0.84       0.85714286 0.87       0.875      0.88       0.88666667
 0.89       0.8975     0.9        0.9025     0.91       0.93
 0.94       0.945      0.95       0.96       0.97       0.98
 0.985      0.99      ]

  UserWarning,

2022-10-31 11:03:26,504:INFO:Calculating mean and std
2022-10-31 11:03:26,504:INFO:Creating metrics dataframe
2022-10-31 11:03:26,504:INFO:Uploading results into container
2022-10-31 11:03:26,504:INFO:Uploading model into container now
2022-10-31 11:03:26,504:INFO:master_model_container: 27
2022-10-31 11:03:26,519:INFO:display_container: 2
2022-10-31 11:03:26,519:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3360)
2022-10-31 11:03:26,519:INFO:create_model() successfully completed......................................
2022-10-31 11:03:26,629:WARNING:create_model() for ExtraTreesRegressor(n_jobs=-1, random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:26,629:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:26,629:INFO:Initializing create_model()
2022-10-31 11:03:26,629:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:26,629:INFO:Checking exceptions
2022-10-31 11:03:26,629:INFO:Importing libraries
2022-10-31 11:03:26,629:INFO:Copying training dataset
2022-10-31 11:03:26,645:INFO:Defining folds
2022-10-31 11:03:26,645:INFO:Declaring metric variables
2022-10-31 11:03:26,645:INFO:Importing untrained model
2022-10-31 11:03:26,645:INFO:Extra Trees Regressor Imported successfully
2022-10-31 11:03:26,645:INFO:Starting cross validation
2022-10-31 11:03:26,645:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:30,878:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.075      0.08       0.09       0.1        0.11
 0.12       0.13       0.14       0.15       0.15       0.16
 0.1625     0.17       0.18       0.19       0.2        0.20666667
 0.21       0.22       0.23       0.235      0.24       0.25
 0.26       0.27       0.275      0.28       0.28571429 0.3
 0.31       0.32       0.33       0.33333333 0.34       0.345
 0.35       0.36       0.37       0.38       0.39       0.4075
 0.41       0.42       0.42857143 0.43       0.44       0.45
 0.45428571 0.46       0.47       0.48       0.49       0.5
 0.50071429 0.51       0.52       0.52941176 0.53       0.53285714
 0.54       0.55       0.56       0.57       0.57142857 0.58
 0.58333333 0.59       0.6        0.6        0.61       0.61666667
 0.62       0.625      0.63       0.64       0.655      0.66
 0.66666667 0.67       0.68       0.684      0.685      0.69
 0.69666667 0.71       0.71428571 0.72       0.73       0.74
 0.74       0.74666667 0.75       0.76       0.76923077 0.77
 0.77777778 0.78       0.78923077 0.79       0.8        0.8
 0.82       0.83       0.83333333 0.84       0.85       0.85714286
 0.86       0.87       0.88       0.884375   0.88888889 0.89
 0.9        0.91       0.92       0.93       0.93666667 0.94
 0.95       0.96       0.97       0.975      0.98       0.98461538
 0.99       0.99142857]

  UserWarning,

2022-10-31 11:03:31,067:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.04666667 0.05
 0.06       0.065      0.07       0.08       0.09       0.095
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.165      0.17       0.18       0.19       0.2
 0.2        0.21       0.22       0.23       0.23846154 0.24
 0.25       0.26       0.265      0.27       0.275      0.28
 0.29       0.3        0.31       0.31       0.33       0.33333333
 0.335      0.34       0.35       0.36       0.37       0.38
 0.39       0.4        0.4075     0.41       0.42       0.42857143
 0.44       0.45       0.45454545 0.455      0.46       0.47
 0.48       0.48666667 0.48666667 0.49       0.5        0.51
 0.52       0.52272727 0.53       0.54       0.55       0.55333333
 0.55555556 0.56       0.57       0.58       0.58333333 0.59
 0.59166667 0.595      0.6        0.6        0.61       0.61538462
 0.62       0.63       0.63636364 0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.71428571 0.71615385 0.72       0.73       0.74
 0.75       0.76       0.77       0.79       0.795      0.8
 0.8        0.82       0.83333333 0.84       0.84666667 0.85
 0.86       0.865      0.87       0.875      0.88888889 0.9
 0.9        0.91       0.9175     0.92       0.93       0.94
 0.944      0.95       0.96       0.97       0.98       0.99      ]

  UserWarning,

2022-10-31 11:03:31,200:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.006      0.01       0.02       0.03       0.04       0.05
 0.06       0.07       0.08       0.09       0.09666667 0.1
 0.11       0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.18428571 0.19       0.2        0.2
 0.21       0.22       0.225      0.23       0.24       0.25
 0.26       0.27       0.28       0.29       0.3        0.301
 0.31       0.32       0.33       0.33333333 0.34       0.34857143
 0.35       0.355      0.36       0.365      0.37       0.375
 0.38       0.39       0.4        0.41       0.41666667 0.42
 0.43       0.44       0.44444444 0.445      0.45       0.46
 0.47       0.48       0.49       0.5        0.51       0.52
 0.54       0.55       0.555      0.56       0.57142857 0.58
 0.582      0.58333333 0.59       0.6        0.6        0.61
 0.62       0.625      0.63       0.638      0.64       0.65
 0.65666667 0.66       0.66666667 0.67       0.69       0.6975
 0.7        0.7        0.71       0.71428571 0.72       0.73
 0.74       0.75       0.77       0.78       0.8        0.8
 0.81       0.81583333 0.82       0.83       0.83333333 0.84
 0.845      0.855      0.85714286 0.86       0.87       0.875
 0.89       0.9        0.902      0.92       0.93       0.94
 0.95       0.96       0.97       0.975      0.98       0.99
 0.9925    ]

  UserWarning,

2022-10-31 11:03:31,270:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.12
 0.125      0.13       0.135      0.14       0.1475     0.15
 0.16       0.17       0.18       0.19       0.2        0.21
 0.22       0.23       0.235      0.24       0.25       0.26
 0.265      0.27       0.28       0.29       0.3        0.305
 0.31       0.32       0.33       0.33333333 0.34       0.35
 0.37       0.378      0.38       0.39       0.395      0.4
 0.4        0.41       0.42       0.43       0.44       0.45
 0.46       0.47       0.475      0.47857143 0.48       0.49
 0.495      0.5        0.51       0.52       0.53       0.54
 0.55       0.56       0.562      0.57       0.58       0.58333333
 0.59       0.6        0.6        0.61       0.62       0.63
 0.63636364 0.64       0.65       0.66       0.66666667 0.67
 0.68       0.69       0.69230769 0.7        0.71       0.72
 0.735      0.74       0.75       0.76       0.76923077 0.77
 0.77777778 0.785      0.79       0.8        0.81       0.81076923
 0.815      0.83       0.83333333 0.84       0.84       0.85
 0.85714286 0.87       0.88       0.8875     0.89       0.9
 0.91       0.92       0.92666667 0.93       0.94       0.95
 0.9525     0.96       0.97       0.975      0.9775     0.98
 0.99       0.9925    ]

  UserWarning,

2022-10-31 11:03:31,287:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.025      0.03       0.035      0.04
 0.05       0.06       0.07       0.075      0.08       0.08425
 0.09       0.1        0.1        0.11       0.12       0.13
 0.14       0.15       0.16       0.17       0.18       0.19
 0.2        0.20525    0.21       0.22       0.23       0.24
 0.25       0.26       0.27       0.28       0.29       0.3
 0.31       0.32       0.33       0.33333333 0.34       0.35
 0.36       0.365      0.37       0.375      0.38       0.39
 0.4        0.4        0.41       0.41       0.42       0.42857143
 0.43       0.44       0.44444444 0.45       0.46       0.47
 0.48       0.49       0.5        0.51       0.51666667 0.52
 0.53       0.54       0.54545455 0.55       0.56       0.57142857
 0.58       0.58333333 0.59       0.6        0.6        0.61
 0.62       0.62       0.62333333 0.64       0.65       0.655
 0.66       0.66666667 0.67       0.68       0.69       0.69230769
 0.7        0.705      0.71       0.71428571 0.72       0.73
 0.74       0.75       0.76       0.77       0.78       0.8
 0.8        0.82       0.83333333 0.84       0.84142857 0.84615385
 0.85       0.85714286 0.87       0.875      0.88       0.9
 0.91       0.91692308 0.92       0.92857143 0.93       0.94
 0.945      0.95       0.96       0.965      0.97       0.97666667
 0.98       0.99       0.995     ]

  UserWarning,

2022-10-31 11:03:31,361:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.08       0.09       0.1        0.11       0.11545455
 0.12       0.13       0.14       0.15       0.16       0.17
 0.18       0.19       0.2        0.2        0.21       0.22
 0.23       0.23833333 0.24       0.25       0.26       0.27
 0.27666667 0.28       0.29       0.3        0.31       0.32
 0.33       0.33333333 0.34       0.35       0.36       0.37
 0.38       0.39       0.4        0.4        0.41       0.42
 0.42857143 0.43       0.44       0.44       0.45       0.46
 0.47       0.49       0.5        0.51       0.52       0.53
 0.55       0.56       0.57       0.57142857 0.58       0.58333333
 0.59       0.6        0.6        0.61       0.61333333 0.61818182
 0.63       0.64       0.65       0.66       0.66666667 0.67
 0.68       0.69       0.7        0.7        0.71       0.71428571
 0.72       0.72727273 0.73       0.75       0.76       0.77
 0.78571429 0.8        0.81       0.81818182 0.83       0.83166667
 0.83333333 0.84       0.85       0.86       0.87       0.88888889
 0.89       0.89666667 0.9        0.91       0.92       0.93
 0.94       0.94333333 0.94333333 0.95       0.952      0.96
 0.9675     0.97       0.97666667 0.98       0.982      0.985
 0.98666667 0.99      ]

  UserWarning,

2022-10-31 11:03:31,408:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.052
 0.06       0.07       0.08       0.08333333 0.09       0.1
 0.11       0.12       0.13       0.14       0.15       0.16
 0.17       0.18       0.19       0.2        0.21       0.22
 0.23       0.24       0.25       0.26       0.27       0.28
 0.28571429 0.29       0.3        0.31       0.32       0.33
 0.33333333 0.34       0.35       0.36       0.36875    0.37
 0.375      0.38       0.39       0.4        0.4        0.41
 0.41666667 0.42       0.43       0.431      0.45454545 0.455
 0.46       0.46333333 0.47       0.48       0.49       0.5
 0.51       0.5175     0.52       0.53       0.54       0.56
 0.57       0.58       0.58333333 0.59       0.6        0.61
 0.62       0.625      0.63       0.63       0.64       0.64285714
 0.65       0.66       0.66666667 0.68       0.69       0.69230769
 0.7        0.71       0.725      0.73       0.74       0.75
 0.75       0.76       0.77       0.785      0.79       0.8
 0.8        0.802      0.805      0.82       0.825      0.83
 0.83333333 0.84666667 0.85       0.855      0.85714286 0.86
 0.87       0.875      0.885      0.91       0.912      0.92
 0.93       0.94       0.94375    0.95       0.95125    0.955
 0.96       0.962      0.9675     0.97       0.97285714 0.98
 0.985      0.99      ]

  UserWarning,

2022-10-31 11:03:31,424:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.015      0.02       0.03       0.04       0.05
 0.06       0.07       0.08       0.09       0.1        0.11
 0.12       0.13       0.14       0.1475     0.15       0.16
 0.17       0.18       0.19       0.195      0.2        0.2
 0.21       0.22       0.23       0.24       0.25       0.255
 0.26       0.27       0.272      0.28       0.29       0.29285714
 0.295      0.3        0.31       0.32       0.33       0.33333333
 0.335      0.34       0.35       0.36       0.37       0.38
 0.39       0.4        0.4        0.41       0.42       0.42857143
 0.43       0.44       0.445      0.45       0.46       0.47
 0.49       0.5        0.51       0.52       0.52666667 0.53
 0.54       0.546      0.55       0.55555556 0.56       0.565
 0.57       0.58       0.58333333 0.59       0.6        0.62
 0.63       0.64       0.65       0.66       0.66333333 0.66666667
 0.67       0.69       0.7        0.71       0.71428571 0.73
 0.73       0.74       0.75       0.77       0.78       0.79
 0.795      0.79666667 0.8        0.8        0.81       0.81818182
 0.82       0.82333333 0.83       0.84       0.85       0.86
 0.87       0.875      0.885      0.89       0.9        0.9025
 0.91       0.92       0.94       0.95       0.96       0.97
 0.9725     0.97375    0.97571429 0.98       0.99      ]

  UserWarning,

2022-10-31 11:03:32,756:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.055
 0.06       0.06       0.06666667 0.07       0.08       0.08333333
 0.09       0.1        0.11       0.12       0.13       0.14
 0.15       0.16       0.17       0.18       0.19       0.2
 0.205      0.21       0.22       0.23       0.24       0.25
 0.26       0.265      0.27       0.2725     0.2775     0.28
 0.29       0.3        0.31       0.315      0.32       0.33
 0.33333333 0.34       0.35       0.36       0.37       0.375
 0.38       0.38333333 0.39       0.4        0.42       0.43
 0.43       0.44       0.44857143 0.45       0.46       0.47
 0.4725     0.49       0.5        0.505      0.51       0.54
 0.54545455 0.55       0.56       0.57       0.57142857 0.58
 0.59       0.6        0.6        0.61       0.62       0.625
 0.62666667 0.63       0.63636364 0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.71428571 0.73       0.75       0.75333333 0.76
 0.77       0.77777778 0.78       0.79857143 0.8        0.805
 0.81       0.81818182 0.82       0.83       0.834      0.85
 0.85714286 0.86       0.87       0.88       0.885      0.9
 0.90444444 0.91       0.92       0.93       0.94       0.95
 0.96       0.97       0.97142857 0.98       0.985      0.99
 0.99714286]

  UserWarning,

2022-10-31 11:03:32,886:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.01       0.02       0.03       0.04       0.05       0.06
 0.07       0.075      0.08       0.086      0.09       0.095
 0.1        0.11       0.12       0.13       0.14       0.15
 0.16       0.17       0.18       0.19       0.2        0.21
 0.22       0.23       0.24       0.25       0.26       0.265
 0.27       0.28       0.29       0.3        0.31       0.32
 0.33       0.33333333 0.34       0.34666667 0.35       0.36
 0.36666667 0.37       0.375      0.38       0.39       0.4
 0.4        0.41       0.41333333 0.42       0.43       0.44
 0.44444444 0.45       0.455      0.46       0.47       0.47
 0.48       0.49       0.5        0.505      0.52       0.525
 0.54       0.54545455 0.55       0.56       0.57       0.57142857
 0.57833333 0.58       0.59       0.6        0.61       0.61538462
 0.62       0.625      0.63       0.64       0.65       0.66
 0.66666667 0.67       0.68       0.69       0.7        0.7
 0.71       0.72       0.73       0.74       0.75       0.76
 0.77       0.775      0.78       0.78333333 0.78571429 0.79
 0.8        0.8        0.81071429 0.81818182 0.82       0.83333333
 0.84       0.85714286 0.87       0.875      0.88       0.88666667
 0.89       0.8975     0.9        0.9025     0.91       0.93
 0.94       0.945      0.95       0.96       0.97       0.98
 0.985      0.99      ]

  UserWarning,

2022-10-31 11:03:32,888:INFO:Calculating mean and std
2022-10-31 11:03:32,889:INFO:Creating metrics dataframe
2022-10-31 11:03:32,897:INFO:Uploading results into container
2022-10-31 11:03:32,897:INFO:Uploading model into container now
2022-10-31 11:03:32,898:INFO:master_model_container: 28
2022-10-31 11:03:32,898:INFO:display_container: 2
2022-10-31 11:03:32,899:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=3360)
2022-10-31 11:03:32,899:INFO:create_model() successfully completed......................................
2022-10-31 11:03:33,011:ERROR:create_model() for ExtraTreesRegressor(n_jobs=-1, random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:33,027:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:33,027:INFO:Initializing AdaBoost Regressor
2022-10-31 11:03:33,027:INFO:Total runtime is 2.0006176829338074 minutes
2022-10-31 11:03:33,027:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:33,027:INFO:Initializing create_model()
2022-10-31 11:03:33,027:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:33,027:INFO:Checking exceptions
2022-10-31 11:03:33,027:INFO:Importing libraries
2022-10-31 11:03:33,027:INFO:Copying training dataset
2022-10-31 11:03:33,027:INFO:Defining folds
2022-10-31 11:03:33,027:INFO:Declaring metric variables
2022-10-31 11:03:33,027:INFO:Importing untrained model
2022-10-31 11:03:33,027:INFO:AdaBoost Regressor Imported successfully
2022-10-31 11:03:33,027:INFO:Starting cross validation
2022-10-31 11:03:33,027:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:35,392:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.13380282 0.17437722 0.21538462 0.2192029  0.23214286 0.27136929
 0.31149927 0.31538462 0.33499377 0.3500821  0.43068283 0.51304348
 0.51666667 0.51935298 0.52335456 0.52421442 0.54798464 0.58250497
 0.61321672 0.66149871 0.88557214 0.96440129]

  UserWarning,

2022-10-31 11:03:35,423:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.14423077 0.16606498 0.20478723 0.20866142 0.25631068 0.30379747
 0.30577576 0.30777311 0.32191781 0.34278156 0.35       0.35826772
 0.35971223 0.39511202 0.39607201 0.41678225 0.42887854 0.48717949
 0.52068558 0.57029703 0.57359307 0.57729941 0.58710562 0.61024499
 0.65549598 0.86190476 0.97755611]

  UserWarning,

2022-10-31 11:03:35,481:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.11368015 0.1440678  0.18604651 0.22136422 0.24077329 0.28494041
 0.28550405 0.34766544 0.40677966 0.41614907 0.50793651 0.51305335
 0.53374486 0.6        0.60664112 0.80555556 0.83168317 0.98478261]

  UserWarning,

2022-10-31 11:03:35,580:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.07855626 0.12413793 0.12438625 0.13735178 0.14020619 0.15730337
 0.19618834 0.20844327 0.25786164 0.25865052 0.27538071 0.28151261
 0.29732869 0.29824561 0.33131923 0.33669065 0.34008097 0.34088763
 0.34353741 0.34754098 0.34960938 0.36651163 0.37267081 0.37280702
 0.38541667 0.42070863 0.42208934 0.42790531 0.46002077 0.48421053
 0.50886974 0.52536005 0.534375   0.54201681 0.55323383 0.55450237
 0.56997972 0.57197452 0.67579909 0.72699069 0.7408377  0.75733333
 0.78835979 0.9504644 ]

  UserWarning,

2022-10-31 11:03:35,595:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.13424658 0.15273775 0.1691475  0.21351351 0.21462264 0.25282167
 0.25505051 0.29585799 0.3260573  0.42628093 0.4275     0.45025792
 0.46875    0.49681159 0.5288839  0.56615385 0.58461538 0.58906883
 0.60683761 0.72383721 0.75821596 0.80555556 0.9630485 ]

  UserWarning,

2022-10-31 11:03:35,611:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.11184211 0.1255814  0.15079365 0.19354839 0.2        0.20330969
 0.24933452 0.27858985 0.29223181 0.31147541 0.31532748 0.33904762
 0.35472973 0.3581127  0.40307935 0.40458015 0.49747984 0.51412429
 0.5186984  0.52339545 0.53521127 0.54545455 0.5461361  0.55927052
 0.59495352 0.60425101 0.61702128 0.65686275 0.71985816 0.78891821
 0.97633136]

  UserWarning,

2022-10-31 11:03:35,656:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.12917595 0.14130435 0.1565762  0.16901408 0.19193021 0.21367521
 0.22585925 0.24333333 0.24816176 0.25111111 0.26081258 0.27340824
 0.27631579 0.28039216 0.29354207 0.33080425 0.35416667 0.35657686
 0.39572349 0.41330998 0.47381302 0.5225     0.525      0.54518693
 0.54755672 0.5587886  0.56179775 0.57067371 0.57875    0.62430939
 0.63364055 0.65914787 0.78592375 0.9494382 ]

  UserWarning,

2022-10-31 11:03:35,698:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.07692308 0.12286689 0.20824295 0.22335025 0.22641509 0.23801917
 0.24025974 0.26415094 0.2897351  0.29638723 0.30510204 0.30639098
 0.31034483 0.31181102 0.3153457  0.32569721 0.3315829  0.33778148
 0.35897436 0.3715342  0.37552743 0.38181818 0.38436189 0.38650307
 0.39325843 0.40458015 0.41176471 0.4619883  0.475      0.51375483
 0.51441242 0.52038043 0.52160356 0.52607261 0.53321118 0.54043393
 0.54542873 0.54985755 0.55771567 0.55869074 0.56292352 0.59471366
 0.59747847 0.62237978 0.62723214 0.6291834  0.6541555  0.70540541
 0.75598086 0.75942029 0.91401869]

  UserWarning,

2022-10-31 11:03:36,591:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.12899543 0.14247312 0.15923567 0.17699115 0.18843683 0.23037975
 0.23193916 0.2606727  0.34020619 0.41215914 0.43713674 0.44444444
 0.49578415 0.51363847 0.51570681 0.53130148 0.55128205 0.55845568
 0.5593692  0.57869416 0.63837209 0.67132867 0.78448276 0.86165049
 0.95280236]

  UserWarning,

2022-10-31 11:03:36,622:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.15236427 0.1887287  0.19582851 0.20798898 0.228125   0.23780488
 0.26       0.26890756 0.28710462 0.28719008 0.29545455 0.29814875
 0.3046595  0.34375    0.36576239 0.48333333 0.50761421 0.5148265
 0.52429039 0.53772291 0.54461822 0.58232044 0.62229905 0.63027656
 0.64864865 0.69444444 0.81818182 0.96174863]

  UserWarning,

2022-10-31 11:03:36,622:INFO:Calculating mean and std
2022-10-31 11:03:36,622:INFO:Creating metrics dataframe
2022-10-31 11:03:36,622:INFO:Uploading results into container
2022-10-31 11:03:36,622:INFO:Uploading model into container now
2022-10-31 11:03:36,622:INFO:master_model_container: 29
2022-10-31 11:03:36,622:INFO:display_container: 2
2022-10-31 11:03:36,622:INFO:AdaBoostRegressor(random_state=3360)
2022-10-31 11:03:36,622:INFO:create_model() successfully completed......................................
2022-10-31 11:03:36,746:WARNING:create_model() for AdaBoostRegressor(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:36,746:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:36,746:INFO:Initializing create_model()
2022-10-31 11:03:36,746:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:36,746:INFO:Checking exceptions
2022-10-31 11:03:36,746:INFO:Importing libraries
2022-10-31 11:03:36,746:INFO:Copying training dataset
2022-10-31 11:03:36,746:INFO:Defining folds
2022-10-31 11:03:36,746:INFO:Declaring metric variables
2022-10-31 11:03:36,746:INFO:Importing untrained model
2022-10-31 11:03:36,746:INFO:AdaBoost Regressor Imported successfully
2022-10-31 11:03:36,746:INFO:Starting cross validation
2022-10-31 11:03:36,746:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:39,158:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.13380282 0.17437722 0.21538462 0.2192029  0.23214286 0.27136929
 0.31149927 0.31538462 0.33499377 0.3500821  0.43068283 0.51304348
 0.51666667 0.51935298 0.52335456 0.52421442 0.54798464 0.58250497
 0.61321672 0.66149871 0.88557214 0.96440129]

  UserWarning,

2022-10-31 11:03:39,267:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.13424658 0.15273775 0.1691475  0.21351351 0.21462264 0.25282167
 0.25505051 0.29585799 0.3260573  0.42628093 0.4275     0.45025792
 0.46875    0.49681159 0.5288839  0.56615385 0.58461538 0.58906883
 0.60683761 0.72383721 0.75821596 0.80555556 0.9630485 ]

  UserWarning,

2022-10-31 11:03:39,274:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.14423077 0.16606498 0.20478723 0.20866142 0.25631068 0.30379747
 0.30577576 0.30777311 0.32191781 0.34278156 0.35       0.35826772
 0.35971223 0.39511202 0.39607201 0.41678225 0.42887854 0.48717949
 0.52068558 0.57029703 0.57359307 0.57729941 0.58710562 0.61024499
 0.65549598 0.86190476 0.97755611]

  UserWarning,

2022-10-31 11:03:39,277:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.11368015 0.1440678  0.18604651 0.22136422 0.24077329 0.28494041
 0.28550405 0.34766544 0.40677966 0.41614907 0.50793651 0.51305335
 0.53374486 0.6        0.60664112 0.80555556 0.83168317 0.98478261]

  UserWarning,

2022-10-31 11:03:39,327:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.07855626 0.12413793 0.12438625 0.13735178 0.14020619 0.15730337
 0.19618834 0.20844327 0.25786164 0.25865052 0.27538071 0.28151261
 0.29732869 0.29824561 0.33131923 0.33669065 0.34008097 0.34088763
 0.34353741 0.34754098 0.34960938 0.36651163 0.37267081 0.37280702
 0.38541667 0.42070863 0.42208934 0.42790531 0.46002077 0.48421053
 0.50886974 0.52536005 0.534375   0.54201681 0.55323383 0.55450237
 0.56997972 0.57197452 0.67579909 0.72699069 0.7408377  0.75733333
 0.78835979 0.9504644 ]

  UserWarning,

2022-10-31 11:03:39,374:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.11184211 0.1255814  0.15079365 0.19354839 0.2        0.20330969
 0.24933452 0.27858985 0.29223181 0.31147541 0.31532748 0.33904762
 0.35472973 0.3581127  0.40307935 0.40458015 0.49747984 0.51412429
 0.5186984  0.52339545 0.53521127 0.54545455 0.5461361  0.55927052
 0.59495352 0.60425101 0.61702128 0.65686275 0.71985816 0.78891821
 0.97633136]

  UserWarning,

2022-10-31 11:03:39,374:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.12917595 0.14130435 0.1565762  0.16901408 0.19193021 0.21367521
 0.22585925 0.24333333 0.24816176 0.25111111 0.26081258 0.27340824
 0.27631579 0.28039216 0.29354207 0.33080425 0.35416667 0.35657686
 0.39572349 0.41330998 0.47381302 0.5225     0.525      0.54518693
 0.54755672 0.5587886  0.56179775 0.57067371 0.57875    0.62430939
 0.63364055 0.65914787 0.78592375 0.9494382 ]

  UserWarning,

2022-10-31 11:03:39,492:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.07692308 0.12286689 0.20824295 0.22335025 0.22641509 0.23801917
 0.24025974 0.26415094 0.2897351  0.29638723 0.30510204 0.30639098
 0.31034483 0.31181102 0.3153457  0.32569721 0.3315829  0.33778148
 0.35897436 0.3715342  0.37552743 0.38181818 0.38436189 0.38650307
 0.39325843 0.40458015 0.41176471 0.4619883  0.475      0.51375483
 0.51441242 0.52038043 0.52160356 0.52607261 0.53321118 0.54043393
 0.54542873 0.54985755 0.55771567 0.55869074 0.56292352 0.59471366
 0.59747847 0.62237978 0.62723214 0.6291834  0.6541555  0.70540541
 0.75598086 0.75942029 0.91401869]

  UserWarning,

2022-10-31 11:03:40,374:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.12899543 0.14247312 0.15923567 0.17699115 0.18843683 0.23037975
 0.23193916 0.2606727  0.34020619 0.41215914 0.43713674 0.44444444
 0.49578415 0.51363847 0.51570681 0.53130148 0.55128205 0.55845568
 0.5593692  0.57869416 0.63837209 0.67132867 0.78448276 0.86165049
 0.95280236]

  UserWarning,

2022-10-31 11:03:40,405:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.15236427 0.1887287  0.19582851 0.20798898 0.228125   0.23780488
 0.26       0.26890756 0.28710462 0.28719008 0.29545455 0.29814875
 0.3046595  0.34375    0.36576239 0.48333333 0.50761421 0.5148265
 0.52429039 0.53772291 0.54461822 0.58232044 0.62229905 0.63027656
 0.64864865 0.69444444 0.81818182 0.96174863]

  UserWarning,

2022-10-31 11:03:40,405:INFO:Calculating mean and std
2022-10-31 11:03:40,405:INFO:Creating metrics dataframe
2022-10-31 11:03:40,405:INFO:Uploading results into container
2022-10-31 11:03:40,405:INFO:Uploading model into container now
2022-10-31 11:03:40,405:INFO:master_model_container: 30
2022-10-31 11:03:40,405:INFO:display_container: 2
2022-10-31 11:03:40,405:INFO:AdaBoostRegressor(random_state=3360)
2022-10-31 11:03:40,405:INFO:create_model() successfully completed......................................
2022-10-31 11:03:40,527:ERROR:create_model() for AdaBoostRegressor(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:40,527:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:40,527:INFO:Initializing Gradient Boosting Regressor
2022-10-31 11:03:40,527:INFO:Total runtime is 2.1256136298179626 minutes
2022-10-31 11:03:40,527:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:40,527:INFO:Initializing create_model()
2022-10-31 11:03:40,527:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:40,527:INFO:Checking exceptions
2022-10-31 11:03:40,527:INFO:Importing libraries
2022-10-31 11:03:40,527:INFO:Copying training dataset
2022-10-31 11:03:40,542:INFO:Defining folds
2022-10-31 11:03:40,542:INFO:Declaring metric variables
2022-10-31 11:03:40,542:INFO:Importing untrained model
2022-10-31 11:03:40,542:INFO:Gradient Boosting Regressor Imported successfully
2022-10-31 11:03:40,542:INFO:Starting cross validation
2022-10-31 11:03:40,542:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:43,863:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.22398376e-01 -9.44646978e-02 -7.49599760e-02 -6.40750129e-02
 -5.80327643e-02 -5.24759869e-02 -4.87953326e-02 -4.22092143e-02
 -4.09273240e-02 -4.07671753e-02 -3.49021456e-02 -3.17590928e-02
 -2.83087044e-02 -2.50680843e-02 -1.10483594e-02 -9.75754108e-03
 -7.59431885e-03 -3.66527742e-03 -9.92386404e-04 -6.37070354e-04
 -1.93764065e-04  1.21188218e-03  2.45582349e-03  7.60051490e-03
  8.79009888e-03  1.22249759e-02  1.62360236e-02  2.09851008e-02
  2.50049736e-02  2.76985505e-02  2.77201435e-02  2.87171696e-02
  2.93957941e-02  3.50954767e-02  3.66618336e-02  3.93959232e-02
  4.00745511e-02  4.01425152e-02  4.22788871e-02  4.59142135e-02
  5.16506272e-02  5.39736564e-02  5.44826145e-02  5.56846531e-02
  5.63646948e-02  5.86920513e-02  6.10529758e-02  6.42428821e-02
  7.17073995e-02  7.22087898e-02  7.28331574e-02  7.34545165e-02
  7.81245465e-02  8.14821559e-02  8.16677801e-02  8.19702063e-02
  8.63382066e-02  8.67362610e-02  8.80431051e-02  8.85117788e-02
  8.86202367e-02  9.01539104e-02  9.04345362e-02  9.06312986e-02
  9.09804657e-02  9.12512702e-02  9.14053873e-02  9.17868017e-02
  9.20607907e-02  9.35068849e-02  9.45594025e-02  9.50176014e-02
  9.55321032e-02  9.57338835e-02  9.57779883e-02  9.60141756e-02
  9.90418622e-02  1.02574932e-01  1.02962391e-01  1.04403641e-01
  1.07375544e-01  1.08691779e-01  1.09451317e-01  1.10317720e-01
  1.12387433e-01  1.13262759e-01  1.14467735e-01  1.14533030e-01
  1.14718821e-01  1.14769218e-01  1.15434023e-01  1.16965708e-01
  1.17127799e-01  1.20106279e-01  1.20924019e-01  1.20930180e-01
  1.25913461e-01  1.28798993e-01  1.28870816e-01  1.29329519e-01
  1.30010086e-01  1.30239067e-01  1.31276394e-01  1.33330973e-01
  1.36418151e-01  1.36503765e-01  1.36553784e-01  1.38822791e-01
  1.40313815e-01  1.41652041e-01  1.44815427e-01  1.45298220e-01
  1.45343140e-01  1.49577661e-01  1.49746626e-01  1.50066113e-01
  1.50097281e-01  1.50706410e-01  1.55096695e-01  1.55175753e-01
  1.57805879e-01  1.58028900e-01  1.58519101e-01  1.59484468e-01
  1.61072570e-01  1.62077111e-01  1.62492726e-01  1.62759248e-01
  1.63562832e-01  1.63877791e-01  1.66028261e-01  1.66170238e-01
  1.66207511e-01  1.66828097e-01  1.67629596e-01  1.69342396e-01
  1.69894199e-01  1.70775353e-01  1.73538294e-01  1.73705272e-01
  1.74760605e-01  1.76644353e-01  1.78070915e-01  1.78590748e-01
  1.81164031e-01  1.82480716e-01  1.82845770e-01  1.82986125e-01
  1.83838896e-01  1.85822030e-01  1.86590928e-01  1.87904858e-01
  1.90194631e-01  1.90266667e-01  1.90637830e-01  1.93049386e-01
  1.96280190e-01  1.96291406e-01  1.96768341e-01  1.97246096e-01
  2.00008498e-01  2.00143739e-01  2.03634389e-01  2.05091484e-01
  2.08949673e-01  2.11893013e-01  2.14488581e-01  2.14900414e-01
  2.15485004e-01  2.19372849e-01  2.19778923e-01  2.20887888e-01
  2.24750193e-01  2.25794895e-01  2.27136571e-01  2.27503805e-01
  2.27928141e-01  2.28936097e-01  2.29143709e-01  2.30929374e-01
  2.31877781e-01  2.36272846e-01  2.47276947e-01  2.50239807e-01
  2.52472914e-01  2.52640061e-01  2.53737439e-01  2.53750221e-01
  2.54085900e-01  2.55906841e-01  2.62062124e-01  2.66853818e-01
  2.68146879e-01  2.68828536e-01  2.69502776e-01  2.70104174e-01
  2.70728540e-01  2.72613344e-01  2.72840942e-01  2.78178262e-01
  2.81230510e-01  2.82454485e-01  2.84036750e-01  2.85991305e-01
  2.89966391e-01  2.90318802e-01  2.92700229e-01  2.97717037e-01
  2.98488401e-01  2.99685725e-01  3.05580379e-01  3.08159300e-01
  3.14414255e-01  3.23768937e-01  3.32956334e-01  3.38644759e-01
  3.41809996e-01  3.47434508e-01  3.53863426e-01  3.55505568e-01
  3.60440169e-01  3.62550912e-01  3.69014662e-01  3.70139772e-01
  3.73261186e-01  3.75329969e-01  3.86834234e-01  3.90701443e-01
  3.94973340e-01  3.96182310e-01  4.04778356e-01  4.04951758e-01
  4.05589276e-01  4.11567980e-01  4.14263530e-01  4.14902157e-01
  4.15589166e-01  4.21796503e-01  4.24672825e-01  4.24807072e-01
  4.25210297e-01  4.27496078e-01  4.27977717e-01  4.50177130e-01
  4.51975053e-01  4.52377231e-01  4.57475393e-01  4.59340828e-01
  4.65770867e-01  4.72572351e-01  4.79513769e-01  4.81611092e-01
  4.82324760e-01  4.89600297e-01  4.96755198e-01  5.00244663e-01
  5.00337183e-01  5.04435713e-01  5.05408357e-01  5.09902690e-01
  5.11102728e-01  5.11577314e-01  5.14098748e-01  5.15914592e-01
  5.18026572e-01  5.18544193e-01  5.20888327e-01  5.21226684e-01
  5.22528193e-01  5.23297078e-01  5.23474301e-01  5.23551728e-01
  5.24707236e-01  5.25792190e-01  5.27677497e-01  5.33101250e-01
  5.33855387e-01  5.34146200e-01  5.45390016e-01  5.47004188e-01
  5.47817054e-01  5.48297249e-01  5.49077076e-01  5.55185196e-01
  5.56686896e-01  5.56775744e-01  5.57802760e-01  5.59417836e-01
  5.61328601e-01  5.62847336e-01  5.65186416e-01  5.67410071e-01
  5.68048172e-01  5.71916894e-01  5.72127173e-01  5.76045424e-01
  5.78464725e-01  5.80217917e-01  5.82998324e-01  5.83196081e-01
  5.87579170e-01  5.90003730e-01  5.93963030e-01  5.95371346e-01
  5.95437077e-01  5.95751073e-01  5.96796886e-01  5.96916094e-01
  5.97219767e-01  5.98627350e-01  5.99454496e-01  6.01200140e-01
  6.04122846e-01  6.05415907e-01  6.05547934e-01  6.05744150e-01
  6.07187264e-01  6.07347079e-01  6.08088022e-01  6.08568674e-01
  6.08988818e-01  6.09381084e-01  6.12653528e-01  6.12951763e-01
  6.14454867e-01  6.15360014e-01  6.15515284e-01  6.16808345e-01
  6.17506813e-01  6.18101257e-01  6.18741843e-01  6.24895036e-01
  6.25253439e-01  6.28779280e-01  6.29498119e-01  6.30034832e-01
  6.39171740e-01  6.41396180e-01  6.41795319e-01  6.45407327e-01
  6.47393977e-01  6.50138477e-01  6.52328177e-01  6.53894301e-01
  6.58868235e-01  6.60241613e-01  6.64247565e-01  6.69892158e-01
  6.73194829e-01  6.77424074e-01  6.78830113e-01  6.80592174e-01
  6.83442192e-01  6.84557351e-01  6.89122856e-01  6.90924195e-01
  6.91984612e-01  6.95430062e-01  6.96367701e-01  7.00574342e-01
  7.05248608e-01  7.06395956e-01  7.11117049e-01  7.27285108e-01
  7.31032492e-01  7.33744373e-01  7.34597782e-01  7.37762946e-01
  7.41354561e-01  7.44135966e-01  7.44863797e-01  7.53953816e-01
  7.58102728e-01  7.59554317e-01  7.62343953e-01  7.68323855e-01
  7.70910139e-01  7.71697511e-01  7.79302474e-01  7.80305739e-01
  7.81674420e-01  7.83447617e-01  7.88955997e-01  7.97123701e-01
  8.03190258e-01  8.03818271e-01  8.14641784e-01  8.15102710e-01
  8.19879112e-01  8.27319365e-01  8.29014093e-01  8.46260574e-01
  8.46655253e-01  8.50238258e-01  8.52415464e-01  8.52637060e-01
  8.52954335e-01  8.54228345e-01  8.54680951e-01  8.55347397e-01
  8.56346248e-01  8.58998678e-01  8.60519255e-01  8.67088185e-01
  8.67133991e-01  8.75865311e-01  8.77819612e-01  8.80472152e-01
  8.81066597e-01  8.86900415e-01  8.87233280e-01  8.90804585e-01
  8.91465921e-01  8.91551745e-01  8.91662069e-01  8.95918288e-01
  8.99624537e-01  9.02087613e-01  9.08460988e-01  9.11433529e-01
  9.11873970e-01  9.13602948e-01  9.19101818e-01  9.22551913e-01
  9.22901855e-01  9.24851414e-01  9.25361719e-01  9.25911831e-01
  9.50021877e-01  9.57635911e-01  9.59696162e-01  9.62340039e-01
  9.69502401e-01  9.71117165e-01  9.75858168e-01  9.76318451e-01
  9.76678924e-01  9.76937212e-01  9.77796382e-01  9.82152269e-01
  9.86056439e-01  9.86717775e-01  9.90441405e-01  9.93880137e-01
  1.00037627e+00  1.00073099e+00  1.00441982e+00  1.00503858e+00
  1.00925871e+00  1.01025363e+00  1.01378417e+00  1.01428847e+00
  1.01481914e+00  1.02441206e+00  1.02744891e+00  1.03142762e+00
  1.03261745e+00  1.05453772e+00  1.05748668e+00  1.08710079e+00
  1.14833436e+00  1.16814659e+00  1.18890152e+00]

  UserWarning,

2022-10-31 11:03:43,926:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.07089942 -0.05418152 -0.05081927 -0.04557608 -0.04214437 -0.04004129
 -0.03219272 -0.03032763 -0.0278789   0.00260455  0.0037187   0.00651512
  0.01630905  0.01720772  0.01800833  0.0202917   0.02857377  0.03078481
  0.0327164   0.0332743   0.03776086  0.038615    0.0392829   0.04424048
  0.04518405  0.05040131  0.05735257  0.0578007   0.06078397  0.06163829
  0.06354957  0.0652211   0.06564159  0.06759094  0.06853197  0.06986258
  0.07274376  0.07402388  0.07505921  0.07650571  0.0772483   0.07803895
  0.07812517  0.08096922  0.08272902  0.08534441  0.08563338  0.08923121
  0.09399117  0.09875982  0.09927168  0.10109952  0.10125484  0.10350972
  0.1035388   0.10450702  0.1088458   0.10944413  0.11049363  0.1134519
  0.11405335  0.11521779  0.11541259  0.11550025  0.1162431   0.1164703
  0.11808909  0.11827651  0.1190098   0.11955781  0.11966782  0.12009879
  0.120355    0.12100138  0.12206975  0.12243247  0.12273302  0.12396163
  0.12456763  0.12565443  0.12687065  0.12768353  0.12772058  0.12834911
  0.13220267  0.13236128  0.13241625  0.13302082  0.13428713  0.13457187
  0.13478303  0.13501052  0.13594503  0.13601498  0.13786787  0.13792151
  0.13826152  0.13830335  0.13849319  0.13956623  0.14152404  0.14189484
  0.14397125  0.14445357  0.14584054  0.14586461  0.1475035   0.14797066
  0.14823779  0.1482857   0.14981199  0.15496334  0.15569965  0.15696834
  0.15790455  0.15829199  0.15829676  0.16037631  0.16050777  0.16133925
  0.16258513  0.16293061  0.16379326  0.16575337  0.16657331  0.16686349
  0.16726231  0.17031643  0.17125107  0.17156053  0.17274234  0.17634352
  0.17652383  0.17819116  0.17827333  0.17854013  0.17953502  0.18237043
  0.18246158  0.18300444  0.18349001  0.18452723  0.18828945  0.19140646
  0.19201673  0.1927682   0.19451727  0.19507323  0.19691532  0.1980785
  0.19820232  0.20032358  0.20095653  0.20332735  0.20434969  0.20504978
  0.20585538  0.20636022  0.2088035   0.20945372  0.20999175  0.21103797
  0.2114141   0.21160781  0.21294408  0.21547533  0.21655517  0.21758798
  0.21831085  0.22074517  0.22559411  0.22981499  0.23427324  0.23923312
  0.2395701   0.24289553  0.24611227  0.24664461  0.24996276  0.25201029
  0.25488093  0.2558307   0.25587109  0.26058763  0.26249122  0.26578135
  0.27016443  0.27462613  0.27473178  0.27749543  0.27965943  0.28508838
  0.2852224   0.28707497  0.28870515  0.28896215  0.28972074  0.29322927
  0.29598553  0.29629873  0.30125197  0.30196188  0.30205923  0.30382313
  0.30511653  0.31023187  0.31475693  0.31911399  0.32400049  0.3257017
  0.32673417  0.32914353  0.32942483  0.33032464  0.33131515  0.33461352
  0.335384    0.33843338  0.3395765   0.340864    0.34196961  0.34290279
  0.34383081  0.34401172  0.34597022  0.34821093  0.35570989  0.36374288
  0.36562552  0.37129845  0.37543076  0.37598374  0.37729194  0.37812596
  0.38168167  0.3898338   0.39545767  0.39680704  0.39764452  0.40180644
  0.40293376  0.40441842  0.40633948  0.4118823   0.41743718  0.42021299
  0.42403583  0.42651511  0.42885142  0.44105051  0.45309428  0.45848234
  0.45981651  0.46750555  0.46997273  0.4721833   0.47244665  0.47514367
  0.47687779  0.47688114  0.48188708  0.48210589  0.48350614  0.48365005
  0.48449127  0.48485316  0.4871458   0.48921015  0.49045163  0.49134603
  0.49259493  0.49466595  0.49701748  0.4976402   0.49779213  0.50085572
  0.50119658  0.50137963  0.50490972  0.50816886  0.50877371  0.51182678
  0.51254938  0.51506043  0.51816804  0.52400531  0.52738757  0.52897232
  0.53044201  0.53093043  0.53639456  0.53642862  0.53821204  0.53900307
  0.53938688  0.54097025  0.54123399  0.54546362  0.5456789   0.54588307
  0.5494406   0.55267229  0.55543646  0.55740266  0.55780888  0.56104182
  0.56343473  0.56666765  0.56829107  0.56900231  0.56942134  0.57128998
  0.5725449   0.5736042   0.57545011  0.57564916  0.57930052  0.58029284
  0.5803181   0.58099304  0.58485827  0.58502348  0.58527773  0.58694952
  0.58760334  0.58888141  0.59032966  0.5907225   0.59084488  0.59116088
  0.59150555  0.59438548  0.59525016  0.59612697  0.59716866  0.59826994
  0.59883821  0.59906324  0.59950287  0.60037209  0.60071259  0.60169253
  0.60244334  0.60333036  0.60375219  0.60873425  0.60946058  0.6120489
  0.61600096  0.61612648  0.61730157  0.61780928  0.61841415  0.6210208
  0.6349926   0.63549541  0.64055849  0.64474294  0.64783379  0.64832693
  0.65322598  0.65886886  0.65940476  0.66718368  0.67018034  0.6716185
  0.67297564  0.6733701   0.67805867  0.67885879  0.6820834   0.68676115
  0.68807     0.6886781   0.69102827  0.69587461  0.69609039  0.69725609
  0.70012327  0.7001831   0.70246476  0.70285408  0.70311174  0.70871871
  0.71169844  0.72433096  0.7337482   0.73486095  0.73792798  0.73815873
  0.7394433   0.74040201  0.74461317  0.74811229  0.75135048  0.76643564
  0.77609591  0.77937862  0.78879977  0.78929802  0.7957849   0.80335952
  0.81683186  0.82879633  0.83045071  0.83222354  0.83977901  0.84252518
  0.84413033  0.85001011  0.85545638  0.85721117  0.86340408  0.86566356
  0.86775539  0.87047675  0.8751545   0.87646336  0.87942162  0.88081467
  0.88117186  0.88377294  0.88625562  0.8881856   0.89067225  0.89334134
  0.89353079  0.8940231   0.89461308  0.89769266  0.9024527   0.90505074
  0.91399562  0.91436372  0.91451631  0.91594573  0.91919406  0.92050292
  0.92138964  0.92346118  0.92384375  0.92444997  0.92832937  0.93096253
  0.94328047  0.94731457  0.95544373  0.95797534  0.96029319  0.96174482
  0.96464451  0.96497094  0.9662798   0.96642257  0.96923807  0.97068969
  0.97247859  0.97406561  0.97724909  0.98513431  0.98639696  0.98731274
  0.98935641  0.99620586  0.99651351  1.00001387  1.00088362  1.00219247
  1.00515074  1.00545839  1.00891687  1.01979941  1.03162761  1.04164285
  1.07569151  1.17716827]

  UserWarning,

2022-10-31 11:03:44,044:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.12890599 -0.09540904 -0.08242129 -0.07244729 -0.06221032 -0.04915087
 -0.0441037  -0.03880002 -0.03233119 -0.01902658 -0.01427952 -0.00465646
 -0.00181497  0.00685231  0.00692341  0.0178951   0.0183161   0.01843342
  0.01938122  0.02059146  0.02140068  0.0242388   0.02612092  0.02763935
  0.03244661  0.03684071  0.03768854  0.03867627  0.03899295  0.03911128
  0.03913436  0.04019868  0.04919207  0.04990634  0.050574    0.05188913
  0.0543993   0.05506676  0.0556278   0.05562884  0.05625382  0.05827216
  0.06050951  0.06142473  0.06250246  0.06464735  0.06660222  0.06852892
  0.06915976  0.06972622  0.07087208  0.07186686  0.07413023  0.07535317
  0.07613088  0.08020983  0.08147273  0.08298765  0.08332668  0.08439298
  0.0846858   0.08693119  0.0877907   0.08851581  0.09153702  0.09318676
  0.09327978  0.09491671  0.09548924  0.09591232  0.09669221  0.0974736
  0.09877934  0.10024071  0.10149129  0.10317498  0.10406154  0.10540513
  0.10548131  0.10656328  0.10781522  0.10807925  0.1088644   0.11006084
  0.11012153  0.11193253  0.11323517  0.11459441  0.11728061  0.11803235
  0.11859272  0.11885399  0.12022427  0.12108901  0.12118459  0.12174231
  0.1221532   0.12223584  0.12334598  0.12370258  0.12374139  0.12747053
  0.12759131  0.12791352  0.12879762  0.12901878  0.12963394  0.12973982
  0.13014228  0.13153366  0.13232499  0.13344242  0.13380779  0.13412114
  0.13574771  0.13859044  0.13897188  0.13980387  0.14099991  0.14157811
  0.14327599  0.14372708  0.14500551  0.14817279  0.14832792  0.14836791
  0.14879657  0.14931161  0.15106505  0.15138882  0.15503844  0.15658782
  0.15675408  0.15743921  0.16399117  0.16488946  0.16626713  0.16657524
  0.1669544   0.16722318  0.16870811  0.16924406  0.17210467  0.17215948
  0.17338009  0.17607804  0.17709777  0.17757452  0.17783512  0.17838789
  0.17978053  0.18066251  0.18098819  0.18165511  0.18177492  0.18288045
  0.18432096  0.18485691  0.18584815  0.18838319  0.18854742  0.1892387
  0.18952855  0.18953249  0.18984923  0.19386687  0.19500037  0.19738302
  0.1989648   0.20001182  0.20172868  0.20330638  0.20788634  0.20876179
  0.20886031  0.21123594  0.21215902  0.21265698  0.21445835  0.21523405
  0.21648427  0.21698806  0.21753917  0.22444217  0.22463144  0.2249616
  0.22624675  0.2312062   0.23222257  0.23352108  0.23935924  0.24092313
  0.24217308  0.25290391  0.25465164  0.25810786  0.26132007  0.26139256
  0.26444518  0.26543248  0.2686615   0.26880432  0.27292532  0.27473359
  0.27535192  0.27710683  0.28366163  0.28546083  0.28852046  0.29882689
  0.30094637  0.30471083  0.30770568  0.30838235  0.30989299  0.31358771
  0.31941905  0.32045982  0.32419544  0.32690206  0.3367136   0.33859205
  0.34073161  0.34564805  0.34936138  0.35208389  0.35472988  0.35518294
  0.35768724  0.35787643  0.36226319  0.36605237  0.38136774  0.38419689
  0.38503919  0.38569034  0.38583654  0.38750236  0.39739965  0.39998515
  0.40287587  0.40386737  0.40392392  0.40530374  0.40613628  0.41224556
  0.41381414  0.4185425   0.42067813  0.42634895  0.42928164  0.4303292
  0.43453818  0.43545355  0.43704121  0.44186832  0.44222205  0.44895069
  0.4503741   0.45047154  0.46457546  0.46901622  0.46946406  0.47228992
  0.47386965  0.48412565  0.48568619  0.4958002   0.49655194  0.49915817
  0.50166696  0.50873808  0.51561747  0.51635613  0.51796677  0.5229321
  0.52454133  0.52607644  0.52655358  0.5296739   0.5308699   0.5308925
  0.53183707  0.53232799  0.53406537  0.53577324  0.53649053  0.53760175
  0.5428026   0.54580541  0.5469621   0.54846091  0.54944931  0.55236625
  0.55393436  0.55848869  0.55891172  0.55992044  0.56051184  0.56755028
  0.56769005  0.56844179  0.56900463  0.57018018  0.57881272  0.58156397
  0.58238989  0.58384619  0.58439787  0.58604203  0.5893525   0.58942009
  0.59492035  0.5953352   0.59591076  0.59659049  0.59874599  0.6004678
  0.60104213  0.60201276  0.60247348  0.60670263  0.60808427  0.61144925
  0.61153044  0.61267415  0.6139451   0.61490813  0.61907289  0.61929234
  0.62065061  0.62140235  0.62400858  0.62691427  0.62716777  0.62894626
  0.63038165  0.63541916  0.6415056   0.64380859  0.64381668  0.6454208
  0.64639964  0.64878065  0.64992014  0.65222076  0.65334178  0.65361424
  0.65581056  0.6597268   0.66132507  0.66372168  0.66380287  0.66592506
  0.67292304  0.67367478  0.67494854  0.67635336  0.68195865  0.68265408
  0.68479077  0.68965463  0.6920006   0.69355364  0.70049389  0.70164852
  0.70193358  0.70301545  0.70671957  0.70970178  0.71058575  0.7111728
  0.71642532  0.71989133  0.72151361  0.72182652  0.72490872  0.72770062
  0.73208802  0.73843793  0.74561832  0.7457119   0.75392095  0.75651827
  0.76591868  0.76596171  0.77508924  0.78488338  0.79164743  0.80860755
  0.81023336  0.81213529  0.83663254  0.8574959   0.85774782  0.86026964
  0.86430886  0.86673425  0.86723767  0.87022274  0.87052462  0.87204102
  0.87315208  0.87487782  0.8763372   0.8783003   0.87957847  0.87972597
  0.88047771  0.8806904   0.88185752  0.88460816  0.88540724  0.88553856
  0.8862903   0.88758887  0.88889653  0.89094684  0.90095508  0.90341237
  0.90606002  0.90847321  0.91138283  0.91150053  0.91310434  0.92396823
  0.92904873  0.9531591   0.96607076  0.96740149  0.97227863  0.97729706
  0.97749845  0.9794472   0.9799033   0.9809006   0.98147999  0.98223173
  0.98339429  0.98483796  0.98564485  0.98669981  0.98774309  0.99033615
  0.99237416  0.99410147  0.99460485  0.99484621  0.99559795  0.99748887
  0.99820418  0.99919941  1.00081777  1.00249655  1.00255738  1.00744197
  1.0088223   1.0100482   1.01085509  1.0158882   1.02005644  1.02012776
  1.04751346  1.11126809]

  UserWarning,

2022-10-31 11:03:44,092:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09365094 -0.06046011 -0.05611808 -0.04269824 -0.00872823 -0.00365088
  0.00160231  0.01218283  0.01290112  0.01641981  0.01711318  0.0171167
  0.02569719  0.0273994   0.02890274  0.0346055   0.0360682   0.03803713
  0.03982229  0.04775145  0.04895878  0.05674523  0.05870213  0.05929919
  0.059455    0.06298757  0.06605153  0.06605367  0.0669699   0.06766586
  0.07089114  0.07116376  0.07445157  0.07749228  0.07810512  0.08030394
  0.08130922  0.08309994  0.08386022  0.08515283  0.08727558  0.08821556
  0.09260449  0.09393548  0.09437902  0.09819489  0.09854537  0.10000279
  0.10061054  0.10098565  0.10198259  0.10217165  0.10310193  0.10433919
  0.10442417  0.10443924  0.10508666  0.10573644  0.10592333  0.10658911
  0.1077024   0.11058226  0.11215652  0.11246937  0.11352692  0.11427495
  0.11585659  0.11620723  0.11713324  0.11909438  0.12022398  0.12062018
  0.12133393  0.12229038  0.12248915  0.12296688  0.12335075  0.12426689
  0.12659585  0.12945494  0.12960424  0.13098356  0.13203398  0.13279548
  0.13436403  0.13509575  0.13637396  0.13699832  0.13849635  0.13851464
  0.14220702  0.14239487  0.14322074  0.14334927  0.14376345  0.1446203
  0.1454817   0.14706059  0.14789942  0.15170183  0.15246382  0.15419086
  0.15538953  0.15546928  0.15613338  0.15653549  0.15662352  0.15837077
  0.15926853  0.15976217  0.16182452  0.16428628  0.16465258  0.1653839
  0.16559478  0.1656055   0.16683075  0.16701685  0.16848697  0.17090469
  0.17121754  0.17232618  0.17271264  0.17526195  0.17861973  0.17904969
  0.17955077  0.1821185   0.18573544  0.18910785  0.18988012  0.19001017
  0.19030893  0.19442069  0.19474508  0.1950009   0.19555253  0.19696864
  0.19818187  0.19867074  0.20027591  0.20324571  0.20376863  0.20426484
  0.20465031  0.20519066  0.20571751  0.20899684  0.20982275  0.21078487
  0.21148726  0.21207032  0.2124168   0.21242199  0.21529881  0.21591384
  0.22042659  0.222607    0.22410802  0.22506436  0.22604935  0.23020719
  0.23383327  0.2353301   0.2363087   0.23714462  0.23805521  0.23931903
  0.24135543  0.24450554  0.2455469   0.24622896  0.24724538  0.24797917
  0.24878105  0.24897492  0.25038076  0.25165104  0.25744278  0.26123958
  0.26241182  0.26635053  0.2688514   0.2698005   0.27433722  0.27994864
  0.28030807  0.28642844  0.29014906  0.2934894   0.30211472  0.30272042
  0.30782595  0.31053278  0.31357679  0.31395799  0.32384741  0.32752176
  0.3308173   0.33566829  0.33597959  0.34439523  0.34503447  0.34837037
  0.34922092  0.35038834  0.35055306  0.35671824  0.35936861  0.36441523
  0.36630251  0.36736482  0.368158    0.37193629  0.37561331  0.38507599
  0.38510092  0.38599767  0.39336506  0.39786737  0.39818901  0.39922198
  0.3999577   0.40159033  0.40921597  0.40947637  0.41413378  0.41455108
  0.41520379  0.43372347  0.43614119  0.43651444  0.43784539  0.43790999
  0.44131662  0.44898795  0.4602737   0.4677136   0.47293613  0.47376551
  0.47553284  0.47777497  0.47785063  0.48062918  0.48809574  0.49351672
  0.49609706  0.4972508   0.49804602  0.49849611  0.49931839  0.50133585
  0.50467244  0.50490827  0.51101736  0.5134326   0.51971768  0.5200125
  0.52156271  0.52186516  0.52455357  0.52877082  0.53446952  0.53835122
  0.53845182  0.5389728   0.54152057  0.54183308  0.54261138  0.5428232
  0.54466017  0.55763238  0.55773533  0.55936828  0.56087791  0.56383328
  0.56452354  0.56553445  0.56611298  0.56761405  0.57203544  0.57240386
  0.57316189  0.57347501  0.57909776  0.57929987  0.58019889  0.58030532
  0.58105243  0.5817472   0.58350937  0.58372012  0.58532162  0.58763842
  0.58925397  0.58933908  0.59175679  0.59207329  0.59326913  0.59413315
  0.59716902  0.59773413  0.59845388  0.59950718  0.59961801  0.59965567
  0.60039405  0.60292174  0.60297061  0.60389228  0.60460356  0.60543578
  0.60673989  0.60686005  0.60891118  0.61234205  0.61299861  0.61486498
  0.6152148   0.61584466  0.61775444  0.61818623  0.61883006  0.62104157
  0.62597358  0.63090893  0.63222094  0.63476789  0.63483502  0.63909378
  0.64297098  0.64755267  0.64848016  0.64886741  0.64997039  0.65042033
  0.6518545   0.65201433  0.65553039  0.66016568  0.66083032  0.66355543
  0.66565489  0.66672051  0.6699705   0.67238822  0.67269561  0.67360492
  0.67375127  0.67873937  0.68102548  0.681664    0.68295703  0.68347988
  0.68523499  0.68636683  0.6879601   0.68909194  0.68999746  0.69164915
  0.69176532  0.69343221  0.69386346  0.69535568  0.69952214  0.7014695
  0.7043826   0.70756812  0.7085467   0.71013217  0.71233716  0.71527778
  0.72368286  0.7307032   0.73105176  0.73377687  0.73819686  0.74240474
  0.74403829  0.7458384   0.74732425  0.75180103  0.75255267  0.75346523
  0.7556976   0.75743842  0.75967383  0.76621148  0.76972895  0.76985253
  0.77379795  0.77786215  0.77897612  0.78243782  0.78365817  0.78732515
  0.7899843   0.80001644  0.8114601   0.81939029  0.82130545  0.82541654
  0.83873377  0.84262616  0.84628722  0.84786682  0.85276959  0.85327956
  0.85569728  0.85706034  0.86008398  0.86039391  0.8669111   0.86854405
  0.8699212   0.87312276  0.87635208  0.88718041  0.88795791  0.88839923
  0.88959813  0.89096118  0.89236803  0.90027244  0.90081195  0.90308067
  0.90365852  0.90376633  0.90377411  0.90769008  0.90795129  0.91149316
  0.91327237  0.91671221  0.91708203  0.9261732   0.9292859   0.9303576
  0.93601864  0.94959498  0.95898806  0.96042331  0.96206511  0.96284103
  0.96420408  0.96927397  0.97405484  0.97528389  0.97568779  0.97973254
  0.98144842  0.98210195  0.98395279  0.98438567  0.985844    0.98593488
  0.98865999  0.99107771  0.99244076  0.99273241  0.99288703  0.99512244
  0.99705781  1.00229153  1.00392448  1.00564951  1.00583182  1.00747914
  1.01218947  1.01408068  1.01593693  1.01962341  1.02175621  1.02206552
  1.04110505  1.06051034  1.11150267]

  UserWarning,

2022-10-31 11:03:44,110:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.30339421e-01 -1.20798169e-01 -1.14746424e-01 -8.73233875e-02
 -6.17283515e-02 -5.95272141e-02 -5.13991633e-02 -4.83437944e-02
 -4.05056471e-02 -2.48049728e-02 -1.72117615e-02 -1.20957085e-02
 -1.76305976e-04  1.11507613e-02  1.28661540e-02  1.40129858e-02
  3.89784863e-02  3.92032197e-02  5.80870189e-02  5.95788400e-02
  6.20617632e-02  6.51407975e-02  6.63825321e-02  6.64219522e-02
  6.65321761e-02  6.68900538e-02  7.17234170e-02  7.54900336e-02
  7.73666995e-02  7.77311313e-02  8.13814362e-02  8.38835572e-02
  8.43222690e-02  8.58856247e-02  8.84553352e-02  8.85156938e-02
  9.02085331e-02  9.10985773e-02  9.11583302e-02  9.33905279e-02
  9.49230317e-02  9.55249355e-02  9.71166213e-02  9.92423502e-02
  9.99148776e-02  1.00066195e-01  1.00776181e-01  1.01862624e-01
  1.03552949e-01  1.03718351e-01  1.04260250e-01  1.05698560e-01
  1.07101497e-01  1.07115549e-01  1.07633341e-01  1.07886163e-01
  1.08064048e-01  1.09394795e-01  1.10394172e-01  1.10652733e-01
  1.12167813e-01  1.12238760e-01  1.13075786e-01  1.14305508e-01
  1.15500039e-01  1.15733885e-01  1.17192878e-01  1.17645650e-01
  1.17977471e-01  1.18375332e-01  1.18411373e-01  1.19859198e-01
  1.21322607e-01  1.21734138e-01  1.22405334e-01  1.23477012e-01
  1.23644860e-01  1.24697499e-01  1.24834718e-01  1.24885206e-01
  1.25793308e-01  1.26227947e-01  1.27421153e-01  1.27853556e-01
  1.27872499e-01  1.29651988e-01  1.30549278e-01  1.32086361e-01
  1.32440923e-01  1.33043497e-01  1.33172949e-01  1.34369127e-01
  1.34713732e-01  1.35883550e-01  1.36093268e-01  1.36606104e-01
  1.37615974e-01  1.37786653e-01  1.39874511e-01  1.40534366e-01
  1.41994105e-01  1.44593234e-01  1.44813120e-01  1.45081037e-01
  1.45418310e-01  1.45638589e-01  1.47380112e-01  1.47649082e-01
  1.47991676e-01  1.48109408e-01  1.48120280e-01  1.49934384e-01
  1.50012197e-01  1.50558128e-01  1.50783375e-01  1.50828797e-01
  1.51247291e-01  1.51544122e-01  1.51977205e-01  1.52928985e-01
  1.53370174e-01  1.54800146e-01  1.56501995e-01  1.56875366e-01
  1.58474582e-01  1.59062649e-01  1.60430548e-01  1.60716828e-01
  1.61501205e-01  1.61738706e-01  1.62174667e-01  1.62260413e-01
  1.63199129e-01  1.63780485e-01  1.63801695e-01  1.64607964e-01
  1.65035248e-01  1.67918343e-01  1.68553032e-01  1.68733291e-01
  1.68766443e-01  1.70809364e-01  1.71142197e-01  1.71524184e-01
  1.73405308e-01  1.75049050e-01  1.75115495e-01  1.75267782e-01
  1.75697186e-01  1.76116235e-01  1.76656953e-01  1.77834072e-01
  1.79165994e-01  1.79592256e-01  1.80099037e-01  1.82225380e-01
  1.83593311e-01  1.85608242e-01  1.88222948e-01  1.88752853e-01
  1.88937786e-01  1.89921970e-01  1.94449103e-01  1.94561251e-01
  1.96852304e-01  1.98077636e-01  2.06333749e-01  2.08253806e-01
  2.08699558e-01  2.10154515e-01  2.13019194e-01  2.13713756e-01
  2.14292473e-01  2.14493479e-01  2.17123825e-01  2.17499149e-01
  2.20753177e-01  2.22439093e-01  2.22462119e-01  2.23068010e-01
  2.24391784e-01  2.38293855e-01  2.43919171e-01  2.50205322e-01
  2.51307513e-01  2.51953130e-01  2.51995769e-01  2.55571348e-01
  2.55791740e-01  2.61219837e-01  2.63081307e-01  2.63437230e-01
  2.66161359e-01  2.66782751e-01  2.69974290e-01  2.75523468e-01
  2.75559512e-01  2.76440870e-01  2.81108938e-01  2.82667544e-01
  2.85909470e-01  2.93951963e-01  2.94498243e-01  3.02289454e-01
  3.03432139e-01  3.12705567e-01  3.13204261e-01  3.18465751e-01
  3.23119986e-01  3.28179296e-01  3.30553636e-01  3.30610352e-01
  3.33079386e-01  3.34813897e-01  3.37436594e-01  3.38716503e-01
  3.47390098e-01  3.48218530e-01  3.48965764e-01  3.57774597e-01
  3.57870969e-01  3.59749114e-01  3.59755772e-01  3.59853504e-01
  3.63369110e-01  3.63768032e-01  3.63973631e-01  3.64403136e-01
  3.68792707e-01  3.75095103e-01  3.81962794e-01  3.85378402e-01
  3.85454005e-01  3.87395718e-01  3.91153725e-01  3.91187057e-01
  3.97392047e-01  4.00175649e-01  4.00624607e-01  4.01871023e-01
  4.11644693e-01  4.13400763e-01  4.14893705e-01  4.15634759e-01
  4.21852620e-01  4.22275786e-01  4.22518785e-01  4.23927957e-01
  4.27853624e-01  4.29312904e-01  4.35641290e-01  4.44368851e-01
  4.45195932e-01  4.51927425e-01  4.52191343e-01  4.61206846e-01
  4.70585117e-01  4.70818239e-01  4.73888461e-01  4.75392171e-01
  4.88170523e-01  4.89567755e-01  4.94006225e-01  4.94512658e-01
  4.98821804e-01  5.03899738e-01  5.04815878e-01  5.06438483e-01
  5.06884376e-01  5.13547351e-01  5.14293992e-01  5.19803447e-01
  5.20359457e-01  5.20487596e-01  5.25116834e-01  5.26866953e-01
  5.28161726e-01  5.28696955e-01  5.32091198e-01  5.33535343e-01
  5.35622064e-01  5.38117365e-01  5.38438049e-01  5.42030226e-01
  5.42117689e-01  5.42733255e-01  5.47450774e-01  5.47491813e-01
  5.48034428e-01  5.48486146e-01  5.50613277e-01  5.53133496e-01
  5.53251665e-01  5.53934554e-01  5.60957101e-01  5.63971293e-01
  5.64956925e-01  5.67937077e-01  5.68188599e-01  5.71861359e-01
  5.74561359e-01  5.75678965e-01  5.78804738e-01  5.81029118e-01
  5.85065661e-01  5.88930534e-01  5.90302627e-01  5.93448622e-01
  5.93988074e-01  5.94107193e-01  5.99658232e-01  6.00287194e-01
  6.02615064e-01  6.03841378e-01  6.05219239e-01  6.05986989e-01
  6.06611883e-01  6.11856046e-01  6.12076275e-01  6.12104410e-01
  6.13257121e-01  6.14461418e-01  6.15646294e-01  6.15687793e-01
  6.15890124e-01  6.18030393e-01  6.18250622e-01  6.20484207e-01
  6.22954313e-01  6.25856841e-01  6.26871626e-01  6.27406968e-01
  6.27809511e-01  6.28902403e-01  6.31084026e-01  6.32021382e-01
  6.33823315e-01  6.38037513e-01  6.39245002e-01  6.43298775e-01
  6.47472896e-01  6.49264587e-01  6.58685032e-01  6.61065827e-01
  6.63154970e-01  6.63570757e-01  6.64057185e-01  6.65038602e-01
  6.68100775e-01  6.69863189e-01  6.73252917e-01  6.79841545e-01
  6.81220603e-01  6.81987156e-01  6.83355533e-01  6.84592528e-01
  6.88076442e-01  6.88204173e-01  6.89257288e-01  6.89590904e-01
  6.90350720e-01  6.90681814e-01  6.91862660e-01  6.94250789e-01
  6.96856161e-01  6.99881646e-01  7.04791933e-01  7.05753384e-01
  7.07015657e-01  7.08890959e-01  7.12204424e-01  7.12987740e-01
  7.14083328e-01  7.16688700e-01  7.24952956e-01  7.25983964e-01
  7.28707736e-01  7.44880160e-01  7.46305610e-01  7.48870509e-01
  7.50888527e-01  7.57603433e-01  7.62804674e-01  7.65283883e-01
  7.67788625e-01  7.79098391e-01  7.81606836e-01  7.94258224e-01
  7.94785310e-01  7.98777747e-01  8.01459315e-01  8.22127629e-01
  8.23155586e-01  8.24711196e-01  8.36426857e-01  8.42137477e-01
  8.47551915e-01  8.52962169e-01  8.54607238e-01  8.64212510e-01
  8.66358120e-01  8.72447406e-01  8.72964732e-01  8.76017425e-01
  8.77852951e-01  8.78621753e-01  8.79030180e-01  8.86483867e-01
  8.86503159e-01  8.86748905e-01  8.87958235e-01  8.89302911e-01
  8.91077434e-01  9.01196609e-01  9.02861702e-01  9.03558745e-01
  9.04956400e-01  9.06190110e-01  9.06540763e-01  9.07833259e-01
  9.09807231e-01  9.10438630e-01  9.12268071e-01  9.13303689e-01
  9.22242503e-01  9.24079072e-01  9.24908636e-01  9.26867539e-01
  9.27677791e-01  9.31666590e-01  9.36734640e-01  9.60954630e-01
  9.61618331e-01  9.63100241e-01  9.64946936e-01  9.69083430e-01
  9.69189527e-01  9.70370373e-01  9.72468336e-01  9.73310205e-01
  9.75363874e-01  9.75787204e-01  9.76968050e-01  9.81416772e-01
  9.81961551e-01  9.83562383e-01  9.87683061e-01  9.90004044e-01
  9.93138397e-01  9.95284007e-01  9.95743768e-01  9.95826016e-01
  9.97889379e-01  1.00369737e+00  1.00397867e+00  1.00515951e+00
  1.00818531e+00  1.01015301e+00  1.01144145e+00  1.01620591e+00
  1.01668018e+00  1.02360047e+00  1.02444081e+00  1.04572004e+00
  1.04773615e+00  1.05014687e+00  1.06887225e+00  1.09204294e+00]

  UserWarning,

2022-10-31 11:03:44,157:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.16509254e-01 -1.05470173e-01 -1.01693538e-01 -9.23713324e-02
 -5.27819590e-02 -2.89387937e-02 -2.29602122e-02 -1.98821073e-02
 -6.55438174e-03 -2.69525345e-03 -2.45992444e-03  3.19076306e-04
  1.82469895e-03  1.84220206e-03  5.01343226e-03  9.27952836e-03
  1.74409014e-02  2.18190429e-02  2.53644135e-02  2.71327376e-02
  2.92539440e-02  3.26376438e-02  3.58335537e-02  3.58839984e-02
  3.69646812e-02  4.01867651e-02  4.09406101e-02  4.53913287e-02
  4.87227286e-02  5.69434087e-02  6.19741449e-02  6.21525433e-02
  6.40209382e-02  6.67663968e-02  6.71287175e-02  6.89456460e-02
  7.06068777e-02  7.10702079e-02  7.14286776e-02  7.54250692e-02
  7.58505900e-02  7.72539971e-02  8.24835633e-02  8.27900257e-02
  8.37968319e-02  8.38783678e-02  8.83455178e-02  8.93695076e-02
  9.01295376e-02  9.04995006e-02  9.08419649e-02  9.28630427e-02
  9.33960046e-02  9.50934685e-02  9.57365342e-02  9.65592731e-02
  9.67986978e-02  9.68438108e-02  9.70149891e-02  9.70910444e-02
  9.71320301e-02  9.71499725e-02  9.83704915e-02  1.00463288e-01
  1.01065653e-01  1.03683800e-01  1.04280405e-01  1.04688035e-01
  1.05508755e-01  1.06788632e-01  1.07283170e-01  1.08395967e-01
  1.09403148e-01  1.09975905e-01  1.11168685e-01  1.12703558e-01
  1.12750132e-01  1.13566484e-01  1.14789022e-01  1.14823536e-01
  1.16159549e-01  1.16219191e-01  1.18145863e-01  1.19469776e-01
  1.20036369e-01  1.20332540e-01  1.21132198e-01  1.22885334e-01
  1.24816208e-01  1.25560608e-01  1.26191592e-01  1.26473367e-01
  1.26724839e-01  1.26868384e-01  1.28596633e-01  1.29154068e-01
  1.30728346e-01  1.32132222e-01  1.32407789e-01  1.32458712e-01
  1.32534462e-01  1.32596857e-01  1.32768105e-01  1.33629211e-01
  1.33952510e-01  1.35979279e-01  1.37914883e-01  1.38021343e-01
  1.38637989e-01  1.40015785e-01  1.40767241e-01  1.41977365e-01
  1.42211303e-01  1.43609163e-01  1.43640888e-01  1.44885394e-01
  1.45355130e-01  1.46743012e-01  1.46979912e-01  1.47090554e-01
  1.47427500e-01  1.48686355e-01  1.49455423e-01  1.50276463e-01
  1.50639840e-01  1.52591879e-01  1.52616895e-01  1.52766970e-01
  1.53111212e-01  1.53535048e-01  1.53825945e-01  1.54093096e-01
  1.54355315e-01  1.54545437e-01  1.55128773e-01  1.56157577e-01
  1.56171696e-01  1.56434086e-01  1.56997618e-01  1.59626508e-01
  1.59789079e-01  1.61517120e-01  1.63082090e-01  1.63149444e-01
  1.64808956e-01  1.64827135e-01  1.66640750e-01  1.68642082e-01
  1.68672890e-01  1.71082758e-01  1.72795345e-01  1.73692425e-01
  1.75488103e-01  1.77344031e-01  1.77634928e-01  1.82689750e-01
  1.85975199e-01  1.86106322e-01  1.86272866e-01  1.87045858e-01
  1.88012365e-01  1.89208175e-01  1.89582579e-01  1.89710371e-01
  1.90026587e-01  1.92048757e-01  1.92195665e-01  1.95121923e-01
  1.98874664e-01  1.99598140e-01  1.99748422e-01  2.00119183e-01
  2.01561374e-01  2.01852271e-01  2.02664905e-01  2.08879265e-01
  2.09499942e-01  2.12007428e-01  2.12377715e-01  2.13303187e-01
  2.14710521e-01  2.15483067e-01  2.15658133e-01  2.16550013e-01
  2.17868733e-01  2.18554050e-01  2.18651762e-01  2.18682353e-01
  2.18792259e-01  2.22054718e-01  2.23219465e-01  2.29265422e-01
  2.29294141e-01  2.30723743e-01  2.31162636e-01  2.32798834e-01
  2.36423777e-01  2.36730978e-01  2.38108039e-01  2.42885786e-01
  2.43009602e-01  2.51292501e-01  2.52093020e-01  2.56708590e-01
  2.59912727e-01  2.61926438e-01  2.62467923e-01  2.63643785e-01
  2.63945847e-01  2.69268986e-01  2.76059480e-01  2.79266936e-01
  2.86351859e-01  2.86675378e-01  2.87694233e-01  2.89116621e-01
  2.90034889e-01  2.95846879e-01  2.97076399e-01  2.98339986e-01
  3.13337448e-01  3.20129685e-01  3.26438380e-01  3.27498981e-01
  3.34611194e-01  3.35031145e-01  3.35793908e-01  3.36910579e-01
  3.40744073e-01  3.41563279e-01  3.56834219e-01  3.61798150e-01
  3.62511034e-01  3.63364133e-01  3.65367245e-01  3.67975633e-01
  3.72623667e-01  3.77159548e-01  3.85890897e-01  3.93580600e-01
  3.97797834e-01  3.98006983e-01  3.99178725e-01  4.02328522e-01
  4.08812025e-01  4.09614528e-01  4.11159415e-01  4.14113993e-01
  4.28310553e-01  4.35936759e-01  4.36194170e-01  4.43071529e-01
  4.43840262e-01  4.47837965e-01  4.54777181e-01  4.58125380e-01
  4.60025110e-01  4.61190570e-01  4.61215315e-01  4.66341984e-01
  4.66700531e-01  4.68370306e-01  4.74375617e-01  4.85130499e-01
  4.89200097e-01  4.93874304e-01  4.95528541e-01  4.95673069e-01
  4.98007025e-01  4.99435445e-01  5.00222362e-01  5.01606289e-01
  5.02567708e-01  5.03630682e-01  5.03983907e-01  5.04933899e-01
  5.08299483e-01  5.13143298e-01  5.17332156e-01  5.18665069e-01
  5.19999305e-01  5.20352157e-01  5.28755872e-01  5.29174860e-01
  5.30661776e-01  5.32820724e-01  5.36042412e-01  5.39481516e-01
  5.40954258e-01  5.42917671e-01  5.43071767e-01  5.44512084e-01
  5.44621837e-01  5.45196382e-01  5.49696454e-01  5.51543526e-01
  5.55531724e-01  5.56171291e-01  5.57635250e-01  5.57788227e-01
  5.61736991e-01  5.67300844e-01  5.70084870e-01  5.73625130e-01
  5.75048801e-01  5.76450727e-01  5.77304727e-01  5.79597487e-01
  5.79638452e-01  5.81124324e-01  5.81170553e-01  5.82728951e-01
  5.84445117e-01  5.86042393e-01  5.87727995e-01  5.88251161e-01
  5.90246973e-01  5.91691126e-01  5.96205392e-01  5.96752063e-01
  5.97122927e-01  6.00719728e-01  6.02074675e-01  6.02512948e-01
  6.04159692e-01  6.06116305e-01  6.08587176e-01  6.08729145e-01
  6.09072254e-01  6.11292339e-01  6.14036185e-01  6.18125461e-01
  6.18359032e-01  6.18584871e-01  6.23322963e-01  6.23432502e-01
  6.31653999e-01  6.32719280e-01  6.37243450e-01  6.40984087e-01
  6.41609961e-01  6.41886104e-01  6.42207381e-01  6.50981782e-01
  6.52061717e-01  6.57274503e-01  6.59385887e-01  6.61084694e-01
  6.61657451e-01  6.63721303e-01  6.65497798e-01  6.71277308e-01
  6.71854453e-01  6.72415957e-01  6.75892314e-01  6.76189870e-01
  6.81153801e-01  6.81302343e-01  6.81666044e-01  6.82229102e-01
  6.83076053e-01  6.85702486e-01  6.87240278e-01  6.87986017e-01
  6.89711150e-01  6.90550117e-01  6.92941038e-01  6.95160159e-01
  6.99708845e-01  7.00677038e-01  7.02454659e-01  7.04556476e-01
  7.08735515e-01  7.15095893e-01  7.15703572e-01  7.17503406e-01
  7.19179332e-01  7.21065275e-01  7.42905476e-01  7.53788013e-01
  7.55012266e-01  7.55426715e-01  7.56757732e-01  7.59354311e-01
  7.74866052e-01  7.78459045e-01  7.79549831e-01  7.79557122e-01
  7.89285911e-01  7.96838236e-01  7.97545059e-01  8.04986220e-01
  8.06503193e-01  8.10498642e-01  8.14175304e-01  8.19527245e-01
  8.19754229e-01  8.21445369e-01  8.24601859e-01  8.26484898e-01
  8.29042642e-01  8.29975554e-01  8.35896219e-01  8.37380699e-01
  8.43548530e-01  8.46248290e-01  8.47907736e-01  8.52871667e-01
  8.55514145e-01  8.56847596e-01  8.57083117e-01  8.57420353e-01
  8.59361363e-01  8.59829067e-01  8.60229917e-01  8.63407522e-01
  8.65526504e-01  8.70490434e-01  8.70760862e-01  8.71667454e-01
  8.74466363e-01  8.75039120e-01  8.76631385e-01  8.79886751e-01
  8.81036645e-01  8.84717100e-01  8.87486858e-01  8.90730282e-01
  8.91591857e-01  8.95804433e-01  8.95964437e-01  9.02335867e-01
  9.09001044e-01  9.13279711e-01  9.14646794e-01  9.15614987e-01
  9.19610725e-01  9.19636496e-01  9.24159411e-01  9.26385461e-01
  9.29007042e-01  9.43477547e-01  9.48796068e-01  9.53627593e-01
  9.53946269e-01  9.54157162e-01  9.60087219e-01  9.60578352e-01
  9.62439587e-01  9.67403518e-01  9.68713518e-01  9.70090969e-01
  9.71952204e-01  9.76799835e-01  9.77520397e-01  9.82940785e-01
  9.85719511e-01  9.88650500e-01  9.91232551e-01  9.92270608e-01
  9.95010465e-01  9.95022210e-01  9.96183018e-01  9.99725870e-01
  1.00153614e+00  1.00468980e+00  1.00559280e+00  1.00591413e+00
  1.00923849e+00  1.01146454e+00  1.01310106e+00  1.01408612e+00
  1.01984431e+00  1.02506315e+00  1.02582478e+00  1.02756156e+00
  1.02864943e+00  1.03020342e+00  1.03248479e+00  1.04265339e+00
  1.04494336e+00  1.09798246e+00  1.12939787e+00]

  UserWarning,

2022-10-31 11:03:44,157:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.78261298e-02 -8.28232593e-02 -6.03722382e-02 -4.26876535e-02
 -3.95344817e-02 -3.94495370e-02 -3.41271445e-02 -3.26028574e-02
 -3.17385092e-02 -3.06673183e-02 -1.88399943e-02 -1.47889240e-02
 -9.93013178e-03 -9.45676026e-03 -8.17146080e-04 -4.73264514e-04
  4.55918085e-04  1.48524046e-03  2.15710560e-03  3.13663860e-03
  6.43970817e-03  1.26526658e-02  2.16315178e-02  2.29861207e-02
  2.63631225e-02  2.93558580e-02  3.08282169e-02  3.50858148e-02
  3.56534434e-02  3.85458920e-02  4.05866146e-02  4.12126991e-02
  4.59698072e-02  4.97393717e-02  5.36504841e-02  5.63973533e-02
  5.74478547e-02  5.76874249e-02  5.79100478e-02  5.82879144e-02
  5.86370656e-02  5.87433807e-02  5.94437640e-02  6.04104094e-02
  6.10160931e-02  6.11433847e-02  6.17016263e-02  6.79755275e-02
  6.93602354e-02  6.97460990e-02  7.49086126e-02  7.55783349e-02
  7.59709662e-02  7.69174865e-02  7.95292761e-02  7.97412070e-02
  7.98142756e-02  8.02628222e-02  8.17541714e-02  8.28001387e-02
  8.56049516e-02  8.57681636e-02  8.61468690e-02  8.71237548e-02
  8.78702333e-02  8.79572060e-02  8.83300587e-02  8.93029463e-02
  9.10470228e-02  9.13583887e-02  9.20804112e-02  9.40689997e-02
  9.42902148e-02  9.45973470e-02  9.55139297e-02  9.58049455e-02
  9.83815552e-02  9.87694262e-02  1.02219096e-01  1.03625596e-01
  1.04207883e-01  1.04259567e-01  1.04809528e-01  1.08457766e-01
  1.08478845e-01  1.08523177e-01  1.10085202e-01  1.12198228e-01
  1.13427233e-01  1.15424698e-01  1.15451874e-01  1.15913101e-01
  1.19209439e-01  1.20618393e-01  1.21872373e-01  1.24214889e-01
  1.24616313e-01  1.27578424e-01  1.28240085e-01  1.29649951e-01
  1.29869849e-01  1.30487116e-01  1.31344357e-01  1.33377250e-01
  1.34069910e-01  1.34319507e-01  1.34765145e-01  1.37211291e-01
  1.38193774e-01  1.38207545e-01  1.41302897e-01  1.41452844e-01
  1.42385039e-01  1.43304927e-01  1.43771262e-01  1.48127825e-01
  1.49229726e-01  1.49715301e-01  1.50082999e-01  1.50326022e-01
  1.50356176e-01  1.52187262e-01  1.52655697e-01  1.53567615e-01
  1.54278031e-01  1.54322885e-01  1.54337487e-01  1.55367725e-01
  1.55549710e-01  1.55822677e-01  1.57466500e-01  1.58752095e-01
  1.59158677e-01  1.59713164e-01  1.60461492e-01  1.63391847e-01
  1.63648188e-01  1.65613850e-01  1.65668050e-01  1.67633552e-01
  1.69225782e-01  1.69757911e-01  1.69831504e-01  1.70282933e-01
  1.70697601e-01  1.72440163e-01  1.73880627e-01  1.74118998e-01
  1.74793534e-01  1.75027917e-01  1.75232835e-01  1.81199367e-01
  1.83344390e-01  1.87277488e-01  1.89535318e-01  1.89881209e-01
  1.90474290e-01  1.91225782e-01  1.93698772e-01  1.94343695e-01
  1.94780804e-01  2.00507159e-01  2.00717102e-01  2.01108237e-01
  2.03175364e-01  2.04182210e-01  2.04958429e-01  2.06951096e-01
  2.08783489e-01  2.09217477e-01  2.12155701e-01  2.12450760e-01
  2.13027107e-01  2.15647278e-01  2.15693618e-01  2.16173381e-01
  2.16299340e-01  2.16423380e-01  2.17119440e-01  2.17872988e-01
  2.26608337e-01  2.26724832e-01  2.29035368e-01  2.29952034e-01
  2.30457073e-01  2.31873304e-01  2.33107921e-01  2.33337815e-01
  2.33951529e-01  2.36423530e-01  2.36813192e-01  2.40246581e-01
  2.41218988e-01  2.41579953e-01  2.44709553e-01  2.45518894e-01
  2.46193585e-01  2.46550125e-01  2.47406730e-01  2.48417391e-01
  2.48913555e-01  2.49308192e-01  2.50353844e-01  2.51519458e-01
  2.52748705e-01  2.53696112e-01  2.60475500e-01  2.65591707e-01
  2.66581546e-01  2.67042897e-01  2.72267952e-01  2.80211640e-01
  2.87172041e-01  2.90077535e-01  2.93276577e-01  2.93326472e-01
  2.98198564e-01  3.01516096e-01  3.18660624e-01  3.22516997e-01
  3.22594092e-01  3.25391300e-01  3.31207570e-01  3.43093260e-01
  3.46868591e-01  3.49041711e-01  3.56739975e-01  3.59804004e-01
  3.62448648e-01  3.74925827e-01  3.77837553e-01  3.78658300e-01
  3.79653057e-01  3.79790246e-01  3.83816511e-01  3.87994018e-01
  3.89453349e-01  3.91676568e-01  3.93615259e-01  3.97910656e-01
  4.02086516e-01  4.03969646e-01  4.13205994e-01  4.13751917e-01
  4.14465787e-01  4.19338105e-01  4.33407910e-01  4.35416204e-01
  4.37390778e-01  4.39648091e-01  4.46579478e-01  4.48550486e-01
  4.49072366e-01  4.49377573e-01  4.49507936e-01  4.52362788e-01
  4.54497401e-01  4.58366019e-01  4.60361172e-01  4.61451352e-01
  4.64004715e-01  4.64317137e-01  4.69074518e-01  4.70376898e-01
  4.75220007e-01  4.76782704e-01  4.78332317e-01  4.83789863e-01
  4.85383822e-01  4.86626108e-01  4.88944099e-01  5.01463600e-01
  5.02199101e-01  5.02357303e-01  5.07069256e-01  5.08015371e-01
  5.11357750e-01  5.16612755e-01  5.26561013e-01  5.26562697e-01
  5.26672610e-01  5.35213166e-01  5.40181649e-01  5.41696779e-01
  5.43072067e-01  5.47029665e-01  5.49388657e-01  5.50879963e-01
  5.51207285e-01  5.53127203e-01  5.54376515e-01  5.54654851e-01
  5.55759136e-01  5.57604287e-01  5.61095600e-01  5.64479869e-01
  5.66401940e-01  5.72467441e-01  5.73840838e-01  5.78067112e-01
  5.78500857e-01  5.78812494e-01  5.78929674e-01  5.79659190e-01
  5.82526143e-01  5.82814429e-01  5.84728420e-01  5.87824100e-01
  5.88363748e-01  5.88977779e-01  5.89601493e-01  5.92291256e-01
  5.92837519e-01  5.93213176e-01  5.93497500e-01  5.94037996e-01
  5.96551168e-01  5.97219490e-01  5.98855171e-01  5.99029445e-01
  5.99350073e-01  5.99967046e-01  5.99986610e-01  6.00950096e-01
  6.01340113e-01  6.04406362e-01  6.05047397e-01  6.05449093e-01
  6.06005838e-01  6.06317485e-01  6.10745283e-01  6.11084525e-01
  6.11970538e-01  6.13156611e-01  6.13276825e-01  6.14458932e-01
  6.16213619e-01  6.18133983e-01  6.18972881e-01  6.19210807e-01
  6.19596646e-01  6.20673470e-01  6.21841539e-01  6.25991403e-01
  6.27222547e-01  6.31966378e-01  6.32503117e-01  6.34131693e-01
  6.34173259e-01  6.34392011e-01  6.36577013e-01  6.37983079e-01
  6.40878999e-01  6.41080818e-01  6.42741143e-01  6.42804263e-01
  6.42889880e-01  6.44755869e-01  6.47739308e-01  6.53070223e-01
  6.56742989e-01  6.59101933e-01  6.62246272e-01  6.65271415e-01
  6.66467890e-01  6.67263580e-01  6.67682743e-01  6.68985064e-01
  6.72660115e-01  6.74122778e-01  6.74786348e-01  6.76249010e-01
  6.76501891e-01  6.80175892e-01  6.81748678e-01  6.83355054e-01
  6.86368117e-01  6.95405131e-01  6.95502889e-01  6.97918083e-01
  7.04642891e-01  7.06908466e-01  7.07596355e-01  7.08729590e-01
  7.10623434e-01  7.10821287e-01  7.12238486e-01  7.15515638e-01
  7.15774763e-01  7.26718326e-01  7.29410523e-01  7.30525042e-01
  7.33131290e-01  7.33921601e-01  7.48610092e-01  7.56887420e-01
  7.57664406e-01  7.62878358e-01  7.68291238e-01  7.69909763e-01
  7.74339714e-01  7.78148381e-01  7.89660585e-01  7.91340506e-01
  8.06568417e-01  8.07619969e-01  8.08841301e-01  8.28581560e-01
  8.38608093e-01  8.43985395e-01  8.50095744e-01  8.52763631e-01
  8.54201693e-01  8.55120889e-01  8.58308948e-01  8.61274084e-01
  8.62269539e-01  8.64987733e-01  8.67667298e-01  8.68662784e-01
  8.70125446e-01  8.70705821e-01  8.73277576e-01  8.84656805e-01
  8.89190644e-01  8.91283245e-01  8.96297809e-01  8.98836174e-01
  9.03441910e-01  9.03923277e-01  9.06297158e-01  9.06527205e-01
  9.07155559e-01  9.07598328e-01  9.10830610e-01  9.10908241e-01
  9.11037311e-01  9.12319346e-01  9.12740173e-01  9.15445402e-01
  9.18266516e-01  9.19158136e-01  9.22724739e-01  9.38151348e-01
  9.47810361e-01  9.54363967e-01  9.55199061e-01  9.59626859e-01
  9.61359010e-01  9.63340508e-01  9.67015559e-01  9.68478222e-01
  9.71630351e-01  9.73890440e-01  9.76018915e-01  9.76229084e-01
  9.76240683e-01  9.81279140e-01  1.00179469e+00  1.00550833e+00
  1.00595110e+00  1.00665408e+00  1.00894875e+00  1.00918339e+00
  1.01056590e+00  1.01064605e+00  1.01103318e+00  1.01289612e+00
  1.01379818e+00  1.02514630e+00  1.09448959e+00  1.13459359e+00]

  UserWarning,

2022-10-31 11:03:44,244:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.23161073e-01 -1.09500160e-01 -1.08007622e-01 -5.29695922e-02
 -2.29989441e-02 -2.19854193e-02 -2.05871017e-02  5.72114716e-04
  3.71842308e-03  5.34946738e-03  6.33380198e-03  6.50683849e-03
  6.98175707e-03  8.63818549e-03  2.81026169e-02  2.82325084e-02
  2.88496960e-02  2.98284586e-02  3.30759644e-02  3.60167811e-02
  3.87803438e-02  3.90583662e-02  4.00111805e-02  4.10048342e-02
  4.47361050e-02  4.71530053e-02  5.41448101e-02  5.48628829e-02
  5.70012760e-02  5.79415439e-02  5.80756856e-02  5.97148211e-02
  6.16238790e-02  6.26018549e-02  6.42497365e-02  6.48630725e-02
  6.63757931e-02  6.84260872e-02  6.99800525e-02  7.24057228e-02
  7.70530021e-02  7.86061907e-02  8.12698334e-02  8.27730541e-02
  8.28297246e-02  8.36522216e-02  8.48417302e-02  8.54068647e-02
  8.62537590e-02  9.10365826e-02  9.34010802e-02  9.37789973e-02
  9.38295181e-02  9.66902112e-02  9.71533844e-02  9.89889568e-02
  1.00699059e-01  1.01260974e-01  1.01308113e-01  1.02659078e-01
  1.03050046e-01  1.03212960e-01  1.03214726e-01  1.03373664e-01
  1.04359901e-01  1.08503050e-01  1.10033666e-01  1.11077195e-01
  1.11334046e-01  1.11375658e-01  1.13583273e-01  1.14006867e-01
  1.14304310e-01  1.15227666e-01  1.15256672e-01  1.16254968e-01
  1.16533955e-01  1.16645574e-01  1.16832426e-01  1.18097951e-01
  1.18617608e-01  1.18796625e-01  1.20674259e-01  1.21656873e-01
  1.22663061e-01  1.23296169e-01  1.23362027e-01  1.23760570e-01
  1.23839497e-01  1.24573334e-01  1.24741720e-01  1.30408262e-01
  1.30532213e-01  1.30863872e-01  1.31285605e-01  1.34440438e-01
  1.34697214e-01  1.35056693e-01  1.35740254e-01  1.36730857e-01
  1.38386519e-01  1.38918127e-01  1.39071912e-01  1.39352046e-01
  1.40227967e-01  1.42976927e-01  1.43036844e-01  1.43354845e-01
  1.44316376e-01  1.45103745e-01  1.46226081e-01  1.46234440e-01
  1.46895404e-01  1.46914215e-01  1.48686837e-01  1.49241581e-01
  1.49288552e-01  1.52994853e-01  1.53883112e-01  1.55309190e-01
  1.57650817e-01  1.58885637e-01  1.59936705e-01  1.60781599e-01
  1.61416096e-01  1.62025143e-01  1.62189427e-01  1.62398338e-01
  1.63059135e-01  1.63839402e-01  1.66044967e-01  1.66396902e-01
  1.67356320e-01  1.70736806e-01  1.72122488e-01  1.75021044e-01
  1.77813472e-01  1.78594073e-01  1.78677147e-01  1.82504389e-01
  1.85857219e-01  1.86411481e-01  1.86651205e-01  1.88207153e-01
  1.90425692e-01  1.91056039e-01  1.91668727e-01  1.94265602e-01
  1.94630865e-01  1.94909409e-01  1.95688675e-01  1.96280481e-01
  1.96575282e-01  1.96715022e-01  1.99537380e-01  2.01432033e-01
  2.01923833e-01  2.02560961e-01  2.02983838e-01  2.04131599e-01
  2.07435829e-01  2.07486295e-01  2.09115524e-01  2.09682352e-01
  2.09863814e-01  2.10786434e-01  2.11422863e-01  2.13637108e-01
  2.16820059e-01  2.19387045e-01  2.20418939e-01  2.25199478e-01
  2.28311339e-01  2.29019240e-01  2.29134356e-01  2.32273332e-01
  2.32627185e-01  2.34676670e-01  2.35027046e-01  2.36446936e-01
  2.37761930e-01  2.39384943e-01  2.39471057e-01  2.41567484e-01
  2.42266623e-01  2.42708896e-01  2.42982255e-01  2.43254039e-01
  2.45069396e-01  2.45314158e-01  2.51661298e-01  2.53240622e-01
  2.53584692e-01  2.54739527e-01  2.58078738e-01  2.59684374e-01
  2.61473277e-01  2.63132673e-01  2.65005990e-01  2.66273034e-01
  2.71014719e-01  2.75200698e-01  2.75214812e-01  2.78720302e-01
  2.79127449e-01  2.89749176e-01  2.90416270e-01  2.97809076e-01
  2.98160757e-01  2.98497328e-01  3.04180163e-01  3.10832197e-01
  3.14929224e-01  3.18276920e-01  3.18840885e-01  3.31575327e-01
  3.33113120e-01  3.37397116e-01  3.38926643e-01  3.41194063e-01
  3.43454498e-01  3.49130407e-01  3.51823659e-01  3.68453883e-01
  3.68771941e-01  3.76746742e-01  3.81997161e-01  3.82218998e-01
  3.83789875e-01  3.90605441e-01  3.93654780e-01  3.94561136e-01
  3.94723115e-01  3.94932018e-01  3.95893346e-01  3.97243352e-01
  4.00439174e-01  4.02227278e-01  4.05434972e-01  4.06707471e-01
  4.10087881e-01  4.12898838e-01  4.16115451e-01  4.16783443e-01
  4.19264646e-01  4.20480983e-01  4.22260827e-01  4.22292222e-01
  4.23303204e-01  4.26474088e-01  4.26860800e-01  4.27535160e-01
  4.31540417e-01  4.36772878e-01  4.37299051e-01  4.45955541e-01
  4.48200194e-01  4.51589934e-01  4.57834388e-01  4.58695137e-01
  4.59758818e-01  4.62346970e-01  4.62499695e-01  4.71088740e-01
  4.73893144e-01  4.78014857e-01  4.81928574e-01  4.82626456e-01
  4.86113683e-01  4.86343141e-01  4.94175599e-01  4.96304898e-01
  5.03356368e-01  5.04216300e-01  5.05403877e-01  5.06308383e-01
  5.07459410e-01  5.15277348e-01  5.15619277e-01  5.15992171e-01
  5.16194173e-01  5.16593668e-01  5.22570366e-01  5.26230390e-01
  5.29995430e-01  5.32404281e-01  5.33735821e-01  5.37569030e-01
  5.37841289e-01  5.38283761e-01  5.42233474e-01  5.44528013e-01
  5.48236524e-01  5.49438210e-01  5.55118258e-01  5.57831190e-01
  5.60686608e-01  5.62020202e-01  5.67892064e-01  5.68193818e-01
  5.71418739e-01  5.71428754e-01  5.72910419e-01  5.76085804e-01
  5.76892048e-01  5.78472159e-01  5.79219733e-01  5.80705625e-01
  5.83016862e-01  5.88262151e-01  5.88804790e-01  5.94274207e-01
  5.97638053e-01  5.97986006e-01  5.99417429e-01  6.00333797e-01
  6.02316364e-01  6.04437257e-01  6.04688195e-01  6.05206307e-01
  6.05358828e-01  6.07300289e-01  6.07602569e-01  6.07980487e-01
  6.08489194e-01  6.11796776e-01  6.11834491e-01  6.12589653e-01
  6.12964412e-01  6.13342882e-01  6.17917071e-01  6.18053692e-01
  6.18157783e-01  6.23676167e-01  6.24423561e-01  6.26132232e-01
  6.26236322e-01  6.28104266e-01  6.28208356e-01  6.31220248e-01
  6.34604340e-01  6.34989924e-01  6.36775186e-01  6.36860864e-01
  6.38825555e-01  6.42003815e-01  6.43809480e-01  6.45729554e-01
  6.50610743e-01  6.52618466e-01  6.53880073e-01  6.59564909e-01
  6.62331870e-01  6.64268442e-01  6.81626114e-01  6.84098400e-01
  6.89768713e-01  6.90280902e-01  6.91971870e-01  6.92780744e-01
  6.98962171e-01  7.00676772e-01  7.04156434e-01  7.04747775e-01
  7.05125692e-01  7.08819790e-01  7.09442639e-01  7.09820556e-01
  7.10819555e-01  7.13924795e-01  7.15302989e-01  7.16709487e-01
  7.22002438e-01  7.23381528e-01  7.25353562e-01  7.27678714e-01
  7.30909434e-01  7.32999447e-01  7.36750833e-01  7.41877386e-01
  7.44754483e-01  7.50068464e-01  7.52544957e-01  7.53669981e-01
  7.54839039e-01  7.59673847e-01  7.61413648e-01  7.66087334e-01
  7.71775839e-01  7.72249570e-01  7.74069380e-01  7.78372269e-01
  7.79309640e-01  7.79982963e-01  7.81076404e-01  7.82228802e-01
  7.82383263e-01  7.88999328e-01  7.92170922e-01  7.94606649e-01
  7.95754427e-01  7.97139488e-01  7.97517405e-01  8.00082271e-01
  8.02348218e-01  8.04255570e-01  8.05992860e-01  8.10262181e-01
  8.16706859e-01  8.28013935e-01  8.29742805e-01  8.40209599e-01
  8.41803770e-01  8.44021759e-01  8.44655233e-01  8.45033150e-01
  8.46985808e-01  8.47084730e-01  8.50017076e-01  8.50719058e-01
  8.53487373e-01  8.54434943e-01  8.55210447e-01  8.58672020e-01
  8.61542490e-01  8.63288986e-01  8.65261020e-01  8.71719787e-01
  8.79959497e-01  8.81770360e-01  8.88424210e-01  8.89682903e-01
  8.90595744e-01  8.94158026e-01  8.94458641e-01  8.99465122e-01
  8.99906047e-01  9.00530503e-01  9.00805444e-01  9.01024916e-01
  9.01183361e-01  9.04212151e-01  9.05661434e-01  9.11272208e-01
  9.16327654e-01  9.16716367e-01  9.17778458e-01  9.21411231e-01
  9.24295187e-01  9.25140204e-01  9.25370196e-01  9.26298473e-01
  9.43438356e-01  9.48469015e-01  9.51449377e-01  9.51827294e-01
  9.62004590e-01  9.70083130e-01  9.72055164e-01  9.74491864e-01
  9.77039089e-01  9.79540747e-01  9.80075137e-01  9.82158752e-01
  9.82403495e-01  9.84765689e-01  9.88627568e-01  9.89674749e-01
  9.92944515e-01  9.93322432e-01  9.94327916e-01  9.96880435e-01
  9.96987889e-01  9.98166000e-01  1.00084110e+00  1.00187681e+00
  1.00349973e+00  1.00688340e+00  1.00726899e+00  1.00782975e+00
  1.00939782e+00  1.01044141e+00  1.01355030e+00  1.01758841e+00
  1.01970562e+00  1.02334867e+00  1.03351146e+00  1.04514672e+00
  1.06349189e+00  1.10239155e+00  1.14391170e+00]

  UserWarning,

2022-10-31 11:03:45,618:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08232029 -0.0617261  -0.03624115 -0.02260416 -0.01469573 -0.01072538
 -0.00827165  0.00198281  0.00871283  0.00891355  0.01625603  0.02836561
  0.02961991  0.03284073  0.03440232  0.0348366   0.03495971  0.03521831
  0.03556561  0.03669133  0.03917579  0.04173726  0.04178869  0.04293348
  0.04345737  0.04477995  0.045761    0.04684527  0.0526391   0.0566254
  0.05738809  0.05789505  0.06057052  0.0608769   0.06287494  0.06571212
  0.07061535  0.07300958  0.07413814  0.07473105  0.07596924  0.07700236
  0.07832604  0.079417    0.08063281  0.0837792   0.08634895  0.08731249
  0.08899522  0.0890382   0.08905331  0.08960593  0.09002897  0.09055553
  0.09230154  0.09246277  0.09254097  0.09267753  0.09517194  0.09539057
  0.09736174  0.09781892  0.0985341   0.10053657  0.10174038  0.10312557
  0.10349507  0.10441607  0.10707811  0.10799362  0.1085713   0.10882361
  0.10885585  0.10920768  0.10995774  0.11018018  0.11143571  0.11313467
  0.11337435  0.11552725  0.11757515  0.11795891  0.11810704  0.11822026
  0.11957326  0.12019479  0.12065171  0.12131116  0.12319144  0.12331908
  0.12365156  0.1243989   0.12610289  0.12670793  0.12806091  0.12927016
  0.12986296  0.13034119  0.13052609  0.13111451  0.13265126  0.1339545
  0.13407229  0.13537605  0.13638616  0.1371762   0.13931626  0.13936595
  0.14140737  0.14214331  0.14252671  0.14281992  0.14501125  0.14573997
  0.14591809  0.14623905  0.14786495  0.14853072  0.1493185   0.15059174
  0.15581684  0.15661341  0.15948024  0.16007854  0.16029943  0.16254861
  0.16260918  0.16439381  0.16453174  0.16541014  0.16658302  0.16677802
  0.16692034  0.16807662  0.16909089  0.17099157  0.17151319  0.17152255
  0.17173088  0.17207187  0.17269343  0.17271831  0.17699559  0.17885843
  0.17918944  0.18044886  0.18086702  0.1813751   0.18166591  0.18235789
  0.1829447   0.18589299  0.18591331  0.18592081  0.18741425  0.18953832
  0.18963178  0.19024793  0.19152523  0.19300683  0.19306865  0.19411725
  0.20129057  0.20143768  0.20178397  0.20353888  0.20489124  0.20646803
  0.20996592  0.21077987  0.21194891  0.21201512  0.21289643  0.21717969
  0.21840151  0.21844486  0.22078768  0.22352244  0.22451428  0.22464699
  0.2250016   0.22674797  0.23831917  0.23884349  0.23948776  0.24066931
  0.24370493  0.25695973  0.25777851  0.25869441  0.25887502  0.25896706
  0.2594523   0.26841666  0.27757656  0.28162566  0.2825846   0.28615581
  0.29199372  0.29862123  0.29986967  0.30072985  0.30108528  0.3013457
  0.3080017   0.3086056   0.31139481  0.31533452  0.31570443  0.3221361
  0.32222764  0.32814804  0.32899539  0.33723813  0.34072045  0.34392114
  0.34965892  0.3498082   0.35876835  0.36067559  0.36595125  0.3674282
  0.36964396  0.37148818  0.37603212  0.37878201  0.38140383  0.39647057
  0.4021943   0.40491699  0.41644692  0.41652058  0.42208802  0.42946567
  0.43096077  0.43210906  0.43225722  0.4400727   0.44430075  0.447883
  0.45607401  0.46255377  0.46371776  0.46380524  0.4656067   0.46758989
  0.47593069  0.48169914  0.48568239  0.48979329  0.4922056   0.49232481
  0.49364111  0.50001951  0.50014405  0.50424054  0.50447749  0.50517027
  0.51039966  0.5135388   0.52145953  0.52276878  0.52702415  0.52901777
  0.53190079  0.53535855  0.53788159  0.54068643  0.54539427  0.54804701
  0.5484952   0.548776    0.55124294  0.552363    0.55273603  0.5536037
  0.55378342  0.55480572  0.55605664  0.55737487  0.55821166  0.56333595
  0.56508101  0.56765512  0.56883134  0.57547326  0.57576006  0.57633129
  0.57832378  0.57924578  0.58111843  0.58307185  0.58443598  0.58559744
  0.58679527  0.58846225  0.59109143  0.59667829  0.59686908  0.59859081
  0.60448699  0.60565595  0.60836357  0.60858715  0.61259227  0.61329978
  0.6145941   0.61477046  0.61731011  0.61762794  0.61868565  0.61998232
  0.62123652  0.6234766   0.6236002   0.62407386  0.62651848  0.62998373
  0.6302242   0.63258743  0.63305543  0.63322465  0.63599661  0.63890409
  0.63931137  0.64856495  0.65291243  0.66468279  0.66502049  0.66665845
  0.66711607  0.6706181   0.67148045  0.6726654   0.67675695  0.6770248
  0.67930782  0.68407559  0.68567213  0.68577197  0.68857305  0.68928056
  0.69050805  0.69232676  0.69569213  0.69864465  0.69991035  0.70056158
  0.70217225  0.70396539  0.70464793  0.7097525   0.71260714  0.71440371
  0.71511001  0.71836466  0.7200543   0.7226177   0.72421332  0.72449881
  0.7314227   0.73706574  0.73909305  0.74317264  0.74676336  0.74882165
  0.75065927  0.75348963  0.75739382  0.76136088  0.76186695  0.76403624
  0.76451257  0.76587276  0.76984618  0.77864641  0.78018534  0.78100136
  0.7887846   0.79499246  0.79522291  0.80645701  0.80848275  0.81779299
  0.81956487  0.8202712   0.82457007  0.83115455  0.8349717   0.8413887
  0.84243482  0.8479119   0.85202915  0.8528965   0.85639715  0.86048869
  0.86292035  0.86303957  0.8632681   0.86407207  0.8662726   0.87001412
  0.87093616  0.87123268  0.87478021  0.88004063  0.88563596  0.88895046
  0.88915747  0.89518961  0.89577721  0.89642342  0.89704317  0.89815837
  0.89978796  0.90113471  0.90356637  0.90464666  0.90691862  0.91099153
  0.91104119  0.91261213  0.91452211  0.91684019  0.92219703  0.92302972
  0.93484327  0.95103248  0.95512402  0.95755568  0.95767489  0.95930796
  0.95991498  0.96090793  0.962972    0.9638098   0.96481286  0.9687442
  0.97175324  0.97459825  0.98836807  0.99245962  0.99286553  0.99489128
  0.99501049  0.99621406  0.99824352  0.99831798  1.00249477  1.00509656
  1.00795515  1.01274676  1.02839256  1.06977257  1.07292979  1.12872908
  1.15678537  1.15881111  1.17279305  1.17400671]

  UserWarning,

2022-10-31 11:03:45,668:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.17492351e-01 -1.15918733e-01 -9.59553907e-02 -6.58625843e-02
 -5.91635477e-02 -5.11462224e-02 -4.65989489e-02 -4.60254713e-02
 -4.15025812e-02 -3.21465102e-02 -8.89186973e-03 -8.05497089e-03
  5.83806007e-04  1.42526370e-02  2.11093772e-02  2.12385183e-02
  2.56235447e-02  4.44847149e-02  4.58118406e-02  4.67513486e-02
  4.87028441e-02  4.89675019e-02  5.05028385e-02  5.14918730e-02
  5.45297852e-02  5.48630125e-02  5.56156231e-02  5.57920830e-02
  5.66896669e-02  5.76764772e-02  5.78913174e-02  6.07718085e-02
  6.40746281e-02  6.43435686e-02  6.47371371e-02  7.44197486e-02
  7.45009468e-02  7.51843793e-02  7.64847245e-02  7.73409847e-02
  7.77403066e-02  7.77769379e-02  7.96707065e-02  8.30539092e-02
  8.32226123e-02  8.55842313e-02  8.71250413e-02  8.83332244e-02
  8.88062510e-02  8.92949166e-02  8.98207824e-02  9.00685458e-02
  9.09905236e-02  9.20475579e-02  9.25712297e-02  9.41464291e-02
  9.54651082e-02  9.65976919e-02  9.75637197e-02  9.81072617e-02
  9.93209577e-02  1.00264397e-01  1.03664171e-01  1.03932620e-01
  1.04001348e-01  1.05510380e-01  1.05517670e-01  1.09579571e-01
  1.10219089e-01  1.10403037e-01  1.11290532e-01  1.11547787e-01
  1.12241028e-01  1.12518402e-01  1.12889945e-01  1.13975665e-01
  1.14034388e-01  1.15683754e-01  1.16915826e-01  1.17187418e-01
  1.17792421e-01  1.18031718e-01  1.19629507e-01  1.20684715e-01
  1.21843018e-01  1.22491277e-01  1.22795460e-01  1.22852403e-01
  1.23235895e-01  1.24784062e-01  1.25421491e-01  1.26196712e-01
  1.28069472e-01  1.28310932e-01  1.30640268e-01  1.32161270e-01
  1.33777951e-01  1.35216810e-01  1.35733142e-01  1.36929044e-01
  1.39319891e-01  1.40835877e-01  1.40853855e-01  1.41014594e-01
  1.44425728e-01  1.44841354e-01  1.45131543e-01  1.45257193e-01
  1.45842598e-01  1.46048697e-01  1.46077275e-01  1.47069911e-01
  1.47214368e-01  1.50978846e-01  1.55623681e-01  1.58355434e-01
  1.59293173e-01  1.59404343e-01  1.59804052e-01  1.61144376e-01
  1.61343938e-01  1.62900582e-01  1.64827034e-01  1.65090684e-01
  1.65586169e-01  1.65707661e-01  1.66288723e-01  1.70241956e-01
  1.74408833e-01  1.76467820e-01  1.76843450e-01  1.77693746e-01
  1.77895621e-01  1.78406949e-01  1.78550750e-01  1.79003821e-01
  1.80953468e-01  1.82138102e-01  1.82924865e-01  1.83162196e-01
  1.84618980e-01  1.85285006e-01  1.86891815e-01  1.89427401e-01
  1.89596020e-01  1.92060663e-01  1.94920323e-01  1.98647249e-01
  2.01251669e-01  2.02502749e-01  2.03558854e-01  2.04068662e-01
  2.05382713e-01  2.05441403e-01  2.05848855e-01  2.06366182e-01
  2.06795117e-01  2.16176931e-01  2.17301265e-01  2.19292146e-01
  2.19960692e-01  2.22307086e-01  2.25573134e-01  2.28605004e-01
  2.30427134e-01  2.32620321e-01  2.35771109e-01  2.36316943e-01
  2.41867124e-01  2.42590818e-01  2.43964891e-01  2.44217321e-01
  2.46552840e-01  2.48284171e-01  2.49401125e-01  2.53853059e-01
  2.54505545e-01  2.55635901e-01  2.59462513e-01  2.59657061e-01
  2.59814226e-01  2.64550927e-01  2.66532670e-01  2.69178116e-01
  2.71135437e-01  2.73752679e-01  2.75209256e-01  2.78682654e-01
  2.80747408e-01  2.85343736e-01  2.87706766e-01  2.88343470e-01
  2.95007521e-01  2.97909500e-01  2.99002844e-01  3.10572796e-01
  3.11202634e-01  3.15032353e-01  3.28853377e-01  3.36852544e-01
  3.40895455e-01  3.44939453e-01  3.45706209e-01  3.47095888e-01
  3.50074227e-01  3.50140674e-01  3.51905482e-01  3.52129725e-01
  3.53890834e-01  3.60073512e-01  3.62998403e-01  3.64329534e-01
  3.66040487e-01  3.68213141e-01  3.74924702e-01  3.75879368e-01
  3.78033726e-01  3.79578734e-01  3.80735401e-01  3.83663165e-01
  3.84240193e-01  3.85079355e-01  3.86369223e-01  3.88921009e-01
  3.90855427e-01  3.93728096e-01  3.98341014e-01  4.00358154e-01
  4.07894082e-01  4.09936027e-01  4.11036413e-01  4.18858835e-01
  4.20537328e-01  4.24500093e-01  4.31757499e-01  4.34321337e-01
  4.35139911e-01  4.35719722e-01  4.37141619e-01  4.37827910e-01
  4.40022568e-01  4.44183628e-01  4.49059665e-01  4.49314945e-01
  4.59233215e-01  4.59529480e-01  4.62483294e-01  4.68010204e-01
  4.69206652e-01  4.70975697e-01  4.74496169e-01  4.74703565e-01
  4.80373221e-01  4.88568745e-01  4.88681149e-01  4.93013635e-01
  4.95049463e-01  5.06382392e-01  5.10624368e-01  5.11354947e-01
  5.11402629e-01  5.18078306e-01  5.19073680e-01  5.20459996e-01
  5.20909228e-01  5.21564976e-01  5.21650179e-01  5.23090147e-01
  5.23652562e-01  5.26670955e-01  5.27069301e-01  5.29210214e-01
  5.30450931e-01  5.31786084e-01  5.32340108e-01  5.34162658e-01
  5.34261016e-01  5.34853821e-01  5.34926141e-01  5.35298807e-01
  5.36592429e-01  5.38843680e-01  5.39821871e-01  5.42332874e-01
  5.42515697e-01  5.43712910e-01  5.45523303e-01  5.46923728e-01
  5.47327380e-01  5.51134019e-01  5.51564718e-01  5.55605380e-01
  5.57240823e-01  5.57619270e-01  5.58600855e-01  5.60545973e-01
  5.60773143e-01  5.61135540e-01  5.65238009e-01  5.68407574e-01
  5.69161431e-01  5.70977592e-01  5.73251109e-01  5.75191180e-01
  5.75858153e-01  5.76602672e-01  5.76843984e-01  5.77513070e-01
  5.79896486e-01  5.82010435e-01  5.83797853e-01  5.84236429e-01
  5.84359464e-01  5.84410430e-01  5.84514888e-01  5.87834503e-01
  5.89430825e-01  5.89549580e-01  5.90502250e-01  5.90862235e-01
  5.91225193e-01  5.91776192e-01  5.94121344e-01  5.96214981e-01
  6.01836500e-01  6.02042385e-01  6.03538973e-01  6.04480624e-01
  6.09783864e-01  6.10971497e-01  6.14390245e-01  6.14475874e-01
  6.17752889e-01  6.18301342e-01  6.18399729e-01  6.18593528e-01
  6.19293005e-01  6.20420362e-01  6.21113604e-01  6.23198786e-01
  6.24685476e-01  6.26308445e-01  6.26707856e-01  6.28502083e-01
  6.29342162e-01  6.30136852e-01  6.33117723e-01  6.37376472e-01
  6.41570513e-01  6.44998481e-01  6.49042813e-01  6.52056226e-01
  6.54316579e-01  6.57394772e-01  6.57795060e-01  6.60056162e-01
  6.60819037e-01  6.64124716e-01  6.67883705e-01  6.68246045e-01
  6.70667346e-01  6.71015304e-01  6.74051558e-01  6.77045421e-01
  6.77738663e-01  6.79117215e-01  6.81310535e-01  6.82933504e-01
  6.86316304e-01  6.87094129e-01  6.91494635e-01  6.93039662e-01
  6.93117604e-01  6.95867148e-01  6.96067849e-01  7.00477425e-01
  7.04825212e-01  7.07405979e-01  7.09360985e-01  7.09744035e-01
  7.16409738e-01  7.19773583e-01  7.22885047e-01  7.29175040e-01
  7.34657926e-01  7.35259776e-01  7.39635226e-01  7.68601629e-01
  7.72682481e-01  7.88002195e-01  7.94404143e-01  7.98512432e-01
  7.98690849e-01  8.10675074e-01  8.11344839e-01  8.18392814e-01
  8.23028671e-01  8.37269477e-01  8.37719754e-01  8.41846250e-01
  8.47715031e-01  8.49047347e-01  8.50097908e-01  8.50809643e-01
  8.57129789e-01  8.60403787e-01  8.61719834e-01  8.65827579e-01
  8.67985466e-01  8.70750930e-01  8.72550937e-01  8.74086370e-01
  8.76122810e-01  8.77745778e-01  8.77831698e-01  8.79939416e-01
  8.82871586e-01  8.85200984e-01  8.87716545e-01  8.92384951e-01
  8.94521804e-01  8.95866256e-01  9.03102132e-01  9.03962086e-01
  9.04072046e-01  9.05531387e-01  9.11777983e-01  9.19643224e-01
  9.21071814e-01  9.21723783e-01  9.31420216e-01  9.32208574e-01
  9.33437698e-01  9.43990644e-01  9.46315048e-01  9.50520824e-01
  9.52525222e-01  9.55923651e-01  9.57244182e-01  9.59296240e-01
  9.64628614e-01  9.64632661e-01  9.66737485e-01  9.67153885e-01
  9.72612638e-01  9.73460844e-01  9.77032716e-01  9.78655685e-01
  9.80849322e-01  9.84227565e-01  9.86724476e-01  9.86765005e-01
  9.88829299e-01  9.93802368e-01  9.94007631e-01  9.95167130e-01
  9.95552658e-01  9.95630600e-01  9.97824237e-01  1.00103375e+00
  1.00222794e+00  1.00294114e+00  1.00393130e+00  1.00445925e+00
  1.00775711e+00  1.01132898e+00  1.01336324e+00  1.01514559e+00
  1.01854157e+00  1.01860596e+00  1.02202968e+00  1.02828160e+00
  1.03023177e+00  1.04411504e+00  1.09439972e+00  1.13650055e+00
  1.14560828e+00  1.14772935e+00  1.16066825e+00]

  UserWarning,

2022-10-31 11:03:45,669:INFO:Calculating mean and std
2022-10-31 11:03:45,670:INFO:Creating metrics dataframe
2022-10-31 11:03:45,674:INFO:Uploading results into container
2022-10-31 11:03:45,674:INFO:Uploading model into container now
2022-10-31 11:03:45,675:INFO:master_model_container: 31
2022-10-31 11:03:45,675:INFO:display_container: 2
2022-10-31 11:03:45,675:INFO:GradientBoostingRegressor(random_state=3360)
2022-10-31 11:03:45,675:INFO:create_model() successfully completed......................................
2022-10-31 11:03:45,802:WARNING:create_model() for GradientBoostingRegressor(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:45,802:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:45,802:INFO:Initializing create_model()
2022-10-31 11:03:45,802:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:45,802:INFO:Checking exceptions
2022-10-31 11:03:45,802:INFO:Importing libraries
2022-10-31 11:03:45,802:INFO:Copying training dataset
2022-10-31 11:03:45,818:INFO:Defining folds
2022-10-31 11:03:45,818:INFO:Declaring metric variables
2022-10-31 11:03:45,818:INFO:Importing untrained model
2022-10-31 11:03:45,818:INFO:Gradient Boosting Regressor Imported successfully
2022-10-31 11:03:45,818:INFO:Starting cross validation
2022-10-31 11:03:45,818:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:49,154:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.09365094 -0.06046011 -0.05611808 -0.04269824 -0.00872823 -0.00365088
  0.00160231  0.01218283  0.01290112  0.01641981  0.01711318  0.0171167
  0.02569719  0.0273994   0.02890274  0.0346055   0.0360682   0.03803713
  0.03982229  0.04775145  0.04895878  0.05674523  0.05870213  0.05929919
  0.059455    0.06298757  0.06605153  0.06605367  0.0669699   0.06766586
  0.07089114  0.07116376  0.07445157  0.07749228  0.07810512  0.08030394
  0.08130922  0.08309994  0.08386022  0.08515283  0.08727558  0.08821556
  0.09260449  0.09393548  0.09437902  0.09819489  0.09854537  0.10000279
  0.10061054  0.10098565  0.10198259  0.10217165  0.10310193  0.10433919
  0.10442417  0.10443924  0.10508666  0.10573644  0.10592333  0.10658911
  0.1077024   0.11058226  0.11215652  0.11246937  0.11352692  0.11427495
  0.11585659  0.11620723  0.11713324  0.11909438  0.12022398  0.12062018
  0.12133393  0.12229038  0.12248915  0.12296688  0.12335075  0.12426689
  0.12659585  0.12945494  0.12960424  0.13098356  0.13203398  0.13279548
  0.13436403  0.13509575  0.13637396  0.13699832  0.13849635  0.13851464
  0.14220702  0.14239487  0.14322074  0.14334927  0.14376345  0.1446203
  0.1454817   0.14706059  0.14789942  0.15170183  0.15246382  0.15419086
  0.15538953  0.15546928  0.15613338  0.15653549  0.15662352  0.15837077
  0.15926853  0.15976217  0.16182452  0.16428628  0.16465258  0.1653839
  0.16559478  0.1656055   0.16683075  0.16701685  0.16848697  0.17090469
  0.17121754  0.17232618  0.17271264  0.17526195  0.17861973  0.17904969
  0.17955077  0.1821185   0.18573544  0.18910785  0.18988012  0.19001017
  0.19030893  0.19442069  0.19474508  0.1950009   0.19555253  0.19696864
  0.19818187  0.19867074  0.20027591  0.20324571  0.20376863  0.20426484
  0.20465031  0.20519066  0.20571751  0.20899684  0.20982275  0.21078487
  0.21148726  0.21207032  0.2124168   0.21242199  0.21529881  0.21591384
  0.22042659  0.222607    0.22410802  0.22506436  0.22604935  0.23020719
  0.23383327  0.2353301   0.2363087   0.23714462  0.23805521  0.23931903
  0.24135543  0.24450554  0.2455469   0.24622896  0.24724538  0.24797917
  0.24878105  0.24897492  0.25038076  0.25165104  0.25744278  0.26123958
  0.26241182  0.26635053  0.2688514   0.2698005   0.27433722  0.27994864
  0.28030807  0.28642844  0.29014906  0.2934894   0.30211472  0.30272042
  0.30782595  0.31053278  0.31357679  0.31395799  0.32384741  0.32752176
  0.3308173   0.33566829  0.33597959  0.34439523  0.34503447  0.34837037
  0.34922092  0.35038834  0.35055306  0.35671824  0.35936861  0.36441523
  0.36630251  0.36736482  0.368158    0.37193629  0.37561331  0.38507599
  0.38510092  0.38599767  0.39336506  0.39786737  0.39818901  0.39922198
  0.3999577   0.40159033  0.40921597  0.40947637  0.41413378  0.41455108
  0.41520379  0.43372347  0.43614119  0.43651444  0.43784539  0.43790999
  0.44131662  0.44898795  0.4602737   0.4677136   0.47293613  0.47376551
  0.47553284  0.47777497  0.47785063  0.48062918  0.48809574  0.49351672
  0.49609706  0.4972508   0.49804602  0.49849611  0.49931839  0.50133585
  0.50467244  0.50490827  0.51101736  0.5134326   0.51971768  0.5200125
  0.52156271  0.52186516  0.52455357  0.52877082  0.53446952  0.53835122
  0.53845182  0.5389728   0.54152057  0.54183308  0.54261138  0.5428232
  0.54466017  0.55763238  0.55773533  0.55936828  0.56087791  0.56383328
  0.56452354  0.56553445  0.56611298  0.56761405  0.57203544  0.57240386
  0.57316189  0.57347501  0.57909776  0.57929987  0.58019889  0.58030532
  0.58105243  0.5817472   0.58350937  0.58372012  0.58532162  0.58763842
  0.58925397  0.58933908  0.59175679  0.59207329  0.59326913  0.59413315
  0.59716902  0.59773413  0.59845388  0.59950718  0.59961801  0.59965567
  0.60039405  0.60292174  0.60297061  0.60389228  0.60460356  0.60543578
  0.60673989  0.60686005  0.60891118  0.61234205  0.61299861  0.61486498
  0.6152148   0.61584466  0.61775444  0.61818623  0.61883006  0.62104157
  0.62597358  0.63090893  0.63222094  0.63476789  0.63483502  0.63909378
  0.64297098  0.64755267  0.64848016  0.64886741  0.64997039  0.65042033
  0.6518545   0.65201433  0.65553039  0.66016568  0.66083032  0.66355543
  0.66565489  0.66672051  0.6699705   0.67238822  0.67269561  0.67360492
  0.67375127  0.67873937  0.68102548  0.681664    0.68295703  0.68347988
  0.68523499  0.68636683  0.6879601   0.68909194  0.68999746  0.69164915
  0.69176532  0.69343221  0.69386346  0.69535568  0.69952214  0.7014695
  0.7043826   0.70756812  0.7085467   0.71013217  0.71233716  0.71527778
  0.72368286  0.7307032   0.73105176  0.73377687  0.73819686  0.74240474
  0.74403829  0.7458384   0.74732425  0.75180103  0.75255267  0.75346523
  0.7556976   0.75743842  0.75967383  0.76621148  0.76972895  0.76985253
  0.77379795  0.77786215  0.77897612  0.78243782  0.78365817  0.78732515
  0.7899843   0.80001644  0.8114601   0.81939029  0.82130545  0.82541654
  0.83873377  0.84262616  0.84628722  0.84786682  0.85276959  0.85327956
  0.85569728  0.85706034  0.86008398  0.86039391  0.8669111   0.86854405
  0.8699212   0.87312276  0.87635208  0.88718041  0.88795791  0.88839923
  0.88959813  0.89096118  0.89236803  0.90027244  0.90081195  0.90308067
  0.90365852  0.90376633  0.90377411  0.90769008  0.90795129  0.91149316
  0.91327237  0.91671221  0.91708203  0.9261732   0.9292859   0.9303576
  0.93601864  0.94959498  0.95898806  0.96042331  0.96206511  0.96284103
  0.96420408  0.96927397  0.97405484  0.97528389  0.97568779  0.97973254
  0.98144842  0.98210195  0.98395279  0.98438567  0.985844    0.98593488
  0.98865999  0.99107771  0.99244076  0.99273241  0.99288703  0.99512244
  0.99705781  1.00229153  1.00392448  1.00564951  1.00583182  1.00747914
  1.01218947  1.01408068  1.01593693  1.01962341  1.02175621  1.02206552
  1.04110505  1.06051034  1.11150267]

  UserWarning,

2022-10-31 11:03:49,217:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.22398376e-01 -9.44646978e-02 -7.49599760e-02 -6.40750129e-02
 -5.80327643e-02 -5.24759869e-02 -4.87953326e-02 -4.22092143e-02
 -4.09273240e-02 -4.07671753e-02 -3.49021456e-02 -3.17590928e-02
 -2.83087044e-02 -2.50680843e-02 -1.10483594e-02 -9.75754108e-03
 -7.59431885e-03 -3.66527742e-03 -9.92386404e-04 -6.37070354e-04
 -1.93764065e-04  1.21188218e-03  2.45582349e-03  7.60051490e-03
  8.79009888e-03  1.22249759e-02  1.62360236e-02  2.09851008e-02
  2.50049736e-02  2.76985505e-02  2.77201435e-02  2.87171696e-02
  2.93957941e-02  3.50954767e-02  3.66618336e-02  3.93959232e-02
  4.00745511e-02  4.01425152e-02  4.22788871e-02  4.59142135e-02
  5.16506272e-02  5.39736564e-02  5.44826145e-02  5.56846531e-02
  5.63646948e-02  5.86920513e-02  6.10529758e-02  6.42428821e-02
  7.17073995e-02  7.22087898e-02  7.28331574e-02  7.34545165e-02
  7.81245465e-02  8.14821559e-02  8.16677801e-02  8.19702063e-02
  8.63382066e-02  8.67362610e-02  8.80431051e-02  8.85117788e-02
  8.86202367e-02  9.01539104e-02  9.04345362e-02  9.06312986e-02
  9.09804657e-02  9.12512702e-02  9.14053873e-02  9.17868017e-02
  9.20607907e-02  9.35068849e-02  9.45594025e-02  9.50176014e-02
  9.55321032e-02  9.57338835e-02  9.57779883e-02  9.60141756e-02
  9.90418622e-02  1.02574932e-01  1.02962391e-01  1.04403641e-01
  1.07375544e-01  1.08691779e-01  1.09451317e-01  1.10317720e-01
  1.12387433e-01  1.13262759e-01  1.14467735e-01  1.14533030e-01
  1.14718821e-01  1.14769218e-01  1.15434023e-01  1.16965708e-01
  1.17127799e-01  1.20106279e-01  1.20924019e-01  1.20930180e-01
  1.25913461e-01  1.28798993e-01  1.28870816e-01  1.29329519e-01
  1.30010086e-01  1.30239067e-01  1.31276394e-01  1.33330973e-01
  1.36418151e-01  1.36503765e-01  1.36553784e-01  1.38822791e-01
  1.40313815e-01  1.41652041e-01  1.44815427e-01  1.45298220e-01
  1.45343140e-01  1.49577661e-01  1.49746626e-01  1.50066113e-01
  1.50097281e-01  1.50706410e-01  1.55096695e-01  1.55175753e-01
  1.57805879e-01  1.58028900e-01  1.58519101e-01  1.59484468e-01
  1.61072570e-01  1.62077111e-01  1.62492726e-01  1.62759248e-01
  1.63562832e-01  1.63877791e-01  1.66028261e-01  1.66170238e-01
  1.66207511e-01  1.66828097e-01  1.67629596e-01  1.69342396e-01
  1.69894199e-01  1.70775353e-01  1.73538294e-01  1.73705272e-01
  1.74760605e-01  1.76644353e-01  1.78070915e-01  1.78590748e-01
  1.81164031e-01  1.82480716e-01  1.82845770e-01  1.82986125e-01
  1.83838896e-01  1.85822030e-01  1.86590928e-01  1.87904858e-01
  1.90194631e-01  1.90266667e-01  1.90637830e-01  1.93049386e-01
  1.96280190e-01  1.96291406e-01  1.96768341e-01  1.97246096e-01
  2.00008498e-01  2.00143739e-01  2.03634389e-01  2.05091484e-01
  2.08949673e-01  2.11893013e-01  2.14488581e-01  2.14900414e-01
  2.15485004e-01  2.19372849e-01  2.19778923e-01  2.20887888e-01
  2.24750193e-01  2.25794895e-01  2.27136571e-01  2.27503805e-01
  2.27928141e-01  2.28936097e-01  2.29143709e-01  2.30929374e-01
  2.31877781e-01  2.36272846e-01  2.47276947e-01  2.50239807e-01
  2.52472914e-01  2.52640061e-01  2.53737439e-01  2.53750221e-01
  2.54085900e-01  2.55906841e-01  2.62062124e-01  2.66853818e-01
  2.68146879e-01  2.68828536e-01  2.69502776e-01  2.70104174e-01
  2.70728540e-01  2.72613344e-01  2.72840942e-01  2.78178262e-01
  2.81230510e-01  2.82454485e-01  2.84036750e-01  2.85991305e-01
  2.89966391e-01  2.90318802e-01  2.92700229e-01  2.97717037e-01
  2.98488401e-01  2.99685725e-01  3.05580379e-01  3.08159300e-01
  3.14414255e-01  3.23768937e-01  3.32956334e-01  3.38644759e-01
  3.41809996e-01  3.47434508e-01  3.53863426e-01  3.55505568e-01
  3.60440169e-01  3.62550912e-01  3.69014662e-01  3.70139772e-01
  3.73261186e-01  3.75329969e-01  3.86834234e-01  3.90701443e-01
  3.94973340e-01  3.96182310e-01  4.04778356e-01  4.04951758e-01
  4.05589276e-01  4.11567980e-01  4.14263530e-01  4.14902157e-01
  4.15589166e-01  4.21796503e-01  4.24672825e-01  4.24807072e-01
  4.25210297e-01  4.27496078e-01  4.27977717e-01  4.50177130e-01
  4.51975053e-01  4.52377231e-01  4.57475393e-01  4.59340828e-01
  4.65770867e-01  4.72572351e-01  4.79513769e-01  4.81611092e-01
  4.82324760e-01  4.89600297e-01  4.96755198e-01  5.00244663e-01
  5.00337183e-01  5.04435713e-01  5.05408357e-01  5.09902690e-01
  5.11102728e-01  5.11577314e-01  5.14098748e-01  5.15914592e-01
  5.18026572e-01  5.18544193e-01  5.20888327e-01  5.21226684e-01
  5.22528193e-01  5.23297078e-01  5.23474301e-01  5.23551728e-01
  5.24707236e-01  5.25792190e-01  5.27677497e-01  5.33101250e-01
  5.33855387e-01  5.34146200e-01  5.45390016e-01  5.47004188e-01
  5.47817054e-01  5.48297249e-01  5.49077076e-01  5.55185196e-01
  5.56686896e-01  5.56775744e-01  5.57802760e-01  5.59417836e-01
  5.61328601e-01  5.62847336e-01  5.65186416e-01  5.67410071e-01
  5.68048172e-01  5.71916894e-01  5.72127173e-01  5.76045424e-01
  5.78464725e-01  5.80217917e-01  5.82998324e-01  5.83196081e-01
  5.87579170e-01  5.90003730e-01  5.93963030e-01  5.95371346e-01
  5.95437077e-01  5.95751073e-01  5.96796886e-01  5.96916094e-01
  5.97219767e-01  5.98627350e-01  5.99454496e-01  6.01200140e-01
  6.04122846e-01  6.05415907e-01  6.05547934e-01  6.05744150e-01
  6.07187264e-01  6.07347079e-01  6.08088022e-01  6.08568674e-01
  6.08988818e-01  6.09381084e-01  6.12653528e-01  6.12951763e-01
  6.14454867e-01  6.15360014e-01  6.15515284e-01  6.16808345e-01
  6.17506813e-01  6.18101257e-01  6.18741843e-01  6.24895036e-01
  6.25253439e-01  6.28779280e-01  6.29498119e-01  6.30034832e-01
  6.39171740e-01  6.41396180e-01  6.41795319e-01  6.45407327e-01
  6.47393977e-01  6.50138477e-01  6.52328177e-01  6.53894301e-01
  6.58868235e-01  6.60241613e-01  6.64247565e-01  6.69892158e-01
  6.73194829e-01  6.77424074e-01  6.78830113e-01  6.80592174e-01
  6.83442192e-01  6.84557351e-01  6.89122856e-01  6.90924195e-01
  6.91984612e-01  6.95430062e-01  6.96367701e-01  7.00574342e-01
  7.05248608e-01  7.06395956e-01  7.11117049e-01  7.27285108e-01
  7.31032492e-01  7.33744373e-01  7.34597782e-01  7.37762946e-01
  7.41354561e-01  7.44135966e-01  7.44863797e-01  7.53953816e-01
  7.58102728e-01  7.59554317e-01  7.62343953e-01  7.68323855e-01
  7.70910139e-01  7.71697511e-01  7.79302474e-01  7.80305739e-01
  7.81674420e-01  7.83447617e-01  7.88955997e-01  7.97123701e-01
  8.03190258e-01  8.03818271e-01  8.14641784e-01  8.15102710e-01
  8.19879112e-01  8.27319365e-01  8.29014093e-01  8.46260574e-01
  8.46655253e-01  8.50238258e-01  8.52415464e-01  8.52637060e-01
  8.52954335e-01  8.54228345e-01  8.54680951e-01  8.55347397e-01
  8.56346248e-01  8.58998678e-01  8.60519255e-01  8.67088185e-01
  8.67133991e-01  8.75865311e-01  8.77819612e-01  8.80472152e-01
  8.81066597e-01  8.86900415e-01  8.87233280e-01  8.90804585e-01
  8.91465921e-01  8.91551745e-01  8.91662069e-01  8.95918288e-01
  8.99624537e-01  9.02087613e-01  9.08460988e-01  9.11433529e-01
  9.11873970e-01  9.13602948e-01  9.19101818e-01  9.22551913e-01
  9.22901855e-01  9.24851414e-01  9.25361719e-01  9.25911831e-01
  9.50021877e-01  9.57635911e-01  9.59696162e-01  9.62340039e-01
  9.69502401e-01  9.71117165e-01  9.75858168e-01  9.76318451e-01
  9.76678924e-01  9.76937212e-01  9.77796382e-01  9.82152269e-01
  9.86056439e-01  9.86717775e-01  9.90441405e-01  9.93880137e-01
  1.00037627e+00  1.00073099e+00  1.00441982e+00  1.00503858e+00
  1.00925871e+00  1.01025363e+00  1.01378417e+00  1.01428847e+00
  1.01481914e+00  1.02441206e+00  1.02744891e+00  1.03142762e+00
  1.03261745e+00  1.05453772e+00  1.05748668e+00  1.08710079e+00
  1.14833436e+00  1.16814659e+00  1.18890152e+00]

  UserWarning,

2022-10-31 11:03:49,287:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.12890599 -0.09540904 -0.08242129 -0.07244729 -0.06221032 -0.04915087
 -0.0441037  -0.03880002 -0.03233119 -0.01902658 -0.01427952 -0.00465646
 -0.00181497  0.00685231  0.00692341  0.0178951   0.0183161   0.01843342
  0.01938122  0.02059146  0.02140068  0.0242388   0.02612092  0.02763935
  0.03244661  0.03684071  0.03768854  0.03867627  0.03899295  0.03911128
  0.03913436  0.04019868  0.04919207  0.04990634  0.050574    0.05188913
  0.0543993   0.05506676  0.0556278   0.05562884  0.05625382  0.05827216
  0.06050951  0.06142473  0.06250246  0.06464735  0.06660222  0.06852892
  0.06915976  0.06972622  0.07087208  0.07186686  0.07413023  0.07535317
  0.07613088  0.08020983  0.08147273  0.08298765  0.08332668  0.08439298
  0.0846858   0.08693119  0.0877907   0.08851581  0.09153702  0.09318676
  0.09327978  0.09491671  0.09548924  0.09591232  0.09669221  0.0974736
  0.09877934  0.10024071  0.10149129  0.10317498  0.10406154  0.10540513
  0.10548131  0.10656328  0.10781522  0.10807925  0.1088644   0.11006084
  0.11012153  0.11193253  0.11323517  0.11459441  0.11728061  0.11803235
  0.11859272  0.11885399  0.12022427  0.12108901  0.12118459  0.12174231
  0.1221532   0.12223584  0.12334598  0.12370258  0.12374139  0.12747053
  0.12759131  0.12791352  0.12879762  0.12901878  0.12963394  0.12973982
  0.13014228  0.13153366  0.13232499  0.13344242  0.13380779  0.13412114
  0.13574771  0.13859044  0.13897188  0.13980387  0.14099991  0.14157811
  0.14327599  0.14372708  0.14500551  0.14817279  0.14832792  0.14836791
  0.14879657  0.14931161  0.15106505  0.15138882  0.15503844  0.15658782
  0.15675408  0.15743921  0.16399117  0.16488946  0.16626713  0.16657524
  0.1669544   0.16722318  0.16870811  0.16924406  0.17210467  0.17215948
  0.17338009  0.17607804  0.17709777  0.17757452  0.17783512  0.17838789
  0.17978053  0.18066251  0.18098819  0.18165511  0.18177492  0.18288045
  0.18432096  0.18485691  0.18584815  0.18838319  0.18854742  0.1892387
  0.18952855  0.18953249  0.18984923  0.19386687  0.19500037  0.19738302
  0.1989648   0.20001182  0.20172868  0.20330638  0.20788634  0.20876179
  0.20886031  0.21123594  0.21215902  0.21265698  0.21445835  0.21523405
  0.21648427  0.21698806  0.21753917  0.22444217  0.22463144  0.2249616
  0.22624675  0.2312062   0.23222257  0.23352108  0.23935924  0.24092313
  0.24217308  0.25290391  0.25465164  0.25810786  0.26132007  0.26139256
  0.26444518  0.26543248  0.2686615   0.26880432  0.27292532  0.27473359
  0.27535192  0.27710683  0.28366163  0.28546083  0.28852046  0.29882689
  0.30094637  0.30471083  0.30770568  0.30838235  0.30989299  0.31358771
  0.31941905  0.32045982  0.32419544  0.32690206  0.3367136   0.33859205
  0.34073161  0.34564805  0.34936138  0.35208389  0.35472988  0.35518294
  0.35768724  0.35787643  0.36226319  0.36605237  0.38136774  0.38419689
  0.38503919  0.38569034  0.38583654  0.38750236  0.39739965  0.39998515
  0.40287587  0.40386737  0.40392392  0.40530374  0.40613628  0.41224556
  0.41381414  0.4185425   0.42067813  0.42634895  0.42928164  0.4303292
  0.43453818  0.43545355  0.43704121  0.44186832  0.44222205  0.44895069
  0.4503741   0.45047154  0.46457546  0.46901622  0.46946406  0.47228992
  0.47386965  0.48412565  0.48568619  0.4958002   0.49655194  0.49915817
  0.50166696  0.50873808  0.51561747  0.51635613  0.51796677  0.5229321
  0.52454133  0.52607644  0.52655358  0.5296739   0.5308699   0.5308925
  0.53183707  0.53232799  0.53406537  0.53577324  0.53649053  0.53760175
  0.5428026   0.54580541  0.5469621   0.54846091  0.54944931  0.55236625
  0.55393436  0.55848869  0.55891172  0.55992044  0.56051184  0.56755028
  0.56769005  0.56844179  0.56900463  0.57018018  0.57881272  0.58156397
  0.58238989  0.58384619  0.58439787  0.58604203  0.5893525   0.58942009
  0.59492035  0.5953352   0.59591076  0.59659049  0.59874599  0.6004678
  0.60104213  0.60201276  0.60247348  0.60670263  0.60808427  0.61144925
  0.61153044  0.61267415  0.6139451   0.61490813  0.61907289  0.61929234
  0.62065061  0.62140235  0.62400858  0.62691427  0.62716777  0.62894626
  0.63038165  0.63541916  0.6415056   0.64380859  0.64381668  0.6454208
  0.64639964  0.64878065  0.64992014  0.65222076  0.65334178  0.65361424
  0.65581056  0.6597268   0.66132507  0.66372168  0.66380287  0.66592506
  0.67292304  0.67367478  0.67494854  0.67635336  0.68195865  0.68265408
  0.68479077  0.68965463  0.6920006   0.69355364  0.70049389  0.70164852
  0.70193358  0.70301545  0.70671957  0.70970178  0.71058575  0.7111728
  0.71642532  0.71989133  0.72151361  0.72182652  0.72490872  0.72770062
  0.73208802  0.73843793  0.74561832  0.7457119   0.75392095  0.75651827
  0.76591868  0.76596171  0.77508924  0.78488338  0.79164743  0.80860755
  0.81023336  0.81213529  0.83663254  0.8574959   0.85774782  0.86026964
  0.86430886  0.86673425  0.86723767  0.87022274  0.87052462  0.87204102
  0.87315208  0.87487782  0.8763372   0.8783003   0.87957847  0.87972597
  0.88047771  0.8806904   0.88185752  0.88460816  0.88540724  0.88553856
  0.8862903   0.88758887  0.88889653  0.89094684  0.90095508  0.90341237
  0.90606002  0.90847321  0.91138283  0.91150053  0.91310434  0.92396823
  0.92904873  0.9531591   0.96607076  0.96740149  0.97227863  0.97729706
  0.97749845  0.9794472   0.9799033   0.9809006   0.98147999  0.98223173
  0.98339429  0.98483796  0.98564485  0.98669981  0.98774309  0.99033615
  0.99237416  0.99410147  0.99460485  0.99484621  0.99559795  0.99748887
  0.99820418  0.99919941  1.00081777  1.00249655  1.00255738  1.00744197
  1.0088223   1.0100482   1.01085509  1.0158882   1.02005644  1.02012776
  1.04751346  1.11126809]

  UserWarning,

387  1.00088362  1.00219247
  1.00515074  1.00545839  1.00891687  1.01979941  1.03162761  1.04164285
  1.07569151  1.17716827]

  UserWarning,

2022-10-31 11:03:49,354:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.23161073e-01 -1.09500160e-01 -1.08007622e-01 -5.29695922e-02
 -2.29989441e-02 -2.19854193e-02 -2.05871017e-02  5.72114716e-04
  3.71842308e-03  5.34946738e-03  6.33380198e-03  6.50683849e-03
  6.98175707e-03  8.63818549e-03  2.81026169e-02  2.82325084e-02
  2.88496960e-02  2.98284586e-02  3.30759644e-02  3.60167811e-02
  3.87803438e-02  3.90583662e-02  4.00111805e-02  4.10048342e-02
  4.47361050e-02  4.71530053e-02  5.41448101e-02  5.48628829e-02
  5.70012760e-02  5.79415439e-02  5.80756856e-02  5.97148211e-02
  6.16238790e-02  6.26018549e-02  6.42497365e-02  6.48630725e-02
  6.63757931e-02  6.84260872e-02  6.99800525e-02  7.24057228e-02
  7.70530021e-02  7.86061907e-02  8.12698334e-02  8.27730541e-02
  8.28297246e-02  8.36522216e-02  8.48417302e-02  8.54068647e-02
  8.62537590e-02  9.10365826e-02  9.34010802e-02  9.37789973e-02
  9.38295181e-02  9.66902112e-02  9.71533844e-02  9.89889568e-02
  1.00699059e-01  1.01260974e-01  1.01308113e-01  1.02659078e-01
  1.03050046e-01  1.03212960e-01  1.03214726e-01  1.03373664e-01
  1.04359901e-01  1.08503050e-01  1.10033666e-01  1.11077195e-01
  1.11334046e-01  1.11375658e-01  1.13583273e-01  1.14006867e-01
  1.14304310e-01  1.15227666e-01  1.15256672e-01  1.16254968e-01
  1.16533955e-01  1.16645574e-01  1.16832426e-01  1.18097951e-01
  1.18617608e-01  1.18796625e-01  1.20674259e-01  1.21656873e-01
  1.22663061e-01  1.23296169e-01  1.23362027e-01  1.23760570e-01
  1.23839497e-01  1.24573334e-01  1.24741720e-01  1.30408262e-01
  1.30532213e-01  1.30863872e-01  1.31285605e-01  1.34440438e-01
  1.34697214e-01  1.35056693e-01  1.35740254e-01  1.36730857e-01
  1.38386519e-01  1.38918127e-01  1.39071912e-01  1.39352046e-01
  1.40227967e-01  1.42976927e-01  1.43036844e-01  1.43354845e-01
  1.44316376e-01  1.45103745e-01  1.46226081e-01  1.46234440e-01
  1.46895404e-01  1.46914215e-01  1.48686837e-01  1.49241581e-01
  1.49288552e-01  1.52994853e-01  1.53883112e-01  1.55309190e-01
  1.57650817e-01  1.58885637e-01  1.59936705e-01  1.60781599e-01
  1.61416096e-01  1.62025143e-01  1.62189427e-01  1.62398338e-01
  1.63059135e-01  1.63839402e-01  1.66044967e-01  1.66396902e-01
  1.67356320e-01  1.70736806e-01  1.72122488e-01  1.75021044e-01
  1.77813472e-01  1.78594073e-01  1.78677147e-01  1.82504389e-01
  1.85857219e-01  1.86411481e-01  1.86651205e-01  1.88207153e-01
  1.90425692e-01  1.91056039e-01  1.91668727e-01  1.94265602e-01
  1.94630865e-01  1.94909409e-01  1.95688675e-01  1.96280481e-01
  1.96575282e-01  1.96715022e-01  1.99537380e-01  2.01432033e-01
  2.01923833e-01  2.02560961e-01  2.02983838e-01  2.04131599e-01
  2.07435829e-01  2.07486295e-01  2.09115524e-01  2.09682352e-01
  2.09863814e-01  2.10786434e-01  2.11422863e-01  2.13637108e-01
  2.16820059e-01  2.19387045e-01  2.20418939e-01  2.25199478e-01
  2.28311339e-01  2.29019240e-01  2.29134356e-01  2.32273332e-01
  2.32627185e-01  2.34676670e-01  2.35027046e-01  2.36446936e-01
  2.37761930e-01  2.39384943e-01  2.39471057e-01  2.41567484e-01
  2.42266623e-01  2.42708896e-01  2.42982255e-01  2.43254039e-01
  2.45069396e-01  2.45314158e-01  2.51661298e-01  2.53240622e-01
  2.53584692e-01  2.54739527e-01  2.58078738e-01  2.59684374e-01
  2.61473277e-01  2.63132673e-01  2.65005990e-01  2.66273034e-01
  2.71014719e-01  2.75200698e-01  2.75214812e-01  2.78720302e-01
  2.79127449e-01  2.89749176e-01  2.90416270e-01  2.97809076e-01
  2.98160757e-01  2.98497328e-01  3.04180163e-01  3.10832197e-01
  3.14929224e-01  3.18276920e-01  3.18840885e-01  3.31575327e-01
  3.33113120e-01  3.37397116e-01  3.38926643e-01  3.41194063e-01
  3.43454498e-01  3.49130407e-01  3.51823659e-01  3.68453883e-01
  3.68771941e-01  3.76746742e-01  3.81997161e-01  3.82218998e-01
  3.83789875e-01  3.90605441e-01  3.93654780e-01  3.94561136e-01
  3.94723115e-01  3.94932018e-01  3.95893346e-01  3.97243352e-01
  4.00439174e-01  4.02227278e-01  4.05434972e-01  4.06707471e-01
  4.10087881e-01  4.12898838e-01  4.16115451e-01  4.16783443e-01
  4.19264646e-01  4.20480983e-01  4.22260827e-01  4.22292222e-01
  4.23303204e-01  4.26474088e-01  4.26860800e-01  4.27535160e-01
  4.31540417e-01  4.36772878e-01  4.37299051e-01  4.45955541e-01
  4.48200194e-01  4.51589934e-01  4.57834388e-01  4.58695137e-01
  4.59758818e-01  4.62346970e-01  4.62499695e-01  4.71088740e-01
  4.73893144e-01  4.78014857e-01  4.81928574e-01  4.82626456e-01
  4.86113683e-01  4.86343141e-01  4.94175599e-01  4.96304898e-01
  5.03356368e-01  5.04216300e-01  5.05403877e-01  5.06308383e-01
  5.07459410e-01  5.15277348e-01  5.15619277e-01  5.15992171e-01
  5.16194173e-01  5.16593668e-01  5.22570366e-01  5.26230390e-01
  5.29995430e-01  5.32404281e-01  5.33735821e-01  5.37569030e-01
  5.37841289e-01  5.38283761e-01  5.42233474e-01  5.44528013e-01
  5.48236524e-01  5.49438210e-01  5.55118258e-01  5.57831190e-01
  5.60686608e-01  5.62020202e-01  5.67892064e-01  5.68193818e-01
  5.71418739e-01  5.71428754e-01  5.72910419e-01  5.76085804e-01
  5.76892048e-01  5.78472159e-01  5.79219733e-01  5.80705625e-01
  5.83016862e-01  5.88262151e-01  5.88804790e-01  5.94274207e-01
  5.97638053e-01  5.97986006e-01  5.99417429e-01  6.00333797e-01
  6.02316364e-01  6.04437257e-01  6.04688195e-01  6.05206307e-01
  6.05358828e-01  6.07300289e-01  6.07602569e-01  6.07980487e-01
  6.08489194e-01  6.11796776e-01  6.11834491e-01  6.12589653e-01
  6.12964412e-01  6.13342882e-01  6.17917071e-01  6.18053692e-01
  6.18157783e-01  6.23676167e-01  6.24423561e-01  6.26132232e-01
  6.26236322e-01  6.28104266e-01  6.28208356e-01  6.31220248e-01
  6.34604340e-01  6.34989924e-01  6.36775186e-01  6.36860864e-01
  6.38825555e-01  6.42003815e-01  6.43809480e-01  6.45729554e-01
  6.50610743e-01  6.52618466e-01  6.53880073e-01  6.59564909e-01
  6.62331870e-01  6.64268442e-01  6.81626114e-01  6.84098400e-01
  6.89768713e-01  6.90280902e-01  6.91971870e-01  6.92780744e-01
  6.98962171e-01  7.00676772e-01  7.04156434e-01  7.04747775e-01
  7.05125692e-01  7.08819790e-01  7.09442639e-01  7.09820556e-01
  7.10819555e-01  7.13924795e-01  7.15302989e-01  7.16709487e-01
  7.22002438e-01  7.23381528e-01  7.25353562e-01  7.27678714e-01
  7.30909434e-01  7.32999447e-01  7.36750833e-01  7.41877386e-01
  7.44754483e-01  7.50068464e-01  7.52544957e-01  7.53669981e-01
  7.54839039e-01  7.59673847e-01  7.61413648e-01  7.66087334e-01
  7.71775839e-01  7.72249570e-01  7.74069380e-01  7.78372269e-01
  7.79309640e-01  7.79982963e-01  7.81076404e-01  7.82228802e-01
  7.82383263e-01  7.88999328e-01  7.92170922e-01  7.94606649e-01
  7.95754427e-01  7.97139488e-01  7.97517405e-01  8.00082271e-01
  8.02348218e-01  8.04255570e-01  8.05992860e-01  8.10262181e-01
  8.16706859e-01  8.28013935e-01  8.29742805e-01  8.40209599e-01
  8.41803770e-01  8.44021759e-01  8.44655233e-01  8.45033150e-01
  8.46985808e-01  8.47084730e-01  8.50017076e-01  8.50719058e-01
  8.53487373e-01  8.54434943e-01  8.55210447e-01  8.58672020e-01
  8.61542490e-01  8.63288986e-01  8.65261020e-01  8.71719787e-01
  8.79959497e-01  8.81770360e-01  8.88424210e-01  8.89682903e-01
  8.90595744e-01  8.94158026e-01  8.94458641e-01  8.99465122e-01
  8.99906047e-01  9.00530503e-01  9.00805444e-01  9.01024916e-01
  9.01183361e-01  9.04212151e-01  9.05661434e-01  9.11272208e-01
  9.16327654e-01  9.16716367e-01  9.17778458e-01  9.21411231e-01
  9.24295187e-01  9.25140204e-01  9.25370196e-01  9.26298473e-01
  9.43438356e-01  9.48469015e-01  9.51449377e-01  9.51827294e-01
  9.62004590e-01  9.70083130e-01  9.72055164e-01  9.74491864e-01
  9.77039089e-01  9.79540747e-01  9.80075137e-01  9.82158752e-01
  9.82403495e-01  9.84765689e-01  9.88627568e-01  9.89674749e-01
  9.92944515e-01  9.93322432e-01  9.94327916e-01  9.96880435e-01
  9.96987889e-01  9.98166000e-01  1.00084110e+00  1.00187681e+00
  1.00349973e+00  1.00688340e+00  1.00726899e+00  1.00782975e+00
  1.00939782e+00  1.01044141e+00  1.01355030e+00  1.01758841e+00
  1.01970562e+00  1.02334867e+00  1.03351146e+00  1.04514672e+00
  1.06349189e+00  1.10239155e+00  1.14391170e+00]

  UserWarning,

2022-10-31 11:03:49,417:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.78261298e-02 -8.28232593e-02 -6.03722382e-02 -4.26876535e-02
 -3.95344817e-02 -3.94495370e-02 -3.41271445e-02 -3.26028574e-02
 -3.17385092e-02 -3.06673183e-02 -1.88399943e-02 -1.47889240e-02
 -9.93013178e-03 -9.45676026e-03 -8.17146080e-04 -4.73264514e-04
  4.55918085e-04  1.48524046e-03  2.15710560e-03  3.13663860e-03
  6.43970817e-03  1.26526658e-02  2.16315178e-02  2.29861207e-02
  2.63631225e-02  2.93558580e-02  3.08282169e-02  3.50858148e-02
  3.56534434e-02  3.85458920e-02  4.05866146e-02  4.12126991e-02
  4.59698072e-02  4.97393717e-02  5.36504841e-02  5.63973533e-02
  5.74478547e-02  5.76874249e-02  5.79100478e-02  5.82879144e-02
  5.86370656e-02  5.87433807e-02  5.94437640e-02  6.04104094e-02
  6.10160931e-02  6.11433847e-02  6.17016263e-02  6.79755275e-02
  6.93602354e-02  6.97460990e-02  7.49086126e-02  7.55783349e-02
  7.59709662e-02  7.69174865e-02  7.95292761e-02  7.97412070e-02
  7.98142756e-02  8.02628222e-02  8.17541714e-02  8.28001387e-02
  8.56049516e-02  8.57681636e-02  8.61468690e-02  8.71237548e-02
  8.78702333e-02  8.79572060e-02  8.83300587e-02  8.93029463e-02
  9.10470228e-02  9.13583887e-02  9.20804112e-02  9.40689997e-02
  9.42902148e-02  9.45973470e-02  9.55139297e-02  9.58049455e-02
  9.83815552e-02  9.87694262e-02  1.02219096e-01  1.03625596e-01
  1.04207883e-01  1.04259567e-01  1.04809528e-01  1.08457766e-01
  1.08478845e-01  1.08523177e-01  1.10085202e-01  1.12198228e-01
  1.13427233e-01  1.15424698e-01  1.15451874e-01  1.15913101e-01
  1.19209439e-01  1.20618393e-01  1.21872373e-01  1.24214889e-01
  1.24616313e-01  1.27578424e-01  1.28240085e-01  1.29649951e-01
  1.29869849e-01  1.30487116e-01  1.31344357e-01  1.33377250e-01
  1.34069910e-01  1.34319507e-01  1.34765145e-01  1.37211291e-01
  1.38193774e-01  1.38207545e-01  1.41302897e-01  1.41452844e-01
  1.42385039e-01  1.43304927e-01  1.43771262e-01  1.48127825e-01
  1.49229726e-01  1.49715301e-01  1.50082999e-01  1.50326022e-01
  1.50356176e-01  1.52187262e-01  1.52655697e-01  1.53567615e-01
  1.54278031e-01  1.54322885e-01  1.54337487e-01  1.55367725e-01
  1.55549710e-01  1.55822677e-01  1.57466500e-01  1.58752095e-01
  1.59158677e-01  1.59713164e-01  1.60461492e-01  1.63391847e-01
  1.63648188e-01  1.65613850e-01  1.65668050e-01  1.67633552e-01
  1.69225782e-01  1.69757911e-01  1.69831504e-01  1.70282933e-01
  1.70697601e-01  1.72440163e-01  1.73880627e-01  1.74118998e-01
  1.74793534e-01  1.75027917e-01  1.75232835e-01  1.81199367e-01
  1.83344390e-01  1.87277488e-01  1.89535318e-01  1.89881209e-01
  1.90474290e-01  1.91225782e-01  1.93698772e-01  1.94343695e-01
  1.94780804e-01  2.00507159e-01  2.00717102e-01  2.01108237e-01
  2.03175364e-01  2.04182210e-01  2.04958429e-01  2.06951096e-01
  2.08783489e-01  2.09217477e-01  2.12155701e-01  2.12450760e-01
  2.13027107e-01  2.15647278e-01  2.15693618e-01  2.16173381e-01
  2.16299340e-01  2.16423380e-01  2.17119440e-01  2.17872988e-01
  2.26608337e-01  2.26724832e-01  2.29035368e-01  2.29952034e-01
  2.30457073e-01  2.31873304e-01  2.33107921e-01  2.33337815e-01
  2.33951529e-01  2.36423530e-01  2.36813192e-01  2.40246581e-01
  2.41218988e-01  2.41579953e-01  2.44709553e-01  2.45518894e-01
  2.46193585e-01  2.46550125e-01  2.47406730e-01  2.48417391e-01
  2.48913555e-01  2.49308192e-01  2.50353844e-01  2.51519458e-01
  2.52748705e-01  2.53696112e-01  2.60475500e-01  2.65591707e-01
  2.66581546e-01  2.67042897e-01  2.72267952e-01  2.80211640e-01
  2.87172041e-01  2.90077535e-01  2.93276577e-01  2.93326472e-01
  2.98198564e-01  3.01516096e-01  3.18660624e-01  3.22516997e-01
  3.22594092e-01  3.25391300e-01  3.31207570e-01  3.43093260e-01
  3.46868591e-01  3.49041711e-01  3.56739975e-01  3.59804004e-01
  3.62448648e-01  3.74925827e-01  3.77837553e-01  3.78658300e-01
  3.79653057e-01  3.79790246e-01  3.83816511e-01  3.87994018e-01
  3.89453349e-01  3.91676568e-01  3.93615259e-01  3.97910656e-01
  4.02086516e-01  4.03969646e-01  4.13205994e-01  4.13751917e-01
  4.14465787e-01  4.19338105e-01  4.33407910e-01  4.35416204e-01
  4.37390778e-01  4.39648091e-01  4.46579478e-01  4.48550486e-01
  4.49072366e-01  4.49377573e-01  4.49507936e-01  4.52362788e-01
  4.54497401e-01  4.58366019e-01  4.60361172e-01  4.61451352e-01
  4.64004715e-01  4.64317137e-01  4.69074518e-01  4.70376898e-01
  4.75220007e-01  4.76782704e-01  4.78332317e-01  4.83789863e-01
  4.85383822e-01  4.86626108e-01  4.88944099e-01  5.01463600e-01
  5.02199101e-01  5.02357303e-01  5.07069256e-01  5.08015371e-01
  5.11357750e-01  5.16612755e-01  5.26561013e-01  5.26562697e-01
  5.26672610e-01  5.35213166e-01  5.40181649e-01  5.41696779e-01
  5.43072067e-01  5.47029665e-01  5.49388657e-01  5.50879963e-01
  5.51207285e-01  5.53127203e-01  5.54376515e-01  5.54654851e-01
  5.55759136e-01  5.57604287e-01  5.61095600e-01  5.64479869e-01
  5.66401940e-01  5.72467441e-01  5.73840838e-01  5.78067112e-01
  5.78500857e-01  5.78812494e-01  5.78929674e-01  5.79659190e-01
  5.82526143e-01  5.82814429e-01  5.84728420e-01  5.87824100e-01
  5.88363748e-01  5.88977779e-01  5.89601493e-01  5.92291256e-01
  5.92837519e-01  5.93213176e-01  5.93497500e-01  5.94037996e-01
  5.96551168e-01  5.97219490e-01  5.98855171e-01  5.99029445e-01
  5.99350073e-01  5.99967046e-01  5.99986610e-01  6.00950096e-01
  6.01340113e-01  6.04406362e-01  6.05047397e-01  6.05449093e-01
  6.06005838e-01  6.06317485e-01  6.10745283e-01  6.11084525e-01
  6.11970538e-01  6.13156611e-01  6.13276825e-01  6.14458932e-01
  6.16213619e-01  6.18133983e-01  6.18972881e-01  6.19210807e-01
  6.19596646e-01  6.20673470e-01  6.21841539e-01  6.25991403e-01
  6.27222547e-01  6.31966378e-01  6.32503117e-01  6.34131693e-01
  6.34173259e-01  6.34392011e-01  6.36577013e-01  6.37983079e-01
  6.40878999e-01  6.41080818e-01  6.42741143e-01  6.42804263e-01
  6.42889880e-01  6.44755869e-01  6.47739308e-01  6.53070223e-01
  6.56742989e-01  6.59101933e-01  6.62246272e-01  6.65271415e-01
  6.66467890e-01  6.67263580e-01  6.67682743e-01  6.68985064e-01
  6.72660115e-01  6.74122778e-01  6.74786348e-01  6.76249010e-01
  6.76501891e-01  6.80175892e-01  6.81748678e-01  6.83355054e-01
  6.86368117e-01  6.95405131e-01  6.95502889e-01  6.97918083e-01
  7.04642891e-01  7.06908466e-01  7.07596355e-01  7.08729590e-01
  7.10623434e-01  7.10821287e-01  7.12238486e-01  7.15515638e-01
  7.15774763e-01  7.26718326e-01  7.29410523e-01  7.30525042e-01
  7.33131290e-01  7.33921601e-01  7.48610092e-01  7.56887420e-01
  7.57664406e-01  7.62878358e-01  7.68291238e-01  7.69909763e-01
  7.74339714e-01  7.78148381e-01  7.89660585e-01  7.91340506e-01
  8.06568417e-01  8.07619969e-01  8.08841301e-01  8.28581560e-01
  8.38608093e-01  8.43985395e-01  8.50095744e-01  8.52763631e-01
  8.54201693e-01  8.55120889e-01  8.58308948e-01  8.61274084e-01
  8.62269539e-01  8.64987733e-01  8.67667298e-01  8.68662784e-01
  8.70125446e-01  8.70705821e-01  8.73277576e-01  8.84656805e-01
  8.89190644e-01  8.91283245e-01  8.96297809e-01  8.98836174e-01
  9.03441910e-01  9.03923277e-01  9.06297158e-01  9.06527205e-01
  9.07155559e-01  9.07598328e-01  9.10830610e-01  9.10908241e-01
  9.11037311e-01  9.12319346e-01  9.12740173e-01  9.15445402e-01
  9.18266516e-01  9.19158136e-01  9.22724739e-01  9.38151348e-01
  9.47810361e-01  9.54363967e-01  9.55199061e-01  9.59626859e-01
  9.61359010e-01  9.63340508e-01  9.67015559e-01  9.68478222e-01
  9.71630351e-01  9.73890440e-01  9.76018915e-01  9.76229084e-01
  9.76240683e-01  9.81279140e-01  1.00179469e+00  1.00550833e+00
  1.00595110e+00  1.00665408e+00  1.00894875e+00  1.00918339e+00
  1.01056590e+00  1.01064605e+00  1.01103318e+00  1.01289612e+00
  1.01379818e+00  1.02514630e+00  1.09448959e+00  1.13459359e+00]

  UserWarning,

2022-10-31 11:03:49,417:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.16509254e-01 -1.05470173e-01 -1.01693538e-01 -9.23713324e-02
 -5.27819590e-02 -2.89387937e-02 -2.29602122e-02 -1.98821073e-02
 -6.55438174e-03 -2.69525345e-03 -2.45992444e-03  3.19076306e-04
  1.82469895e-03  1.84220206e-03  5.01343226e-03  9.27952836e-03
  1.74409014e-02  2.18190429e-02  2.53644135e-02  2.71327376e-02
  2.92539440e-02  3.26376438e-02  3.58335537e-02  3.58839984e-02
  3.69646812e-02  4.01867651e-02  4.09406101e-02  4.53913287e-02
  4.87227286e-02  5.69434087e-02  6.19741449e-02  6.21525433e-02
  6.40209382e-02  6.67663968e-02  6.71287175e-02  6.89456460e-02
  7.06068777e-02  7.10702079e-02  7.14286776e-02  7.54250692e-02
  7.58505900e-02  7.72539971e-02  8.24835633e-02  8.27900257e-02
  8.37968319e-02  8.38783678e-02  8.83455178e-02  8.93695076e-02
  9.01295376e-02  9.04995006e-02  9.08419649e-02  9.28630427e-02
  9.33960046e-02  9.50934685e-02  9.57365342e-02  9.65592731e-02
  9.67986978e-02  9.68438108e-02  9.70149891e-02  9.70910444e-02
  9.71320301e-02  9.71499725e-02  9.83704915e-02  1.00463288e-01
  1.01065653e-01  1.03683800e-01  1.04280405e-01  1.04688035e-01
  1.05508755e-01  1.06788632e-01  1.07283170e-01  1.08395967e-01
  1.09403148e-01  1.09975905e-01  1.11168685e-01  1.12703558e-01
  1.12750132e-01  1.13566484e-01  1.14789022e-01  1.14823536e-01
  1.16159549e-01  1.16219191e-01  1.18145863e-01  1.19469776e-01
  1.20036369e-01  1.20332540e-01  1.21132198e-01  1.22885334e-01
  1.24816208e-01  1.25560608e-01  1.26191592e-01  1.26473367e-01
  1.26724839e-01  1.26868384e-01  1.28596633e-01  1.29154068e-01
  1.30728346e-01  1.32132222e-01  1.32407789e-01  1.32458712e-01
  1.32534462e-01  1.32596857e-01  1.32768105e-01  1.33629211e-01
  1.33952510e-01  1.35979279e-01  1.37914883e-01  1.38021343e-01
  1.38637989e-01  1.40015785e-01  1.40767241e-01  1.41977365e-01
  1.42211303e-01  1.43609163e-01  1.43640888e-01  1.44885394e-01
  1.45355130e-01  1.46743012e-01  1.46979912e-01  1.47090554e-01
  1.47427500e-01  1.48686355e-01  1.49455423e-01  1.50276463e-01
  1.50639840e-01  1.52591879e-01  1.52616895e-01  1.52766970e-01
  1.53111212e-01  1.53535048e-01  1.53825945e-01  1.54093096e-01
  1.54355315e-01  1.54545437e-01  1.55128773e-01  1.56157577e-01
  1.56171696e-01  1.56434086e-01  1.56997618e-01  1.59626508e-01
  1.59789079e-01  1.61517120e-01  1.63082090e-01  1.63149444e-01
  1.64808956e-01  1.64827135e-01  1.66640750e-01  1.68642082e-01
  1.68672890e-01  1.71082758e-01  1.72795345e-01  1.73692425e-01
  1.75488103e-01  1.77344031e-01  1.77634928e-01  1.82689750e-01
  1.85975199e-01  1.86106322e-01  1.86272866e-01  1.87045858e-01
  1.88012365e-01  1.89208175e-01  1.89582579e-01  1.89710371e-01
  1.90026587e-01  1.92048757e-01  1.92195665e-01  1.95121923e-01
  1.98874664e-01  1.99598140e-01  1.99748422e-01  2.00119183e-01
  2.01561374e-01  2.01852271e-01  2.02664905e-01  2.08879265e-01
  2.09499942e-01  2.12007428e-01  2.12377715e-01  2.13303187e-01
  2.14710521e-01  2.15483067e-01  2.15658133e-01  2.16550013e-01
  2.17868733e-01  2.18554050e-01  2.18651762e-01  2.18682353e-01
  2.18792259e-01  2.22054718e-01  2.23219465e-01  2.29265422e-01
  2.29294141e-01  2.30723743e-01  2.31162636e-01  2.32798834e-01
  2.36423777e-01  2.36730978e-01  2.38108039e-01  2.42885786e-01
  2.43009602e-01  2.51292501e-01  2.52093020e-01  2.56708590e-01
  2.59912727e-01  2.61926438e-01  2.62467923e-01  2.63643785e-01
  2.63945847e-01  2.69268986e-01  2.76059480e-01  2.79266936e-01
  2.86351859e-01  2.86675378e-01  2.87694233e-01  2.89116621e-01
  2.90034889e-01  2.95846879e-01  2.97076399e-01  2.98339986e-01
  3.13337448e-01  3.20129685e-01  3.26438380e-01  3.27498981e-01
  3.34611194e-01  3.35031145e-01  3.35793908e-01  3.36910579e-01
  3.40744073e-01  3.41563279e-01  3.56834219e-01  3.61798150e-01
  3.62511034e-01  3.63364133e-01  3.65367245e-01  3.67975633e-01
  3.72623667e-01  3.77159548e-01  3.85890897e-01  3.93580600e-01
  3.97797834e-01  3.98006983e-01  3.99178725e-01  4.02328522e-01
  4.08812025e-01  4.09614528e-01  4.11159415e-01  4.14113993e-01
  4.28310553e-01  4.35936759e-01  4.36194170e-01  4.43071529e-01
  4.43840262e-01  4.47837965e-01  4.54777181e-01  4.58125380e-01
  4.60025110e-01  4.61190570e-01  4.61215315e-01  4.66341984e-01
  4.66700531e-01  4.68370306e-01  4.74375617e-01  4.85130499e-01
  4.89200097e-01  4.93874304e-01  4.95528541e-01  4.95673069e-01
  4.98007025e-01  4.99435445e-01  5.00222362e-01  5.01606289e-01
  5.02567708e-01  5.03630682e-01  5.03983907e-01  5.04933899e-01
  5.08299483e-01  5.13143298e-01  5.17332156e-01  5.18665069e-01
  5.19999305e-01  5.20352157e-01  5.28755872e-01  5.29174860e-01
  5.30661776e-01  5.32820724e-01  5.36042412e-01  5.39481516e-01
  5.40954258e-01  5.42917671e-01  5.43071767e-01  5.44512084e-01
  5.44621837e-01  5.45196382e-01  5.49696454e-01  5.51543526e-01
  5.55531724e-01  5.56171291e-01  5.57635250e-01  5.57788227e-01
  5.61736991e-01  5.67300844e-01  5.70084870e-01  5.73625130e-01
  5.75048801e-01  5.76450727e-01  5.77304727e-01  5.79597487e-01
  5.79638452e-01  5.81124324e-01  5.81170553e-01  5.82728951e-01
  5.84445117e-01  5.86042393e-01  5.87727995e-01  5.88251161e-01
  5.90246973e-01  5.91691126e-01  5.96205392e-01  5.96752063e-01
  5.97122927e-01  6.00719728e-01  6.02074675e-01  6.02512948e-01
  6.04159692e-01  6.06116305e-01  6.08587176e-01  6.08729145e-01
  6.09072254e-01  6.11292339e-01  6.14036185e-01  6.18125461e-01
  6.18359032e-01  6.18584871e-01  6.23322963e-01  6.23432502e-01
  6.31653999e-01  6.32719280e-01  6.37243450e-01  6.40984087e-01
  6.41609961e-01  6.41886104e-01  6.42207381e-01  6.50981782e-01
  6.52061717e-01  6.57274503e-01  6.59385887e-01  6.61084694e-01
  6.61657451e-01  6.63721303e-01  6.65497798e-01  6.71277308e-01
  6.71854453e-01  6.72415957e-01  6.75892314e-01  6.76189870e-01
  6.81153801e-01  6.81302343e-01  6.81666044e-01  6.82229102e-01
  6.83076053e-01  6.85702486e-01  6.87240278e-01  6.87986017e-01
  6.89711150e-01  6.90550117e-01  6.92941038e-01  6.95160159e-01
  6.99708845e-01  7.00677038e-01  7.02454659e-01  7.04556476e-01
  7.08735515e-01  7.15095893e-01  7.15703572e-01  7.17503406e-01
  7.19179332e-01  7.21065275e-01  7.42905476e-01  7.53788013e-01
  7.55012266e-01  7.55426715e-01  7.56757732e-01  7.59354311e-01
  7.74866052e-01  7.78459045e-01  7.79549831e-01  7.79557122e-01
  7.89285911e-01  7.96838236e-01  7.97545059e-01  8.04986220e-01
  8.06503193e-01  8.10498642e-01  8.14175304e-01  8.19527245e-01
  8.19754229e-01  8.21445369e-01  8.24601859e-01  8.26484898e-01
  8.29042642e-01  8.29975554e-01  8.35896219e-01  8.37380699e-01
  8.43548530e-01  8.46248290e-01  8.47907736e-01  8.52871667e-01
  8.55514145e-01  8.56847596e-01  8.57083117e-01  8.57420353e-01
  8.59361363e-01  8.59829067e-01  8.60229917e-01  8.63407522e-01
  8.65526504e-01  8.70490434e-01  8.70760862e-01  8.71667454e-01
  8.74466363e-01  8.75039120e-01  8.76631385e-01  8.79886751e-01
  8.81036645e-01  8.84717100e-01  8.87486858e-01  8.90730282e-01
  8.91591857e-01  8.95804433e-01  8.95964437e-01  9.02335867e-01
  9.09001044e-01  9.13279711e-01  9.14646794e-01  9.15614987e-01
  9.19610725e-01  9.19636496e-01  9.24159411e-01  9.26385461e-01
  9.29007042e-01  9.43477547e-01  9.48796068e-01  9.53627593e-01
  9.53946269e-01  9.54157162e-01  9.60087219e-01  9.60578352e-01
  9.62439587e-01  9.67403518e-01  9.68713518e-01  9.70090969e-01
  9.71952204e-01  9.76799835e-01  9.77520397e-01  9.82940785e-01
  9.85719511e-01  9.88650500e-01  9.91232551e-01  9.92270608e-01
  9.95010465e-01  9.95022210e-01  9.96183018e-01  9.99725870e-01
  1.00153614e+00  1.00468980e+00  1.00559280e+00  1.00591413e+00
  1.00923849e+00  1.01146454e+00  1.01310106e+00  1.01408612e+00
  1.01984431e+00  1.02506315e+00  1.02582478e+00  1.02756156e+00
  1.02864943e+00  1.03020342e+00  1.03248479e+00  1.04265339e+00
  1.04494336e+00  1.09798246e+00  1.12939787e+00]

  UserWarning,

2022-10-31 11:03:49,470:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.30339421e-01 -1.20798169e-01 -1.14746424e-01 -8.73233875e-02
 -6.17283515e-02 -5.95272141e-02 -5.13991633e-02 -4.83437944e-02
 -4.05056471e-02 -2.48049728e-02 -1.72117615e-02 -1.20957085e-02
 -1.76305976e-04  1.11507613e-02  1.28661540e-02  1.40129858e-02
  3.89784863e-02  3.92032197e-02  5.80870189e-02  5.95788400e-02
  6.20617632e-02  6.51407975e-02  6.63825321e-02  6.64219522e-02
  6.65321761e-02  6.68900538e-02  7.17234170e-02  7.54900336e-02
  7.73666995e-02  7.77311313e-02  8.13814362e-02  8.38835572e-02
  8.43222690e-02  8.58856247e-02  8.84553352e-02  8.85156938e-02
  9.02085331e-02  9.10985773e-02  9.11583302e-02  9.33905279e-02
  9.49230317e-02  9.55249355e-02  9.71166213e-02  9.92423502e-02
  9.99148776e-02  1.00066195e-01  1.00776181e-01  1.01862624e-01
  1.03552949e-01  1.03718351e-01  1.04260250e-01  1.05698560e-01
  1.07101497e-01  1.07115549e-01  1.07633341e-01  1.07886163e-01
  1.08064048e-01  1.09394795e-01  1.10394172e-01  1.10652733e-01
  1.12167813e-01  1.12238760e-01  1.13075786e-01  1.14305508e-01
  1.15500039e-01  1.15733885e-01  1.17192878e-01  1.17645650e-01
  1.17977471e-01  1.18375332e-01  1.18411373e-01  1.19859198e-01
  1.21322607e-01  1.21734138e-01  1.22405334e-01  1.23477012e-01
  1.23644860e-01  1.24697499e-01  1.24834718e-01  1.24885206e-01
  1.25793308e-01  1.26227947e-01  1.27421153e-01  1.27853556e-01
  1.27872499e-01  1.29651988e-01  1.30549278e-01  1.32086361e-01
  1.32440923e-01  1.33043497e-01  1.33172949e-01  1.34369127e-01
  1.34713732e-01  1.35883550e-01  1.36093268e-01  1.36606104e-01
  1.37615974e-01  1.37786653e-01  1.39874511e-01  1.40534366e-01
  1.41994105e-01  1.44593234e-01  1.44813120e-01  1.45081037e-01
  1.45418310e-01  1.45638589e-01  1.47380112e-01  1.47649082e-01
  1.47991676e-01  1.48109408e-01  1.48120280e-01  1.49934384e-01
  1.50012197e-01  1.50558128e-01  1.50783375e-01  1.50828797e-01
  1.51247291e-01  1.51544122e-01  1.51977205e-01  1.52928985e-01
  1.53370174e-01  1.54800146e-01  1.56501995e-01  1.56875366e-01
  1.58474582e-01  1.59062649e-01  1.60430548e-01  1.60716828e-01
  1.61501205e-01  1.61738706e-01  1.62174667e-01  1.62260413e-01
  1.63199129e-01  1.63780485e-01  1.63801695e-01  1.64607964e-01
  1.65035248e-01  1.67918343e-01  1.68553032e-01  1.68733291e-01
  1.68766443e-01  1.70809364e-01  1.71142197e-01  1.71524184e-01
  1.73405308e-01  1.75049050e-01  1.75115495e-01  1.75267782e-01
  1.75697186e-01  1.76116235e-01  1.76656953e-01  1.77834072e-01
  1.79165994e-01  1.79592256e-01  1.80099037e-01  1.82225380e-01
  1.83593311e-01  1.85608242e-01  1.88222948e-01  1.88752853e-01
  1.88937786e-01  1.89921970e-01  1.94449103e-01  1.94561251e-01
  1.96852304e-01  1.98077636e-01  2.06333749e-01  2.08253806e-01
  2.08699558e-01  2.10154515e-01  2.13019194e-01  2.13713756e-01
  2.14292473e-01  2.14493479e-01  2.17123825e-01  2.17499149e-01
  2.20753177e-01  2.22439093e-01  2.22462119e-01  2.23068010e-01
  2.24391784e-01  2.38293855e-01  2.43919171e-01  2.50205322e-01
  2.51307513e-01  2.51953130e-01  2.51995769e-01  2.55571348e-01
  2.55791740e-01  2.61219837e-01  2.63081307e-01  2.63437230e-01
  2.66161359e-01  2.66782751e-01  2.69974290e-01  2.75523468e-01
  2.75559512e-01  2.76440870e-01  2.81108938e-01  2.82667544e-01
  2.85909470e-01  2.93951963e-01  2.94498243e-01  3.02289454e-01
  3.03432139e-01  3.12705567e-01  3.13204261e-01  3.18465751e-01
  3.23119986e-01  3.28179296e-01  3.30553636e-01  3.30610352e-01
  3.33079386e-01  3.34813897e-01  3.37436594e-01  3.38716503e-01
  3.47390098e-01  3.48218530e-01  3.48965764e-01  3.57774597e-01
  3.57870969e-01  3.59749114e-01  3.59755772e-01  3.59853504e-01
  3.63369110e-01  3.63768032e-01  3.63973631e-01  3.64403136e-01
  3.68792707e-01  3.75095103e-01  3.81962794e-01  3.85378402e-01
  3.85454005e-01  3.87395718e-01  3.91153725e-01  3.91187057e-01
  3.97392047e-01  4.00175649e-01  4.00624607e-01  4.01871023e-01
  4.11644693e-01  4.13400763e-01  4.14893705e-01  4.15634759e-01
  4.21852620e-01  4.22275786e-01  4.22518785e-01  4.23927957e-01
  4.27853624e-01  4.29312904e-01  4.35641290e-01  4.44368851e-01
  4.45195932e-01  4.51927425e-01  4.52191343e-01  4.61206846e-01
  4.70585117e-01  4.70818239e-01  4.73888461e-01  4.75392171e-01
  4.88170523e-01  4.89567755e-01  4.94006225e-01  4.94512658e-01
  4.98821804e-01  5.03899738e-01  5.04815878e-01  5.06438483e-01
  5.06884376e-01  5.13547351e-01  5.14293992e-01  5.19803447e-01
  5.20359457e-01  5.20487596e-01  5.25116834e-01  5.26866953e-01
  5.28161726e-01  5.28696955e-01  5.32091198e-01  5.33535343e-01
  5.35622064e-01  5.38117365e-01  5.38438049e-01  5.42030226e-01
  5.42117689e-01  5.42733255e-01  5.47450774e-01  5.47491813e-01
  5.48034428e-01  5.48486146e-01  5.50613277e-01  5.53133496e-01
  5.53251665e-01  5.53934554e-01  5.60957101e-01  5.63971293e-01
  5.64956925e-01  5.67937077e-01  5.68188599e-01  5.71861359e-01
  5.74561359e-01  5.75678965e-01  5.78804738e-01  5.81029118e-01
  5.85065661e-01  5.88930534e-01  5.90302627e-01  5.93448622e-01
  5.93988074e-01  5.94107193e-01  5.99658232e-01  6.00287194e-01
  6.02615064e-01  6.03841378e-01  6.05219239e-01  6.05986989e-01
  6.06611883e-01  6.11856046e-01  6.12076275e-01  6.12104410e-01
  6.13257121e-01  6.14461418e-01  6.15646294e-01  6.15687793e-01
  6.15890124e-01  6.18030393e-01  6.18250622e-01  6.20484207e-01
  6.22954313e-01  6.25856841e-01  6.26871626e-01  6.27406968e-01
  6.27809511e-01  6.28902403e-01  6.31084026e-01  6.32021382e-01
  6.33823315e-01  6.38037513e-01  6.39245002e-01  6.43298775e-01
  6.47472896e-01  6.49264587e-01  6.58685032e-01  6.61065827e-01
  6.63154970e-01  6.63570757e-01  6.64057185e-01  6.65038602e-01
  6.68100775e-01  6.69863189e-01  6.73252917e-01  6.79841545e-01
  6.81220603e-01  6.81987156e-01  6.83355533e-01  6.84592528e-01
  6.88076442e-01  6.88204173e-01  6.89257288e-01  6.89590904e-01
  6.90350720e-01  6.90681814e-01  6.91862660e-01  6.94250789e-01
  6.96856161e-01  6.99881646e-01  7.04791933e-01  7.05753384e-01
  7.07015657e-01  7.08890959e-01  7.12204424e-01  7.12987740e-01
  7.14083328e-01  7.16688700e-01  7.24952956e-01  7.25983964e-01
  7.28707736e-01  7.44880160e-01  7.46305610e-01  7.48870509e-01
  7.50888527e-01  7.57603433e-01  7.62804674e-01  7.65283883e-01
  7.67788625e-01  7.79098391e-01  7.81606836e-01  7.94258224e-01
  7.94785310e-01  7.98777747e-01  8.01459315e-01  8.22127629e-01
  8.23155586e-01  8.24711196e-01  8.36426857e-01  8.42137477e-01
  8.47551915e-01  8.52962169e-01  8.54607238e-01  8.64212510e-01
  8.66358120e-01  8.72447406e-01  8.72964732e-01  8.76017425e-01
  8.77852951e-01  8.78621753e-01  8.79030180e-01  8.86483867e-01
  8.86503159e-01  8.86748905e-01  8.87958235e-01  8.89302911e-01
  8.91077434e-01  9.01196609e-01  9.02861702e-01  9.03558745e-01
  9.04956400e-01  9.06190110e-01  9.06540763e-01  9.07833259e-01
  9.09807231e-01  9.10438630e-01  9.12268071e-01  9.13303689e-01
  9.22242503e-01  9.24079072e-01  9.24908636e-01  9.26867539e-01
  9.27677791e-01  9.31666590e-01  9.36734640e-01  9.60954630e-01
  9.61618331e-01  9.63100241e-01  9.64946936e-01  9.69083430e-01
  9.69189527e-01  9.70370373e-01  9.72468336e-01  9.73310205e-01
  9.75363874e-01  9.75787204e-01  9.76968050e-01  9.81416772e-01
  9.81961551e-01  9.83562383e-01  9.87683061e-01  9.90004044e-01
  9.93138397e-01  9.95284007e-01  9.95743768e-01  9.95826016e-01
  9.97889379e-01  1.00369737e+00  1.00397867e+00  1.00515951e+00
  1.00818531e+00  1.01015301e+00  1.01144145e+00  1.01620591e+00
  1.01668018e+00  1.02360047e+00  1.02444081e+00  1.04572004e+00
  1.04773615e+00  1.05014687e+00  1.06887225e+00  1.09204294e+00]

  UserWarning,

2022-10-31 11:03:50,865:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.17492351e-01 -1.15918733e-01 -9.59553907e-02 -6.58625843e-02
 -5.91635477e-02 -5.11462224e-02 -4.65989489e-02 -4.60254713e-02
 -4.15025812e-02 -3.21465102e-02 -8.89186973e-03 -8.05497089e-03
  5.83806007e-04  1.42526370e-02  2.11093772e-02  2.12385183e-02
  2.56235447e-02  4.44847149e-02  4.58118406e-02  4.67513486e-02
  4.87028441e-02  4.89675019e-02  5.05028385e-02  5.14918730e-02
  5.45297852e-02  5.48630125e-02  5.56156231e-02  5.57920830e-02
  5.66896669e-02  5.76764772e-02  5.78913174e-02  6.07718085e-02
  6.40746281e-02  6.43435686e-02  6.47371371e-02  7.44197486e-02
  7.45009468e-02  7.51843793e-02  7.64847245e-02  7.73409847e-02
  7.77403066e-02  7.77769379e-02  7.96707065e-02  8.30539092e-02
  8.32226123e-02  8.55842313e-02  8.71250413e-02  8.83332244e-02
  8.88062510e-02  8.92949166e-02  8.98207824e-02  9.00685458e-02
  9.09905236e-02  9.20475579e-02  9.25712297e-02  9.41464291e-02
  9.54651082e-02  9.65976919e-02  9.75637197e-02  9.81072617e-02
  9.93209577e-02  1.00264397e-01  1.03664171e-01  1.03932620e-01
  1.04001348e-01  1.05510380e-01  1.05517670e-01  1.09579571e-01
  1.10219089e-01  1.10403037e-01  1.11290532e-01  1.11547787e-01
  1.12241028e-01  1.12518402e-01  1.12889945e-01  1.13975665e-01
  1.14034388e-01  1.15683754e-01  1.16915826e-01  1.17187418e-01
  1.17792421e-01  1.18031718e-01  1.19629507e-01  1.20684715e-01
  1.21843018e-01  1.22491277e-01  1.22795460e-01  1.22852403e-01
  1.23235895e-01  1.24784062e-01  1.25421491e-01  1.26196712e-01
  1.28069472e-01  1.28310932e-01  1.30640268e-01  1.32161270e-01
  1.33777951e-01  1.35216810e-01  1.35733142e-01  1.36929044e-01
  1.39319891e-01  1.40835877e-01  1.40853855e-01  1.41014594e-01
  1.44425728e-01  1.44841354e-01  1.45131543e-01  1.45257193e-01
  1.45842598e-01  1.46048697e-01  1.46077275e-01  1.47069911e-01
  1.47214368e-01  1.50978846e-01  1.55623681e-01  1.58355434e-01
  1.59293173e-01  1.59404343e-01  1.59804052e-01  1.61144376e-01
  1.61343938e-01  1.62900582e-01  1.64827034e-01  1.65090684e-01
  1.65586169e-01  1.65707661e-01  1.66288723e-01  1.70241956e-01
  1.74408833e-01  1.76467820e-01  1.76843450e-01  1.77693746e-01
  1.77895621e-01  1.78406949e-01  1.78550750e-01  1.79003821e-01
  1.80953468e-01  1.82138102e-01  1.82924865e-01  1.83162196e-01
  1.84618980e-01  1.85285006e-01  1.86891815e-01  1.89427401e-01
  1.89596020e-01  1.92060663e-01  1.94920323e-01  1.98647249e-01
  2.01251669e-01  2.02502749e-01  2.03558854e-01  2.04068662e-01
  2.05382713e-01  2.05441403e-01  2.05848855e-01  2.06366182e-01
  2.06795117e-01  2.16176931e-01  2.17301265e-01  2.19292146e-01
  2.19960692e-01  2.22307086e-01  2.25573134e-01  2.28605004e-01
  2.30427134e-01  2.32620321e-01  2.35771109e-01  2.36316943e-01
  2.41867124e-01  2.42590818e-01  2.43964891e-01  2.44217321e-01
  2.46552840e-01  2.48284171e-01  2.49401125e-01  2.53853059e-01
  2.54505545e-01  2.55635901e-01  2.59462513e-01  2.59657061e-01
  2.59814226e-01  2.64550927e-01  2.66532670e-01  2.69178116e-01
  2.71135437e-01  2.73752679e-01  2.75209256e-01  2.78682654e-01
  2.80747408e-01  2.85343736e-01  2.87706766e-01  2.88343470e-01
  2.95007521e-01  2.97909500e-01  2.99002844e-01  3.10572796e-01
  3.11202634e-01  3.15032353e-01  3.28853377e-01  3.36852544e-01
  3.40895455e-01  3.44939453e-01  3.45706209e-01  3.47095888e-01
  3.50074227e-01  3.50140674e-01  3.51905482e-01  3.52129725e-01
  3.53890834e-01  3.60073512e-01  3.62998403e-01  3.64329534e-01
  3.66040487e-01  3.68213141e-01  3.74924702e-01  3.75879368e-01
  3.78033726e-01  3.79578734e-01  3.80735401e-01  3.83663165e-01
  3.84240193e-01  3.85079355e-01  3.86369223e-01  3.88921009e-01
  3.90855427e-01  3.93728096e-01  3.98341014e-01  4.00358154e-01
  4.07894082e-01  4.09936027e-01  4.11036413e-01  4.18858835e-01
  4.20537328e-01  4.24500093e-01  4.31757499e-01  4.34321337e-01
  4.35139911e-01  4.35719722e-01  4.37141619e-01  4.37827910e-01
  4.40022568e-01  4.44183628e-01  4.49059665e-01  4.49314945e-01
  4.59233215e-01  4.59529480e-01  4.62483294e-01  4.68010204e-01
  4.69206652e-01  4.70975697e-01  4.74496169e-01  4.74703565e-01
  4.80373221e-01  4.88568745e-01  4.88681149e-01  4.93013635e-01
  4.95049463e-01  5.06382392e-01  5.10624368e-01  5.11354947e-01
  5.11402629e-01  5.18078306e-01  5.19073680e-01  5.20459996e-01
  5.20909228e-01  5.21564976e-01  5.21650179e-01  5.23090147e-01
  5.23652562e-01  5.26670955e-01  5.27069301e-01  5.29210214e-01
  5.30450931e-01  5.31786084e-01  5.32340108e-01  5.34162658e-01
  5.34261016e-01  5.34853821e-01  5.34926141e-01  5.35298807e-01
  5.36592429e-01  5.38843680e-01  5.39821871e-01  5.42332874e-01
  5.42515697e-01  5.43712910e-01  5.45523303e-01  5.46923728e-01
  5.47327380e-01  5.51134019e-01  5.51564718e-01  5.55605380e-01
  5.57240823e-01  5.57619270e-01  5.58600855e-01  5.60545973e-01
  5.60773143e-01  5.61135540e-01  5.65238009e-01  5.68407574e-01
  5.69161431e-01  5.70977592e-01  5.73251109e-01  5.75191180e-01
  5.75858153e-01  5.76602672e-01  5.76843984e-01  5.77513070e-01
  5.79896486e-01  5.82010435e-01  5.83797853e-01  5.84236429e-01
  5.84359464e-01  5.84410430e-01  5.84514888e-01  5.87834503e-01
  5.89430825e-01  5.89549580e-01  5.90502250e-01  5.90862235e-01
  5.91225193e-01  5.91776192e-01  5.94121344e-01  5.96214981e-01
  6.01836500e-01  6.02042385e-01  6.03538973e-01  6.04480624e-01
  6.09783864e-01  6.10971497e-01  6.14390245e-01  6.14475874e-01
  6.17752889e-01  6.18301342e-01  6.18399729e-01  6.18593528e-01
  6.19293005e-01  6.20420362e-01  6.21113604e-01  6.23198786e-01
  6.24685476e-01  6.26308445e-01  6.26707856e-01  6.28502083e-01
  6.29342162e-01  6.30136852e-01  6.33117723e-01  6.37376472e-01
  6.41570513e-01  6.44998481e-01  6.49042813e-01  6.52056226e-01
  6.54316579e-01  6.57394772e-01  6.57795060e-01  6.60056162e-01
  6.60819037e-01  6.64124716e-01  6.67883705e-01  6.68246045e-01
  6.70667346e-01  6.71015304e-01  6.74051558e-01  6.77045421e-01
  6.77738663e-01  6.79117215e-01  6.81310535e-01  6.82933504e-01
  6.86316304e-01  6.87094129e-01  6.91494635e-01  6.93039662e-01
  6.93117604e-01  6.95867148e-01  6.96067849e-01  7.00477425e-01
  7.04825212e-01  7.07405979e-01  7.09360985e-01  7.09744035e-01
  7.16409738e-01  7.19773583e-01  7.22885047e-01  7.29175040e-01
  7.34657926e-01  7.35259776e-01  7.39635226e-01  7.68601629e-01
  7.72682481e-01  7.88002195e-01  7.94404143e-01  7.98512432e-01
  7.98690849e-01  8.10675074e-01  8.11344839e-01  8.18392814e-01
  8.23028671e-01  8.37269477e-01  8.37719754e-01  8.41846250e-01
  8.47715031e-01  8.49047347e-01  8.50097908e-01  8.50809643e-01
  8.57129789e-01  8.60403787e-01  8.61719834e-01  8.65827579e-01
  8.67985466e-01  8.70750930e-01  8.72550937e-01  8.74086370e-01
  8.76122810e-01  8.77745778e-01  8.77831698e-01  8.79939416e-01
  8.82871586e-01  8.85200984e-01  8.87716545e-01  8.92384951e-01
  8.94521804e-01  8.95866256e-01  9.03102132e-01  9.03962086e-01
  9.04072046e-01  9.05531387e-01  9.11777983e-01  9.19643224e-01
  9.21071814e-01  9.21723783e-01  9.31420216e-01  9.32208574e-01
  9.33437698e-01  9.43990644e-01  9.46315048e-01  9.50520824e-01
  9.52525222e-01  9.55923651e-01  9.57244182e-01  9.59296240e-01
  9.64628614e-01  9.64632661e-01  9.66737485e-01  9.67153885e-01
  9.72612638e-01  9.73460844e-01  9.77032716e-01  9.78655685e-01
  9.80849322e-01  9.84227565e-01  9.86724476e-01  9.86765005e-01
  9.88829299e-01  9.93802368e-01  9.94007631e-01  9.95167130e-01
  9.95552658e-01  9.95630600e-01  9.97824237e-01  1.00103375e+00
  1.00222794e+00  1.00294114e+00  1.00393130e+00  1.00445925e+00
  1.00775711e+00  1.01132898e+00  1.01336324e+00  1.01514559e+00
  1.01854157e+00  1.01860596e+00  1.02202968e+00  1.02828160e+00
  1.03023177e+00  1.04411504e+00  1.09439972e+00  1.13650055e+00
  1.14560828e+00  1.14772935e+00  1.16066825e+00]

  UserWarning,

2022-10-31 11:03:50,892:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08232029 -0.0617261  -0.03624115 -0.02260416 -0.01469573 -0.01072538
 -0.00827165  0.00198281  0.00871283  0.00891355  0.01625603  0.02836561
  0.02961991  0.03284073  0.03440232  0.0348366   0.03495971  0.03521831
  0.03556561  0.03669133  0.03917579  0.04173726  0.04178869  0.04293348
  0.04345737  0.04477995  0.045761    0.04684527  0.0526391   0.0566254
  0.05738809  0.05789505  0.06057052  0.0608769   0.06287494  0.06571212
  0.07061535  0.07300958  0.07413814  0.07473105  0.07596924  0.07700236
  0.07832604  0.079417    0.08063281  0.0837792   0.08634895  0.08731249
  0.08899522  0.0890382   0.08905331  0.08960593  0.09002897  0.09055553
  0.09230154  0.09246277  0.09254097  0.09267753  0.09517194  0.09539057
  0.09736174  0.09781892  0.0985341   0.10053657  0.10174038  0.10312557
  0.10349507  0.10441607  0.10707811  0.10799362  0.1085713   0.10882361
  0.10885585  0.10920768  0.10995774  0.11018018  0.11143571  0.11313467
  0.11337435  0.11552725  0.11757515  0.11795891  0.11810704  0.11822026
  0.11957326  0.12019479  0.12065171  0.12131116  0.12319144  0.12331908
  0.12365156  0.1243989   0.12610289  0.12670793  0.12806091  0.12927016
  0.12986296  0.13034119  0.13052609  0.13111451  0.13265126  0.1339545
  0.13407229  0.13537605  0.13638616  0.1371762   0.13931626  0.13936595
  0.14140737  0.14214331  0.14252671  0.14281992  0.14501125  0.14573997
  0.14591809  0.14623905  0.14786495  0.14853072  0.1493185   0.15059174
  0.15581684  0.15661341  0.15948024  0.16007854  0.16029943  0.16254861
  0.16260918  0.16439381  0.16453174  0.16541014  0.16658302  0.16677802
  0.16692034  0.16807662  0.16909089  0.17099157  0.17151319  0.17152255
  0.17173088  0.17207187  0.17269343  0.17271831  0.17699559  0.17885843
  0.17918944  0.18044886  0.18086702  0.1813751   0.18166591  0.18235789
  0.1829447   0.18589299  0.18591331  0.18592081  0.18741425  0.18953832
  0.18963178  0.19024793  0.19152523  0.19300683  0.19306865  0.19411725
  0.20129057  0.20143768  0.20178397  0.20353888  0.20489124  0.20646803
  0.20996592  0.21077987  0.21194891  0.21201512  0.21289643  0.21717969
  0.21840151  0.21844486  0.22078768  0.22352244  0.22451428  0.22464699
  0.2250016   0.22674797  0.23831917  0.23884349  0.23948776  0.24066931
  0.24370493  0.25695973  0.25777851  0.25869441  0.25887502  0.25896706
  0.2594523   0.26841666  0.27757656  0.28162566  0.2825846   0.28615581
  0.29199372  0.29862123  0.29986967  0.30072985  0.30108528  0.3013457
  0.3080017   0.3086056   0.31139481  0.31533452  0.31570443  0.3221361
  0.32222764  0.32814804  0.32899539  0.33723813  0.34072045  0.34392114
  0.34965892  0.3498082   0.35876835  0.36067559  0.36595125  0.3674282
  0.36964396  0.37148818  0.37603212  0.37878201  0.38140383  0.39647057
  0.4021943   0.40491699  0.41644692  0.41652058  0.42208802  0.42946567
  0.43096077  0.43210906  0.43225722  0.4400727   0.44430075  0.447883
  0.45607401  0.46255377  0.46371776  0.46380524  0.4656067   0.46758989
  0.47593069  0.48169914  0.48568239  0.48979329  0.4922056   0.49232481
  0.49364111  0.50001951  0.50014405  0.50424054  0.50447749  0.50517027
  0.51039966  0.5135388   0.52145953  0.52276878  0.52702415  0.52901777
  0.53190079  0.53535855  0.53788159  0.54068643  0.54539427  0.54804701
  0.5484952   0.548776    0.55124294  0.552363    0.55273603  0.5536037
  0.55378342  0.55480572  0.55605664  0.55737487  0.55821166  0.56333595
  0.56508101  0.56765512  0.56883134  0.57547326  0.57576006  0.57633129
  0.57832378  0.57924578  0.58111843  0.58307185  0.58443598  0.58559744
  0.58679527  0.58846225  0.59109143  0.59667829  0.59686908  0.59859081
  0.60448699  0.60565595  0.60836357  0.60858715  0.61259227  0.61329978
  0.6145941   0.61477046  0.61731011  0.61762794  0.61868565  0.61998232
  0.62123652  0.6234766   0.6236002   0.62407386  0.62651848  0.62998373
  0.6302242   0.63258743  0.63305543  0.63322465  0.63599661  0.63890409
  0.63931137  0.64856495  0.65291243  0.66468279  0.66502049  0.66665845
  0.66711607  0.6706181   0.67148045  0.6726654   0.67675695  0.6770248
  0.67930782  0.68407559  0.68567213  0.68577197  0.68857305  0.68928056
  0.69050805  0.69232676  0.69569213  0.69864465  0.69991035  0.70056158
  0.70217225  0.70396539  0.70464793  0.7097525   0.71260714  0.71440371
  0.71511001  0.71836466  0.7200543   0.7226177   0.72421332  0.72449881
  0.7314227   0.73706574  0.73909305  0.74317264  0.74676336  0.74882165
  0.75065927  0.75348963  0.75739382  0.76136088  0.76186695  0.76403624
  0.76451257  0.76587276  0.76984618  0.77864641  0.78018534  0.78100136
  0.7887846   0.79499246  0.79522291  0.80645701  0.80848275  0.81779299
  0.81956487  0.8202712   0.82457007  0.83115455  0.8349717   0.8413887
  0.84243482  0.8479119   0.85202915  0.8528965   0.85639715  0.86048869
  0.86292035  0.86303957  0.8632681   0.86407207  0.8662726   0.87001412
  0.87093616  0.87123268  0.87478021  0.88004063  0.88563596  0.88895046
  0.88915747  0.89518961  0.89577721  0.89642342  0.89704317  0.89815837
  0.89978796  0.90113471  0.90356637  0.90464666  0.90691862  0.91099153
  0.91104119  0.91261213  0.91452211  0.91684019  0.92219703  0.92302972
  0.93484327  0.95103248  0.95512402  0.95755568  0.95767489  0.95930796
  0.95991498  0.96090793  0.962972    0.9638098   0.96481286  0.9687442
  0.97175324  0.97459825  0.98836807  0.99245962  0.99286553  0.99489128
  0.99501049  0.99621406  0.99824352  0.99831798  1.00249477  1.00509656
  1.00795515  1.01274676  1.02839256  1.06977257  1.07292979  1.12872908
  1.15678537  1.15881111  1.17279305  1.17400671]

  UserWarning,

2022-10-31 11:03:50,892:INFO:Calculating mean and std
2022-10-31 11:03:50,892:INFO:Creating metrics dataframe
2022-10-31 11:03:50,892:INFO:Uploading results into container
2022-10-31 11:03:50,892:INFO:Uploading model into container now
2022-10-31 11:03:50,892:INFO:master_model_container: 32
2022-10-31 11:03:50,892:INFO:display_container: 2
2022-10-31 11:03:50,892:INFO:GradientBoostingRegressor(random_state=3360)
2022-10-31 11:03:50,892:INFO:create_model() successfully completed......................................
2022-10-31 11:03:51,033:ERROR:create_model() for GradientBoostingRegressor(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:51,033:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:51,033:INFO:Initializing Light Gradient Boosting Machine
2022-10-31 11:03:51,033:INFO:Total runtime is 2.300712589422862 minutes
2022-10-31 11:03:51,033:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:51,033:INFO:Initializing create_model()
2022-10-31 11:03:51,033:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:51,033:INFO:Checking exceptions
2022-10-31 11:03:51,033:INFO:Importing libraries
2022-10-31 11:03:51,033:INFO:Copying training dataset
2022-10-31 11:03:51,033:INFO:Defining folds
2022-10-31 11:03:51,033:INFO:Declaring metric variables
2022-10-31 11:03:51,033:INFO:Importing untrained model
2022-10-31 11:03:51,033:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-31 11:03:51,033:INFO:Starting cross validation
2022-10-31 11:03:51,033:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:53,600:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.21887257e-01 -8.54925327e-02 -7.94643625e-02 -7.10508369e-02
 -6.99412922e-02 -5.74292095e-02 -5.27005164e-02 -4.91852114e-02
 -4.88294324e-02 -3.90512065e-02 -3.77164195e-02 -2.67805768e-02
 -2.51889256e-02 -2.06296099e-02 -1.92713465e-02 -8.73136256e-03
 -7.57706659e-03 -6.95111688e-03 -6.64392604e-03 -5.06904124e-03
 -1.63313582e-03 -1.25431604e-03  5.37726418e-04  1.28110038e-03
  3.93629500e-03  5.42299364e-03  8.72611566e-03  1.07218137e-02
  1.19494881e-02  1.19617415e-02  1.67100089e-02  1.76213996e-02
  1.81177393e-02  1.81559136e-02  1.87126846e-02  1.97415339e-02
  2.37042361e-02  2.51373730e-02  2.60034500e-02  2.89717534e-02
  2.99427756e-02  3.00799325e-02  3.03224544e-02  3.06479601e-02
  3.17465125e-02  3.31008046e-02  3.36882606e-02  3.51025143e-02
  3.53020185e-02  3.62235216e-02  3.85476390e-02  3.95209414e-02
  4.07695707e-02  4.45092611e-02  4.73443825e-02  4.77975914e-02
  4.83884911e-02  4.84409461e-02  5.08408247e-02  5.09840957e-02
  5.18854207e-02  5.28489176e-02  5.42352865e-02  5.67334022e-02
  5.79265763e-02  5.94088341e-02  6.21762948e-02  6.25423413e-02
  6.96163062e-02  6.96768557e-02  6.97504962e-02  7.06138319e-02
  7.11925878e-02  7.25806330e-02  7.27792066e-02  7.30663930e-02
  7.56700718e-02  7.59041241e-02  7.61991104e-02  7.92695026e-02
  8.08682929e-02  8.08701028e-02  8.19211154e-02  8.20990120e-02
  8.32510687e-02  8.39420273e-02  8.51541723e-02  8.57314488e-02
  8.58593004e-02  8.59176565e-02  8.68789814e-02  8.73381928e-02
  8.78975345e-02  8.92261080e-02  9.40785994e-02  9.86734113e-02
  1.00465960e-01  1.02047824e-01  1.03561452e-01  1.04915807e-01
  1.05841843e-01  1.09427804e-01  1.12662049e-01  1.14876114e-01
  1.16379101e-01  1.16390103e-01  1.17530324e-01  1.19713863e-01
  1.26524596e-01  1.28618877e-01  1.31591015e-01  1.36185392e-01
  1.36933629e-01  1.38251005e-01  1.38767415e-01  1.38902857e-01
  1.38951161e-01  1.42618225e-01  1.44392048e-01  1.44590369e-01
  1.45118888e-01  1.45943115e-01  1.46125717e-01  1.46493675e-01
  1.47217973e-01  1.47680471e-01  1.47837123e-01  1.48389578e-01
  1.51401893e-01  1.52225023e-01  1.53129226e-01  1.54773404e-01
  1.56857426e-01  1.57011945e-01  1.57142822e-01  1.57207710e-01
  1.59157712e-01  1.60565973e-01  1.61924638e-01  1.61978890e-01
  1.62048999e-01  1.62519499e-01  1.63196340e-01  1.66463401e-01
  1.67484752e-01  1.67585005e-01  1.67695779e-01  1.68934018e-01
  1.69736991e-01  1.73533357e-01  1.74434611e-01  1.75624935e-01
  1.77084426e-01  1.83196191e-01  1.83235154e-01  1.84570529e-01
  1.84884702e-01  1.85693098e-01  1.85754890e-01  1.86686072e-01
  1.87873524e-01  1.89571423e-01  1.89741904e-01  1.90333721e-01
  1.92590247e-01  1.95096383e-01  1.95140716e-01  1.96300910e-01
  1.96893178e-01  1.99839719e-01  1.99969141e-01  2.00819805e-01
  2.01647505e-01  2.01778677e-01  2.02580515e-01  2.02800365e-01
  2.03220518e-01  2.05188676e-01  2.05924134e-01  2.06578634e-01
  2.07838235e-01  2.11430764e-01  2.13937749e-01  2.18924599e-01
  2.19633207e-01  2.23107910e-01  2.23952545e-01  2.27925150e-01
  2.28772016e-01  2.30842587e-01  2.33612488e-01  2.36248119e-01
  2.36606764e-01  2.41464971e-01  2.43453033e-01  2.44745863e-01
  2.46947581e-01  2.46979457e-01  2.48882377e-01  2.51456443e-01
  2.55064640e-01  2.56153052e-01  2.58268529e-01  2.60024932e-01
  2.62018020e-01  2.64194988e-01  2.65005064e-01  2.65488940e-01
  2.65578772e-01  2.66670063e-01  2.67049127e-01  2.67148363e-01
  2.68851399e-01  2.68952281e-01  2.71999081e-01  2.79080617e-01
  2.85747208e-01  2.89068290e-01  2.96863489e-01  2.98238893e-01
  2.99195544e-01  3.00741693e-01  3.02475537e-01  3.03615134e-01
  3.03682410e-01  3.05438999e-01  3.08323038e-01  3.13297524e-01
  3.23643921e-01  3.24649031e-01  3.40753664e-01  3.45608718e-01
  3.46081730e-01  3.47497690e-01  3.48106564e-01  3.49816952e-01
  3.49977548e-01  3.51943604e-01  3.52750195e-01  3.56341444e-01
  3.56364109e-01  3.59951459e-01  3.61673042e-01  3.66307072e-01
  3.66359365e-01  3.66638320e-01  3.69794246e-01  3.70996061e-01
  3.71044751e-01  3.74648975e-01  3.79465098e-01  3.83101407e-01
  3.91600661e-01  3.91680438e-01  3.94412123e-01  3.94966762e-01
  3.96541818e-01  3.97100066e-01  3.99838132e-01  3.99931157e-01
  4.01946072e-01  4.02970824e-01  4.06617964e-01  4.07408404e-01
  4.07742422e-01  4.09669216e-01  4.18104915e-01  4.29495434e-01
  4.33018300e-01  4.33727708e-01  4.34806241e-01  4.46827860e-01
  4.53891593e-01  4.55190690e-01  4.56780346e-01  4.60824441e-01
  4.62177339e-01  4.70316529e-01  4.85133172e-01  4.89938946e-01
  4.93655956e-01  4.94555247e-01  4.98699052e-01  5.02160753e-01
  5.03232404e-01  5.05715648e-01  5.10649018e-01  5.14472952e-01
  5.14595044e-01  5.15945492e-01  5.17960048e-01  5.18795085e-01
  5.28322009e-01  5.29634650e-01  5.30211083e-01  5.30761830e-01
  5.31954108e-01  5.33201841e-01  5.35737756e-01  5.35919807e-01
  5.36147846e-01  5.37874562e-01  5.38085873e-01  5.38844357e-01
  5.45089204e-01  5.46765451e-01  5.47793944e-01  5.50549130e-01
  5.53016944e-01  5.53516074e-01  5.56501768e-01  5.57553549e-01
  5.58673278e-01  5.60315900e-01  5.61254233e-01  5.63233175e-01
  5.64783182e-01  5.65657604e-01  5.65875462e-01  5.65901514e-01
  5.66625046e-01  5.68890512e-01  5.69512588e-01  5.70047798e-01
  5.70837727e-01  5.72108498e-01  5.72994831e-01  5.76334188e-01
  5.78996825e-01  5.79586030e-01  5.81017186e-01  5.84283531e-01
  5.84422632e-01  5.87783696e-01  5.89742015e-01  5.96376819e-01
  6.00025575e-01  6.03357771e-01  6.06360712e-01  6.07044308e-01
  6.10385075e-01  6.10766756e-01  6.11023321e-01  6.14570631e-01
  6.16454386e-01  6.18090996e-01  6.18894363e-01  6.21755013e-01
  6.22684265e-01  6.23082840e-01  6.28447501e-01  6.38323474e-01
  6.38645333e-01  6.41219274e-01  6.42168521e-01  6.42762121e-01
  6.44251370e-01  6.44332715e-01  6.45988952e-01  6.46060559e-01
  6.49214524e-01  6.52482990e-01  6.53144635e-01  6.54691120e-01
  6.55697487e-01  6.58980920e-01  6.60965521e-01  6.61857121e-01
  6.62555934e-01  6.66611436e-01  6.70156809e-01  6.71029862e-01
  6.73535523e-01  6.74112918e-01  6.74240822e-01  6.78330576e-01
  6.80243631e-01  6.80955326e-01  6.82172021e-01  6.83576787e-01
  6.83633895e-01  6.84708146e-01  6.84845714e-01  6.87044751e-01
  6.92285520e-01  6.98225028e-01  7.00724388e-01  7.07240355e-01
  7.09134365e-01  7.14730872e-01  7.18341453e-01  7.32138897e-01
  7.32667565e-01  7.33940529e-01  7.36178775e-01  7.37908334e-01
  7.38717615e-01  7.51997236e-01  7.60703017e-01  7.63819462e-01
  7.67259347e-01  7.69318368e-01  7.69608073e-01  7.75821407e-01
  7.75868010e-01  7.76209867e-01  7.76979105e-01  7.80281276e-01
  7.81030656e-01  7.81579297e-01  7.83984903e-01  7.84096743e-01
  7.95725161e-01  7.98103863e-01  8.22730404e-01  8.23617359e-01
  8.24850821e-01  8.28287644e-01  8.29897254e-01  8.46139138e-01
  8.48350299e-01  8.50481975e-01  8.50485823e-01  8.51241110e-01
  8.51974353e-01  8.55301030e-01  8.57040236e-01  8.66474959e-01
  8.76457618e-01  8.76936714e-01  8.81331269e-01  8.83692315e-01
  8.84920268e-01  8.87699857e-01  8.90313482e-01  8.93891260e-01
  8.96793807e-01  8.97378934e-01  8.97491806e-01  8.97606974e-01
  8.97651671e-01  8.98230384e-01  8.99229364e-01  8.99763961e-01
  9.00520374e-01  9.01336940e-01  9.02246525e-01  9.03695476e-01
  9.03814675e-01  9.04855115e-01  9.07519031e-01  9.07651637e-01
  9.08155253e-01  9.09033966e-01  9.12987417e-01  9.13030387e-01
  9.13674710e-01  9.15612936e-01  9.15617000e-01  9.16462721e-01
  9.17460607e-01  9.18592027e-01  9.20905797e-01  9.29693521e-01
  9.33878066e-01  9.34716465e-01  9.36306606e-01  9.40942537e-01
  9.42195048e-01  9.43548406e-01  9.43724941e-01  9.50961105e-01
  9.51775703e-01  9.52686369e-01  9.53115042e-01  9.53549961e-01
  9.54794866e-01  9.55965540e-01  9.56593466e-01  9.57325129e-01
  9.57364644e-01  9.60516325e-01  9.61054300e-01  9.61388250e-01
  9.62546182e-01  9.63524713e-01  9.64491107e-01  9.67255243e-01
  9.67370411e-01  9.68039996e-01  9.69190377e-01  9.69618353e-01
  9.70255142e-01  9.71813662e-01  9.73848147e-01  9.74266036e-01
  9.76581911e-01  9.78542596e-01  9.80187816e-01  9.81391412e-01
  9.81816225e-01  9.83147294e-01  9.83876695e-01  9.87852565e-01
  9.88594455e-01  9.90730918e-01  9.90789234e-01  9.91173687e-01
  9.91202589e-01  9.91703169e-01  9.92226641e-01  9.92516335e-01
  9.93126475e-01  9.94461448e-01  9.96719538e-01  9.96824558e-01
  9.98117749e-01  9.98269474e-01  9.98315906e-01  1.00076842e+00
  1.00133642e+00  1.00335271e+00  1.00562506e+00  1.00707292e+00
  1.00830492e+00  1.01097204e+00  1.01183984e+00  1.01722230e+00
  1.01749136e+00  1.03024085e+00  1.03127983e+00  1.03320575e+00
  1.03669154e+00  1.04056936e+00  1.04319791e+00  1.04630663e+00
  1.05434131e+00  1.05653575e+00  1.05950887e+00  1.06095856e+00
  1.06212722e+00  1.06511872e+00  1.06751861e+00  1.06925268e+00
  1.07851093e+00  1.08522957e+00  1.09232336e+00  1.18084279e+00]

  UserWarning,

2022-10-31 11:03:53,647:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.61166922e-01 -1.41113584e-01 -1.22745389e-01 -1.20433443e-01
 -1.00095948e-01 -9.75135260e-02 -7.81162300e-02 -7.37391203e-02
 -6.52813885e-02 -6.20441402e-02 -5.97366163e-02 -5.06357554e-02
 -4.26616753e-02 -3.87042253e-02 -2.57515135e-02 -2.51958326e-02
 -1.93549480e-02 -1.89445948e-02 -1.68601966e-02 -1.34979611e-02
 -6.60028747e-03 -5.63543706e-03 -4.12350534e-03 -3.69959011e-03
 -1.30906181e-03 -1.15040174e-03 -3.42969320e-04  8.64419205e-04
  5.50760500e-03  6.09589585e-03  6.13436231e-03  9.45348271e-03
  1.18005937e-02  1.29632979e-02  1.32799825e-02  1.56452891e-02
  1.58848062e-02  1.70105445e-02  1.78271072e-02  1.94530237e-02
  2.15320510e-02  2.17290143e-02  2.22288873e-02  2.28106632e-02
  2.46455227e-02  2.58938917e-02  2.70994576e-02  2.73988834e-02
  3.00267743e-02  3.25688392e-02  3.40942672e-02  3.42727105e-02
  3.53589462e-02  3.70802190e-02  3.88839793e-02  3.93916906e-02
  3.96677430e-02  3.99931547e-02  4.14616807e-02  4.18074258e-02
  4.20001715e-02  4.30048464e-02  4.31613122e-02  4.62964647e-02
  4.83877197e-02  4.87139073e-02  4.95485842e-02  5.02447042e-02
  5.14471709e-02  5.46611020e-02  5.49620376e-02  5.64847099e-02
  5.64854022e-02  5.65775815e-02  5.70178980e-02  5.91849422e-02
  6.02414060e-02  6.16308256e-02  6.17945949e-02  6.68779811e-02
  6.80740250e-02  6.82800959e-02  6.87126847e-02  6.90928243e-02
  7.03550697e-02  7.08645194e-02  7.17039898e-02  7.20767166e-02
  7.33932630e-02  7.55380371e-02  7.62856033e-02  7.64790337e-02
  7.80350782e-02  7.96859910e-02  8.22946753e-02  8.23666321e-02
  8.41845075e-02  8.48088996e-02  8.66005692e-02  8.67510795e-02
  8.81924305e-02  8.83898449e-02  8.88123650e-02  8.95038464e-02
  9.07869996e-02  9.14709105e-02  9.16674423e-02  9.38887819e-02
  9.44224004e-02  9.78645937e-02  9.87377733e-02  1.01734100e-01
  1.02000663e-01  1.03315119e-01  1.03465933e-01  1.04585824e-01
  1.04882279e-01  1.05076421e-01  1.05897137e-01  1.06373677e-01
  1.07338790e-01  1.07511906e-01  1.08343883e-01  1.09293925e-01
  1.09304760e-01  1.13285699e-01  1.13473295e-01  1.15818453e-01
  1.16244264e-01  1.16497079e-01  1.17950315e-01  1.21051091e-01
  1.22298594e-01  1.23980901e-01  1.24716009e-01  1.25204409e-01
  1.25455363e-01  1.25466880e-01  1.26127963e-01  1.26863166e-01
  1.27950663e-01  1.28974009e-01  1.29166204e-01  1.29534844e-01
  1.30104547e-01  1.30397335e-01  1.30881986e-01  1.32395202e-01
  1.33125519e-01  1.34497849e-01  1.34707480e-01  1.36961917e-01
  1.37271663e-01  1.37517700e-01  1.38736713e-01  1.40242117e-01
  1.40500563e-01  1.43358091e-01  1.43891701e-01  1.45003794e-01
  1.45130374e-01  1.45427278e-01  1.45887488e-01  1.53498190e-01
  1.54982938e-01  1.57947351e-01  1.59053999e-01  1.63725042e-01
  1.63791030e-01  1.65261799e-01  1.65298637e-01  1.66635603e-01
  1.70629693e-01  1.72400343e-01  1.74076402e-01  1.75140544e-01
  1.75217104e-01  1.77506249e-01  1.79139864e-01  1.80366209e-01
  1.82051928e-01  1.82359742e-01  1.87451989e-01  1.88209559e-01
  1.88824173e-01  1.89210036e-01  1.89253185e-01  1.89298382e-01
  1.89323956e-01  1.89791720e-01  1.93024717e-01  1.93061550e-01
  1.93737404e-01  1.97521997e-01  2.00282248e-01  2.05651821e-01
  2.07722368e-01  2.08593544e-01  2.12093664e-01  2.12114382e-01
  2.12401804e-01  2.15464071e-01  2.16686615e-01  2.21200311e-01
  2.21869304e-01  2.22131123e-01  2.22678797e-01  2.26226131e-01
  2.27376093e-01  2.30236218e-01  2.31759293e-01  2.32729042e-01
  2.35647881e-01  2.37494014e-01  2.43096988e-01  2.44356305e-01
  2.45401471e-01  2.57108367e-01  2.59777942e-01  2.62430835e-01
  2.62862058e-01  2.65086122e-01  2.67323193e-01  2.77935681e-01
  2.78438231e-01  2.78972421e-01  2.79043637e-01  2.83520681e-01
  2.84238482e-01  2.84966707e-01  2.85592775e-01  2.88072487e-01
  2.91673619e-01  2.91981899e-01  2.92116752e-01  2.92253051e-01
  2.95086677e-01  2.98904164e-01  2.98937531e-01  3.00374472e-01
  3.05841464e-01  3.06182313e-01  3.06926872e-01  3.11648503e-01
  3.12186069e-01  3.13126892e-01  3.19487611e-01  3.20617313e-01
  3.23522427e-01  3.28867418e-01  3.31836580e-01  3.32821629e-01
  3.33093583e-01  3.33657138e-01  3.37639046e-01  3.43786914e-01
  3.54252336e-01  3.56549846e-01  3.66448193e-01  3.66537318e-01
  3.70990084e-01  3.72546652e-01  3.85786615e-01  4.02409371e-01
  4.06893165e-01  4.07975930e-01  4.17193306e-01  4.19380681e-01
  4.25725282e-01  4.28978989e-01  4.30376604e-01  4.31009041e-01
  4.31047992e-01  4.32065057e-01  4.33646955e-01  4.41262070e-01
  4.46354700e-01  4.51491282e-01  4.53837750e-01  4.59045312e-01
  4.61087834e-01  4.63196183e-01  4.65696482e-01  4.69804686e-01
  4.75058808e-01  4.75676142e-01  4.76864288e-01  4.77537264e-01
  4.78039884e-01  4.78634218e-01  4.83765698e-01  4.84202813e-01
  4.84977906e-01  4.87534369e-01  4.91896284e-01  4.91943979e-01
  4.96456377e-01  4.98260247e-01  5.01079125e-01  5.04589961e-01
  5.07563221e-01  5.08622631e-01  5.09483825e-01  5.10715741e-01
  5.11238164e-01  5.14567582e-01  5.20770741e-01  5.26590399e-01
  5.27587812e-01  5.28581824e-01  5.33548202e-01  5.34503983e-01
  5.37091055e-01  5.38999326e-01  5.44846717e-01  5.45853493e-01
  5.45950091e-01  5.54082965e-01  5.56245984e-01  5.56557978e-01
  5.58208004e-01  5.64261634e-01  5.64872016e-01  5.64944422e-01
  5.66411709e-01  5.67020067e-01  5.69751464e-01  5.73045258e-01
  5.73401924e-01  5.75237419e-01  5.79001932e-01  5.79536952e-01
  5.80777469e-01  5.82748259e-01  5.83064015e-01  5.87982080e-01
  5.91226264e-01  5.95920790e-01  5.97541093e-01  5.99721686e-01
  6.09737333e-01  6.12239331e-01  6.13026018e-01  6.16577294e-01
  6.17917045e-01  6.20063633e-01  6.22148832e-01  6.23695077e-01
  6.23925704e-01  6.26922595e-01  6.31347790e-01  6.32129409e-01
  6.32454734e-01  6.34977466e-01  6.37645253e-01  6.40615287e-01
  6.40786151e-01  6.42231859e-01  6.48840947e-01  6.55077968e-01
  6.58043696e-01  6.61132287e-01  6.61201688e-01  6.63386361e-01
  6.64562430e-01  6.65521350e-01  6.68301530e-01  6.68613705e-01
  6.71653817e-01  6.72925520e-01  6.73534929e-01  6.77403703e-01
  6.77469497e-01  6.79584404e-01  6.85114286e-01  6.85389015e-01
  6.91635007e-01  6.92724568e-01  6.93311483e-01  6.94179789e-01
  7.05488706e-01  7.06760266e-01  7.23843731e-01  7.24862210e-01
  7.24864531e-01  7.25370427e-01  7.25965401e-01  7.29886194e-01
  7.39369188e-01  7.39453391e-01  7.39754963e-01  7.43389512e-01
  7.43428268e-01  7.45796317e-01  7.48946686e-01  7.49389449e-01
  7.49696462e-01  7.56624712e-01  7.59883913e-01  7.69556949e-01
  7.70046879e-01  7.71442616e-01  7.73007897e-01  7.73397894e-01
  7.73858600e-01  7.78851505e-01  7.97316277e-01  7.98092506e-01
  8.08988950e-01  8.09413288e-01  8.11797315e-01  8.15559292e-01
  8.16570612e-01  8.24765627e-01  8.25102967e-01  8.25408221e-01
  8.28355608e-01  8.28459381e-01  8.29865768e-01  8.33963179e-01
  8.36567442e-01  8.36697041e-01  8.41234525e-01  8.42880863e-01
  8.48476920e-01  8.53494849e-01  8.54420616e-01  8.56578401e-01
  8.62754531e-01  8.64255052e-01  8.65608176e-01  8.69237850e-01
  8.70344331e-01  8.76843391e-01  8.77121254e-01  8.85047096e-01
  8.87593783e-01  8.92153239e-01  8.95470364e-01  8.96449194e-01
  8.97844831e-01  8.98835741e-01  8.99166939e-01  9.01645572e-01
  9.05957193e-01  9.06293796e-01  9.06346267e-01  9.10502599e-01
  9.13507967e-01  9.16557738e-01  9.20975904e-01  9.21361970e-01
  9.23665494e-01  9.24866718e-01  9.26487200e-01  9.26731528e-01
  9.29376976e-01  9.29715981e-01  9.30593516e-01  9.33021774e-01
  9.33307310e-01  9.34947321e-01  9.35699563e-01  9.36276725e-01
  9.38058055e-01  9.40443040e-01  9.48791396e-01  9.49364585e-01
  9.50764796e-01  9.53430097e-01  9.53687225e-01  9.54767713e-01
  9.55793526e-01  9.59794756e-01  9.64708731e-01  9.66588177e-01
  9.67255418e-01  9.67403499e-01  9.68150840e-01  9.69235476e-01
  9.70263481e-01  9.70930023e-01  9.71791739e-01  9.76202851e-01
  9.78348748e-01  9.79755814e-01  9.80938033e-01  9.81091878e-01
  9.82095464e-01  9.82592827e-01  9.83314133e-01  9.83468692e-01
  9.83825547e-01  9.84441854e-01  9.86015379e-01  9.86845160e-01
  9.87458848e-01  9.87726651e-01  9.87995437e-01  9.88226038e-01
  9.89823564e-01  9.89885073e-01  9.90620731e-01  9.91515073e-01
  9.91923143e-01  9.92608865e-01  9.93373353e-01  9.95087075e-01
  9.96292913e-01  9.96617957e-01  9.99788193e-01  1.00203979e+00
  1.00265004e+00  1.00451571e+00  1.00470891e+00  1.00577384e+00
  1.00960341e+00  1.01110782e+00  1.01127463e+00  1.01378716e+00
  1.01748069e+00  1.02062774e+00  1.03042643e+00  1.04820956e+00
  1.06821071e+00  1.07567685e+00  1.07658236e+00  1.11238069e+00]

  UserWarning,

2022-10-31 11:03:53,689:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.37596419e-01 -1.94256336e-01 -1.12023306e-01 -1.10471550e-01
 -8.02354037e-02 -7.46557621e-02 -7.29400346e-02 -5.02265231e-02
 -4.98018070e-02 -4.88968604e-02 -4.87589048e-02 -3.52019601e-02
 -3.47808944e-02 -2.21765949e-02 -1.71202922e-02 -1.65342595e-02
 -1.48395215e-02 -1.14935059e-02 -1.11339409e-02 -6.42626831e-03
 -2.51185150e-03 -1.75283714e-03 -3.76571320e-04  3.68992462e-03
  1.48199726e-02  1.48308981e-02  1.56338846e-02  1.72519115e-02
  1.96802142e-02  2.30002197e-02  2.42667965e-02  2.89406562e-02
  3.27934830e-02  3.36371562e-02  3.63848667e-02  3.84501156e-02
  3.85283477e-02  4.11921111e-02  4.27705629e-02  4.33800212e-02
  4.47036674e-02  4.52421779e-02  4.52424471e-02  4.54885472e-02
  4.56770695e-02  4.84379658e-02  4.89620914e-02  5.24547209e-02
  5.40338655e-02  5.45208511e-02  5.45667046e-02  5.65716748e-02
  5.81017618e-02  6.37834986e-02  6.63498353e-02  6.78703009e-02
  6.88981842e-02  7.24853792e-02  7.39871802e-02  7.43862445e-02
  7.45928999e-02  7.63693839e-02  7.72370876e-02  7.89345578e-02
  8.04858884e-02  8.05021224e-02  8.06632887e-02  8.11406947e-02
  8.12964858e-02  8.17984910e-02  8.22686521e-02  8.26359081e-02
  8.48272710e-02  8.58043465e-02  8.71859161e-02  8.74732908e-02
  8.76692261e-02  8.95257898e-02  8.96245760e-02  9.19327820e-02
  9.24451097e-02  9.35649451e-02  9.40696268e-02  9.47194435e-02
  9.49935107e-02  9.53621166e-02  9.53815289e-02  9.60087653e-02
  9.60113229e-02  9.60444873e-02  9.65046360e-02  9.74869762e-02
  1.00501831e-01  1.01506517e-01  1.02309743e-01  1.03925354e-01
  1.04347941e-01  1.06270281e-01  1.06304757e-01  1.06910393e-01
  1.07567044e-01  1.11501702e-01  1.14987997e-01  1.15187004e-01
  1.15344534e-01  1.15985138e-01  1.16722470e-01  1.17296224e-01
  1.17936947e-01  1.18771837e-01  1.21347924e-01  1.23468972e-01
  1.24653867e-01  1.26347970e-01  1.27633721e-01  1.27845475e-01
  1.27959713e-01  1.31497115e-01  1.32973124e-01  1.33384107e-01
  1.33802671e-01  1.33865845e-01  1.34448422e-01  1.36827746e-01
  1.38144906e-01  1.38235779e-01  1.41328287e-01  1.47492076e-01
  1.47538528e-01  1.47573248e-01  1.48345919e-01  1.51264399e-01
  1.53597473e-01  1.54586926e-01  1.55418456e-01  1.57976084e-01
  1.63401264e-01  1.64245430e-01  1.66348657e-01  1.67310401e-01
  1.69227742e-01  1.71138601e-01  1.73599098e-01  1.75646282e-01
  1.75819030e-01  1.77599450e-01  1.78363860e-01  1.80553948e-01
  1.82002818e-01  1.82260398e-01  1.82905651e-01  1.86638649e-01
  1.86684261e-01  1.87218307e-01  1.89298583e-01  1.91094378e-01
  1.91429438e-01  1.93074841e-01  1.93371702e-01  1.93750008e-01
  1.94383446e-01  1.95057832e-01  1.95123010e-01  2.00328957e-01
  2.00964074e-01  2.02213685e-01  2.02345571e-01  2.04373178e-01
  2.04472986e-01  2.05042849e-01  2.08240177e-01  2.08831137e-01
  2.10036137e-01  2.11610288e-01  2.17528523e-01  2.18045067e-01
  2.18082732e-01  2.19526619e-01  2.22755275e-01  2.23260729e-01
  2.23899320e-01  2.23902225e-01  2.26632144e-01  2.26852704e-01
  2.27955517e-01  2.30600547e-01  2.30849832e-01  2.31105217e-01
  2.33492890e-01  2.35398227e-01  2.36091300e-01  2.39394558e-01
  2.42232865e-01  2.42554909e-01  2.43141893e-01  2.44061582e-01
  2.48016979e-01  2.53374155e-01  2.54609969e-01  2.57854033e-01
  2.58117695e-01  2.60100085e-01  2.61022766e-01  2.61134622e-01
  2.62064046e-01  2.66292245e-01  2.66479342e-01  2.66849358e-01
  2.68914288e-01  2.70441958e-01  2.70448634e-01  2.71309973e-01
  2.78121290e-01  2.81043686e-01  2.84001591e-01  2.84133167e-01
  2.85628651e-01  2.86436976e-01  2.88638832e-01  2.88823978e-01
  2.91800978e-01  2.92798000e-01  2.95317393e-01  2.97081778e-01
  2.99840131e-01  3.06168454e-01  3.07355157e-01  3.08791917e-01
  3.09817607e-01  3.10628211e-01  3.11091725e-01  3.11522985e-01
  3.13309069e-01  3.15448638e-01  3.16415147e-01  3.17007780e-01
  3.18442815e-01  3.19258316e-01  3.20882965e-01  3.24903438e-01
  3.27778148e-01  3.30288847e-01  3.33673278e-01  3.38285333e-01
  3.39447918e-01  3.40167715e-01  3.42326053e-01  3.43103623e-01
  3.47314793e-01  3.49062954e-01  3.49428959e-01  3.56037489e-01
  3.65470661e-01  3.68140756e-01  3.71481663e-01  3.72507987e-01
  3.72581788e-01  3.74053080e-01  3.76196430e-01  3.77390033e-01
  3.79506194e-01  3.79970861e-01  3.86377631e-01  3.88654701e-01
  3.92688728e-01  3.93542074e-01  3.93734228e-01  3.94552072e-01
  3.97492564e-01  4.01595479e-01  4.04651155e-01  4.10005722e-01
  4.19656396e-01  4.26774873e-01  4.27222227e-01  4.28438096e-01
  4.29911312e-01  4.32572477e-01  4.34338958e-01  4.34888284e-01
  4.35826152e-01  4.43326555e-01  4.43912895e-01  4.45853864e-01
  4.50210741e-01  4.55136429e-01  4.55426158e-01  4.59905772e-01
  4.61449198e-01  4.63755195e-01  4.64514593e-01  4.65508753e-01
  4.68583363e-01  4.70875689e-01  4.74672808e-01  4.75170790e-01
  4.83810955e-01  4.85014873e-01  4.88396612e-01  4.89242056e-01
  4.90134546e-01  4.93704888e-01  4.97850551e-01  4.99447895e-01
  5.03806351e-01  5.08051294e-01  5.09498285e-01  5.15135711e-01
  5.15362482e-01  5.15477524e-01  5.17390154e-01  5.21717684e-01
  5.25178364e-01  5.26744173e-01  5.26800393e-01  5.27300911e-01
  5.29233961e-01  5.29525276e-01  5.34222172e-01  5.34590148e-01
  5.36514342e-01  5.41217274e-01  5.43130948e-01  5.44560737e-01
  5.45435063e-01  5.45527419e-01  5.47512725e-01  5.47768476e-01
  5.49783478e-01  5.49804603e-01  5.53877176e-01  5.55187105e-01
  5.55392926e-01  5.64044340e-01  5.64533323e-01  5.65319893e-01
  5.67866778e-01  5.74077401e-01  5.78279280e-01  5.83615323e-01
  5.83740562e-01  5.88292088e-01  5.92754743e-01  5.95084977e-01
  5.98546453e-01  6.04042493e-01  6.05123151e-01  6.12086990e-01
  6.13159326e-01  6.14569568e-01  6.15861045e-01  6.16126165e-01
  6.18596828e-01  6.18842427e-01  6.19728737e-01  6.21036556e-01
  6.25119587e-01  6.28566341e-01  6.32999004e-01  6.33273326e-01
  6.40665237e-01  6.43310034e-01  6.46139954e-01  6.47101992e-01
  6.51286223e-01  6.52945834e-01  6.54434234e-01  6.57453917e-01
  6.63977901e-01  6.64097661e-01  6.64313090e-01  6.64974024e-01
  6.66857794e-01  6.68691233e-01  6.69190141e-01  6.69469109e-01
  6.71073599e-01  6.76628732e-01  6.77789487e-01  6.78219373e-01
  6.78273314e-01  6.80560781e-01  6.82348332e-01  6.85064163e-01
  6.85634958e-01  6.87079165e-01  6.90208217e-01  6.93748325e-01
  6.94265922e-01  6.95565184e-01  6.95620531e-01  6.95898837e-01
  6.96501615e-01  6.99679251e-01  7.00317357e-01  7.01332819e-01
  7.01396090e-01  7.05771496e-01  7.08316845e-01  7.12307136e-01
  7.12408388e-01  7.13836385e-01  7.18526752e-01  7.19933030e-01
  7.20025051e-01  7.23479188e-01  7.24718154e-01  7.25380645e-01
  7.30379778e-01  7.34294793e-01  7.34315958e-01  7.35335516e-01
  7.36597964e-01  7.39454604e-01  7.39556297e-01  7.39639539e-01
  7.42821770e-01  7.46293730e-01  7.48362149e-01  7.50080255e-01
  7.51072293e-01  7.55213752e-01  7.56923129e-01  7.57051876e-01
  7.70108267e-01  7.71652272e-01  7.77952037e-01  7.83894598e-01
  7.84493392e-01  7.86303699e-01  7.89710285e-01  7.90204850e-01
  7.97034197e-01  7.98474375e-01  7.98590167e-01  7.99187873e-01
  8.00802803e-01  8.01723794e-01  8.06604661e-01  8.10367710e-01
  8.38076253e-01  8.40906581e-01  8.48821468e-01  8.51488672e-01
  8.54951835e-01  8.56621714e-01  8.65096363e-01  8.65684449e-01
  8.70038295e-01  8.74074002e-01  8.77254237e-01  8.80010296e-01
  8.82197396e-01  8.90046499e-01  8.97718703e-01  8.98168441e-01
  8.98677745e-01  9.00501872e-01  9.00906803e-01  9.04896316e-01
  9.05402158e-01  9.08531253e-01  9.09867316e-01  9.10515550e-01
  9.12291226e-01  9.13615209e-01  9.19037030e-01  9.19629243e-01
  9.21366089e-01  9.21601656e-01  9.22123873e-01  9.23902866e-01
  9.26799102e-01  9.32341353e-01  9.32438098e-01  9.33233290e-01
  9.36158284e-01  9.36945094e-01  9.39407022e-01  9.41852480e-01
  9.42326408e-01  9.43982730e-01  9.44807743e-01  9.45107770e-01
  9.46378912e-01  9.46526656e-01  9.47509085e-01  9.50972211e-01
  9.53756454e-01  9.54805741e-01  9.58418711e-01  9.58630153e-01
  9.65592226e-01  9.67941148e-01  9.68679993e-01  9.68761535e-01
  9.69481399e-01  9.70005206e-01  9.70226895e-01  9.70321834e-01
  9.74135030e-01  9.74249267e-01  9.74532043e-01  9.74539503e-01
  9.75650780e-01  9.75953187e-01  9.76010136e-01  9.78066489e-01
  9.78104338e-01  9.79338366e-01  9.79526937e-01  9.80747796e-01
  9.83473097e-01  9.85000122e-01  9.85489182e-01  9.85803543e-01
  9.88140079e-01  9.88256794e-01  9.89527262e-01  9.89775524e-01
  9.89889760e-01  9.91291274e-01  9.93788312e-01  9.94062301e-01
  9.94533525e-01  9.95004077e-01  9.95161361e-01  9.95698717e-01
  1.00386135e+00  1.00453077e+00  1.00524474e+00  1.00596027e+00
  1.01137236e+00  1.01327134e+00  1.01604588e+00  1.02072696e+00
  1.02082201e+00  1.02133592e+00  1.04292107e+00  1.05277406e+00
  1.08277826e+00  1.11366661e+00  1.11490094e+00  1.16976726e+00
  1.21152181e+00]

  UserWarning,

2022-10-31 11:03:53,705:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.06060264e-01 -5.35373194e-02 -5.16455819e-02 -4.91547597e-02
 -4.76165588e-02 -4.61344090e-02 -3.64640808e-02 -2.97624379e-02
 -2.66602006e-02 -2.27577801e-02 -1.81787048e-02 -1.44066893e-02
 -9.65324552e-03 -6.75955845e-03 -4.68889405e-03 -4.48527152e-03
 -1.70749078e-03 -1.68781741e-03 -4.94941646e-04  6.28116001e-03
  6.29943851e-03  8.52210562e-03  1.18069975e-02  1.47236143e-02
  1.49404489e-02  1.54334142e-02  1.64213997e-02  2.36784259e-02
  2.81978493e-02  2.82096499e-02  2.99106874e-02  3.04126275e-02
  3.13350122e-02  3.58603771e-02  3.72739865e-02  3.74410579e-02
  3.85910616e-02  3.96769280e-02  3.98866148e-02  4.30438082e-02
  4.57722432e-02  4.86877322e-02  4.97914154e-02  5.05212131e-02
  5.15757605e-02  5.17944996e-02  5.39079062e-02  5.52238318e-02
  5.70500331e-02  5.72002033e-02  6.37354478e-02  6.41294355e-02
  6.46418269e-02  6.56781583e-02  6.64502599e-02  6.64999189e-02
  6.75587148e-02  6.94485269e-02  6.97258779e-02  7.05904775e-02
  7.11461361e-02  7.13634787e-02  7.16064177e-02  7.17906282e-02
  7.25231188e-02  7.26413714e-02  7.31552600e-02  7.32960283e-02
  7.36454311e-02  7.57277997e-02  7.57568625e-02  7.58005571e-02
  7.72962927e-02  7.73987850e-02  7.89879833e-02  8.03305953e-02
  8.14767294e-02  8.15901849e-02  8.19250732e-02  8.19513902e-02
  8.56131333e-02  8.61883814e-02  8.64983242e-02  8.77353917e-02
  9.24547102e-02  9.33353528e-02  9.51372514e-02  9.58712113e-02
  9.80508098e-02  9.84926600e-02  1.00698354e-01  1.01848567e-01
  1.02404276e-01  1.04231819e-01  1.04242644e-01  1.04524307e-01
  1.04787548e-01  1.08702177e-01  1.08708364e-01  1.09262659e-01
  1.09723252e-01  1.11054168e-01  1.15165041e-01  1.15523345e-01
  1.15856507e-01  1.18190947e-01  1.22486294e-01  1.26253002e-01
  1.26557011e-01  1.27669562e-01  1.28221202e-01  1.29144592e-01
  1.29886733e-01  1.30063081e-01  1.30418090e-01  1.33620120e-01
  1.39315628e-01  1.39975416e-01  1.40001900e-01  1.40913841e-01
  1.40936053e-01  1.43496587e-01  1.44109875e-01  1.44453269e-01
  1.49510784e-01  1.53225973e-01  1.55839650e-01  1.55910396e-01
  1.60545190e-01  1.60790530e-01  1.61668975e-01  1.64266040e-01
  1.66353805e-01  1.67055443e-01  1.67684332e-01  1.69905666e-01
  1.71816228e-01  1.72141973e-01  1.74257624e-01  1.74852389e-01
  1.75489415e-01  1.76067850e-01  1.77973731e-01  1.80899324e-01
  1.81675455e-01  1.84302359e-01  1.85871849e-01  1.87664787e-01
  1.91215440e-01  1.93088086e-01  1.94228379e-01  1.95016550e-01
  1.95647545e-01  1.95762720e-01  1.96821300e-01  1.98697109e-01
  1.98759831e-01  1.99982702e-01  2.00832943e-01  2.03951093e-01
  2.04724068e-01  2.05877531e-01  2.06855623e-01  2.07462210e-01
  2.08953240e-01  2.09667382e-01  2.11171942e-01  2.13683016e-01
  2.16970209e-01  2.17545186e-01  2.17645548e-01  2.20261775e-01
  2.22422980e-01  2.29139657e-01  2.29398065e-01  2.29829350e-01
  2.32376961e-01  2.32391185e-01  2.39703512e-01  2.40261239e-01
  2.46864858e-01  2.47014500e-01  2.48500254e-01  2.52645015e-01
  2.53618760e-01  2.55012348e-01  2.55062673e-01  2.60494286e-01
  2.60976401e-01  2.61138299e-01  2.62622490e-01  2.64312060e-01
  2.68073184e-01  2.68473631e-01  2.72107971e-01  2.72993872e-01
  2.76208771e-01  2.78909464e-01  2.80149920e-01  2.84329864e-01
  2.88192167e-01  2.89061833e-01  2.89528534e-01  2.90213879e-01
  2.92796694e-01  2.93849187e-01  2.96739385e-01  2.96968296e-01
  2.97478168e-01  2.98183162e-01  2.98275209e-01  3.03528198e-01
  3.04414795e-01  3.04997612e-01  3.05442958e-01  3.06463496e-01
  3.08222325e-01  3.08281526e-01  3.12034506e-01  3.13225715e-01
  3.16538384e-01  3.21056873e-01  3.23807565e-01  3.32553195e-01
  3.32816768e-01  3.33127200e-01  3.33875333e-01  3.35611275e-01
  3.39822166e-01  3.45324227e-01  3.45480905e-01  3.46420104e-01
  3.47478770e-01  3.52856091e-01  3.53332557e-01  3.53997505e-01
  3.58771479e-01  3.71748323e-01  3.73614143e-01  3.77819324e-01
  3.91012484e-01  4.00003578e-01  4.00147713e-01  4.01416086e-01
  4.02007716e-01  4.02814033e-01  4.06270010e-01  4.13078910e-01
  4.13646885e-01  4.15374129e-01  4.22358946e-01  4.24232770e-01
  4.32050219e-01  4.33730431e-01  4.38658414e-01  4.38857864e-01
  4.39874359e-01  4.40269983e-01  4.45968325e-01  4.48872310e-01
  4.49659738e-01  4.51284687e-01  4.52506261e-01  4.57061997e-01
  4.60484475e-01  4.64655418e-01  4.70410901e-01  4.71715551e-01
  4.72903954e-01  4.76422399e-01  4.77976644e-01  4.80086327e-01
  4.83074217e-01  4.84933046e-01  4.93430982e-01  4.97663105e-01
  4.99219919e-01  4.99241678e-01  5.03686710e-01  5.05125631e-01
  5.06799975e-01  5.08676703e-01  5.09084330e-01  5.13323076e-01
  5.15368043e-01  5.16079634e-01  5.17264380e-01  5.23682510e-01
  5.24263629e-01  5.26941639e-01  5.32279395e-01  5.33367852e-01
  5.35670090e-01  5.37633948e-01  5.38981601e-01  5.39095699e-01
  5.41178944e-01  5.45162580e-01  5.45668325e-01  5.46817390e-01
  5.55269565e-01  5.56118074e-01  5.57635748e-01  5.64374327e-01
  5.64715245e-01  5.65243232e-01  5.67198546e-01  5.67607953e-01
  5.67719740e-01  5.68996592e-01  5.69109733e-01  5.69443551e-01
  5.71331445e-01  5.72981702e-01  5.73124543e-01  5.73214748e-01
  5.75113068e-01  5.75204366e-01  5.76396639e-01  5.78446395e-01
  5.80165389e-01  5.81372985e-01  5.82993376e-01  5.84614918e-01
  5.93738281e-01  5.95755517e-01  5.99898556e-01  6.01026592e-01
  6.01156141e-01  6.04618694e-01  6.06295018e-01  6.07589493e-01
  6.08276594e-01  6.10566849e-01  6.11094690e-01  6.14614903e-01
  6.16863026e-01  6.21360656e-01  6.21926693e-01  6.22570685e-01
  6.22703433e-01  6.24151720e-01  6.26487494e-01  6.32004497e-01
  6.32442621e-01  6.35639426e-01  6.37987866e-01  6.40621815e-01
  6.40795569e-01  6.42569915e-01  6.44137698e-01  6.49481681e-01
  6.51906668e-01  6.53129646e-01  6.54768346e-01  6.55881746e-01
  6.59831687e-01  6.60965673e-01  6.62881397e-01  6.63792898e-01
  6.65797185e-01  6.67910743e-01  6.71019560e-01  6.71444824e-01
  6.76210640e-01  6.80356222e-01  6.82141745e-01  6.82463646e-01
  6.83115520e-01  6.89497963e-01  6.90037883e-01  6.91940701e-01
  6.95945776e-01  6.98173025e-01  6.98806956e-01  6.99751297e-01
  7.04691050e-01  7.06591028e-01  7.09189028e-01  7.09949164e-01
  7.12205548e-01  7.16746554e-01  7.18078936e-01  7.19620372e-01
  7.26519153e-01  7.36154080e-01  7.37987160e-01  7.38262644e-01
  7.39932338e-01  7.40179202e-01  7.43878795e-01  7.45377828e-01
  7.49847122e-01  7.52472093e-01  7.59899238e-01  7.63997708e-01
  7.71004193e-01  7.71483478e-01  7.74459979e-01  7.76398209e-01
  7.76768642e-01  7.80807634e-01  7.84061123e-01  7.84871279e-01
  7.91054337e-01  7.94716001e-01  7.95762829e-01  8.08580277e-01
  8.09393368e-01  8.13103238e-01  8.16603921e-01  8.27360455e-01
  8.27562939e-01  8.31244651e-01  8.35220755e-01  8.41044474e-01
  8.43848567e-01  8.44980103e-01  8.50111718e-01  8.51358691e-01
  8.51525562e-01  8.55138282e-01  8.55445979e-01  8.56184207e-01
  8.56673295e-01  8.61019305e-01  8.63035375e-01  8.65103233e-01
  8.66068810e-01  8.66276423e-01  8.68206538e-01  8.72382771e-01
  8.74206515e-01  8.76225495e-01  8.77172443e-01  8.77948559e-01
  8.78965639e-01  8.85401761e-01  8.86057283e-01  8.88302301e-01
  8.90611378e-01  8.91610227e-01  8.91875939e-01  8.96734684e-01
  8.97577219e-01  9.03439179e-01  9.03709505e-01  9.06232698e-01
  9.07780961e-01  9.10499666e-01  9.11491107e-01  9.12498165e-01
  9.16561822e-01  9.19164942e-01  9.19244805e-01  9.19361261e-01
  9.21669343e-01  9.21696169e-01  9.22955359e-01  9.23609577e-01
  9.26876073e-01  9.27964937e-01  9.29209232e-01  9.31826797e-01
  9.31864443e-01  9.32522620e-01  9.32534974e-01  9.32846071e-01
  9.33200885e-01  9.37503962e-01  9.38238679e-01  9.38894249e-01
  9.40223627e-01  9.40954771e-01  9.41414509e-01  9.41511664e-01
  9.41852445e-01  9.43991049e-01  9.46350843e-01  9.47505609e-01
  9.49063455e-01  9.52224544e-01  9.54636781e-01  9.56193177e-01
  9.57377035e-01  9.58216657e-01  9.61143569e-01  9.61895188e-01
  9.63715024e-01  9.64893710e-01  9.67044178e-01  9.67389019e-01
  9.68361666e-01  9.68643367e-01  9.69116748e-01  9.69501260e-01
  9.69736515e-01  9.69789642e-01  9.70838605e-01  9.71155605e-01
  9.72168460e-01  9.75132828e-01  9.75708112e-01  9.76080530e-01
  9.76392464e-01  9.76721842e-01  9.81878370e-01  9.81931150e-01
  9.83448503e-01  9.84796487e-01  9.85737122e-01  9.86239876e-01
  9.88176767e-01  9.88209893e-01  9.88612059e-01  9.89072064e-01
  9.90423249e-01  9.90740929e-01  9.91449159e-01  9.91825330e-01
  9.92168420e-01  9.92909581e-01  9.93696871e-01  9.94134407e-01
  9.94322780e-01  9.95376045e-01  9.97078854e-01  9.99101046e-01
  1.00079156e+00  1.00165734e+00  1.00335807e+00  1.00425629e+00
  1.00564135e+00  1.01343679e+00  1.02070844e+00  1.02595044e+00
  1.02851115e+00  1.04972112e+00  1.05718610e+00  1.06161934e+00
  1.07071977e+00  1.08160698e+00  1.08535898e+00  1.14431396e+00
  1.22998309e+00]

  UserWarning,

2022-10-31 11:03:53,736:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.20084184 -0.13582909 -0.10226986 -0.10036909 -0.09098774 -0.09059253
 -0.08628922 -0.0836738  -0.0808127  -0.07674612 -0.07546329 -0.07247207
 -0.06431126 -0.05585144 -0.05318709 -0.05120556 -0.04777614 -0.04735596
 -0.03722775 -0.03498673 -0.02440395 -0.02421155 -0.02404776 -0.02269296
 -0.02128748 -0.02033924 -0.0172188  -0.01699566 -0.01374913 -0.01336546
 -0.01235795 -0.01124303 -0.00914772 -0.00883171 -0.00448867  0.00851642
  0.00894124  0.01345512  0.01345949  0.01540172  0.01770841  0.02298323
  0.02298661  0.02315321  0.02716823  0.02839223  0.03118983  0.03169949
  0.03264162  0.03286613  0.03332715  0.03396621  0.03439599  0.03558185
  0.03565477  0.03955375  0.03958318  0.03966448  0.04248815  0.04607319
  0.04646773  0.04672136  0.04847932  0.04878869  0.04964849  0.04998703
  0.05311829  0.05506444  0.05635685  0.06405875  0.06435329  0.06550552
  0.06583468  0.06595645  0.06866056  0.07006489  0.07078345  0.07085033
  0.07103561  0.07268326  0.0728734   0.07382165  0.07626766  0.07693135
  0.07928863  0.07954439  0.07977341  0.08261276  0.08270133  0.08398613
  0.08424578  0.08499372  0.08589151  0.08631416  0.08767875  0.08907819
  0.08987432  0.09051004  0.093819    0.09533756  0.09633901  0.09634389
  0.09744675  0.09785885  0.10481158  0.10915674  0.10918438  0.10932514
  0.10951996  0.11058914  0.1113399   0.11145986  0.11147222  0.11173995
  0.11312072  0.11473022  0.1149224   0.11596105  0.11724219  0.11897499
  0.12022455  0.12031238  0.12273842  0.12309052  0.12564098  0.12804294
  0.12824671  0.12988812  0.13086457  0.13275518  0.13341903  0.13518326
  0.13559382  0.13685482  0.1375013   0.13856893  0.13932136  0.13966525
  0.14064965  0.14095256  0.14299177  0.14412337  0.14429821  0.14505984
  0.14574824  0.148037    0.1505814   0.1507785   0.151851    0.15337613
  0.15451207  0.15553607  0.15802573  0.15816351  0.15867822  0.15899926
  0.16412838  0.16453729  0.16471011  0.16541794  0.16834799  0.17014811
  0.17164119  0.17371969  0.17414702  0.17490562  0.17649595  0.18017254
  0.18113004  0.18198946  0.18386012  0.18429637  0.18565655  0.18570541
  0.18744488  0.18800832  0.18949474  0.19119806  0.19183999  0.1954519
  0.20274939  0.20375196  0.20474201  0.20492561  0.20986626  0.20996535
  0.21014229  0.21288978  0.21666746  0.21854546  0.21875454  0.2191792
  0.22064477  0.22158013  0.22200331  0.22374645  0.22571324  0.22695788
  0.22799986  0.22996959  0.23039745  0.230632    0.23103145  0.23124672
  0.23165794  0.23339573  0.24039706  0.24175288  0.24224381  0.24581029
  0.24678262  0.24754999  0.24798054  0.2524196   0.25382753  0.2541651
  0.25661827  0.25842545  0.25968468  0.26048943  0.26164527  0.2641428
  0.26442634  0.26475122  0.26774985  0.27089277  0.27435601  0.27482808
  0.27542346  0.27690793  0.27837141  0.27991579  0.29836587  0.29998091
  0.30552042  0.30658468  0.30966369  0.3127884   0.31518616  0.31655332
  0.32224808  0.32596752  0.32996861  0.33152861  0.34105389  0.34639564
  0.3477012   0.350481    0.35387404  0.35683917  0.36147853  0.36411753
  0.37059159  0.37350295  0.37666195  0.37683358  0.38396658  0.38479349
  0.38600757  0.3961886   0.40446157  0.40568467  0.41037091  0.41348959
  0.41406615  0.41558485  0.41860347  0.42064009  0.42295516  0.42318431
  0.42376637  0.42818998  0.4365891   0.43670839  0.43832339  0.44026625
  0.44408496  0.44584149  0.44680016  0.45234018  0.45732226  0.45948504
  0.45949328  0.4610289   0.46304405  0.46443777  0.46474311  0.46501092
  0.46770953  0.47244328  0.47528726  0.48296276  0.48687337  0.48871185
  0.48884542  0.49179466  0.49439699  0.49474247  0.49990834  0.50180712
  0.50256122  0.51549093  0.5213607   0.524152    0.52756913  0.52958171
  0.53371255  0.5382611   0.53941234  0.5399081   0.53991511  0.5419284
  0.5446476   0.54568417  0.54695845  0.54710693  0.54998852  0.55676866
  0.55829511  0.5589521   0.5661035   0.56804477  0.57002429  0.57068687
  0.57408336  0.5747394   0.57507579  0.57563935  0.57793939  0.5780665
  0.58263875  0.58278508  0.58613513  0.59006419  0.59292892  0.59384997
  0.59635988  0.59687605  0.59807736  0.59982843  0.60044182  0.6026915
  0.60282267  0.60322856  0.60343202  0.60415172  0.60418902  0.60566131
  0.6064578   0.60699162  0.61004279  0.61061159  0.61184115  0.61211298
  0.61361264  0.61392051  0.61408182  0.61497263  0.61522297  0.61938099
  0.62083012  0.62241081  0.62709093  0.62824141  0.63669027  0.64092258
  0.64228027  0.64432414  0.64468416  0.64479891  0.64971895  0.64986583
  0.6558076   0.65686816  0.65744907  0.66181433  0.66467897  0.66668038
  0.66926234  0.66965481  0.67258996  0.67651485  0.67852387  0.67859407
  0.68478463  0.68517061  0.68757089  0.68825361  0.68843427  0.69029171
  0.69403292  0.69522339  0.69598624  0.69877213  0.69925869  0.70868456
  0.71016703  0.7124645   0.71318947  0.71457578  0.71477525  0.71561883
  0.71657044  0.71731922  0.72012864  0.72237744  0.72299664  0.72553828
  0.73101779  0.73102486  0.73274034  0.73299434  0.73462248  0.73560064
  0.73625505  0.74239986  0.74766143  0.74864835  0.75139244  0.75437718
  0.75577783  0.75852634  0.75883215  0.7624791   0.76459705  0.76478215
  0.77836394  0.78019237  0.7861367   0.79055356  0.79233787  0.8000705
  0.80790087  0.80981589  0.81048282  0.81088375  0.81291599  0.8157525
  0.81907865  0.82235529  0.83445024  0.84337421  0.84603134  0.84727206
  0.85686077  0.86084129  0.86133894  0.8681075   0.87041835  0.87173073
  0.87224299  0.87738816  0.88018323  0.8813727   0.88189632  0.88503552
  0.88543612  0.88672313  0.88936461  0.89140586  0.89257499  0.89491054
  0.8988125   0.90018961  0.90035243  0.90217923  0.90447661  0.90509033
  0.90711922  0.90897893  0.90949588  0.90974699  0.91430557  0.91448144
  0.91464832  0.91778267  0.91806469  0.92013199  0.92111739  0.92250125
  0.92632663  0.92735159  0.92824577  0.92834046  0.92837041  0.92994545
  0.93147624  0.93798913  0.94440582  0.95283766  0.95719687  0.95776707
  0.96324081  0.96630977  0.96717328  0.96816726  0.96921575  0.97044501
  0.97104913  0.97152986  0.97205615  0.97372391  0.97380095  0.97433266
  0.97567565  0.97641917  0.9772676   0.9774954   0.97838432  0.98075686
  0.98286396  0.98702137  0.9873938   0.98801061  0.98819988  0.9888498
  0.98986178  0.99028282  0.99108288  0.99152953  0.99173529  0.99250767
  0.99327827  0.99602715  0.99651184  0.99692694  1.00156918  1.00164687
  1.00202332  1.00210589  1.00212779  1.00267433  1.0061107   1.00703437
  1.00774439  1.01226808  1.02914356  1.02983722  1.03606361  1.05983346
  1.08865032  1.09906036  1.1270348   1.13419674  1.23098254]

  UserWarning,

2022-10-31 11:03:53,736:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.56734975e-01 -1.41578626e-01 -1.35417870e-01 -9.67077691e-02
 -8.46484465e-02 -8.31542570e-02 -7.54757841e-02 -7.30937154e-02
 -5.71372516e-02 -4.84759073e-02 -3.64589561e-02 -3.39099066e-02
 -3.21971695e-02 -3.06352153e-02 -2.99826249e-02 -1.38508802e-02
 -1.26362932e-02 -9.18456625e-03 -7.37423661e-03 -7.05510625e-03
 -3.52164728e-03 -7.49133707e-04  1.96023553e-03  2.74430958e-03
  4.10124952e-03  6.89493995e-03  9.17084403e-03  1.12746581e-02
  1.16431025e-02  1.25282894e-02  1.41780289e-02  1.55241512e-02
  1.57955198e-02  1.65047523e-02  2.10462143e-02  2.11222756e-02
  2.17516601e-02  2.41523611e-02  2.62730827e-02  2.82570720e-02
  2.87740345e-02  2.89911993e-02  2.97716924e-02  3.19619868e-02
  3.32882513e-02  3.44109762e-02  3.47057269e-02  3.51181962e-02
  3.51915461e-02  3.55111714e-02  3.61486666e-02  3.75507396e-02
  3.84465321e-02  4.06902274e-02  4.55837344e-02  4.98166841e-02
  5.08945858e-02  5.09188377e-02  5.49293376e-02  5.54519180e-02
  5.86057361e-02  5.98247382e-02  6.32618328e-02  6.43022396e-02
  6.57269268e-02  6.63359837e-02  7.06903486e-02  7.14305917e-02
  7.19801239e-02  7.24468003e-02  7.37011726e-02  7.54765300e-02
  7.66155721e-02  7.73800349e-02  7.74913829e-02  7.82246832e-02
  8.32280653e-02  8.56862107e-02  8.74200957e-02  9.05074688e-02
  9.10098102e-02  9.21552153e-02  9.22322166e-02  9.28057579e-02
  9.30465891e-02  9.45047447e-02  9.50669145e-02  9.56041823e-02
  9.56309453e-02  9.78802905e-02  9.90322257e-02  1.01133879e-01
  1.03180074e-01  1.04138634e-01  1.05555965e-01  1.06836719e-01
  1.07379083e-01  1.08966651e-01  1.09174785e-01  1.09367606e-01
  1.12958234e-01  1.14427994e-01  1.15658183e-01  1.16648678e-01
  1.18099370e-01  1.18654259e-01  1.19251194e-01  1.20193659e-01
  1.20476987e-01  1.21113951e-01  1.26795323e-01  1.27014223e-01
  1.28232960e-01  1.29961047e-01  1.31262959e-01  1.32640815e-01
  1.35192281e-01  1.37188419e-01  1.40402390e-01  1.41023812e-01
  1.41696440e-01  1.42894700e-01  1.43865493e-01  1.45190842e-01
  1.46427419e-01  1.47080849e-01  1.47600922e-01  1.48803218e-01
  1.48985710e-01  1.49326261e-01  1.49767777e-01  1.50240344e-01
  1.50785192e-01  1.52452403e-01  1.53571817e-01  1.54698891e-01
  1.56078266e-01  1.56081089e-01  1.56405224e-01  1.59282493e-01
  1.61109443e-01  1.62543542e-01  1.63280299e-01  1.69177497e-01
  1.69541335e-01  1.71221058e-01  1.71293321e-01  1.71575892e-01
  1.74552629e-01  1.74852712e-01  1.74933130e-01  1.75754222e-01
  1.77096839e-01  1.77815995e-01  1.80310111e-01  1.80326140e-01
  1.83701045e-01  1.88234091e-01  1.92419867e-01  1.93269135e-01
  1.94637750e-01  1.95617065e-01  1.96455131e-01  1.97943811e-01
  1.98445530e-01  1.99486714e-01  2.00945428e-01  2.01975089e-01
  2.03790219e-01  2.04535441e-01  2.07621115e-01  2.08020224e-01
  2.08477964e-01  2.11594331e-01  2.13562538e-01  2.13728342e-01
  2.21264612e-01  2.21377845e-01  2.21517063e-01  2.24263260e-01
  2.25119767e-01  2.30893024e-01  2.30954005e-01  2.32995733e-01
  2.33215667e-01  2.34486009e-01  2.34579315e-01  2.36224830e-01
  2.36728216e-01  2.36769508e-01  2.37030258e-01  2.37725217e-01
  2.40403687e-01  2.40654191e-01  2.41023836e-01  2.41176170e-01
  2.42798800e-01  2.43624606e-01  2.45916630e-01  2.48754863e-01
  2.50671391e-01  2.51654895e-01  2.52182417e-01  2.54259148e-01
  2.57670332e-01  2.61662455e-01  2.63440895e-01  2.63979092e-01
  2.65226613e-01  2.69017281e-01  2.69286527e-01  2.69828168e-01
  2.70455959e-01  2.73533917e-01  2.73642634e-01  2.78446381e-01
  2.81188225e-01  2.82833788e-01  2.84253457e-01  2.87904964e-01
  2.89474730e-01  2.91080884e-01  2.94010421e-01  2.95349729e-01
  2.96489985e-01  2.96716781e-01  2.98857841e-01  3.03497094e-01
  3.03526349e-01  3.04109785e-01  3.05558480e-01  3.05660722e-01
  3.08065934e-01  3.08440024e-01  3.10046118e-01  3.16197790e-01
  3.17139634e-01  3.18082974e-01  3.23249133e-01  3.23977785e-01
  3.27274111e-01  3.27803886e-01  3.29100435e-01  3.35176719e-01
  3.36548623e-01  3.40020298e-01  3.45651342e-01  3.48049584e-01
  3.52060119e-01  3.56719215e-01  3.57377211e-01  3.58610759e-01
  3.63458825e-01  3.67485018e-01  3.69189541e-01  3.76206158e-01
  3.82041197e-01  3.86614869e-01  3.88171648e-01  3.90530063e-01
  3.93940451e-01  3.95910479e-01  3.97398988e-01  4.05056457e-01
  4.17934533e-01  4.19307063e-01  4.19686402e-01  4.20925702e-01
  4.22040178e-01  4.23178582e-01  4.27025121e-01  4.27443926e-01
  4.29008371e-01  4.39105253e-01  4.41466559e-01  4.46457351e-01
  4.48091822e-01  4.50429258e-01  4.51117632e-01  4.55682778e-01
  4.56495202e-01  4.60709032e-01  4.62685861e-01  4.65602735e-01
  4.70693050e-01  4.74418655e-01  4.74770693e-01  4.75881259e-01
  4.75924010e-01  4.76327044e-01  4.77830948e-01  4.80333856e-01
  4.80859191e-01  4.90878903e-01  4.98560646e-01  5.07557125e-01
  5.13563804e-01  5.18297384e-01  5.22533079e-01  5.22692086e-01
  5.23513361e-01  5.32132416e-01  5.36146166e-01  5.39328081e-01
  5.39643606e-01  5.39651899e-01  5.44296420e-01  5.44448252e-01
  5.46651917e-01  5.47337514e-01  5.48778816e-01  5.49873229e-01
  5.53269062e-01  5.53451831e-01  5.57367951e-01  5.57509555e-01
  5.59288057e-01  5.62065672e-01  5.62838322e-01  5.64246372e-01
  5.64667431e-01  5.65431815e-01  5.68184685e-01  5.70757960e-01
  5.71428900e-01  5.71730444e-01  5.72719525e-01  5.76631124e-01
  5.77022351e-01  5.83996167e-01  5.84442274e-01  5.85450528e-01
  5.88980967e-01  5.90076967e-01  5.90212251e-01  5.90235796e-01
  5.92334719e-01  5.95067694e-01  5.95444876e-01  5.97813663e-01
  5.98243898e-01  6.00737354e-01  6.01941334e-01  6.04152942e-01
  6.06546881e-01  6.14968758e-01  6.15845916e-01  6.19974109e-01
  6.21025228e-01  6.21877700e-01  6.22798731e-01  6.26432788e-01
  6.26498666e-01  6.34445071e-01  6.36833161e-01  6.40445835e-01
  6.43363776e-01  6.47912420e-01  6.49679362e-01  6.56340222e-01
  6.58182205e-01  6.59341340e-01  6.64770239e-01  6.66790671e-01
  6.68055500e-01  6.74572965e-01  6.74819679e-01  6.75191825e-01
  6.76949142e-01  6.84177825e-01  6.84787260e-01  6.88456673e-01
  6.95155135e-01  6.98205963e-01  7.00102897e-01  7.02713222e-01
  7.09658856e-01  7.10480877e-01  7.10651758e-01  7.11440767e-01
  7.14337938e-01  7.14972382e-01  7.20175560e-01  7.23013610e-01
  7.23280301e-01  7.25385944e-01  7.34689528e-01  7.37721415e-01
  7.37820441e-01  7.40292084e-01  7.40481181e-01  7.43921133e-01
  7.45489283e-01  7.47121856e-01  7.48534093e-01  7.48589994e-01
  7.50039402e-01  7.51255763e-01  7.52046518e-01  7.52190009e-01
  7.52306075e-01  7.54149545e-01  7.56537772e-01  7.57047277e-01
  7.57672594e-01  7.60690995e-01  7.61548963e-01  7.61580380e-01
  7.71445484e-01  7.74297205e-01  7.77384691e-01  7.81981818e-01
  7.82715491e-01  7.85436587e-01  7.91729203e-01  7.93346638e-01
  8.03353944e-01  8.06531217e-01  8.11038426e-01  8.14824041e-01
  8.22325616e-01  8.27879006e-01  8.27927909e-01  8.30467590e-01
  8.30607662e-01  8.34549502e-01  8.37456242e-01  8.40563394e-01
  8.42545642e-01  8.43278086e-01  8.48740045e-01  8.49301496e-01
  8.49990396e-01  8.51636843e-01  8.54718606e-01  8.56851986e-01
  8.58491624e-01  8.62732117e-01  8.64391593e-01  8.64406298e-01
  8.65981299e-01  8.66105041e-01  8.67798895e-01  8.69079107e-01
  8.69468934e-01  8.69759053e-01  8.75032956e-01  8.75105338e-01
  8.76967548e-01  8.79007091e-01  8.79729083e-01  8.80217017e-01
  8.80748894e-01  8.82932804e-01  8.83661622e-01  8.87340979e-01
  8.88605230e-01  8.88935743e-01  8.92543758e-01  8.93834544e-01
  8.94793406e-01  8.94922665e-01  8.97217526e-01  9.00494101e-01
  9.03786439e-01  9.10560429e-01  9.10925884e-01  9.13465206e-01
  9.13718490e-01  9.13981219e-01  9.14692785e-01  9.21126829e-01
  9.22809102e-01  9.25876458e-01  9.29788443e-01  9.32134224e-01
  9.34871289e-01  9.35399917e-01  9.36415824e-01  9.39121061e-01
  9.40097407e-01  9.46277535e-01  9.46408598e-01  9.47950614e-01
  9.56931183e-01  9.60365750e-01  9.62329181e-01  9.64516329e-01
  9.65437876e-01  9.65666930e-01  9.66151868e-01  9.67102567e-01
  9.67379584e-01  9.67962627e-01  9.70624373e-01  9.71145866e-01
  9.71538774e-01  9.75593106e-01  9.76490103e-01  9.76640724e-01
  9.77065366e-01  9.77990025e-01  9.79284305e-01  9.79541610e-01
  9.80751418e-01  9.81189960e-01  9.81331466e-01  9.81596098e-01
  9.82831670e-01  9.83271531e-01  9.84397471e-01  9.84768054e-01
  9.85592291e-01  9.87681127e-01  9.87877029e-01  9.88315571e-01
  9.89409798e-01  9.89848340e-01  9.90770144e-01  9.92593553e-01
  9.92768943e-01  9.93969485e-01  9.94028076e-01  9.94853462e-01
  9.95307249e-01  9.95560845e-01  9.97914174e-01  9.98645043e-01
  1.00002465e+00  1.00087709e+00  1.00266530e+00  1.00281468e+00
  1.00888551e+00  1.00918597e+00  1.01190825e+00  1.01200716e+00
  1.01486733e+00  1.01609059e+00  1.02089616e+00  1.02232097e+00
  1.04289423e+00  1.04712888e+00  1.05415936e+00  1.05554886e+00
  1.05890169e+00  1.06037599e+00  1.07277530e+00  1.09160603e+00
  1.10540822e+00  1.12299529e+00  1.12300489e+00  1.17677416e+00]

  UserWarning,

2022-10-31 11:03:53,752:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.39623486e-02 -5.57129546e-02 -5.25683789e-02 -5.01818942e-02
 -4.85975588e-02 -4.28083302e-02 -4.27329005e-02 -4.24241313e-02
 -4.20465437e-02 -4.13637343e-02 -3.70209445e-02 -3.68397687e-02
 -3.40240561e-02 -3.37699142e-02 -3.28415613e-02 -2.72333406e-02
 -2.34590260e-02 -1.59154153e-02 -1.34973998e-02 -1.24035583e-02
 -1.21416685e-02 -1.15397098e-02 -1.11928866e-02 -1.01300814e-02
 -5.99777841e-03 -2.57586592e-05  3.38645341e-03  5.93789952e-03
  7.15225858e-03  7.65708711e-03  7.70844031e-03  9.33032355e-03
  1.11366884e-02  1.41722090e-02  1.91764144e-02  1.95810795e-02
  2.04428096e-02  2.12941239e-02  2.79723261e-02  2.96962337e-02
  3.07499454e-02  3.17494803e-02  3.23425684e-02  3.26562862e-02
  3.41164758e-02  3.52773518e-02  3.60049969e-02  3.93255660e-02
  4.17106727e-02  4.48079608e-02  4.86274772e-02  5.29277521e-02
  5.43622831e-02  5.86841832e-02  5.94736029e-02  5.99623033e-02
  6.06983611e-02  6.08114496e-02  6.24474217e-02  6.36656435e-02
  6.46381409e-02  6.50588775e-02  6.52079848e-02  6.61569438e-02
  6.74115694e-02  7.31521106e-02  7.33100946e-02  7.46204237e-02
  7.53950044e-02  7.69126978e-02  7.70626257e-02  7.73217411e-02
  7.93590074e-02  8.36709493e-02  8.37680308e-02  8.42331419e-02
  8.45981609e-02  8.69073033e-02  8.70003233e-02  8.77142378e-02
  8.77625616e-02  8.79312244e-02  8.88908729e-02  8.89821250e-02
  8.93307811e-02  9.03289560e-02  9.47651620e-02  9.50296746e-02
  9.59656831e-02  9.93678086e-02  1.00298718e-01  1.01722463e-01
  1.02014128e-01  1.02366097e-01  1.05510786e-01  1.08468275e-01
  1.08937305e-01  1.11078999e-01  1.12170987e-01  1.12584505e-01
  1.14202775e-01  1.14697402e-01  1.14873529e-01  1.15429153e-01
  1.15935975e-01  1.15940499e-01  1.16349946e-01  1.16748804e-01
  1.19446108e-01  1.21561560e-01  1.21597730e-01  1.21597841e-01
  1.26782438e-01  1.29369051e-01  1.30583281e-01  1.32412161e-01
  1.35463767e-01  1.36852173e-01  1.37013806e-01  1.37026048e-01
  1.37662825e-01  1.38630097e-01  1.40496074e-01  1.44255274e-01
  1.48086371e-01  1.50514264e-01  1.51044286e-01  1.51462351e-01
  1.51801004e-01  1.53996017e-01  1.54521421e-01  1.58760204e-01
  1.59949744e-01  1.60357560e-01  1.60702841e-01  1.61169175e-01
  1.64978976e-01  1.65128783e-01  1.66255621e-01  1.66781582e-01
  1.67806842e-01  1.68090488e-01  1.68574317e-01  1.68577656e-01
  1.68759446e-01  1.69993675e-01  1.70744465e-01  1.71306042e-01
  1.73735371e-01  1.73745475e-01  1.74523912e-01  1.77638536e-01
  1.78026205e-01  1.80141762e-01  1.81138214e-01  1.84561222e-01
  1.84919123e-01  1.85623228e-01  1.86418219e-01  1.86495758e-01
  1.87836831e-01  1.91534588e-01  1.92710239e-01  1.93146909e-01
  1.95744292e-01  1.97580628e-01  2.00275900e-01  2.04361424e-01
  2.04526161e-01  2.05506175e-01  2.05612825e-01  2.06877169e-01
  2.07029911e-01  2.08355653e-01  2.09628039e-01  2.10523164e-01
  2.10956099e-01  2.11944886e-01  2.19844134e-01  2.21016597e-01
  2.21693943e-01  2.24364978e-01  2.25366934e-01  2.26780905e-01
  2.26991122e-01  2.34987940e-01  2.35731935e-01  2.36113752e-01
  2.39829749e-01  2.45031263e-01  2.46219439e-01  2.46896439e-01
  2.50437169e-01  2.51372170e-01  2.55503850e-01  2.56078608e-01
  2.56423401e-01  2.57664354e-01  2.59472063e-01  2.61602695e-01
  2.65473411e-01  2.67407547e-01  2.68449006e-01  2.69842180e-01
  2.70184030e-01  2.70223838e-01  2.72990880e-01  2.75266982e-01
  2.77938338e-01  2.80879161e-01  2.83047040e-01  2.85531005e-01
  2.85962652e-01  2.87441819e-01  2.94134393e-01  2.96585790e-01
  2.99136397e-01  3.02466816e-01  3.02630648e-01  3.03548974e-01
  3.08105217e-01  3.09696748e-01  3.14486642e-01  3.16576650e-01
  3.18735198e-01  3.22878130e-01  3.43445683e-01  3.49024648e-01
  3.51773084e-01  3.51933685e-01  3.53700338e-01  3.54365882e-01
  3.57547394e-01  3.60811211e-01  3.69821811e-01  3.73599631e-01
  3.76530877e-01  3.87544607e-01  3.89785498e-01  3.92624115e-01
  4.00304846e-01  4.01338862e-01  4.02695085e-01  4.03281627e-01
  4.07897323e-01  4.10614374e-01  4.14491649e-01  4.18452863e-01
  4.18502854e-01  4.19032515e-01  4.20280889e-01  4.21444746e-01
  4.22372058e-01  4.23613671e-01  4.27992291e-01  4.29840513e-01
  4.30615750e-01  4.34762504e-01  4.41606814e-01  4.44883381e-01
  4.47157042e-01  4.57740531e-01  4.57842050e-01  4.58039991e-01
  4.58129784e-01  4.60726445e-01  4.66339058e-01  4.69236670e-01
  4.70709834e-01  4.71155555e-01  4.71479928e-01  4.71926861e-01
  4.73573660e-01  4.82796936e-01  4.85243938e-01  4.88158550e-01
  4.88505974e-01  4.89406321e-01  4.89905204e-01  4.97096170e-01
  4.97593020e-01  4.99038621e-01  5.03851685e-01  5.05901513e-01
  5.06525432e-01  5.14354104e-01  5.15104604e-01  5.19933708e-01
  5.25059866e-01  5.29397856e-01  5.33405064e-01  5.35427681e-01
  5.36826999e-01  5.37603198e-01  5.39139621e-01  5.39540683e-01
  5.43092183e-01  5.43185320e-01  5.44770924e-01  5.46942290e-01
  5.48431068e-01  5.50920888e-01  5.52693287e-01  5.52813399e-01
  5.52866183e-01  5.56529410e-01  5.58504414e-01  5.59894165e-01
  5.61896378e-01  5.62724847e-01  5.63712862e-01  5.64629400e-01
  5.66002146e-01  5.70430747e-01  5.71692481e-01  5.71701584e-01
  5.72260492e-01  5.73648805e-01  5.74573398e-01  5.76217475e-01
  5.77134396e-01  5.78878896e-01  5.79297056e-01  5.79710870e-01
  5.80031344e-01  5.81319658e-01  5.81974046e-01  5.82633405e-01
  5.83682674e-01  5.85945669e-01  5.86649863e-01  5.91017985e-01
  5.91416035e-01  5.96749790e-01  5.99477793e-01  6.01430175e-01
  6.09735841e-01  6.11560008e-01  6.15516337e-01  6.19323179e-01
  6.24320031e-01  6.24977973e-01  6.26872422e-01  6.29979823e-01
  6.31037574e-01  6.31614253e-01  6.34777182e-01  6.35730287e-01
  6.36945966e-01  6.45905592e-01  6.53176053e-01  6.58592006e-01
  6.59948230e-01  6.63082325e-01  6.63551422e-01  6.68397627e-01
  6.68490786e-01  6.69979430e-01  6.71875065e-01  6.73545166e-01
  6.76257073e-01  6.77297453e-01  6.81475662e-01  6.81901284e-01
  6.84543194e-01  6.84753047e-01  6.85814031e-01  6.86990559e-01
  6.87389987e-01  6.87971104e-01  6.88094181e-01  6.89908312e-01
  6.90688006e-01  6.92439045e-01  6.92462303e-01  6.96613861e-01
  6.98047953e-01  6.98156274e-01  6.98889123e-01  6.99434614e-01
  7.03980550e-01  7.10271365e-01  7.13046023e-01  7.29825779e-01
  7.30152693e-01  7.31687167e-01  7.32134796e-01  7.33627982e-01
  7.34633132e-01  7.40712403e-01  7.42518528e-01  7.43448596e-01
  7.47671181e-01  7.49968667e-01  7.50477190e-01  7.51300752e-01
  7.51420067e-01  7.52740185e-01  7.53444378e-01  7.55442700e-01
  7.55488162e-01  7.56447823e-01  7.56622330e-01  7.58163067e-01
  7.58233530e-01  7.63152571e-01  7.65816684e-01  7.65952596e-01
  7.70092697e-01  7.70158271e-01  7.70791761e-01  7.74973456e-01
  7.79089857e-01  7.81591938e-01  7.88052094e-01  7.89851646e-01
  7.91617802e-01  7.97656634e-01  7.98991374e-01  8.08795742e-01
  8.20634439e-01  8.30807573e-01  8.46793126e-01  8.47179976e-01
  8.53174851e-01  8.53553785e-01  8.54146505e-01  8.64429844e-01
  8.76948107e-01  8.76993024e-01  8.79153174e-01  8.80424011e-01
  8.81302140e-01  8.82371232e-01  8.84613540e-01  8.86454657e-01
  8.87224163e-01  8.88420613e-01  8.88718094e-01  8.88719350e-01
  8.88753771e-01  8.90696473e-01  8.91750622e-01  8.92000239e-01
  8.93642139e-01  8.94261853e-01  8.94668096e-01  8.95372290e-01
  8.99451927e-01  9.08752178e-01  9.14431284e-01  9.14691664e-01
  9.15086245e-01  9.19610399e-01  9.21241867e-01  9.25722146e-01
  9.30360269e-01  9.31274808e-01  9.34155241e-01  9.38215684e-01
  9.38320867e-01  9.38370980e-01  9.38429972e-01  9.39172536e-01
  9.41430084e-01  9.44045381e-01  9.51568788e-01  9.52282318e-01
  9.53022274e-01  9.54692040e-01  9.57069135e-01  9.57471436e-01
  9.58711011e-01  9.59213950e-01  9.59774187e-01  9.60190712e-01
  9.62160146e-01  9.63408769e-01  9.67166920e-01  9.67297168e-01
  9.67461785e-01  9.69237528e-01  9.69791309e-01  9.70572168e-01
  9.70739939e-01  9.71512372e-01  9.71558136e-01  9.71825073e-01
  9.72262329e-01  9.73068693e-01  9.73185208e-01  9.73560569e-01
  9.74779185e-01  9.75881039e-01  9.78901820e-01  9.79661053e-01
  9.80177877e-01  9.81167910e-01  9.82119878e-01  9.82357937e-01
  9.84643611e-01  9.85180731e-01  9.85672777e-01  9.86202013e-01
  9.86473738e-01  9.86947558e-01  9.87651751e-01  9.87948637e-01
  9.89002786e-01  9.89233232e-01  9.90536182e-01  9.90585904e-01
  9.91265294e-01  9.91805792e-01  9.92956469e-01  9.93276458e-01
  9.94848477e-01  9.97427631e-01  9.97777881e-01  9.99088372e-01
  1.00007678e+00  1.00025039e+00  1.00135027e+00  1.00231293e+00
  1.00461122e+00  1.00558179e+00  1.00698957e+00  1.00978864e+00
  1.01046458e+00  1.01217424e+00  1.01393934e+00  1.01784909e+00
  1.02322190e+00  1.02773859e+00  1.03614075e+00  1.03641300e+00
  1.04432375e+00  1.06499612e+00  1.20079047e+00  1.23247506e+00]

  UserWarning,

2022-10-31 11:03:53,799:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.47689987e-01 -1.01130077e-01 -8.88727155e-02 -8.75843533e-02
 -8.07252613e-02 -7.75632675e-02 -7.35808624e-02 -7.23316773e-02
 -5.87473955e-02 -5.56306246e-02 -4.93154529e-02 -3.39113137e-02
 -3.31103951e-02 -2.76155561e-02 -2.41954088e-02 -1.55913619e-02
 -1.36324938e-02 -1.28860659e-02 -1.16087553e-02 -1.06524740e-02
 -8.05550688e-03 -6.28122657e-03 -3.85687734e-03 -1.87324343e-03
 -3.58459757e-04  2.12418285e-03  4.24573656e-03  6.25743826e-03
  6.62009054e-03  1.26359376e-02  1.35544082e-02  1.40126131e-02
  1.56853888e-02  1.72193896e-02  2.00643969e-02  2.23383091e-02
  2.50113570e-02  2.67229009e-02  2.81644897e-02  2.90801974e-02
  2.97722008e-02  3.05103972e-02  3.06607732e-02  3.14650892e-02
  3.33435049e-02  3.36577697e-02  3.85423134e-02  3.87161883e-02
  3.97371444e-02  4.00583554e-02  4.04370772e-02  4.06333745e-02
  4.21032782e-02  4.25354512e-02  4.34692076e-02  4.41947257e-02
  4.63300528e-02  4.71825324e-02  4.87651962e-02  5.26296379e-02
  5.34646108e-02  5.44374676e-02  5.89253348e-02  6.04492627e-02
  6.14268312e-02  6.14783597e-02  6.17885084e-02  6.18103966e-02
  6.30733217e-02  6.46628628e-02  6.46801657e-02  6.49257961e-02
  6.66050895e-02  6.68990436e-02  6.74785160e-02  6.75206424e-02
  6.82155302e-02  6.98519926e-02  7.04524931e-02  7.06502692e-02
  7.23764632e-02  7.31760887e-02  7.36785101e-02  7.37875566e-02
  7.40757672e-02  7.42385279e-02  7.60197815e-02  7.62178954e-02
  8.08325530e-02  8.23926795e-02  8.24689758e-02  8.38640280e-02
  8.75209308e-02  8.75819297e-02  8.77037222e-02  9.57883516e-02
  9.83849616e-02  9.86088591e-02  1.00140106e-01  1.01977617e-01
  1.04579391e-01  1.05193382e-01  1.05777904e-01  1.06682527e-01
  1.07122988e-01  1.11102453e-01  1.12692944e-01  1.15292009e-01
  1.17718243e-01  1.18621417e-01  1.19084703e-01  1.21758245e-01
  1.22094780e-01  1.22405950e-01  1.22889861e-01  1.25010660e-01
  1.25114601e-01  1.27430337e-01  1.28801583e-01  1.31202479e-01
  1.31956971e-01  1.32484656e-01  1.34663300e-01  1.35244710e-01
  1.35717063e-01  1.36001483e-01  1.36558850e-01  1.37438714e-01
  1.38615042e-01  1.40735907e-01  1.41921593e-01  1.42746790e-01
  1.43514063e-01  1.45327070e-01  1.46342822e-01  1.48650467e-01
  1.48694947e-01  1.51396084e-01  1.52131362e-01  1.54684183e-01
  1.56112235e-01  1.56438004e-01  1.56902833e-01  1.58220372e-01
  1.59277201e-01  1.60625492e-01  1.60671693e-01  1.64302522e-01
  1.65801112e-01  1.67085614e-01  1.67266030e-01  1.67932863e-01
  1.69570382e-01  1.70448948e-01  1.71843192e-01  1.74076454e-01
  1.74724832e-01  1.75975743e-01  1.76347947e-01  1.77810739e-01
  1.80175954e-01  1.81316118e-01  1.81778531e-01  1.83855441e-01
  1.84066330e-01  1.84817314e-01  1.86359703e-01  1.86681659e-01
  1.86714343e-01  1.87701264e-01  1.88668619e-01  1.89917164e-01
  1.90293876e-01  1.90694868e-01  1.91689660e-01  1.91960395e-01
  1.92234555e-01  1.93195940e-01  1.94113582e-01  1.95597032e-01
  1.96059238e-01  2.00993399e-01  2.02250072e-01  2.04042307e-01
  2.05043024e-01  2.05111485e-01  2.06801079e-01  2.08966976e-01
  2.09839014e-01  2.09981984e-01  2.10374456e-01  2.16162240e-01
  2.17649117e-01  2.18935331e-01  2.20196079e-01  2.21638121e-01
  2.26424007e-01  2.27149543e-01  2.29632803e-01  2.30085493e-01
  2.32225289e-01  2.32967648e-01  2.34416617e-01  2.34621415e-01
  2.35240049e-01  2.35366116e-01  2.43845900e-01  2.43898425e-01
  2.44439633e-01  2.46897003e-01  2.49068044e-01  2.50986709e-01
  2.51474560e-01  2.53145967e-01  2.55532769e-01  2.56180800e-01
  2.56805132e-01  2.57473205e-01  2.59243979e-01  2.67291723e-01
  2.74392138e-01  2.74598785e-01  2.76715421e-01  2.76925629e-01
  2.78071829e-01  2.79267612e-01  2.80774275e-01  2.84607398e-01
  2.85150487e-01  2.89163304e-01  2.91346299e-01  2.91939308e-01
  2.95616096e-01  3.00162104e-01  3.01844563e-01  3.02174689e-01
  3.02821099e-01  3.03612263e-01  3.04398157e-01  3.07035916e-01
  3.07745548e-01  3.09266266e-01  3.11576289e-01  3.12933918e-01
  3.12934576e-01  3.15150552e-01  3.17330359e-01  3.19550445e-01
  3.23498907e-01  3.24405753e-01  3.26196681e-01  3.27257302e-01
  3.30398616e-01  3.31815491e-01  3.38713374e-01  3.38993088e-01
  3.42643296e-01  3.45291497e-01  3.45939681e-01  3.47200721e-01
  3.53616003e-01  3.56731253e-01  3.57176033e-01  3.62809153e-01
  3.62939735e-01  3.63213497e-01  3.68358845e-01  3.76138174e-01
  3.80116452e-01  3.84852953e-01  3.88205121e-01  3.91440884e-01
  3.92514284e-01  3.94494854e-01  3.94540210e-01  3.99014304e-01
  4.00915433e-01  4.01420813e-01  4.06059912e-01  4.10131765e-01
  4.10734721e-01  4.15100834e-01  4.15872127e-01  4.16028264e-01
  4.21804189e-01  4.23183085e-01  4.28299162e-01  4.33563191e-01
  4.34333846e-01  4.34511771e-01  4.39710378e-01  4.42801278e-01
  4.44191260e-01  4.53484581e-01  4.61569192e-01  4.67838622e-01
  4.71983695e-01  4.73984410e-01  4.77903471e-01  4.79470250e-01
  4.84230302e-01  4.86385062e-01  4.90265177e-01  4.91130749e-01
  4.93121738e-01  4.95212429e-01  4.95294423e-01  4.99067568e-01
  5.04116851e-01  5.11612624e-01  5.16265059e-01  5.17764024e-01
  5.30445259e-01  5.32100804e-01  5.32604079e-01  5.40294902e-01
  5.42728255e-01  5.44998542e-01  5.46657472e-01  5.49250140e-01
  5.51053237e-01  5.51187313e-01  5.51887722e-01  5.59869186e-01
  5.60373637e-01  5.61073980e-01  5.61444679e-01  5.62371528e-01
  5.63333041e-01  5.64342067e-01  5.65796898e-01  5.67121529e-01
  5.67148510e-01  5.71124582e-01  5.72325703e-01  5.78953901e-01
  5.79626897e-01  5.80823857e-01  5.81566511e-01  5.83901698e-01
  5.84242808e-01  5.84577789e-01  5.86973140e-01  5.99043486e-01
  6.02381551e-01  6.05164674e-01  6.10357545e-01  6.10699816e-01
  6.11540507e-01  6.13305109e-01  6.14038369e-01  6.18232554e-01
  6.20177793e-01  6.20366978e-01  6.26535722e-01  6.29198198e-01
  6.32831752e-01  6.37620172e-01  6.39072322e-01  6.45887544e-01
  6.46173818e-01  6.48764471e-01  6.49138129e-01  6.51446213e-01
  6.52390821e-01  6.53246272e-01  6.54742846e-01  6.57773972e-01
  6.61716407e-01  6.62577549e-01  6.62814244e-01  6.62921564e-01
  6.63209832e-01  6.64014627e-01  6.68540721e-01  6.74894252e-01
  6.76182005e-01  6.79582941e-01  6.79813844e-01  6.80402978e-01
  6.82557143e-01  6.85309685e-01  6.85418778e-01  6.86431114e-01
  6.86752123e-01  6.87539963e-01  6.88260974e-01  6.89681646e-01
  6.94384681e-01  6.99809113e-01  7.03346505e-01  7.03858321e-01
  7.04285474e-01  7.05014162e-01  7.10964026e-01  7.11076807e-01
  7.11506943e-01  7.12663105e-01  7.17657405e-01  7.21594569e-01
  7.38593585e-01  7.44297416e-01  7.54521646e-01  7.57411986e-01
  7.61050604e-01  7.62031573e-01  7.69602804e-01  7.79113304e-01
  7.82951720e-01  7.84834573e-01  7.92671107e-01  7.95956079e-01
  7.99691686e-01  8.04092491e-01  8.09059843e-01  8.14718175e-01
  8.15346087e-01  8.22269593e-01  8.22470048e-01  8.23033338e-01
  8.24331149e-01  8.32725756e-01  8.45968205e-01  8.46072459e-01
  8.46976375e-01  8.51423633e-01  8.55359141e-01  8.55687975e-01
  8.56766649e-01  8.57263469e-01  8.60727491e-01  8.61008161e-01
  8.66016023e-01  8.67634919e-01  8.70810332e-01  8.72226403e-01
  8.74856712e-01  8.79494825e-01  8.81664108e-01  8.83601280e-01
  8.84316221e-01  8.84496396e-01  8.88456943e-01  8.88740391e-01
  8.89478721e-01  8.89715053e-01  8.90184017e-01  8.90280330e-01
  8.93266691e-01  8.93772762e-01  8.94306917e-01  8.94493587e-01
  8.95502976e-01  8.96069081e-01  8.97078470e-01  8.97419822e-01
  8.97871801e-01  8.98021410e-01  8.98042887e-01  9.00489080e-01
  9.01646172e-01  9.01971754e-01  9.03151014e-01  9.03228873e-01
  9.05698225e-01  9.06261616e-01  9.07758041e-01  9.10735210e-01
  9.12097942e-01  9.13660507e-01  9.16238921e-01  9.21068688e-01
  9.22576221e-01  9.22875399e-01  9.23210937e-01  9.23606457e-01
  9.24295084e-01  9.25728469e-01  9.27621522e-01  9.27998114e-01
  9.29205621e-01  9.35934267e-01  9.36014488e-01  9.38525593e-01
  9.38637169e-01  9.39666033e-01  9.43845190e-01  9.44393475e-01
  9.47999880e-01  9.48398874e-01  9.50243147e-01  9.50404625e-01
  9.50971470e-01  9.51452164e-01  9.51949865e-01  9.52370926e-01
  9.52768825e-01  9.54186872e-01  9.56818282e-01  9.58332141e-01
  9.60489808e-01  9.61757351e-01  9.63647964e-01  9.65585136e-01
  9.68416909e-01  9.69372697e-01  9.69909863e-01  9.70491300e-01
  9.71485357e-01  9.72836098e-01  9.74158486e-01  9.74474006e-01
  9.74688043e-01  9.75150604e-01  9.76762461e-01  9.77189188e-01
  9.77189248e-01  9.77864274e-01  9.78080794e-01  9.78273514e-01
  9.78699634e-01  9.79478790e-01  9.80077433e-01  9.82846420e-01
  9.83764495e-01  9.84408985e-01  9.85115236e-01  9.85367072e-01
  9.91904695e-01  9.93623877e-01  9.93667536e-01  9.93886499e-01
  9.94454173e-01  9.95449064e-01  9.95668423e-01  9.97374012e-01
  9.97780003e-01  1.00056325e+00  1.00398990e+00  1.00831332e+00
  1.00974567e+00  1.01005914e+00  1.01051342e+00  1.01254932e+00
  1.01457046e+00  1.01871311e+00  1.02311808e+00  1.02751204e+00
  1.03544151e+00  1.03593890e+00  1.03912338e+00  1.04315313e+00
  1.06566994e+00  1.08692315e+00  1.08893535e+00  1.09542309e+00]

  UserWarning,

2022-10-31 11:03:54,799:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.34826231e-01 -1.22759002e-01 -1.04111664e-01 -9.05607166e-02
 -8.92792246e-02 -8.01554499e-02 -5.68303908e-02 -5.37196428e-02
 -4.06977745e-02 -3.60542410e-02 -3.59206658e-02 -1.80726194e-02
 -1.72937128e-02 -1.40983431e-02 -5.67903137e-03 -6.81201501e-04
  2.52759833e-03  3.92080503e-03  4.50516065e-03  5.44441857e-03
  5.49077550e-03  8.43679517e-03  8.74160289e-03  9.08633381e-03
  1.15749053e-02  1.16374319e-02  1.47638048e-02  1.53428294e-02
  1.74453116e-02  1.83677595e-02  1.87020561e-02  2.08529450e-02
  2.16804638e-02  2.25777326e-02  2.37908555e-02  2.60932596e-02
  2.76278336e-02  2.92310829e-02  3.16318440e-02  3.19606253e-02
  3.36261013e-02  3.51925401e-02  4.31590240e-02  4.33557459e-02
  4.62030734e-02  4.69456895e-02  4.72193608e-02  5.32365911e-02
  5.47726379e-02  5.54404680e-02  5.61214424e-02  5.82843375e-02
  6.00995537e-02  6.06515756e-02  6.36221950e-02  6.58633650e-02
  6.64737656e-02  6.65430435e-02  6.66722274e-02  6.74866574e-02
  6.88653637e-02  7.10283029e-02  7.12307970e-02  7.13906982e-02
  7.22415517e-02  7.37141043e-02  7.75609722e-02  7.78576397e-02
  7.88358104e-02  8.05478954e-02  8.13186871e-02  8.17589271e-02
  8.40947107e-02  8.42456960e-02  8.71107429e-02  9.18209135e-02
  9.35107538e-02  9.44385770e-02  9.55944789e-02  9.59074375e-02
  9.60064957e-02  9.62569736e-02  9.77396038e-02  1.00279896e-01
  1.03821524e-01  1.03845640e-01  1.03870664e-01  1.05908321e-01
  1.06767551e-01  1.07091909e-01  1.11254097e-01  1.11393154e-01
  1.12517041e-01  1.12954234e-01  1.13553208e-01  1.13693036e-01
  1.14982205e-01  1.17226598e-01  1.17371745e-01  1.17757515e-01
  1.17866970e-01  1.18357502e-01  1.19591780e-01  1.19901231e-01
  1.20346822e-01  1.22439838e-01  1.22538042e-01  1.22558834e-01
  1.23124024e-01  1.25084467e-01  1.26585000e-01  1.27746847e-01
  1.29326240e-01  1.29659033e-01  1.30512535e-01  1.31111657e-01
  1.31739915e-01  1.31937049e-01  1.32187975e-01  1.32428458e-01
  1.32942906e-01  1.33345924e-01  1.35089966e-01  1.38082466e-01
  1.39368250e-01  1.40415087e-01  1.45599203e-01  1.53057475e-01
  1.53281641e-01  1.54770713e-01  1.55225468e-01  1.55470559e-01
  1.58197571e-01  1.60310768e-01  1.62235392e-01  1.62619122e-01
  1.63361865e-01  1.64563569e-01  1.66043511e-01  1.66612844e-01
  1.67753509e-01  1.68141277e-01  1.70171459e-01  1.70176725e-01
  1.70433222e-01  1.70463120e-01  1.70993461e-01  1.76443398e-01
  1.76539905e-01  1.76665534e-01  1.77234815e-01  1.77630793e-01
  1.79619790e-01  1.79975025e-01  1.80342371e-01  1.80937742e-01
  1.81734691e-01  1.81860681e-01  1.82784492e-01  1.85818932e-01
  1.86750545e-01  1.91393513e-01  1.92502132e-01  1.94135737e-01
  1.94328811e-01  1.95074206e-01  1.96260191e-01  1.97791890e-01
  2.01603707e-01  2.02596176e-01  2.02627735e-01  2.03582471e-01
  2.06189314e-01  2.07359563e-01  2.07418690e-01  2.09399915e-01
  2.09897916e-01  2.12732346e-01  2.13909429e-01  2.15042178e-01
  2.18212204e-01  2.20234249e-01  2.22050885e-01  2.23370522e-01
  2.25849525e-01  2.26178572e-01  2.28273230e-01  2.29838328e-01
  2.29851151e-01  2.31529743e-01  2.31739143e-01  2.37918734e-01
  2.46046557e-01  2.49015226e-01  2.50689037e-01  2.52217307e-01
  2.53275727e-01  2.57189524e-01  2.57310962e-01  2.57909460e-01
  2.58434752e-01  2.58524253e-01  2.65870125e-01  2.74371234e-01
  2.74771376e-01  2.75058779e-01  2.75741541e-01  2.76304329e-01
  2.77135666e-01  2.79594889e-01  2.80469160e-01  2.81414056e-01
  2.82511814e-01  2.85059092e-01  2.85539518e-01  2.92795818e-01
  2.93812742e-01  2.95234295e-01  2.96975268e-01  2.97153528e-01
  2.99638747e-01  3.00929810e-01  3.06412118e-01  3.14601984e-01
  3.15782316e-01  3.22665696e-01  3.24371812e-01  3.27660739e-01
  3.29023082e-01  3.41162750e-01  3.42155541e-01  3.43509519e-01
  3.44241092e-01  3.48144178e-01  3.50592550e-01  3.54055400e-01
  3.58099403e-01  3.59579349e-01  3.65282650e-01  3.67688289e-01
  3.68892517e-01  3.78881425e-01  3.83318670e-01  3.83503036e-01
  3.98715341e-01  3.99835051e-01  4.03288036e-01  4.06155525e-01
  4.10135295e-01  4.21144929e-01  4.21725691e-01  4.26858402e-01
  4.40812271e-01  4.41791878e-01  4.42622978e-01  4.52407704e-01
  4.52533963e-01  4.61130821e-01  4.61406653e-01  4.63378651e-01
  4.64102602e-01  4.66801242e-01  4.69585574e-01  4.72687965e-01
  4.77950510e-01  4.81494969e-01  4.84051659e-01  4.87745043e-01
  4.88878361e-01  4.90771720e-01  4.93503154e-01  4.94927684e-01
  4.95430956e-01  4.96017417e-01  5.09810309e-01  5.10525540e-01
  5.11467827e-01  5.12573422e-01  5.13483352e-01  5.24174287e-01
  5.26370867e-01  5.27298735e-01  5.28303923e-01  5.30957908e-01
  5.32269919e-01  5.33888727e-01  5.35628149e-01  5.36410024e-01
  5.36876469e-01  5.39217496e-01  5.40167000e-01  5.42584128e-01
  5.43426733e-01  5.45192729e-01  5.46471141e-01  5.46779241e-01
  5.47699554e-01  5.50880333e-01  5.51209028e-01  5.51907515e-01
  5.52389494e-01  5.53719256e-01  5.54457761e-01  5.54905707e-01
  5.55454101e-01  5.55741857e-01  5.56272099e-01  5.58238906e-01
  5.58796571e-01  5.59702773e-01  5.60418952e-01  5.65172467e-01
  5.69335360e-01  5.71001176e-01  5.72724718e-01  5.73276716e-01
  5.73579143e-01  5.76380626e-01  5.77593782e-01  5.80254819e-01
  5.80474155e-01  5.80505620e-01  5.80703038e-01  5.81204441e-01
  5.82272796e-01  5.86952305e-01  5.88041464e-01  5.88288643e-01
  5.88517720e-01  5.88968137e-01  5.91518083e-01  5.92327012e-01
  5.94849130e-01  5.96742797e-01  5.98563376e-01  6.06836334e-01
  6.08388049e-01  6.10335379e-01  6.19011290e-01  6.23392667e-01
  6.27354642e-01  6.28353524e-01  6.31849722e-01  6.33978643e-01
  6.36060861e-01  6.36687691e-01  6.39325260e-01  6.42330025e-01
  6.42967768e-01  6.46045021e-01  6.49229768e-01  6.51198402e-01
  6.56045604e-01  6.57851096e-01  6.63257133e-01  6.64617665e-01
  6.65232809e-01  6.65686337e-01  6.69174425e-01  6.70324014e-01
  6.71325335e-01  6.74885315e-01  6.76381134e-01  6.78116767e-01
  6.81529513e-01  6.84528886e-01  6.86594079e-01  6.87463193e-01
  6.89927886e-01  6.91287498e-01  6.91321844e-01  6.96286270e-01
  7.01983277e-01  7.02117076e-01  7.08384814e-01  7.14000453e-01
  7.14270332e-01  7.16114098e-01  7.19694558e-01  7.20399912e-01
  7.34278187e-01  7.34979253e-01  7.35690723e-01  7.38471248e-01
  7.39145652e-01  7.40683700e-01  7.46886622e-01  7.48690088e-01
  7.49750419e-01  7.50115814e-01  7.53941503e-01  7.56401403e-01
  7.56939815e-01  7.61911726e-01  7.71025226e-01  7.88846132e-01
  7.91773251e-01  7.92099439e-01  7.93256967e-01  7.96440171e-01
  8.02334341e-01  8.07248014e-01  8.09408166e-01  8.11152899e-01
  8.13489713e-01  8.20107041e-01  8.27964528e-01  8.30058980e-01
  8.30407663e-01  8.33636978e-01  8.35607792e-01  8.49199093e-01
  8.55247089e-01  8.59353966e-01  8.63100533e-01  8.66513443e-01
  8.70262494e-01  8.72567012e-01  8.72875862e-01  8.74542901e-01
  8.78733991e-01  8.79296416e-01  8.80484534e-01  8.81017396e-01
  8.82499373e-01  8.85374312e-01  8.90436649e-01  8.90580567e-01
  8.91728254e-01  8.93057807e-01  8.95309636e-01  8.96479131e-01
  8.97491851e-01  8.98080497e-01  8.98444257e-01  8.98616303e-01
  8.98754056e-01  8.99100195e-01  9.04543182e-01  9.07612779e-01
  9.07763843e-01  9.08116696e-01  9.11718001e-01  9.15666544e-01
  9.15957537e-01  9.16430520e-01  9.16574112e-01  9.18679770e-01
  9.19480195e-01  9.23933416e-01  9.26280379e-01  9.27196816e-01
  9.28108917e-01  9.28613696e-01  9.31033895e-01  9.32121018e-01
  9.32197922e-01  9.32433541e-01  9.33727184e-01  9.34685823e-01
  9.35220681e-01  9.37322375e-01  9.37867974e-01  9.40144714e-01
  9.41756642e-01  9.45093319e-01  9.47067851e-01  9.47413990e-01
  9.47523845e-01  9.50158606e-01  9.51377689e-01  9.52118845e-01
  9.56821175e-01  9.58892949e-01  9.62979035e-01  9.64324757e-01
  9.64416041e-01  9.66391645e-01  9.66962325e-01  9.67719695e-01
  9.68691109e-01  9.70165282e-01  9.70845954e-01  9.71780344e-01
  9.72263699e-01  9.73628557e-01  9.73931255e-01  9.75915753e-01
  9.76861045e-01  9.77408898e-01  9.77952994e-01  9.78755255e-01
  9.79284507e-01  9.79554559e-01  9.81316976e-01  9.81336453e-01
  9.82192128e-01  9.83011967e-01  9.83487364e-01  9.83561853e-01
  9.84792972e-01  9.85184580e-01  9.85395085e-01  9.86017855e-01
  9.86832049e-01  9.87311326e-01  9.87389543e-01  9.87405879e-01
  9.88284756e-01  9.88595190e-01  9.88824518e-01  9.90229044e-01
  9.90467632e-01  9.90976141e-01  9.92426655e-01  9.92628681e-01
  9.93466892e-01  9.93836406e-01  9.95521003e-01  9.96266761e-01
  9.98068105e-01  1.00012417e+00  1.00162144e+00  1.00174078e+00
  1.00386215e+00  1.00414781e+00  1.00476852e+00  1.00772434e+00
  1.01310101e+00  1.01368967e+00  1.01522084e+00  1.01720398e+00
  1.02921063e+00  1.03490911e+00  1.04265323e+00  1.04527678e+00
  1.07169575e+00  1.07497814e+00  1.08193111e+00  1.08631059e+00
  1.09630845e+00  1.10401850e+00  1.20921189e+00  1.24291648e+00]

  UserWarning,

2022-10-31 11:03:54,815:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08801748 -0.08553762 -0.08311123 -0.06628425 -0.06526849 -0.04823493
 -0.04541334 -0.04243311 -0.04075135 -0.03961086 -0.03617548 -0.03542051
 -0.03258018 -0.0309516  -0.02884819 -0.02216673 -0.0189442  -0.01771032
 -0.01691894 -0.01653723 -0.013533   -0.01253745 -0.01241667 -0.01213356
 -0.00978924 -0.00904612 -0.00823484 -0.00561879 -0.00361771 -0.00299146
 -0.002937    0.00606448  0.00636894  0.01008776  0.01075267  0.01755309
  0.02071952  0.02202813  0.02734906  0.0302474   0.03101099  0.03121003
  0.03190817  0.03339337  0.03446593  0.03807093  0.03911556  0.04337924
  0.04346521  0.04580302  0.05166463  0.05221032  0.05441982  0.05456023
  0.05742594  0.06008588  0.0617947   0.06333441  0.06357105  0.06430252
  0.06571263  0.06716023  0.06746866  0.06828827  0.0684648   0.06983372
  0.06999698  0.07021398  0.07077574  0.07267427  0.07367298  0.07368089
  0.07491475  0.07665438  0.07701718  0.07708101  0.07794814  0.07821313
  0.07909283  0.08163926  0.08276324  0.08451171  0.08683796  0.08779315
  0.08794584  0.08901455  0.09060596  0.10217318  0.10403517  0.10416115
  0.10554745  0.10702145  0.10819917  0.10911702  0.10914454  0.10924217
  0.11129427  0.11281088  0.11971706  0.12010429  0.12086032  0.12113815
  0.12189593  0.12237097  0.12407589  0.12553389  0.1268837   0.12783505
  0.12885817  0.12928436  0.12996532  0.13097893  0.13289463  0.13424447
  0.13638866  0.13759831  0.13796292  0.14285224  0.14532515  0.14553755
  0.1494713   0.15271986  0.15376189  0.15577904  0.1573503   0.15844301
  0.15975061  0.15988703  0.15994733  0.16082523  0.16174464  0.1625201
  0.16312874  0.16566199  0.16689254  0.16697394  0.16882048  0.17154631
  0.17333082  0.17526541  0.17809243  0.17885772  0.18020873  0.18088493
  0.1830886   0.18452729  0.18647655  0.18685032  0.18794636  0.18799253
  0.19084439  0.1912402   0.19130149  0.19281669  0.19377051  0.19585316
  0.1970279   0.19759947  0.19845054  0.199057    0.20010984  0.20025488
  0.20092207  0.20276063  0.20308305  0.2031052   0.20348989  0.20401901
  0.20444433  0.2062242   0.20751446  0.20957269  0.21641646  0.22263339
  0.2246281   0.22532253  0.22764559  0.22791367  0.22848416  0.22939484
  0.23777412  0.23832264  0.2441062   0.24430554  0.24505176  0.25046768
  0.25230973  0.25396452  0.25577861  0.25645348  0.25835535  0.26089286
  0.26463704  0.26687987  0.26696048  0.26937288  0.27354968  0.27417059
  0.27599179  0.2794445   0.28369735  0.28392558  0.2866109   0.28947233
  0.29449832  0.29730737  0.30067472  0.30174894  0.30218962  0.30279137
  0.30282291  0.30664592  0.30668173  0.31214371  0.31439887  0.31617435
  0.31727052  0.31920922  0.3200462   0.32277686  0.32289665  0.32852767
  0.32913299  0.3297012   0.33327152  0.33558642  0.33655009  0.34097197
  0.34266234  0.34271688  0.34578869  0.35200756  0.35649283  0.35726361
  0.35758341  0.36172646  0.36710915  0.37790875  0.38040558  0.39135313
  0.39310928  0.40034193  0.40438985  0.4062422   0.40830396  0.40859954
  0.40951255  0.4108737   0.41264869  0.41479624  0.41529402  0.41543816
  0.41749074  0.41895686  0.41895808  0.4209066   0.4239815   0.42507318
  0.42596358  0.42921586  0.4327519   0.43941334  0.44234203  0.44623803
  0.44980461  0.45904534  0.46230683  0.46339729  0.47350523  0.47553112
  0.47692945  0.48007169  0.48304126  0.48509934  0.48576322  0.4887637
  0.48903932  0.49067782  0.492173    0.49323664  0.49703867  0.49769052
  0.5001129   0.5045352   0.51283221  0.51663382  0.52074635  0.52188658
  0.5222903   0.52295272  0.52722176  0.52777764  0.5308659   0.53230255
  0.53425422  0.54138252  0.54153598  0.54501554  0.54590252  0.54850651
  0.5567404   0.55812738  0.55862725  0.56146743  0.56400808  0.56632153
  0.56712356  0.56716667  0.56757799  0.56985622  0.57141889  0.57172184
  0.57237624  0.57244129  0.57397095  0.57513148  0.57708119  0.57794418
  0.57886787  0.57996013  0.58066212  0.58153299  0.58270634  0.58504635
  0.58693381  0.58733482  0.58788359  0.59094921  0.59160482  0.59251755
  0.59277327  0.59328535  0.59431134  0.597824    0.59892273  0.59948953
  0.59991749  0.60149118  0.60184987  0.60237649  0.60267742  0.60620367
  0.60644527  0.60752234  0.61141514  0.61248162  0.61642843  0.62406428
  0.62532637  0.62587364  0.62787603  0.63455853  0.6351212   0.63700725
  0.63750695  0.63755537  0.64362916  0.64904855  0.65265417  0.65541046
  0.65551421  0.65652822  0.65960295  0.66191043  0.66219847  0.66339425
  0.66424657  0.66530191  0.67033177  0.67157827  0.67238731  0.675331
  0.67588997  0.67601528  0.67694778  0.67838961  0.67970131  0.67990177
  0.68036687  0.68186687  0.68318245  0.68462447  0.68610066  0.68925732
  0.69185028  0.69789153  0.69981026  0.70577763  0.70723449  0.70925192
  0.71202496  0.71277282  0.7133213   0.71351707  0.7143448   0.71438472
  0.71676354  0.71845053  0.71891595  0.71947651  0.71993014  0.72004114
  0.72218483  0.72740185  0.72747712  0.72753817  0.73254373  0.74225462
  0.74246929  0.74660496  0.74888318  0.74955848  0.75013679  0.75262055
  0.76280499  0.76455741  0.76570808  0.77467615  0.77568983  0.787125
  0.79430679  0.8013077   0.8036049   0.81362497  0.81632506  0.83359647
  0.83470824  0.85481577  0.86074841  0.86163788  0.86438596  0.87312708
  0.87321247  0.87848862  0.88041977  0.88474034  0.88552558  0.885828
  0.89368857  0.89382832  0.8939543   0.89703908  0.89772073  0.90073702
  0.90199633  0.9027191   0.90703761  0.90711734  0.9080245   0.90812269
  0.90855856  0.90949526  0.91142738  0.91360929  0.91382451  0.91648519
  0.91676855  0.91952316  0.92515442  0.92630006  0.92631098  0.92758183
  0.92922948  0.93080474  0.95105538  0.95159681  0.95365936  0.95520357
  0.95539854  0.96493168  0.9655697   0.9659156   0.96882017  0.96970548
  0.97286537  0.97349167  0.9757438   0.97606031  0.97729196  0.97783429
  0.97797632  0.97857747  0.97961504  0.98017044  0.98027765  0.98126519
  0.98145938  0.98185956  0.98209838  0.98216816  0.98296497  0.98386997
  0.98494776  0.98530111  0.98557567  0.9860716   0.98663527  0.98729883
  0.98819786  0.98893002  0.98918914  0.99076303  0.9907661   0.99242643
  0.9936965   0.99403928  0.99450739  0.99527577  0.99557214  0.99586569
  0.9967075   0.99835258  0.99880096  0.99904131  0.9991537   1.00133827
  1.00164381  1.00256471  1.00293291  1.0034538   1.00357333  1.0047074
  1.00656124  1.00687096  1.00713222  1.00891465  1.01475712  1.02243533
  1.02442135  1.02571907  1.02693857  1.03344019  1.0349128   1.03800929
  1.03888603  1.04699918  1.05033271  1.05215659  1.06503463  1.06646883
  1.0762837   1.07764039  1.09339114  1.10476586  1.14262567  1.15325065]

  UserWarning,

2022-10-31 11:03:54,815:INFO:Calculating mean and std
2022-10-31 11:03:54,815:INFO:Creating metrics dataframe
2022-10-31 11:03:54,815:INFO:Uploading results into container
2022-10-31 11:03:54,815:INFO:Uploading model into container now
2022-10-31 11:03:54,815:INFO:master_model_container: 33
2022-10-31 11:03:54,815:INFO:display_container: 2
2022-10-31 11:03:54,815:INFO:LGBMRegressor(random_state=3360)
2022-10-31 11:03:54,815:INFO:create_model() successfully completed......................................
2022-10-31 11:03:54,952:WARNING:create_model() for LGBMRegressor(random_state=3360) raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:03:54,952:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:54,952:INFO:Initializing create_model()
2022-10-31 11:03:54,952:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:54,952:INFO:Checking exceptions
2022-10-31 11:03:54,952:INFO:Importing libraries
2022-10-31 11:03:54,952:INFO:Copying training dataset
2022-10-31 11:03:54,952:INFO:Defining folds
2022-10-31 11:03:54,952:INFO:Declaring metric variables
2022-10-31 11:03:54,952:INFO:Importing untrained model
2022-10-31 11:03:54,952:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-31 11:03:54,952:INFO:Starting cross validation
2022-10-31 11:03:54,952:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:03:57,503:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-2.37596419e-01 -1.94256336e-01 -1.12023306e-01 -1.10471550e-01
 -8.02354037e-02 -7.46557621e-02 -7.29400346e-02 -5.02265231e-02
 -4.98018070e-02 -4.88968604e-02 -4.87589048e-02 -3.52019601e-02
 -3.47808944e-02 -2.21765949e-02 -1.71202922e-02 -1.65342595e-02
 -1.48395215e-02 -1.14935059e-02 -1.11339409e-02 -6.42626831e-03
 -2.51185150e-03 -1.75283714e-03 -3.76571320e-04  3.68992462e-03
  1.48199726e-02  1.48308981e-02  1.56338846e-02  1.72519115e-02
  1.96802142e-02  2.30002197e-02  2.42667965e-02  2.89406562e-02
  3.27934830e-02  3.36371562e-02  3.63848667e-02  3.84501156e-02
  3.85283477e-02  4.11921111e-02  4.27705629e-02  4.33800212e-02
  4.47036674e-02  4.52421779e-02  4.52424471e-02  4.54885472e-02
  4.56770695e-02  4.84379658e-02  4.89620914e-02  5.24547209e-02
  5.40338655e-02  5.45208511e-02  5.45667046e-02  5.65716748e-02
  5.81017618e-02  6.37834986e-02  6.63498353e-02  6.78703009e-02
  6.88981842e-02  7.24853792e-02  7.39871802e-02  7.43862445e-02
  7.45928999e-02  7.63693839e-02  7.72370876e-02  7.89345578e-02
  8.04858884e-02  8.05021224e-02  8.06632887e-02  8.11406947e-02
  8.12964858e-02  8.17984910e-02  8.22686521e-02  8.26359081e-02
  8.48272710e-02  8.58043465e-02  8.71859161e-02  8.74732908e-02
  8.76692261e-02  8.95257898e-02  8.96245760e-02  9.19327820e-02
  9.24451097e-02  9.35649451e-02  9.40696268e-02  9.47194435e-02
  9.49935107e-02  9.53621166e-02  9.53815289e-02  9.60087653e-02
  9.60113229e-02  9.60444873e-02  9.65046360e-02  9.74869762e-02
  1.00501831e-01  1.01506517e-01  1.02309743e-01  1.03925354e-01
  1.04347941e-01  1.06270281e-01  1.06304757e-01  1.06910393e-01
  1.07567044e-01  1.11501702e-01  1.14987997e-01  1.15187004e-01
  1.15344534e-01  1.15985138e-01  1.16722470e-01  1.17296224e-01
  1.17936947e-01  1.18771837e-01  1.21347924e-01  1.23468972e-01
  1.24653867e-01  1.26347970e-01  1.27633721e-01  1.27845475e-01
  1.27959713e-01  1.31497115e-01  1.32973124e-01  1.33384107e-01
  1.33802671e-01  1.33865845e-01  1.34448422e-01  1.36827746e-01
  1.38144906e-01  1.38235779e-01  1.41328287e-01  1.47492076e-01
  1.47538528e-01  1.47573248e-01  1.48345919e-01  1.51264399e-01
  1.53597473e-01  1.54586926e-01  1.55418456e-01  1.57976084e-01
  1.63401264e-01  1.64245430e-01  1.66348657e-01  1.67310401e-01
  1.69227742e-01  1.71138601e-01  1.73599098e-01  1.75646282e-01
  1.75819030e-01  1.77599450e-01  1.78363860e-01  1.80553948e-01
  1.82002818e-01  1.82260398e-01  1.82905651e-01  1.86638649e-01
  1.86684261e-01  1.87218307e-01  1.89298583e-01  1.91094378e-01
  1.91429438e-01  1.93074841e-01  1.93371702e-01  1.93750008e-01
  1.94383446e-01  1.95057832e-01  1.95123010e-01  2.00328957e-01
  2.00964074e-01  2.02213685e-01  2.02345571e-01  2.04373178e-01
  2.04472986e-01  2.05042849e-01  2.08240177e-01  2.08831137e-01
  2.10036137e-01  2.11610288e-01  2.17528523e-01  2.18045067e-01
  2.18082732e-01  2.19526619e-01  2.22755275e-01  2.23260729e-01
  2.23899320e-01  2.23902225e-01  2.26632144e-01  2.26852704e-01
  2.27955517e-01  2.30600547e-01  2.30849832e-01  2.31105217e-01
  2.33492890e-01  2.35398227e-01  2.36091300e-01  2.39394558e-01
  2.42232865e-01  2.42554909e-01  2.43141893e-01  2.44061582e-01
  2.48016979e-01  2.53374155e-01  2.54609969e-01  2.57854033e-01
  2.58117695e-01  2.60100085e-01  2.61022766e-01  2.61134622e-01
  2.62064046e-01  2.66292245e-01  2.66479342e-01  2.66849358e-01
  2.68914288e-01  2.70441958e-01  2.70448634e-01  2.71309973e-01
  2.78121290e-01  2.81043686e-01  2.84001591e-01  2.84133167e-01
  2.85628651e-01  2.86436976e-01  2.88638832e-01  2.88823978e-01
  2.91800978e-01  2.92798000e-01  2.95317393e-01  2.97081778e-01
  2.99840131e-01  3.06168454e-01  3.07355157e-01  3.08791917e-01
  3.09817607e-01  3.10628211e-01  3.11091725e-01  3.11522985e-01
  3.13309069e-01  3.15448638e-01  3.16415147e-01  3.17007780e-01
  3.18442815e-01  3.19258316e-01  3.20882965e-01  3.24903438e-01
  3.27778148e-01  3.30288847e-01  3.33673278e-01  3.38285333e-01
  3.39447918e-01  3.40167715e-01  3.42326053e-01  3.43103623e-01
  3.47314793e-01  3.49062954e-01  3.49428959e-01  3.56037489e-01
  3.65470661e-01  3.68140756e-01  3.71481663e-01  3.72507987e-01
  3.72581788e-01  3.74053080e-01  3.76196430e-01  3.77390033e-01
  3.79506194e-01  3.79970861e-01  3.86377631e-01  3.88654701e-01
  3.92688728e-01  3.93542074e-01  3.93734228e-01  3.94552072e-01
  3.97492564e-01  4.01595479e-01  4.04651155e-01  4.10005722e-01
  4.19656396e-01  4.26774873e-01  4.27222227e-01  4.28438096e-01
  4.29911312e-01  4.32572477e-01  4.34338958e-01  4.34888284e-01
  4.35826152e-01  4.43326555e-01  4.43912895e-01  4.45853864e-01
  4.50210741e-01  4.55136429e-01  4.55426158e-01  4.59905772e-01
  4.61449198e-01  4.63755195e-01  4.64514593e-01  4.65508753e-01
  4.68583363e-01  4.70875689e-01  4.74672808e-01  4.75170790e-01
  4.83810955e-01  4.85014873e-01  4.88396612e-01  4.89242056e-01
  4.90134546e-01  4.93704888e-01  4.97850551e-01  4.99447895e-01
  5.03806351e-01  5.08051294e-01  5.09498285e-01  5.15135711e-01
  5.15362482e-01  5.15477524e-01  5.17390154e-01  5.21717684e-01
  5.25178364e-01  5.26744173e-01  5.26800393e-01  5.27300911e-01
  5.29233961e-01  5.29525276e-01  5.34222172e-01  5.34590148e-01
  5.36514342e-01  5.41217274e-01  5.43130948e-01  5.44560737e-01
  5.45435063e-01  5.45527419e-01  5.47512725e-01  5.47768476e-01
  5.49783478e-01  5.49804603e-01  5.53877176e-01  5.55187105e-01
  5.55392926e-01  5.64044340e-01  5.64533323e-01  5.65319893e-01
  5.67866778e-01  5.74077401e-01  5.78279280e-01  5.83615323e-01
  5.83740562e-01  5.88292088e-01  5.92754743e-01  5.95084977e-01
  5.98546453e-01  6.04042493e-01  6.05123151e-01  6.12086990e-01
  6.13159326e-01  6.14569568e-01  6.15861045e-01  6.16126165e-01
  6.18596828e-01  6.18842427e-01  6.19728737e-01  6.21036556e-01
  6.25119587e-01  6.28566341e-01  6.32999004e-01  6.33273326e-01
  6.40665237e-01  6.43310034e-01  6.46139954e-01  6.47101992e-01
  6.51286223e-01  6.52945834e-01  6.54434234e-01  6.57453917e-01
  6.63977901e-01  6.64097661e-01  6.64313090e-01  6.64974024e-01
  6.66857794e-01  6.68691233e-01  6.69190141e-01  6.69469109e-01
  6.71073599e-01  6.76628732e-01  6.77789487e-01  6.78219373e-01
  6.78273314e-01  6.80560781e-01  6.82348332e-01  6.85064163e-01
  6.85634958e-01  6.87079165e-01  6.90208217e-01  6.93748325e-01
  6.94265922e-01  6.95565184e-01  6.95620531e-01  6.95898837e-01
  6.96501615e-01  6.99679251e-01  7.00317357e-01  7.01332819e-01
  7.01396090e-01  7.05771496e-01  7.08316845e-01  7.12307136e-01
  7.12408388e-01  7.13836385e-01  7.18526752e-01  7.19933030e-01
  7.20025051e-01  7.23479188e-01  7.24718154e-01  7.25380645e-01
  7.30379778e-01  7.34294793e-01  7.34315958e-01  7.35335516e-01
  7.36597964e-01  7.39454604e-01  7.39556297e-01  7.39639539e-01
  7.42821770e-01  7.46293730e-01  7.48362149e-01  7.50080255e-01
  7.51072293e-01  7.55213752e-01  7.56923129e-01  7.57051876e-01
  7.70108267e-01  7.71652272e-01  7.77952037e-01  7.83894598e-01
  7.84493392e-01  7.86303699e-01  7.89710285e-01  7.90204850e-01
  7.97034197e-01  7.98474375e-01  7.98590167e-01  7.99187873e-01
  8.00802803e-01  8.01723794e-01  8.06604661e-01  8.10367710e-01
  8.38076253e-01  8.40906581e-01  8.48821468e-01  8.51488672e-01
  8.54951835e-01  8.56621714e-01  8.65096363e-01  8.65684449e-01
  8.70038295e-01  8.74074002e-01  8.77254237e-01  8.80010296e-01
  8.82197396e-01  8.90046499e-01  8.97718703e-01  8.98168441e-01
  8.98677745e-01  9.00501872e-01  9.00906803e-01  9.04896316e-01
  9.05402158e-01  9.08531253e-01  9.09867316e-01  9.10515550e-01
  9.12291226e-01  9.13615209e-01  9.19037030e-01  9.19629243e-01
  9.21366089e-01  9.21601656e-01  9.22123873e-01  9.23902866e-01
  9.26799102e-01  9.32341353e-01  9.32438098e-01  9.33233290e-01
  9.36158284e-01  9.36945094e-01  9.39407022e-01  9.41852480e-01
  9.42326408e-01  9.43982730e-01  9.44807743e-01  9.45107770e-01
  9.46378912e-01  9.46526656e-01  9.47509085e-01  9.50972211e-01
  9.53756454e-01  9.54805741e-01  9.58418711e-01  9.58630153e-01
  9.65592226e-01  9.67941148e-01  9.68679993e-01  9.68761535e-01
  9.69481399e-01  9.70005206e-01  9.70226895e-01  9.70321834e-01
  9.74135030e-01  9.74249267e-01  9.74532043e-01  9.74539503e-01
  9.75650780e-01  9.75953187e-01  9.76010136e-01  9.78066489e-01
  9.78104338e-01  9.79338366e-01  9.79526937e-01  9.80747796e-01
  9.83473097e-01  9.85000122e-01  9.85489182e-01  9.85803543e-01
  9.88140079e-01  9.88256794e-01  9.89527262e-01  9.89775524e-01
  9.89889760e-01  9.91291274e-01  9.93788312e-01  9.94062301e-01
  9.94533525e-01  9.95004077e-01  9.95161361e-01  9.95698717e-01
  1.00386135e+00  1.00453077e+00  1.00524474e+00  1.00596027e+00
  1.01137236e+00  1.01327134e+00  1.01604588e+00  1.02072696e+00
  1.02082201e+00  1.02133592e+00  1.04292107e+00  1.05277406e+00
  1.08277826e+00  1.11366661e+00  1.11490094e+00  1.16976726e+00
  1.21152181e+00]

  UserWarning,

2022-10-31 11:03:57,519:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.21887257e-01 -8.54925327e-02 -7.94643625e-02 -7.10508369e-02
 -6.99412922e-02 -5.74292095e-02 -5.27005164e-02 -4.91852114e-02
 -4.88294324e-02 -3.90512065e-02 -3.77164195e-02 -2.67805768e-02
 -2.51889256e-02 -2.06296099e-02 -1.92713465e-02 -8.73136256e-03
 -7.57706659e-03 -6.95111688e-03 -6.64392604e-03 -5.06904124e-03
 -1.63313582e-03 -1.25431604e-03  5.37726418e-04  1.28110038e-03
  3.93629500e-03  5.42299364e-03  8.72611566e-03  1.07218137e-02
  1.19494881e-02  1.19617415e-02  1.67100089e-02  1.76213996e-02
  1.81177393e-02  1.81559136e-02  1.87126846e-02  1.97415339e-02
  2.37042361e-02  2.51373730e-02  2.60034500e-02  2.89717534e-02
  2.99427756e-02  3.00799325e-02  3.03224544e-02  3.06479601e-02
  3.17465125e-02  3.31008046e-02  3.36882606e-02  3.51025143e-02
  3.53020185e-02  3.62235216e-02  3.85476390e-02  3.95209414e-02
  4.07695707e-02  4.45092611e-02  4.73443825e-02  4.77975914e-02
  4.83884911e-02  4.84409461e-02  5.08408247e-02  5.09840957e-02
  5.18854207e-02  5.28489176e-02  5.42352865e-02  5.67334022e-02
  5.79265763e-02  5.94088341e-02  6.21762948e-02  6.25423413e-02
  6.96163062e-02  6.96768557e-02  6.97504962e-02  7.06138319e-02
  7.11925878e-02  7.25806330e-02  7.27792066e-02  7.30663930e-02
  7.56700718e-02  7.59041241e-02  7.61991104e-02  7.92695026e-02
  8.08682929e-02  8.08701028e-02  8.19211154e-02  8.20990120e-02
  8.32510687e-02  8.39420273e-02  8.51541723e-02  8.57314488e-02
  8.58593004e-02  8.59176565e-02  8.68789814e-02  8.73381928e-02
  8.78975345e-02  8.92261080e-02  9.40785994e-02  9.86734113e-02
  1.00465960e-01  1.02047824e-01  1.03561452e-01  1.04915807e-01
  1.05841843e-01  1.09427804e-01  1.12662049e-01  1.14876114e-01
  1.16379101e-01  1.16390103e-01  1.17530324e-01  1.19713863e-01
  1.26524596e-01  1.28618877e-01  1.31591015e-01  1.36185392e-01
  1.36933629e-01  1.38251005e-01  1.38767415e-01  1.38902857e-01
  1.38951161e-01  1.42618225e-01  1.44392048e-01  1.44590369e-01
  1.45118888e-01  1.45943115e-01  1.46125717e-01  1.46493675e-01
  1.47217973e-01  1.47680471e-01  1.47837123e-01  1.48389578e-01
  1.51401893e-01  1.52225023e-01  1.53129226e-01  1.54773404e-01
  1.56857426e-01  1.57011945e-01  1.57142822e-01  1.57207710e-01
  1.59157712e-01  1.60565973e-01  1.61924638e-01  1.61978890e-01
  1.62048999e-01  1.62519499e-01  1.63196340e-01  1.66463401e-01
  1.67484752e-01  1.67585005e-01  1.67695779e-01  1.68934018e-01
  1.69736991e-01  1.73533357e-01  1.74434611e-01  1.75624935e-01
  1.77084426e-01  1.83196191e-01  1.83235154e-01  1.84570529e-01
  1.84884702e-01  1.85693098e-01  1.85754890e-01  1.86686072e-01
  1.87873524e-01  1.89571423e-01  1.89741904e-01  1.90333721e-01
  1.92590247e-01  1.95096383e-01  1.95140716e-01  1.96300910e-01
  1.96893178e-01  1.99839719e-01  1.99969141e-01  2.00819805e-01
  2.01647505e-01  2.01778677e-01  2.02580515e-01  2.02800365e-01
  2.03220518e-01  2.05188676e-01  2.05924134e-01  2.06578634e-01
  2.07838235e-01  2.11430764e-01  2.13937749e-01  2.18924599e-01
  2.19633207e-01  2.23107910e-01  2.23952545e-01  2.27925150e-01
  2.28772016e-01  2.30842587e-01  2.33612488e-01  2.36248119e-01
  2.36606764e-01  2.41464971e-01  2.43453033e-01  2.44745863e-01
  2.46947581e-01  2.46979457e-01  2.48882377e-01  2.51456443e-01
  2.55064640e-01  2.56153052e-01  2.58268529e-01  2.60024932e-01
  2.62018020e-01  2.64194988e-01  2.65005064e-01  2.65488940e-01
  2.65578772e-01  2.66670063e-01  2.67049127e-01  2.67148363e-01
  2.68851399e-01  2.68952281e-01  2.71999081e-01  2.79080617e-01
  2.85747208e-01  2.89068290e-01  2.96863489e-01  2.98238893e-01
  2.99195544e-01  3.00741693e-01  3.02475537e-01  3.03615134e-01
  3.03682410e-01  3.05438999e-01  3.08323038e-01  3.13297524e-01
  3.23643921e-01  3.24649031e-01  3.40753664e-01  3.45608718e-01
  3.46081730e-01  3.47497690e-01  3.48106564e-01  3.49816952e-01
  3.49977548e-01  3.51943604e-01  3.52750195e-01  3.56341444e-01
  3.56364109e-01  3.59951459e-01  3.61673042e-01  3.66307072e-01
  3.66359365e-01  3.66638320e-01  3.69794246e-01  3.70996061e-01
  3.71044751e-01  3.74648975e-01  3.79465098e-01  3.83101407e-01
  3.91600661e-01  3.91680438e-01  3.94412123e-01  3.94966762e-01
  3.96541818e-01  3.97100066e-01  3.99838132e-01  3.99931157e-01
  4.01946072e-01  4.02970824e-01  4.06617964e-01  4.07408404e-01
  4.07742422e-01  4.09669216e-01  4.18104915e-01  4.29495434e-01
  4.33018300e-01  4.33727708e-01  4.34806241e-01  4.46827860e-01
  4.53891593e-01  4.55190690e-01  4.56780346e-01  4.60824441e-01
  4.62177339e-01  4.70316529e-01  4.85133172e-01  4.89938946e-01
  4.93655956e-01  4.94555247e-01  4.98699052e-01  5.02160753e-01
  5.03232404e-01  5.05715648e-01  5.10649018e-01  5.14472952e-01
  5.14595044e-01  5.15945492e-01  5.17960048e-01  5.18795085e-01
  5.28322009e-01  5.29634650e-01  5.30211083e-01  5.30761830e-01
  5.31954108e-01  5.33201841e-01  5.35737756e-01  5.35919807e-01
  5.36147846e-01  5.37874562e-01  5.38085873e-01  5.38844357e-01
  5.45089204e-01  5.46765451e-01  5.47793944e-01  5.50549130e-01
  5.53016944e-01  5.53516074e-01  5.56501768e-01  5.57553549e-01
  5.58673278e-01  5.60315900e-01  5.61254233e-01  5.63233175e-01
  5.64783182e-01  5.65657604e-01  5.65875462e-01  5.65901514e-01
  5.66625046e-01  5.68890512e-01  5.69512588e-01  5.70047798e-01
  5.70837727e-01  5.72108498e-01  5.72994831e-01  5.76334188e-01
  5.78996825e-01  5.79586030e-01  5.81017186e-01  5.84283531e-01
  5.84422632e-01  5.87783696e-01  5.89742015e-01  5.96376819e-01
  6.00025575e-01  6.03357771e-01  6.06360712e-01  6.07044308e-01
  6.10385075e-01  6.10766756e-01  6.11023321e-01  6.14570631e-01
  6.16454386e-01  6.18090996e-01  6.18894363e-01  6.21755013e-01
  6.22684265e-01  6.23082840e-01  6.28447501e-01  6.38323474e-01
  6.38645333e-01  6.41219274e-01  6.42168521e-01  6.42762121e-01
  6.44251370e-01  6.44332715e-01  6.45988952e-01  6.46060559e-01
  6.49214524e-01  6.52482990e-01  6.53144635e-01  6.54691120e-01
  6.55697487e-01  6.58980920e-01  6.60965521e-01  6.61857121e-01
  6.62555934e-01  6.66611436e-01  6.70156809e-01  6.71029862e-01
  6.73535523e-01  6.74112918e-01  6.74240822e-01  6.78330576e-01
  6.80243631e-01  6.80955326e-01  6.82172021e-01  6.83576787e-01
  6.83633895e-01  6.84708146e-01  6.84845714e-01  6.87044751e-01
  6.92285520e-01  6.98225028e-01  7.00724388e-01  7.07240355e-01
  7.09134365e-01  7.14730872e-01  7.18341453e-01  7.32138897e-01
  7.32667565e-01  7.33940529e-01  7.36178775e-01  7.37908334e-01
  7.38717615e-01  7.51997236e-01  7.60703017e-01  7.63819462e-01
  7.67259347e-01  7.69318368e-01  7.69608073e-01  7.75821407e-01
  7.75868010e-01  7.76209867e-01  7.76979105e-01  7.80281276e-01
  7.81030656e-01  7.81579297e-01  7.83984903e-01  7.84096743e-01
  7.95725161e-01  7.98103863e-01  8.22730404e-01  8.23617359e-01
  8.24850821e-01  8.28287644e-01  8.29897254e-01  8.46139138e-01
  8.48350299e-01  8.50481975e-01  8.50485823e-01  8.51241110e-01
  8.51974353e-01  8.55301030e-01  8.57040236e-01  8.66474959e-01
  8.76457618e-01  8.76936714e-01  8.81331269e-01  8.83692315e-01
  8.84920268e-01  8.87699857e-01  8.90313482e-01  8.93891260e-01
  8.96793807e-01  8.97378934e-01  8.97491806e-01  8.97606974e-01
  8.97651671e-01  8.98230384e-01  8.99229364e-01  8.99763961e-01
  9.00520374e-01  9.01336940e-01  9.02246525e-01  9.03695476e-01
  9.03814675e-01  9.04855115e-01  9.07519031e-01  9.07651637e-01
  9.08155253e-01  9.09033966e-01  9.12987417e-01  9.13030387e-01
  9.13674710e-01  9.15612936e-01  9.15617000e-01  9.16462721e-01
  9.17460607e-01  9.18592027e-01  9.20905797e-01  9.29693521e-01
  9.33878066e-01  9.34716465e-01  9.36306606e-01  9.40942537e-01
  9.42195048e-01  9.43548406e-01  9.43724941e-01  9.50961105e-01
  9.51775703e-01  9.52686369e-01  9.53115042e-01  9.53549961e-01
  9.54794866e-01  9.55965540e-01  9.56593466e-01  9.57325129e-01
  9.57364644e-01  9.60516325e-01  9.61054300e-01  9.61388250e-01
  9.62546182e-01  9.63524713e-01  9.64491107e-01  9.67255243e-01
  9.67370411e-01  9.68039996e-01  9.69190377e-01  9.69618353e-01
  9.70255142e-01  9.71813662e-01  9.73848147e-01  9.74266036e-01
  9.76581911e-01  9.78542596e-01  9.80187816e-01  9.81391412e-01
  9.81816225e-01  9.83147294e-01  9.83876695e-01  9.87852565e-01
  9.88594455e-01  9.90730918e-01  9.90789234e-01  9.91173687e-01
  9.91202589e-01  9.91703169e-01  9.92226641e-01  9.92516335e-01
  9.93126475e-01  9.94461448e-01  9.96719538e-01  9.96824558e-01
  9.98117749e-01  9.98269474e-01  9.98315906e-01  1.00076842e+00
  1.00133642e+00  1.00335271e+00  1.00562506e+00  1.00707292e+00
  1.00830492e+00  1.01097204e+00  1.01183984e+00  1.01722230e+00
  1.01749136e+00  1.03024085e+00  1.03127983e+00  1.03320575e+00
  1.03669154e+00  1.04056936e+00  1.04319791e+00  1.04630663e+00
  1.05434131e+00  1.05653575e+00  1.05950887e+00  1.06095856e+00
  1.06212722e+00  1.06511872e+00  1.06751861e+00  1.06925268e+00
  1.07851093e+00  1.08522957e+00  1.09232336e+00  1.18084279e+00]

  UserWarning,

2022-10-31 11:03:57,519:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.20084184 -0.13582909 -0.10226986 -0.10036909 -0.09098774 -0.09059253
 -0.08628922 -0.0836738  -0.0808127  -0.07674612 -0.07546329 -0.07247207
 -0.06431126 -0.05585144 -0.05318709 -0.05120556 -0.04777614 -0.04735596
 -0.03722775 -0.03498673 -0.02440395 -0.02421155 -0.02404776 -0.02269296
 -0.02128748 -0.02033924 -0.0172188  -0.01699566 -0.01374913 -0.01336546
 -0.01235795 -0.01124303 -0.00914772 -0.00883171 -0.00448867  0.00851642
  0.00894124  0.01345512  0.01345949  0.01540172  0.01770841  0.02298323
  0.02298661  0.02315321  0.02716823  0.02839223  0.03118983  0.03169949
  0.03264162  0.03286613  0.03332715  0.03396621  0.03439599  0.03558185
  0.03565477  0.03955375  0.03958318  0.03966448  0.04248815  0.04607319
  0.04646773  0.04672136  0.04847932  0.04878869  0.04964849  0.04998703
  0.05311829  0.05506444  0.05635685  0.06405875  0.06435329  0.06550552
  0.06583468  0.06595645  0.06866056  0.07006489  0.07078345  0.07085033
  0.07103561  0.07268326  0.0728734   0.07382165  0.07626766  0.07693135
  0.07928863  0.07954439  0.07977341  0.08261276  0.08270133  0.08398613
  0.08424578  0.08499372  0.08589151  0.08631416  0.08767875  0.08907819
  0.08987432  0.09051004  0.093819    0.09533756  0.09633901  0.09634389
  0.09744675  0.09785885  0.10481158  0.10915674  0.10918438  0.10932514
  0.10951996  0.11058914  0.1113399   0.11145986  0.11147222  0.11173995
  0.11312072  0.11473022  0.1149224   0.11596105  0.11724219  0.11897499
  0.12022455  0.12031238  0.12273842  0.12309052  0.12564098  0.12804294
  0.12824671  0.12988812  0.13086457  0.13275518  0.13341903  0.13518326
  0.13559382  0.13685482  0.1375013   0.13856893  0.13932136  0.13966525
  0.14064965  0.14095256  0.14299177  0.14412337  0.14429821  0.14505984
  0.14574824  0.148037    0.1505814   0.1507785   0.151851    0.15337613
  0.15451207  0.15553607  0.15802573  0.15816351  0.15867822  0.15899926
  0.16412838  0.16453729  0.16471011  0.16541794  0.16834799  0.17014811
  0.17164119  0.17371969  0.17414702  0.17490562  0.17649595  0.18017254
  0.18113004  0.18198946  0.18386012  0.18429637  0.18565655  0.18570541
  0.18744488  0.18800832  0.18949474  0.19119806  0.19183999  0.1954519
  0.20274939  0.20375196  0.20474201  0.20492561  0.20986626  0.20996535
  0.21014229  0.21288978  0.21666746  0.21854546  0.21875454  0.2191792
  0.22064477  0.22158013  0.22200331  0.22374645  0.22571324  0.22695788
  0.22799986  0.22996959  0.23039745  0.230632    0.23103145  0.23124672
  0.23165794  0.23339573  0.24039706  0.24175288  0.24224381  0.24581029
  0.24678262  0.24754999  0.24798054  0.2524196   0.25382753  0.2541651
  0.25661827  0.25842545  0.25968468  0.26048943  0.26164527  0.2641428
  0.26442634  0.26475122  0.26774985  0.27089277  0.27435601  0.27482808
  0.27542346  0.27690793  0.27837141  0.27991579  0.29836587  0.29998091
  0.30552042  0.30658468  0.30966369  0.3127884   0.31518616  0.31655332
  0.32224808  0.32596752  0.32996861  0.33152861  0.34105389  0.34639564
  0.3477012   0.350481    0.35387404  0.35683917  0.36147853  0.36411753
  0.37059159  0.37350295  0.37666195  0.37683358  0.38396658  0.38479349
  0.38600757  0.3961886   0.40446157  0.40568467  0.41037091  0.41348959
  0.41406615  0.41558485  0.41860347  0.42064009  0.42295516  0.42318431
  0.42376637  0.42818998  0.4365891   0.43670839  0.43832339  0.44026625
  0.44408496  0.44584149  0.44680016  0.45234018  0.45732226  0.45948504
  0.45949328  0.4610289   0.46304405  0.46443777  0.46474311  0.46501092
  0.46770953  0.47244328  0.47528726  0.48296276  0.48687337  0.48871185
  0.48884542  0.49179466  0.49439699  0.49474247  0.49990834  0.50180712
  0.50256122  0.51549093  0.5213607   0.524152    0.52756913  0.52958171
  0.53371255  0.5382611   0.53941234  0.5399081   0.53991511  0.5419284
  0.5446476   0.54568417  0.54695845  0.54710693  0.54998852  0.55676866
  0.55829511  0.5589521   0.5661035   0.56804477  0.57002429  0.57068687
  0.57408336  0.5747394   0.57507579  0.57563935  0.57793939  0.5780665
  0.58263875  0.58278508  0.58613513  0.59006419  0.59292892  0.59384997
  0.59635988  0.59687605  0.59807736  0.59982843  0.60044182  0.6026915
  0.60282267  0.60322856  0.60343202  0.60415172  0.60418902  0.60566131
  0.6064578   0.60699162  0.61004279  0.61061159  0.61184115  0.61211298
  0.61361264  0.61392051  0.61408182  0.61497263  0.61522297  0.61938099
  0.62083012  0.62241081  0.62709093  0.62824141  0.63669027  0.64092258
  0.64228027  0.64432414  0.64468416  0.64479891  0.64971895  0.64986583
  0.6558076   0.65686816  0.65744907  0.66181433  0.66467897  0.66668038
  0.66926234  0.66965481  0.67258996  0.67651485  0.67852387  0.67859407
  0.68478463  0.68517061  0.68757089  0.68825361  0.68843427  0.69029171
  0.69403292  0.69522339  0.69598624  0.69877213  0.69925869  0.70868456
  0.71016703  0.7124645   0.71318947  0.71457578  0.71477525  0.71561883
  0.71657044  0.71731922  0.72012864  0.72237744  0.72299664  0.72553828
  0.73101779  0.73102486  0.73274034  0.73299434  0.73462248  0.73560064
  0.73625505  0.74239986  0.74766143  0.74864835  0.75139244  0.75437718
  0.75577783  0.75852634  0.75883215  0.7624791   0.76459705  0.76478215
  0.77836394  0.78019237  0.7861367   0.79055356  0.79233787  0.8000705
  0.80790087  0.80981589  0.81048282  0.81088375  0.81291599  0.8157525
  0.81907865  0.82235529  0.83445024  0.84337421  0.84603134  0.84727206
  0.85686077  0.86084129  0.86133894  0.8681075   0.87041835  0.87173073
  0.87224299  0.87738816  0.88018323  0.8813727   0.88189632  0.88503552
  0.88543612  0.88672313  0.88936461  0.89140586  0.89257499  0.89491054
  0.8988125   0.90018961  0.90035243  0.90217923  0.90447661  0.90509033
  0.90711922  0.90897893  0.90949588  0.90974699  0.91430557  0.91448144
  0.91464832  0.91778267  0.91806469  0.92013199  0.92111739  0.92250125
  0.92632663  0.92735159  0.92824577  0.92834046  0.92837041  0.92994545
  0.93147624  0.93798913  0.94440582  0.95283766  0.95719687  0.95776707
  0.96324081  0.96630977  0.96717328  0.96816726  0.96921575  0.97044501
  0.97104913  0.97152986  0.97205615  0.97372391  0.97380095  0.97433266
  0.97567565  0.97641917  0.9772676   0.9774954   0.97838432  0.98075686
  0.98286396  0.98702137  0.9873938   0.98801061  0.98819988  0.9888498
  0.98986178  0.99028282  0.99108288  0.99152953  0.99173529  0.99250767
  0.99327827  0.99602715  0.99651184  0.99692694  1.00156918  1.00164687
  1.00202332  1.00210589  1.00212779  1.00267433  1.0061107   1.00703437
  1.00774439  1.01226808  1.02914356  1.02983722  1.03606361  1.05983346
  1.08865032  1.09906036  1.1270348   1.13419674  1.23098254]

  UserWarning,

2022-10-31 11:03:57,566:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.06060264e-01 -5.35373194e-02 -5.16455819e-02 -4.91547597e-02
 -4.76165588e-02 -4.61344090e-02 -3.64640808e-02 -2.97624379e-02
 -2.66602006e-02 -2.27577801e-02 -1.81787048e-02 -1.44066893e-02
 -9.65324552e-03 -6.75955845e-03 -4.68889405e-03 -4.48527152e-03
 -1.70749078e-03 -1.68781741e-03 -4.94941646e-04  6.28116001e-03
  6.29943851e-03  8.52210562e-03  1.18069975e-02  1.47236143e-02
  1.49404489e-02  1.54334142e-02  1.64213997e-02  2.36784259e-02
  2.81978493e-02  2.82096499e-02  2.99106874e-02  3.04126275e-02
  3.13350122e-02  3.58603771e-02  3.72739865e-02  3.74410579e-02
  3.85910616e-02  3.96769280e-02  3.98866148e-02  4.30438082e-02
  4.57722432e-02  4.86877322e-02  4.97914154e-02  5.05212131e-02
  5.15757605e-02  5.17944996e-02  5.39079062e-02  5.52238318e-02
  5.70500331e-02  5.72002033e-02  6.37354478e-02  6.41294355e-02
  6.46418269e-02  6.56781583e-02  6.64502599e-02  6.64999189e-02
  6.75587148e-02  6.94485269e-02  6.97258779e-02  7.05904775e-02
  7.11461361e-02  7.13634787e-02  7.16064177e-02  7.17906282e-02
  7.25231188e-02  7.26413714e-02  7.31552600e-02  7.32960283e-02
  7.36454311e-02  7.57277997e-02  7.57568625e-02  7.58005571e-02
  7.72962927e-02  7.73987850e-02  7.89879833e-02  8.03305953e-02
  8.14767294e-02  8.15901849e-02  8.19250732e-02  8.19513902e-02
  8.56131333e-02  8.61883814e-02  8.64983242e-02  8.77353917e-02
  9.24547102e-02  9.33353528e-02  9.51372514e-02  9.58712113e-02
  9.80508098e-02  9.84926600e-02  1.00698354e-01  1.01848567e-01
  1.02404276e-01  1.04231819e-01  1.04242644e-01  1.04524307e-01
  1.04787548e-01  1.08702177e-01  1.08708364e-01  1.09262659e-01
  1.09723252e-01  1.11054168e-01  1.15165041e-01  1.15523345e-01
  1.15856507e-01  1.18190947e-01  1.22486294e-01  1.26253002e-01
  1.26557011e-01  1.27669562e-01  1.28221202e-01  1.29144592e-01
  1.29886733e-01  1.30063081e-01  1.30418090e-01  1.33620120e-01
  1.39315628e-01  1.39975416e-01  1.40001900e-01  1.40913841e-01
  1.40936053e-01  1.43496587e-01  1.44109875e-01  1.44453269e-01
  1.49510784e-01  1.53225973e-01  1.55839650e-01  1.55910396e-01
  1.60545190e-01  1.60790530e-01  1.61668975e-01  1.64266040e-01
  1.66353805e-01  1.67055443e-01  1.67684332e-01  1.69905666e-01
  1.71816228e-01  1.72141973e-01  1.74257624e-01  1.74852389e-01
  1.75489415e-01  1.76067850e-01  1.77973731e-01  1.80899324e-01
  1.81675455e-01  1.84302359e-01  1.85871849e-01  1.87664787e-01
  1.91215440e-01  1.93088086e-01  1.94228379e-01  1.95016550e-01
  1.95647545e-01  1.95762720e-01  1.96821300e-01  1.98697109e-01
  1.98759831e-01  1.99982702e-01  2.00832943e-01  2.03951093e-01
  2.04724068e-01  2.05877531e-01  2.06855623e-01  2.07462210e-01
  2.08953240e-01  2.09667382e-01  2.11171942e-01  2.13683016e-01
  2.16970209e-01  2.17545186e-01  2.17645548e-01  2.20261775e-01
  2.22422980e-01  2.29139657e-01  2.29398065e-01  2.29829350e-01
  2.32376961e-01  2.32391185e-01  2.39703512e-01  2.40261239e-01
  2.46864858e-01  2.47014500e-01  2.48500254e-01  2.52645015e-01
  2.53618760e-01  2.55012348e-01  2.55062673e-01  2.60494286e-01
  2.60976401e-01  2.61138299e-01  2.62622490e-01  2.64312060e-01
  2.68073184e-01  2.68473631e-01  2.72107971e-01  2.72993872e-01
  2.76208771e-01  2.78909464e-01  2.80149920e-01  2.84329864e-01
  2.88192167e-01  2.89061833e-01  2.89528534e-01  2.90213879e-01
  2.92796694e-01  2.93849187e-01  2.96739385e-01  2.96968296e-01
  2.97478168e-01  2.98183162e-01  2.98275209e-01  3.03528198e-01
  3.04414795e-01  3.04997612e-01  3.05442958e-01  3.06463496e-01
  3.08222325e-01  3.08281526e-01  3.12034506e-01  3.13225715e-01
  3.16538384e-01  3.21056873e-01  3.23807565e-01  3.32553195e-01
  3.32816768e-01  3.33127200e-01  3.33875333e-01  3.35611275e-01
  3.39822166e-01  3.45324227e-01  3.45480905e-01  3.46420104e-01
  3.47478770e-01  3.52856091e-01  3.53332557e-01  3.53997505e-01
  3.58771479e-01  3.71748323e-01  3.73614143e-01  3.77819324e-01
  3.91012484e-01  4.00003578e-01  4.00147713e-01  4.01416086e-01
  4.02007716e-01  4.02814033e-01  4.06270010e-01  4.13078910e-01
  4.13646885e-01  4.15374129e-01  4.22358946e-01  4.24232770e-01
  4.32050219e-01  4.33730431e-01  4.38658414e-01  4.38857864e-01
  4.39874359e-01  4.40269983e-01  4.45968325e-01  4.48872310e-01
  4.49659738e-01  4.51284687e-01  4.52506261e-01  4.57061997e-01
  4.60484475e-01  4.64655418e-01  4.70410901e-01  4.71715551e-01
  4.72903954e-01  4.76422399e-01  4.77976644e-01  4.80086327e-01
  4.83074217e-01  4.84933046e-01  4.93430982e-01  4.97663105e-01
  4.99219919e-01  4.99241678e-01  5.03686710e-01  5.05125631e-01
  5.06799975e-01  5.08676703e-01  5.09084330e-01  5.13323076e-01
  5.15368043e-01  5.16079634e-01  5.17264380e-01  5.23682510e-01
  5.24263629e-01  5.26941639e-01  5.32279395e-01  5.33367852e-01
  5.35670090e-01  5.37633948e-01  5.38981601e-01  5.39095699e-01
  5.41178944e-01  5.45162580e-01  5.45668325e-01  5.46817390e-01
  5.55269565e-01  5.56118074e-01  5.57635748e-01  5.64374327e-01
  5.64715245e-01  5.65243232e-01  5.67198546e-01  5.67607953e-01
  5.67719740e-01  5.68996592e-01  5.69109733e-01  5.69443551e-01
  5.71331445e-01  5.72981702e-01  5.73124543e-01  5.73214748e-01
  5.75113068e-01  5.75204366e-01  5.76396639e-01  5.78446395e-01
  5.80165389e-01  5.81372985e-01  5.82993376e-01  5.84614918e-01
  5.93738281e-01  5.95755517e-01  5.99898556e-01  6.01026592e-01
  6.01156141e-01  6.04618694e-01  6.06295018e-01  6.07589493e-01
  6.08276594e-01  6.10566849e-01  6.11094690e-01  6.14614903e-01
  6.16863026e-01  6.21360656e-01  6.21926693e-01  6.22570685e-01
  6.22703433e-01  6.24151720e-01  6.26487494e-01  6.32004497e-01
  6.32442621e-01  6.35639426e-01  6.37987866e-01  6.40621815e-01
  6.40795569e-01  6.42569915e-01  6.44137698e-01  6.49481681e-01
  6.51906668e-01  6.53129646e-01  6.54768346e-01  6.55881746e-01
  6.59831687e-01  6.60965673e-01  6.62881397e-01  6.63792898e-01
  6.65797185e-01  6.67910743e-01  6.71019560e-01  6.71444824e-01
  6.76210640e-01  6.80356222e-01  6.82141745e-01  6.82463646e-01
  6.83115520e-01  6.89497963e-01  6.90037883e-01  6.91940701e-01
  6.95945776e-01  6.98173025e-01  6.98806956e-01  6.99751297e-01
  7.04691050e-01  7.06591028e-01  7.09189028e-01  7.09949164e-01
  7.12205548e-01  7.16746554e-01  7.18078936e-01  7.19620372e-01
  7.26519153e-01  7.36154080e-01  7.37987160e-01  7.38262644e-01
  7.39932338e-01  7.40179202e-01  7.43878795e-01  7.45377828e-01
  7.49847122e-01  7.52472093e-01  7.59899238e-01  7.63997708e-01
  7.71004193e-01  7.71483478e-01  7.74459979e-01  7.76398209e-01
  7.76768642e-01  7.80807634e-01  7.84061123e-01  7.84871279e-01
  7.91054337e-01  7.94716001e-01  7.95762829e-01  8.08580277e-01
  8.09393368e-01  8.13103238e-01  8.16603921e-01  8.27360455e-01
  8.27562939e-01  8.31244651e-01  8.35220755e-01  8.41044474e-01
  8.43848567e-01  8.44980103e-01  8.50111718e-01  8.51358691e-01
  8.51525562e-01  8.55138282e-01  8.55445979e-01  8.56184207e-01
  8.56673295e-01  8.61019305e-01  8.63035375e-01  8.65103233e-01
  8.66068810e-01  8.66276423e-01  8.68206538e-01  8.72382771e-01
  8.74206515e-01  8.76225495e-01  8.77172443e-01  8.77948559e-01
  8.78965639e-01  8.85401761e-01  8.86057283e-01  8.88302301e-01
  8.90611378e-01  8.91610227e-01  8.91875939e-01  8.96734684e-01
  8.97577219e-01  9.03439179e-01  9.03709505e-01  9.06232698e-01
  9.07780961e-01  9.10499666e-01  9.11491107e-01  9.12498165e-01
  9.16561822e-01  9.19164942e-01  9.19244805e-01  9.19361261e-01
  9.21669343e-01  9.21696169e-01  9.22955359e-01  9.23609577e-01
  9.26876073e-01  9.27964937e-01  9.29209232e-01  9.31826797e-01
  9.31864443e-01  9.32522620e-01  9.32534974e-01  9.32846071e-01
  9.33200885e-01  9.37503962e-01  9.38238679e-01  9.38894249e-01
  9.40223627e-01  9.40954771e-01  9.41414509e-01  9.41511664e-01
  9.41852445e-01  9.43991049e-01  9.46350843e-01  9.47505609e-01
  9.49063455e-01  9.52224544e-01  9.54636781e-01  9.56193177e-01
  9.57377035e-01  9.58216657e-01  9.61143569e-01  9.61895188e-01
  9.63715024e-01  9.64893710e-01  9.67044178e-01  9.67389019e-01
  9.68361666e-01  9.68643367e-01  9.69116748e-01  9.69501260e-01
  9.69736515e-01  9.69789642e-01  9.70838605e-01  9.71155605e-01
  9.72168460e-01  9.75132828e-01  9.75708112e-01  9.76080530e-01
  9.76392464e-01  9.76721842e-01  9.81878370e-01  9.81931150e-01
  9.83448503e-01  9.84796487e-01  9.85737122e-01  9.86239876e-01
  9.88176767e-01  9.88209893e-01  9.88612059e-01  9.89072064e-01
  9.90423249e-01  9.90740929e-01  9.91449159e-01  9.91825330e-01
  9.92168420e-01  9.92909581e-01  9.93696871e-01  9.94134407e-01
  9.94322780e-01  9.95376045e-01  9.97078854e-01  9.99101046e-01
  1.00079156e+00  1.00165734e+00  1.00335807e+00  1.00425629e+00
  1.00564135e+00  1.01343679e+00  1.02070844e+00  1.02595044e+00
  1.02851115e+00  1.04972112e+00  1.05718610e+00  1.06161934e+00
  1.07071977e+00  1.08160698e+00  1.08535898e+00  1.14431396e+00
  1.22998309e+00]

  UserWarning,

2022-10-31 11:03:57,597:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.61166922e-01 -1.41113584e-01 -1.22745389e-01 -1.20433443e-01
 -1.00095948e-01 -9.75135260e-02 -7.81162300e-02 -7.37391203e-02
 -6.52813885e-02 -6.20441402e-02 -5.97366163e-02 -5.06357554e-02
 -4.26616753e-02 -3.87042253e-02 -2.57515135e-02 -2.51958326e-02
 -1.93549480e-02 -1.89445948e-02 -1.68601966e-02 -1.34979611e-02
 -6.60028747e-03 -5.63543706e-03 -4.12350534e-03 -3.69959011e-03
 -1.30906181e-03 -1.15040174e-03 -3.42969320e-04  8.64419205e-04
  5.50760500e-03  6.09589585e-03  6.13436231e-03  9.45348271e-03
  1.18005937e-02  1.29632979e-02  1.32799825e-02  1.56452891e-02
  1.58848062e-02  1.70105445e-02  1.78271072e-02  1.94530237e-02
  2.15320510e-02  2.17290143e-02  2.22288873e-02  2.28106632e-02
  2.46455227e-02  2.58938917e-02  2.70994576e-02  2.73988834e-02
  3.00267743e-02  3.25688392e-02  3.40942672e-02  3.42727105e-02
  3.53589462e-02  3.70802190e-02  3.88839793e-02  3.93916906e-02
  3.96677430e-02  3.99931547e-02  4.14616807e-02  4.18074258e-02
  4.20001715e-02  4.30048464e-02  4.31613122e-02  4.62964647e-02
  4.83877197e-02  4.87139073e-02  4.95485842e-02  5.02447042e-02
  5.14471709e-02  5.46611020e-02  5.49620376e-02  5.64847099e-02
  5.64854022e-02  5.65775815e-02  5.70178980e-02  5.91849422e-02
  6.02414060e-02  6.16308256e-02  6.17945949e-02  6.68779811e-02
  6.80740250e-02  6.82800959e-02  6.87126847e-02  6.90928243e-02
  7.03550697e-02  7.08645194e-02  7.17039898e-02  7.20767166e-02
  7.33932630e-02  7.55380371e-02  7.62856033e-02  7.64790337e-02
  7.80350782e-02  7.96859910e-02  8.22946753e-02  8.23666321e-02
  8.41845075e-02  8.48088996e-02  8.66005692e-02  8.67510795e-02
  8.81924305e-02  8.83898449e-02  8.88123650e-02  8.95038464e-02
  9.07869996e-02  9.14709105e-02  9.16674423e-02  9.38887819e-02
  9.44224004e-02  9.78645937e-02  9.87377733e-02  1.01734100e-01
  1.02000663e-01  1.03315119e-01  1.03465933e-01  1.04585824e-01
  1.04882279e-01  1.05076421e-01  1.05897137e-01  1.06373677e-01
  1.07338790e-01  1.07511906e-01  1.08343883e-01  1.09293925e-01
  1.09304760e-01  1.13285699e-01  1.13473295e-01  1.15818453e-01
  1.16244264e-01  1.16497079e-01  1.17950315e-01  1.21051091e-01
  1.22298594e-01  1.23980901e-01  1.24716009e-01  1.25204409e-01
  1.25455363e-01  1.25466880e-01  1.26127963e-01  1.26863166e-01
  1.27950663e-01  1.28974009e-01  1.29166204e-01  1.29534844e-01
  1.30104547e-01  1.30397335e-01  1.30881986e-01  1.32395202e-01
  1.33125519e-01  1.34497849e-01  1.34707480e-01  1.36961917e-01
  1.37271663e-01  1.37517700e-01  1.38736713e-01  1.40242117e-01
  1.40500563e-01  1.43358091e-01  1.43891701e-01  1.45003794e-01
  1.45130374e-01  1.45427278e-01  1.45887488e-01  1.53498190e-01
  1.54982938e-01  1.57947351e-01  1.59053999e-01  1.63725042e-01
  1.63791030e-01  1.65261799e-01  1.65298637e-01  1.66635603e-01
  1.70629693e-01  1.72400343e-01  1.74076402e-01  1.75140544e-01
  1.75217104e-01  1.77506249e-01  1.79139864e-01  1.80366209e-01
  1.82051928e-01  1.82359742e-01  1.87451989e-01  1.88209559e-01
  1.88824173e-01  1.89210036e-01  1.89253185e-01  1.89298382e-01
  1.89323956e-01  1.89791720e-01  1.93024717e-01  1.93061550e-01
  1.93737404e-01  1.97521997e-01  2.00282248e-01  2.05651821e-01
  2.07722368e-01  2.08593544e-01  2.12093664e-01  2.12114382e-01
  2.12401804e-01  2.15464071e-01  2.16686615e-01  2.21200311e-01
  2.21869304e-01  2.22131123e-01  2.22678797e-01  2.26226131e-01
  2.27376093e-01  2.30236218e-01  2.31759293e-01  2.32729042e-01
  2.35647881e-01  2.37494014e-01  2.43096988e-01  2.44356305e-01
  2.45401471e-01  2.57108367e-01  2.59777942e-01  2.62430835e-01
  2.62862058e-01  2.65086122e-01  2.67323193e-01  2.77935681e-01
  2.78438231e-01  2.78972421e-01  2.79043637e-01  2.83520681e-01
  2.84238482e-01  2.84966707e-01  2.85592775e-01  2.88072487e-01
  2.91673619e-01  2.91981899e-01  2.92116752e-01  2.92253051e-01
  2.95086677e-01  2.98904164e-01  2.98937531e-01  3.00374472e-01
  3.05841464e-01  3.06182313e-01  3.06926872e-01  3.11648503e-01
  3.12186069e-01  3.13126892e-01  3.19487611e-01  3.20617313e-01
  3.23522427e-01  3.28867418e-01  3.31836580e-01  3.32821629e-01
  3.33093583e-01  3.33657138e-01  3.37639046e-01  3.43786914e-01
  3.54252336e-01  3.56549846e-01  3.66448193e-01  3.66537318e-01
  3.70990084e-01  3.72546652e-01  3.85786615e-01  4.02409371e-01
  4.06893165e-01  4.07975930e-01  4.17193306e-01  4.19380681e-01
  4.25725282e-01  4.28978989e-01  4.30376604e-01  4.31009041e-01
  4.31047992e-01  4.32065057e-01  4.33646955e-01  4.41262070e-01
  4.46354700e-01  4.51491282e-01  4.53837750e-01  4.59045312e-01
  4.61087834e-01  4.63196183e-01  4.65696482e-01  4.69804686e-01
  4.75058808e-01  4.75676142e-01  4.76864288e-01  4.77537264e-01
  4.78039884e-01  4.78634218e-01  4.83765698e-01  4.84202813e-01
  4.84977906e-01  4.87534369e-01  4.91896284e-01  4.91943979e-01
  4.96456377e-01  4.98260247e-01  5.01079125e-01  5.04589961e-01
  5.07563221e-01  5.08622631e-01  5.09483825e-01  5.10715741e-01
  5.11238164e-01  5.14567582e-01  5.20770741e-01  5.26590399e-01
  5.27587812e-01  5.28581824e-01  5.33548202e-01  5.34503983e-01
  5.37091055e-01  5.38999326e-01  5.44846717e-01  5.45853493e-01
  5.45950091e-01  5.54082965e-01  5.56245984e-01  5.56557978e-01
  5.58208004e-01  5.64261634e-01  5.64872016e-01  5.64944422e-01
  5.66411709e-01  5.67020067e-01  5.69751464e-01  5.73045258e-01
  5.73401924e-01  5.75237419e-01  5.79001932e-01  5.79536952e-01
  5.80777469e-01  5.82748259e-01  5.83064015e-01  5.87982080e-01
  5.91226264e-01  5.95920790e-01  5.97541093e-01  5.99721686e-01
  6.09737333e-01  6.12239331e-01  6.13026018e-01  6.16577294e-01
  6.17917045e-01  6.20063633e-01  6.22148832e-01  6.23695077e-01
  6.23925704e-01  6.26922595e-01  6.31347790e-01  6.32129409e-01
  6.32454734e-01  6.34977466e-01  6.37645253e-01  6.40615287e-01
  6.40786151e-01  6.42231859e-01  6.48840947e-01  6.55077968e-01
  6.58043696e-01  6.61132287e-01  6.61201688e-01  6.63386361e-01
  6.64562430e-01  6.65521350e-01  6.68301530e-01  6.68613705e-01
  6.71653817e-01  6.72925520e-01  6.73534929e-01  6.77403703e-01
  6.77469497e-01  6.79584404e-01  6.85114286e-01  6.85389015e-01
  6.91635007e-01  6.92724568e-01  6.93311483e-01  6.94179789e-01
  7.05488706e-01  7.06760266e-01  7.23843731e-01  7.24862210e-01
  7.24864531e-01  7.25370427e-01  7.25965401e-01  7.29886194e-01
  7.39369188e-01  7.39453391e-01  7.39754963e-01  7.43389512e-01
  7.43428268e-01  7.45796317e-01  7.48946686e-01  7.49389449e-01
  7.49696462e-01  7.56624712e-01  7.59883913e-01  7.69556949e-01
  7.70046879e-01  7.71442616e-01  7.73007897e-01  7.73397894e-01
  7.73858600e-01  7.78851505e-01  7.97316277e-01  7.98092506e-01
  8.08988950e-01  8.09413288e-01  8.11797315e-01  8.15559292e-01
  8.16570612e-01  8.24765627e-01  8.25102967e-01  8.25408221e-01
  8.28355608e-01  8.28459381e-01  8.29865768e-01  8.33963179e-01
  8.36567442e-01  8.36697041e-01  8.41234525e-01  8.42880863e-01
  8.48476920e-01  8.53494849e-01  8.54420616e-01  8.56578401e-01
  8.62754531e-01  8.64255052e-01  8.65608176e-01  8.69237850e-01
  8.70344331e-01  8.76843391e-01  8.77121254e-01  8.85047096e-01
  8.87593783e-01  8.92153239e-01  8.95470364e-01  8.96449194e-01
  8.97844831e-01  8.98835741e-01  8.99166939e-01  9.01645572e-01
  9.05957193e-01  9.06293796e-01  9.06346267e-01  9.10502599e-01
  9.13507967e-01  9.16557738e-01  9.20975904e-01  9.21361970e-01
  9.23665494e-01  9.24866718e-01  9.26487200e-01  9.26731528e-01
  9.29376976e-01  9.29715981e-01  9.30593516e-01  9.33021774e-01
  9.33307310e-01  9.34947321e-01  9.35699563e-01  9.36276725e-01
  9.38058055e-01  9.40443040e-01  9.48791396e-01  9.49364585e-01
  9.50764796e-01  9.53430097e-01  9.53687225e-01  9.54767713e-01
  9.55793526e-01  9.59794756e-01  9.64708731e-01  9.66588177e-01
  9.67255418e-01  9.67403499e-01  9.68150840e-01  9.69235476e-01
  9.70263481e-01  9.70930023e-01  9.71791739e-01  9.76202851e-01
  9.78348748e-01  9.79755814e-01  9.80938033e-01  9.81091878e-01
  9.82095464e-01  9.82592827e-01  9.83314133e-01  9.83468692e-01
  9.83825547e-01  9.84441854e-01  9.86015379e-01  9.86845160e-01
  9.87458848e-01  9.87726651e-01  9.87995437e-01  9.88226038e-01
  9.89823564e-01  9.89885073e-01  9.90620731e-01  9.91515073e-01
  9.91923143e-01  9.92608865e-01  9.93373353e-01  9.95087075e-01
  9.96292913e-01  9.96617957e-01  9.99788193e-01  1.00203979e+00
  1.00265004e+00  1.00451571e+00  1.00470891e+00  1.00577384e+00
  1.00960341e+00  1.01110782e+00  1.01127463e+00  1.01378716e+00
  1.01748069e+00  1.02062774e+00  1.03042643e+00  1.04820956e+00
  1.06821071e+00  1.07567685e+00  1.07658236e+00  1.11238069e+00]

  UserWarning,

2022-10-31 11:03:57,649:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-8.39623486e-02 -5.57129546e-02 -5.25683789e-02 -5.01818942e-02
 -4.85975588e-02 -4.28083302e-02 -4.27329005e-02 -4.24241313e-02
 -4.20465437e-02 -4.13637343e-02 -3.70209445e-02 -3.68397687e-02
 -3.40240561e-02 -3.37699142e-02 -3.28415613e-02 -2.72333406e-02
 -2.34590260e-02 -1.59154153e-02 -1.34973998e-02 -1.24035583e-02
 -1.21416685e-02 -1.15397098e-02 -1.11928866e-02 -1.01300814e-02
 -5.99777841e-03 -2.57586592e-05  3.38645341e-03  5.93789952e-03
  7.15225858e-03  7.65708711e-03  7.70844031e-03  9.33032355e-03
  1.11366884e-02  1.41722090e-02  1.91764144e-02  1.95810795e-02
  2.04428096e-02  2.12941239e-02  2.79723261e-02  2.96962337e-02
  3.07499454e-02  3.17494803e-02  3.23425684e-02  3.26562862e-02
  3.41164758e-02  3.52773518e-02  3.60049969e-02  3.93255660e-02
  4.17106727e-02  4.48079608e-02  4.86274772e-02  5.29277521e-02
  5.43622831e-02  5.86841832e-02  5.94736029e-02  5.99623033e-02
  6.06983611e-02  6.08114496e-02  6.24474217e-02  6.36656435e-02
  6.46381409e-02  6.50588775e-02  6.52079848e-02  6.61569438e-02
  6.74115694e-02  7.31521106e-02  7.33100946e-02  7.46204237e-02
  7.53950044e-02  7.69126978e-02  7.70626257e-02  7.73217411e-02
  7.93590074e-02  8.36709493e-02  8.37680308e-02  8.42331419e-02
  8.45981609e-02  8.69073033e-02  8.70003233e-02  8.77142378e-02
  8.77625616e-02  8.79312244e-02  8.88908729e-02  8.89821250e-02
  8.93307811e-02  9.03289560e-02  9.47651620e-02  9.50296746e-02
  9.59656831e-02  9.93678086e-02  1.00298718e-01  1.01722463e-01
  1.02014128e-01  1.02366097e-01  1.05510786e-01  1.08468275e-01
  1.08937305e-01  1.11078999e-01  1.12170987e-01  1.12584505e-01
  1.14202775e-01  1.14697402e-01  1.14873529e-01  1.15429153e-01
  1.15935975e-01  1.15940499e-01  1.16349946e-01  1.16748804e-01
  1.19446108e-01  1.21561560e-01  1.21597730e-01  1.21597841e-01
  1.26782438e-01  1.29369051e-01  1.30583281e-01  1.32412161e-01
  1.35463767e-01  1.36852173e-01  1.37013806e-01  1.37026048e-01
  1.37662825e-01  1.38630097e-01  1.40496074e-01  1.44255274e-01
  1.48086371e-01  1.50514264e-01  1.51044286e-01  1.51462351e-01
  1.51801004e-01  1.53996017e-01  1.54521421e-01  1.58760204e-01
  1.59949744e-01  1.60357560e-01  1.60702841e-01  1.61169175e-01
  1.64978976e-01  1.65128783e-01  1.66255621e-01  1.66781582e-01
  1.67806842e-01  1.68090488e-01  1.68574317e-01  1.68577656e-01
  1.68759446e-01  1.69993675e-01  1.70744465e-01  1.71306042e-01
  1.73735371e-01  1.73745475e-01  1.74523912e-01  1.77638536e-01
  1.78026205e-01  1.80141762e-01  1.81138214e-01  1.84561222e-01
  1.84919123e-01  1.85623228e-01  1.86418219e-01  1.86495758e-01
  1.87836831e-01  1.91534588e-01  1.92710239e-01  1.93146909e-01
  1.95744292e-01  1.97580628e-01  2.00275900e-01  2.04361424e-01
  2.04526161e-01  2.05506175e-01  2.05612825e-01  2.06877169e-01
  2.07029911e-01  2.08355653e-01  2.09628039e-01  2.10523164e-01
  2.10956099e-01  2.11944886e-01  2.19844134e-01  2.21016597e-01
  2.21693943e-01  2.24364978e-01  2.25366934e-01  2.26780905e-01
  2.26991122e-01  2.34987940e-01  2.35731935e-01  2.36113752e-01
  2.39829749e-01  2.45031263e-01  2.46219439e-01  2.46896439e-01
  2.50437169e-01  2.51372170e-01  2.55503850e-01  2.56078608e-01
  2.56423401e-01  2.57664354e-01  2.59472063e-01  2.61602695e-01
  2.65473411e-01  2.67407547e-01  2.68449006e-01  2.69842180e-01
  2.70184030e-01  2.70223838e-01  2.72990880e-01  2.75266982e-01
  2.77938338e-01  2.80879161e-01  2.83047040e-01  2.85531005e-01
  2.85962652e-01  2.87441819e-01  2.94134393e-01  2.96585790e-01
  2.99136397e-01  3.02466816e-01  3.02630648e-01  3.03548974e-01
  3.08105217e-01  3.09696748e-01  3.14486642e-01  3.16576650e-01
  3.18735198e-01  3.22878130e-01  3.43445683e-01  3.49024648e-01
  3.51773084e-01  3.51933685e-01  3.53700338e-01  3.54365882e-01
  3.57547394e-01  3.60811211e-01  3.69821811e-01  3.73599631e-01
  3.76530877e-01  3.87544607e-01  3.89785498e-01  3.92624115e-01
  4.00304846e-01  4.01338862e-01  4.02695085e-01  4.03281627e-01
  4.07897323e-01  4.10614374e-01  4.14491649e-01  4.18452863e-01
  4.18502854e-01  4.19032515e-01  4.20280889e-01  4.21444746e-01
  4.22372058e-01  4.23613671e-01  4.27992291e-01  4.29840513e-01
  4.30615750e-01  4.34762504e-01  4.41606814e-01  4.44883381e-01
  4.47157042e-01  4.57740531e-01  4.57842050e-01  4.58039991e-01
  4.58129784e-01  4.60726445e-01  4.66339058e-01  4.69236670e-01
  4.70709834e-01  4.71155555e-01  4.71479928e-01  4.71926861e-01
  4.73573660e-01  4.82796936e-01  4.85243938e-01  4.88158550e-01
  4.88505974e-01  4.89406321e-01  4.89905204e-01  4.97096170e-01
  4.97593020e-01  4.99038621e-01  5.03851685e-01  5.05901513e-01
  5.06525432e-01  5.14354104e-01  5.15104604e-01  5.19933708e-01
  5.25059866e-01  5.29397856e-01  5.33405064e-01  5.35427681e-01
  5.36826999e-01  5.37603198e-01  5.39139621e-01  5.39540683e-01
  5.43092183e-01  5.43185320e-01  5.44770924e-01  5.46942290e-01
  5.48431068e-01  5.50920888e-01  5.52693287e-01  5.52813399e-01
  5.52866183e-01  5.56529410e-01  5.58504414e-01  5.59894165e-01
  5.61896378e-01  5.62724847e-01  5.63712862e-01  5.64629400e-01
  5.66002146e-01  5.70430747e-01  5.71692481e-01  5.71701584e-01
  5.72260492e-01  5.73648805e-01  5.74573398e-01  5.76217475e-01
  5.77134396e-01  5.78878896e-01  5.79297056e-01  5.79710870e-01
  5.80031344e-01  5.81319658e-01  5.81974046e-01  5.82633405e-01
  5.83682674e-01  5.85945669e-01  5.86649863e-01  5.91017985e-01
  5.91416035e-01  5.96749790e-01  5.99477793e-01  6.01430175e-01
  6.09735841e-01  6.11560008e-01  6.15516337e-01  6.19323179e-01
  6.24320031e-01  6.24977973e-01  6.26872422e-01  6.29979823e-01
  6.31037574e-01  6.31614253e-01  6.34777182e-01  6.35730287e-01
  6.36945966e-01  6.45905592e-01  6.53176053e-01  6.58592006e-01
  6.59948230e-01  6.63082325e-01  6.63551422e-01  6.68397627e-01
  6.68490786e-01  6.69979430e-01  6.71875065e-01  6.73545166e-01
  6.76257073e-01  6.77297453e-01  6.81475662e-01  6.81901284e-01
  6.84543194e-01  6.84753047e-01  6.85814031e-01  6.86990559e-01
  6.87389987e-01  6.87971104e-01  6.88094181e-01  6.89908312e-01
  6.90688006e-01  6.92439045e-01  6.92462303e-01  6.96613861e-01
  6.98047953e-01  6.98156274e-01  6.98889123e-01  6.99434614e-01
  7.03980550e-01  7.10271365e-01  7.13046023e-01  7.29825779e-01
  7.30152693e-01  7.31687167e-01  7.32134796e-01  7.33627982e-01
  7.34633132e-01  7.40712403e-01  7.42518528e-01  7.43448596e-01
  7.47671181e-01  7.49968667e-01  7.50477190e-01  7.51300752e-01
  7.51420067e-01  7.52740185e-01  7.53444378e-01  7.55442700e-01
  7.55488162e-01  7.56447823e-01  7.56622330e-01  7.58163067e-01
  7.58233530e-01  7.63152571e-01  7.65816684e-01  7.65952596e-01
  7.70092697e-01  7.70158271e-01  7.70791761e-01  7.74973456e-01
  7.79089857e-01  7.81591938e-01  7.88052094e-01  7.89851646e-01
  7.91617802e-01  7.97656634e-01  7.98991374e-01  8.08795742e-01
  8.20634439e-01  8.30807573e-01  8.46793126e-01  8.47179976e-01
  8.53174851e-01  8.53553785e-01  8.54146505e-01  8.64429844e-01
  8.76948107e-01  8.76993024e-01  8.79153174e-01  8.80424011e-01
  8.81302140e-01  8.82371232e-01  8.84613540e-01  8.86454657e-01
  8.87224163e-01  8.88420613e-01  8.88718094e-01  8.88719350e-01
  8.88753771e-01  8.90696473e-01  8.91750622e-01  8.92000239e-01
  8.93642139e-01  8.94261853e-01  8.94668096e-01  8.95372290e-01
  8.99451927e-01  9.08752178e-01  9.14431284e-01  9.14691664e-01
  9.15086245e-01  9.19610399e-01  9.21241867e-01  9.25722146e-01
  9.30360269e-01  9.31274808e-01  9.34155241e-01  9.38215684e-01
  9.38320867e-01  9.38370980e-01  9.38429972e-01  9.39172536e-01
  9.41430084e-01  9.44045381e-01  9.51568788e-01  9.52282318e-01
  9.53022274e-01  9.54692040e-01  9.57069135e-01  9.57471436e-01
  9.58711011e-01  9.59213950e-01  9.59774187e-01  9.60190712e-01
  9.62160146e-01  9.63408769e-01  9.67166920e-01  9.67297168e-01
  9.67461785e-01  9.69237528e-01  9.69791309e-01  9.70572168e-01
  9.70739939e-01  9.71512372e-01  9.71558136e-01  9.71825073e-01
  9.72262329e-01  9.73068693e-01  9.73185208e-01  9.73560569e-01
  9.74779185e-01  9.75881039e-01  9.78901820e-01  9.79661053e-01
  9.80177877e-01  9.81167910e-01  9.82119878e-01  9.82357937e-01
  9.84643611e-01  9.85180731e-01  9.85672777e-01  9.86202013e-01
  9.86473738e-01  9.86947558e-01  9.87651751e-01  9.87948637e-01
  9.89002786e-01  9.89233232e-01  9.90536182e-01  9.90585904e-01
  9.91265294e-01  9.91805792e-01  9.92956469e-01  9.93276458e-01
  9.94848477e-01  9.97427631e-01  9.97777881e-01  9.99088372e-01
  1.00007678e+00  1.00025039e+00  1.00135027e+00  1.00231293e+00
  1.00461122e+00  1.00558179e+00  1.00698957e+00  1.00978864e+00
  1.01046458e+00  1.01217424e+00  1.01393934e+00  1.01784909e+00
  1.02322190e+00  1.02773859e+00  1.03614075e+00  1.03641300e+00
  1.04432375e+00  1.06499612e+00  1.20079047e+00  1.23247506e+00]

  UserWarning,

2022-10-31 11:03:57,694:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.47689987e-01 -1.01130077e-01 -8.88727155e-02 -8.75843533e-02
 -8.07252613e-02 -7.75632675e-02 -7.35808624e-02 -7.23316773e-02
 -5.87473955e-02 -5.56306246e-02 -4.93154529e-02 -3.39113137e-02
 -3.31103951e-02 -2.76155561e-02 -2.41954088e-02 -1.55913619e-02
 -1.36324938e-02 -1.28860659e-02 -1.16087553e-02 -1.06524740e-02
 -8.05550688e-03 -6.28122657e-03 -3.85687734e-03 -1.87324343e-03
 -3.58459757e-04  2.12418285e-03  4.24573656e-03  6.25743826e-03
  6.62009054e-03  1.26359376e-02  1.35544082e-02  1.40126131e-02
  1.56853888e-02  1.72193896e-02  2.00643969e-02  2.23383091e-02
  2.50113570e-02  2.67229009e-02  2.81644897e-02  2.90801974e-02
  2.97722008e-02  3.05103972e-02  3.06607732e-02  3.14650892e-02
  3.33435049e-02  3.36577697e-02  3.85423134e-02  3.87161883e-02
  3.97371444e-02  4.00583554e-02  4.04370772e-02  4.06333745e-02
  4.21032782e-02  4.25354512e-02  4.34692076e-02  4.41947257e-02
  4.63300528e-02  4.71825324e-02  4.87651962e-02  5.26296379e-02
  5.34646108e-02  5.44374676e-02  5.89253348e-02  6.04492627e-02
  6.14268312e-02  6.14783597e-02  6.17885084e-02  6.18103966e-02
  6.30733217e-02  6.46628628e-02  6.46801657e-02  6.49257961e-02
  6.66050895e-02  6.68990436e-02  6.74785160e-02  6.75206424e-02
  6.82155302e-02  6.98519926e-02  7.04524931e-02  7.06502692e-02
  7.23764632e-02  7.31760887e-02  7.36785101e-02  7.37875566e-02
  7.40757672e-02  7.42385279e-02  7.60197815e-02  7.62178954e-02
  8.08325530e-02  8.23926795e-02  8.24689758e-02  8.38640280e-02
  8.75209308e-02  8.75819297e-02  8.77037222e-02  9.57883516e-02
  9.83849616e-02  9.86088591e-02  1.00140106e-01  1.01977617e-01
  1.04579391e-01  1.05193382e-01  1.05777904e-01  1.06682527e-01
  1.07122988e-01  1.11102453e-01  1.12692944e-01  1.15292009e-01
  1.17718243e-01  1.18621417e-01  1.19084703e-01  1.21758245e-01
  1.22094780e-01  1.22405950e-01  1.22889861e-01  1.25010660e-01
  1.25114601e-01  1.27430337e-01  1.28801583e-01  1.31202479e-01
  1.31956971e-01  1.32484656e-01  1.34663300e-01  1.35244710e-01
  1.35717063e-01  1.36001483e-01  1.36558850e-01  1.37438714e-01
  1.38615042e-01  1.40735907e-01  1.41921593e-01  1.42746790e-01
  1.43514063e-01  1.45327070e-01  1.46342822e-01  1.48650467e-01
  1.48694947e-01  1.51396084e-01  1.52131362e-01  1.54684183e-01
  1.56112235e-01  1.56438004e-01  1.56902833e-01  1.58220372e-01
  1.59277201e-01  1.60625492e-01  1.60671693e-01  1.64302522e-01
  1.65801112e-01  1.67085614e-01  1.67266030e-01  1.67932863e-01
  1.69570382e-01  1.70448948e-01  1.71843192e-01  1.74076454e-01
  1.74724832e-01  1.75975743e-01  1.76347947e-01  1.77810739e-01
  1.80175954e-01  1.81316118e-01  1.81778531e-01  1.83855441e-01
  1.84066330e-01  1.84817314e-01  1.86359703e-01  1.86681659e-01
  1.86714343e-01  1.87701264e-01  1.88668619e-01  1.89917164e-01
  1.90293876e-01  1.90694868e-01  1.91689660e-01  1.91960395e-01
  1.92234555e-01  1.93195940e-01  1.94113582e-01  1.95597032e-01
  1.96059238e-01  2.00993399e-01  2.02250072e-01  2.04042307e-01
  2.05043024e-01  2.05111485e-01  2.06801079e-01  2.08966976e-01
  2.09839014e-01  2.09981984e-01  2.10374456e-01  2.16162240e-01
  2.17649117e-01  2.18935331e-01  2.20196079e-01  2.21638121e-01
  2.26424007e-01  2.27149543e-01  2.29632803e-01  2.30085493e-01
  2.32225289e-01  2.32967648e-01  2.34416617e-01  2.34621415e-01
  2.35240049e-01  2.35366116e-01  2.43845900e-01  2.43898425e-01
  2.44439633e-01  2.46897003e-01  2.49068044e-01  2.50986709e-01
  2.51474560e-01  2.53145967e-01  2.55532769e-01  2.56180800e-01
  2.56805132e-01  2.57473205e-01  2.59243979e-01  2.67291723e-01
  2.74392138e-01  2.74598785e-01  2.76715421e-01  2.76925629e-01
  2.78071829e-01  2.79267612e-01  2.80774275e-01  2.84607398e-01
  2.85150487e-01  2.89163304e-01  2.91346299e-01  2.91939308e-01
  2.95616096e-01  3.00162104e-01  3.01844563e-01  3.02174689e-01
  3.02821099e-01  3.03612263e-01  3.04398157e-01  3.07035916e-01
  3.07745548e-01  3.09266266e-01  3.11576289e-01  3.12933918e-01
  3.12934576e-01  3.15150552e-01  3.17330359e-01  3.19550445e-01
  3.23498907e-01  3.24405753e-01  3.26196681e-01  3.27257302e-01
  3.30398616e-01  3.31815491e-01  3.38713374e-01  3.38993088e-01
  3.42643296e-01  3.45291497e-01  3.45939681e-01  3.47200721e-01
  3.53616003e-01  3.56731253e-01  3.57176033e-01  3.62809153e-01
  3.62939735e-01  3.63213497e-01  3.68358845e-01  3.76138174e-01
  3.80116452e-01  3.84852953e-01  3.88205121e-01  3.91440884e-01
  3.92514284e-01  3.94494854e-01  3.94540210e-01  3.99014304e-01
  4.00915433e-01  4.01420813e-01  4.06059912e-01  4.10131765e-01
  4.10734721e-01  4.15100834e-01  4.15872127e-01  4.16028264e-01
  4.21804189e-01  4.23183085e-01  4.28299162e-01  4.33563191e-01
  4.34333846e-01  4.34511771e-01  4.39710378e-01  4.42801278e-01
  4.44191260e-01  4.53484581e-01  4.61569192e-01  4.67838622e-01
  4.71983695e-01  4.73984410e-01  4.77903471e-01  4.79470250e-01
  4.84230302e-01  4.86385062e-01  4.90265177e-01  4.91130749e-01
  4.93121738e-01  4.95212429e-01  4.95294423e-01  4.99067568e-01
  5.04116851e-01  5.11612624e-01  5.16265059e-01  5.17764024e-01
  5.30445259e-01  5.32100804e-01  5.32604079e-01  5.40294902e-01
  5.42728255e-01  5.44998542e-01  5.46657472e-01  5.49250140e-01
  5.51053237e-01  5.51187313e-01  5.51887722e-01  5.59869186e-01
  5.60373637e-01  5.61073980e-01  5.61444679e-01  5.62371528e-01
  5.63333041e-01  5.64342067e-01  5.65796898e-01  5.67121529e-01
  5.67148510e-01  5.71124582e-01  5.72325703e-01  5.78953901e-01
  5.79626897e-01  5.80823857e-01  5.81566511e-01  5.83901698e-01
  5.84242808e-01  5.84577789e-01  5.86973140e-01  5.99043486e-01
  6.02381551e-01  6.05164674e-01  6.10357545e-01  6.10699816e-01
  6.11540507e-01  6.13305109e-01  6.14038369e-01  6.18232554e-01
  6.20177793e-01  6.20366978e-01  6.26535722e-01  6.29198198e-01
  6.32831752e-01  6.37620172e-01  6.39072322e-01  6.45887544e-01
  6.46173818e-01  6.48764471e-01  6.49138129e-01  6.51446213e-01
  6.52390821e-01  6.53246272e-01  6.54742846e-01  6.57773972e-01
  6.61716407e-01  6.62577549e-01  6.62814244e-01  6.62921564e-01
  6.63209832e-01  6.64014627e-01  6.68540721e-01  6.74894252e-01
  6.76182005e-01  6.79582941e-01  6.79813844e-01  6.80402978e-01
  6.82557143e-01  6.85309685e-01  6.85418778e-01  6.86431114e-01
  6.86752123e-01  6.87539963e-01  6.88260974e-01  6.89681646e-01
  6.94384681e-01  6.99809113e-01  7.03346505e-01  7.03858321e-01
  7.04285474e-01  7.05014162e-01  7.10964026e-01  7.11076807e-01
  7.11506943e-01  7.12663105e-01  7.17657405e-01  7.21594569e-01
  7.38593585e-01  7.44297416e-01  7.54521646e-01  7.57411986e-01
  7.61050604e-01  7.62031573e-01  7.69602804e-01  7.79113304e-01
  7.82951720e-01  7.84834573e-01  7.92671107e-01  7.95956079e-01
  7.99691686e-01  8.04092491e-01  8.09059843e-01  8.14718175e-01
  8.15346087e-01  8.22269593e-01  8.22470048e-01  8.23033338e-01
  8.24331149e-01  8.32725756e-01  8.45968205e-01  8.46072459e-01
  8.46976375e-01  8.51423633e-01  8.55359141e-01  8.55687975e-01
  8.56766649e-01  8.57263469e-01  8.60727491e-01  8.61008161e-01
  8.66016023e-01  8.67634919e-01  8.70810332e-01  8.72226403e-01
  8.74856712e-01  8.79494825e-01  8.81664108e-01  8.83601280e-01
  8.84316221e-01  8.84496396e-01  8.88456943e-01  8.88740391e-01
  8.89478721e-01  8.89715053e-01  8.90184017e-01  8.90280330e-01
  8.93266691e-01  8.93772762e-01  8.94306917e-01  8.94493587e-01
  8.95502976e-01  8.96069081e-01  8.97078470e-01  8.97419822e-01
  8.97871801e-01  8.98021410e-01  8.98042887e-01  9.00489080e-01
  9.01646172e-01  9.01971754e-01  9.03151014e-01  9.03228873e-01
  9.05698225e-01  9.06261616e-01  9.07758041e-01  9.10735210e-01
  9.12097942e-01  9.13660507e-01  9.16238921e-01  9.21068688e-01
  9.22576221e-01  9.22875399e-01  9.23210937e-01  9.23606457e-01
  9.24295084e-01  9.25728469e-01  9.27621522e-01  9.27998114e-01
  9.29205621e-01  9.35934267e-01  9.36014488e-01  9.38525593e-01
  9.38637169e-01  9.39666033e-01  9.43845190e-01  9.44393475e-01
  9.47999880e-01  9.48398874e-01  9.50243147e-01  9.50404625e-01
  9.50971470e-01  9.51452164e-01  9.51949865e-01  9.52370926e-01
  9.52768825e-01  9.54186872e-01  9.56818282e-01  9.58332141e-01
  9.60489808e-01  9.61757351e-01  9.63647964e-01  9.65585136e-01
  9.68416909e-01  9.69372697e-01  9.69909863e-01  9.70491300e-01
  9.71485357e-01  9.72836098e-01  9.74158486e-01  9.74474006e-01
  9.74688043e-01  9.75150604e-01  9.76762461e-01  9.77189188e-01
  9.77189248e-01  9.77864274e-01  9.78080794e-01  9.78273514e-01
  9.78699634e-01  9.79478790e-01  9.80077433e-01  9.82846420e-01
  9.83764495e-01  9.84408985e-01  9.85115236e-01  9.85367072e-01
  9.91904695e-01  9.93623877e-01  9.93667536e-01  9.93886499e-01
  9.94454173e-01  9.95449064e-01  9.95668423e-01  9.97374012e-01
  9.97780003e-01  1.00056325e+00  1.00398990e+00  1.00831332e+00
  1.00974567e+00  1.01005914e+00  1.01051342e+00  1.01254932e+00
  1.01457046e+00  1.01871311e+00  1.02311808e+00  1.02751204e+00
  1.03544151e+00  1.03593890e+00  1.03912338e+00  1.04315313e+00
  1.06566994e+00  1.08692315e+00  1.08893535e+00  1.09542309e+00]

  UserWarning,

2022-10-31 11:03:57,699:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.56734975e-01 -1.41578626e-01 -1.35417870e-01 -9.67077691e-02
 -8.46484465e-02 -8.31542570e-02 -7.54757841e-02 -7.30937154e-02
 -5.71372516e-02 -4.84759073e-02 -3.64589561e-02 -3.39099066e-02
 -3.21971695e-02 -3.06352153e-02 -2.99826249e-02 -1.38508802e-02
 -1.26362932e-02 -9.18456625e-03 -7.37423661e-03 -7.05510625e-03
 -3.52164728e-03 -7.49133707e-04  1.96023553e-03  2.74430958e-03
  4.10124952e-03  6.89493995e-03  9.17084403e-03  1.12746581e-02
  1.16431025e-02  1.25282894e-02  1.41780289e-02  1.55241512e-02
  1.57955198e-02  1.65047523e-02  2.10462143e-02  2.11222756e-02
  2.17516601e-02  2.41523611e-02  2.62730827e-02  2.82570720e-02
  2.87740345e-02  2.89911993e-02  2.97716924e-02  3.19619868e-02
  3.32882513e-02  3.44109762e-02  3.47057269e-02  3.51181962e-02
  3.51915461e-02  3.55111714e-02  3.61486666e-02  3.75507396e-02
  3.84465321e-02  4.06902274e-02  4.55837344e-02  4.98166841e-02
  5.08945858e-02  5.09188377e-02  5.49293376e-02  5.54519180e-02
  5.86057361e-02  5.98247382e-02  6.32618328e-02  6.43022396e-02
  6.57269268e-02  6.63359837e-02  7.06903486e-02  7.14305917e-02
  7.19801239e-02  7.24468003e-02  7.37011726e-02  7.54765300e-02
  7.66155721e-02  7.73800349e-02  7.74913829e-02  7.82246832e-02
  8.32280653e-02  8.56862107e-02  8.74200957e-02  9.05074688e-02
  9.10098102e-02  9.21552153e-02  9.22322166e-02  9.28057579e-02
  9.30465891e-02  9.45047447e-02  9.50669145e-02  9.56041823e-02
  9.56309453e-02  9.78802905e-02  9.90322257e-02  1.01133879e-01
  1.03180074e-01  1.04138634e-01  1.05555965e-01  1.06836719e-01
  1.07379083e-01  1.08966651e-01  1.09174785e-01  1.09367606e-01
  1.12958234e-01  1.14427994e-01  1.15658183e-01  1.16648678e-01
  1.18099370e-01  1.18654259e-01  1.19251194e-01  1.20193659e-01
  1.20476987e-01  1.21113951e-01  1.26795323e-01  1.27014223e-01
  1.28232960e-01  1.29961047e-01  1.31262959e-01  1.32640815e-01
  1.35192281e-01  1.37188419e-01  1.40402390e-01  1.41023812e-01
  1.41696440e-01  1.42894700e-01  1.43865493e-01  1.45190842e-01
  1.46427419e-01  1.47080849e-01  1.47600922e-01  1.48803218e-01
  1.48985710e-01  1.49326261e-01  1.49767777e-01  1.50240344e-01
  1.50785192e-01  1.52452403e-01  1.53571817e-01  1.54698891e-01
  1.56078266e-01  1.56081089e-01  1.56405224e-01  1.59282493e-01
  1.61109443e-01  1.62543542e-01  1.63280299e-01  1.69177497e-01
  1.69541335e-01  1.71221058e-01  1.71293321e-01  1.71575892e-01
  1.74552629e-01  1.74852712e-01  1.74933130e-01  1.75754222e-01
  1.77096839e-01  1.77815995e-01  1.80310111e-01  1.80326140e-01
  1.83701045e-01  1.88234091e-01  1.92419867e-01  1.93269135e-01
  1.94637750e-01  1.95617065e-01  1.96455131e-01  1.97943811e-01
  1.98445530e-01  1.99486714e-01  2.00945428e-01  2.01975089e-01
  2.03790219e-01  2.04535441e-01  2.07621115e-01  2.08020224e-01
  2.08477964e-01  2.11594331e-01  2.13562538e-01  2.13728342e-01
  2.21264612e-01  2.21377845e-01  2.21517063e-01  2.24263260e-01
  2.25119767e-01  2.30893024e-01  2.30954005e-01  2.32995733e-01
  2.33215667e-01  2.34486009e-01  2.34579315e-01  2.36224830e-01
  2.36728216e-01  2.36769508e-01  2.37030258e-01  2.37725217e-01
  2.40403687e-01  2.40654191e-01  2.41023836e-01  2.41176170e-01
  2.42798800e-01  2.43624606e-01  2.45916630e-01  2.48754863e-01
  2.50671391e-01  2.51654895e-01  2.52182417e-01  2.54259148e-01
  2.57670332e-01  2.61662455e-01  2.63440895e-01  2.63979092e-01
  2.65226613e-01  2.69017281e-01  2.69286527e-01  2.69828168e-01
  2.70455959e-01  2.73533917e-01  2.73642634e-01  2.78446381e-01
  2.81188225e-01  2.82833788e-01  2.84253457e-01  2.87904964e-01
  2.89474730e-01  2.91080884e-01  2.94010421e-01  2.95349729e-01
  2.96489985e-01  2.96716781e-01  2.98857841e-01  3.03497094e-01
  3.03526349e-01  3.04109785e-01  3.05558480e-01  3.05660722e-01
  3.08065934e-01  3.08440024e-01  3.10046118e-01  3.16197790e-01
  3.17139634e-01  3.18082974e-01  3.23249133e-01  3.23977785e-01
  3.27274111e-01  3.27803886e-01  3.29100435e-01  3.35176719e-01
  3.36548623e-01  3.40020298e-01  3.45651342e-01  3.48049584e-01
  3.52060119e-01  3.56719215e-01  3.57377211e-01  3.58610759e-01
  3.63458825e-01  3.67485018e-01  3.69189541e-01  3.76206158e-01
  3.82041197e-01  3.86614869e-01  3.88171648e-01  3.90530063e-01
  3.93940451e-01  3.95910479e-01  3.97398988e-01  4.05056457e-01
  4.17934533e-01  4.19307063e-01  4.19686402e-01  4.20925702e-01
  4.22040178e-01  4.23178582e-01  4.27025121e-01  4.27443926e-01
  4.29008371e-01  4.39105253e-01  4.41466559e-01  4.46457351e-01
  4.48091822e-01  4.50429258e-01  4.51117632e-01  4.55682778e-01
  4.56495202e-01  4.60709032e-01  4.62685861e-01  4.65602735e-01
  4.70693050e-01  4.74418655e-01  4.74770693e-01  4.75881259e-01
  4.75924010e-01  4.76327044e-01  4.77830948e-01  4.80333856e-01
  4.80859191e-01  4.90878903e-01  4.98560646e-01  5.07557125e-01
  5.13563804e-01  5.18297384e-01  5.22533079e-01  5.22692086e-01
  5.23513361e-01  5.32132416e-01  5.36146166e-01  5.39328081e-01
  5.39643606e-01  5.39651899e-01  5.44296420e-01  5.44448252e-01
  5.46651917e-01  5.47337514e-01  5.48778816e-01  5.49873229e-01
  5.53269062e-01  5.53451831e-01  5.57367951e-01  5.57509555e-01
  5.59288057e-01  5.62065672e-01  5.62838322e-01  5.64246372e-01
  5.64667431e-01  5.65431815e-01  5.68184685e-01  5.70757960e-01
  5.71428900e-01  5.71730444e-01  5.72719525e-01  5.76631124e-01
  5.77022351e-01  5.83996167e-01  5.84442274e-01  5.85450528e-01
  5.88980967e-01  5.90076967e-01  5.90212251e-01  5.90235796e-01
  5.92334719e-01  5.95067694e-01  5.95444876e-01  5.97813663e-01
  5.98243898e-01  6.00737354e-01  6.01941334e-01  6.04152942e-01
  6.06546881e-01  6.14968758e-01  6.15845916e-01  6.19974109e-01
  6.21025228e-01  6.21877700e-01  6.22798731e-01  6.26432788e-01
  6.26498666e-01  6.34445071e-01  6.36833161e-01  6.40445835e-01
  6.43363776e-01  6.47912420e-01  6.49679362e-01  6.56340222e-01
  6.58182205e-01  6.59341340e-01  6.64770239e-01  6.66790671e-01
  6.68055500e-01  6.74572965e-01  6.74819679e-01  6.75191825e-01
  6.76949142e-01  6.84177825e-01  6.84787260e-01  6.88456673e-01
  6.95155135e-01  6.98205963e-01  7.00102897e-01  7.02713222e-01
  7.09658856e-01  7.10480877e-01  7.10651758e-01  7.11440767e-01
  7.14337938e-01  7.14972382e-01  7.20175560e-01  7.23013610e-01
  7.23280301e-01  7.25385944e-01  7.34689528e-01  7.37721415e-01
  7.37820441e-01  7.40292084e-01  7.40481181e-01  7.43921133e-01
  7.45489283e-01  7.47121856e-01  7.48534093e-01  7.48589994e-01
  7.50039402e-01  7.51255763e-01  7.52046518e-01  7.52190009e-01
  7.52306075e-01  7.54149545e-01  7.56537772e-01  7.57047277e-01
  7.57672594e-01  7.60690995e-01  7.61548963e-01  7.61580380e-01
  7.71445484e-01  7.74297205e-01  7.77384691e-01  7.81981818e-01
  7.82715491e-01  7.85436587e-01  7.91729203e-01  7.93346638e-01
  8.03353944e-01  8.06531217e-01  8.11038426e-01  8.14824041e-01
  8.22325616e-01  8.27879006e-01  8.27927909e-01  8.30467590e-01
  8.30607662e-01  8.34549502e-01  8.37456242e-01  8.40563394e-01
  8.42545642e-01  8.43278086e-01  8.48740045e-01  8.49301496e-01
  8.49990396e-01  8.51636843e-01  8.54718606e-01  8.56851986e-01
  8.58491624e-01  8.62732117e-01  8.64391593e-01  8.64406298e-01
  8.65981299e-01  8.66105041e-01  8.67798895e-01  8.69079107e-01
  8.69468934e-01  8.69759053e-01  8.75032956e-01  8.75105338e-01
  8.76967548e-01  8.79007091e-01  8.79729083e-01  8.80217017e-01
  8.80748894e-01  8.82932804e-01  8.83661622e-01  8.87340979e-01
  8.88605230e-01  8.88935743e-01  8.92543758e-01  8.93834544e-01
  8.94793406e-01  8.94922665e-01  8.97217526e-01  9.00494101e-01
  9.03786439e-01  9.10560429e-01  9.10925884e-01  9.13465206e-01
  9.13718490e-01  9.13981219e-01  9.14692785e-01  9.21126829e-01
  9.22809102e-01  9.25876458e-01  9.29788443e-01  9.32134224e-01
  9.34871289e-01  9.35399917e-01  9.36415824e-01  9.39121061e-01
  9.40097407e-01  9.46277535e-01  9.46408598e-01  9.47950614e-01
  9.56931183e-01  9.60365750e-01  9.62329181e-01  9.64516329e-01
  9.65437876e-01  9.65666930e-01  9.66151868e-01  9.67102567e-01
  9.67379584e-01  9.67962627e-01  9.70624373e-01  9.71145866e-01
  9.71538774e-01  9.75593106e-01  9.76490103e-01  9.76640724e-01
  9.77065366e-01  9.77990025e-01  9.79284305e-01  9.79541610e-01
  9.80751418e-01  9.81189960e-01  9.81331466e-01  9.81596098e-01
  9.82831670e-01  9.83271531e-01  9.84397471e-01  9.84768054e-01
  9.85592291e-01  9.87681127e-01  9.87877029e-01  9.88315571e-01
  9.89409798e-01  9.89848340e-01  9.90770144e-01  9.92593553e-01
  9.92768943e-01  9.93969485e-01  9.94028076e-01  9.94853462e-01
  9.95307249e-01  9.95560845e-01  9.97914174e-01  9.98645043e-01
  1.00002465e+00  1.00087709e+00  1.00266530e+00  1.00281468e+00
  1.00888551e+00  1.00918597e+00  1.01190825e+00  1.01200716e+00
  1.01486733e+00  1.01609059e+00  1.02089616e+00  1.02232097e+00
  1.04289423e+00  1.04712888e+00  1.05415936e+00  1.05554886e+00
  1.05890169e+00  1.06037599e+00  1.07277530e+00  1.09160603e+00
  1.10540822e+00  1.12299529e+00  1.12300489e+00  1.17677416e+00]

  UserWarning,

2022-10-31 11:03:58,703:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-1.34826231e-01 -1.22759002e-01 -1.04111664e-01 -9.05607166e-02
 -8.92792246e-02 -8.01554499e-02 -5.68303908e-02 -5.37196428e-02
 -4.06977745e-02 -3.60542410e-02 -3.59206658e-02 -1.80726194e-02
 -1.72937128e-02 -1.40983431e-02 -5.67903137e-03 -6.81201501e-04
  2.52759833e-03  3.92080503e-03  4.50516065e-03  5.44441857e-03
  5.49077550e-03  8.43679517e-03  8.74160289e-03  9.08633381e-03
  1.15749053e-02  1.16374319e-02  1.47638048e-02  1.53428294e-02
  1.74453116e-02  1.83677595e-02  1.87020561e-02  2.08529450e-02
  2.16804638e-02  2.25777326e-02  2.37908555e-02  2.60932596e-02
  2.76278336e-02  2.92310829e-02  3.16318440e-02  3.19606253e-02
  3.36261013e-02  3.51925401e-02  4.31590240e-02  4.33557459e-02
  4.62030734e-02  4.69456895e-02  4.72193608e-02  5.32365911e-02
  5.47726379e-02  5.54404680e-02  5.61214424e-02  5.82843375e-02
  6.00995537e-02  6.06515756e-02  6.36221950e-02  6.58633650e-02
  6.64737656e-02  6.65430435e-02  6.66722274e-02  6.74866574e-02
  6.88653637e-02  7.10283029e-02  7.12307970e-02  7.13906982e-02
  7.22415517e-02  7.37141043e-02  7.75609722e-02  7.78576397e-02
  7.88358104e-02  8.05478954e-02  8.13186871e-02  8.17589271e-02
  8.40947107e-02  8.42456960e-02  8.71107429e-02  9.18209135e-02
  9.35107538e-02  9.44385770e-02  9.55944789e-02  9.59074375e-02
  9.60064957e-02  9.62569736e-02  9.77396038e-02  1.00279896e-01
  1.03821524e-01  1.03845640e-01  1.03870664e-01  1.05908321e-01
  1.06767551e-01  1.07091909e-01  1.11254097e-01  1.11393154e-01
  1.12517041e-01  1.12954234e-01  1.13553208e-01  1.13693036e-01
  1.14982205e-01  1.17226598e-01  1.17371745e-01  1.17757515e-01
  1.17866970e-01  1.18357502e-01  1.19591780e-01  1.19901231e-01
  1.20346822e-01  1.22439838e-01  1.22538042e-01  1.22558834e-01
  1.23124024e-01  1.25084467e-01  1.26585000e-01  1.27746847e-01
  1.29326240e-01  1.29659033e-01  1.30512535e-01  1.31111657e-01
  1.31739915e-01  1.31937049e-01  1.32187975e-01  1.32428458e-01
  1.32942906e-01  1.33345924e-01  1.35089966e-01  1.38082466e-01
  1.39368250e-01  1.40415087e-01  1.45599203e-01  1.53057475e-01
  1.53281641e-01  1.54770713e-01  1.55225468e-01  1.55470559e-01
  1.58197571e-01  1.60310768e-01  1.62235392e-01  1.62619122e-01
  1.63361865e-01  1.64563569e-01  1.66043511e-01  1.66612844e-01
  1.67753509e-01  1.68141277e-01  1.70171459e-01  1.70176725e-01
  1.70433222e-01  1.70463120e-01  1.70993461e-01  1.76443398e-01
  1.76539905e-01  1.76665534e-01  1.77234815e-01  1.77630793e-01
  1.79619790e-01  1.79975025e-01  1.80342371e-01  1.80937742e-01
  1.81734691e-01  1.81860681e-01  1.82784492e-01  1.85818932e-01
  1.86750545e-01  1.91393513e-01  1.92502132e-01  1.94135737e-01
  1.94328811e-01  1.95074206e-01  1.96260191e-01  1.97791890e-01
  2.01603707e-01  2.02596176e-01  2.02627735e-01  2.03582471e-01
  2.06189314e-01  2.07359563e-01  2.07418690e-01  2.09399915e-01
  2.09897916e-01  2.12732346e-01  2.13909429e-01  2.15042178e-01
  2.18212204e-01  2.20234249e-01  2.22050885e-01  2.23370522e-01
  2.25849525e-01  2.26178572e-01  2.28273230e-01  2.29838328e-01
  2.29851151e-01  2.31529743e-01  2.31739143e-01  2.37918734e-01
  2.46046557e-01  2.49015226e-01  2.50689037e-01  2.52217307e-01
  2.53275727e-01  2.57189524e-01  2.57310962e-01  2.57909460e-01
  2.58434752e-01  2.58524253e-01  2.65870125e-01  2.74371234e-01
  2.74771376e-01  2.75058779e-01  2.75741541e-01  2.76304329e-01
  2.77135666e-01  2.79594889e-01  2.80469160e-01  2.81414056e-01
  2.82511814e-01  2.85059092e-01  2.85539518e-01  2.92795818e-01
  2.93812742e-01  2.95234295e-01  2.96975268e-01  2.97153528e-01
  2.99638747e-01  3.00929810e-01  3.06412118e-01  3.14601984e-01
  3.15782316e-01  3.22665696e-01  3.24371812e-01  3.27660739e-01
  3.29023082e-01  3.41162750e-01  3.42155541e-01  3.43509519e-01
  3.44241092e-01  3.48144178e-01  3.50592550e-01  3.54055400e-01
  3.58099403e-01  3.59579349e-01  3.65282650e-01  3.67688289e-01
  3.68892517e-01  3.78881425e-01  3.83318670e-01  3.83503036e-01
  3.98715341e-01  3.99835051e-01  4.03288036e-01  4.06155525e-01
  4.10135295e-01  4.21144929e-01  4.21725691e-01  4.26858402e-01
  4.40812271e-01  4.41791878e-01  4.42622978e-01  4.52407704e-01
  4.52533963e-01  4.61130821e-01  4.61406653e-01  4.63378651e-01
  4.64102602e-01  4.66801242e-01  4.69585574e-01  4.72687965e-01
  4.77950510e-01  4.81494969e-01  4.84051659e-01  4.87745043e-01
  4.88878361e-01  4.90771720e-01  4.93503154e-01  4.94927684e-01
  4.95430956e-01  4.96017417e-01  5.09810309e-01  5.10525540e-01
  5.11467827e-01  5.12573422e-01  5.13483352e-01  5.24174287e-01
  5.26370867e-01  5.27298735e-01  5.28303923e-01  5.30957908e-01
  5.32269919e-01  5.33888727e-01  5.35628149e-01  5.36410024e-01
  5.36876469e-01  5.39217496e-01  5.40167000e-01  5.42584128e-01
  5.43426733e-01  5.45192729e-01  5.46471141e-01  5.46779241e-01
  5.47699554e-01  5.50880333e-01  5.51209028e-01  5.51907515e-01
  5.52389494e-01  5.53719256e-01  5.54457761e-01  5.54905707e-01
  5.55454101e-01  5.55741857e-01  5.56272099e-01  5.58238906e-01
  5.58796571e-01  5.59702773e-01  5.60418952e-01  5.65172467e-01
  5.69335360e-01  5.71001176e-01  5.72724718e-01  5.73276716e-01
  5.73579143e-01  5.76380626e-01  5.77593782e-01  5.80254819e-01
  5.80474155e-01  5.80505620e-01  5.80703038e-01  5.81204441e-01
  5.82272796e-01  5.86952305e-01  5.88041464e-01  5.88288643e-01
  5.88517720e-01  5.88968137e-01  5.91518083e-01  5.92327012e-01
  5.94849130e-01  5.96742797e-01  5.98563376e-01  6.06836334e-01
  6.08388049e-01  6.10335379e-01  6.19011290e-01  6.23392667e-01
  6.27354642e-01  6.28353524e-01  6.31849722e-01  6.33978643e-01
  6.36060861e-01  6.36687691e-01  6.39325260e-01  6.42330025e-01
  6.42967768e-01  6.46045021e-01  6.49229768e-01  6.51198402e-01
  6.56045604e-01  6.57851096e-01  6.63257133e-01  6.64617665e-01
  6.65232809e-01  6.65686337e-01  6.69174425e-01  6.70324014e-01
  6.71325335e-01  6.74885315e-01  6.76381134e-01  6.78116767e-01
  6.81529513e-01  6.84528886e-01  6.86594079e-01  6.87463193e-01
  6.89927886e-01  6.91287498e-01  6.91321844e-01  6.96286270e-01
  7.01983277e-01  7.02117076e-01  7.08384814e-01  7.14000453e-01
  7.14270332e-01  7.16114098e-01  7.19694558e-01  7.20399912e-01
  7.34278187e-01  7.34979253e-01  7.35690723e-01  7.38471248e-01
  7.39145652e-01  7.40683700e-01  7.46886622e-01  7.48690088e-01
  7.49750419e-01  7.50115814e-01  7.53941503e-01  7.56401403e-01
  7.56939815e-01  7.61911726e-01  7.71025226e-01  7.88846132e-01
  7.91773251e-01  7.92099439e-01  7.93256967e-01  7.96440171e-01
  8.02334341e-01  8.07248014e-01  8.09408166e-01  8.11152899e-01
  8.13489713e-01  8.20107041e-01  8.27964528e-01  8.30058980e-01
  8.30407663e-01  8.33636978e-01  8.35607792e-01  8.49199093e-01
  8.55247089e-01  8.59353966e-01  8.63100533e-01  8.66513443e-01
  8.70262494e-01  8.72567012e-01  8.72875862e-01  8.74542901e-01
  8.78733991e-01  8.79296416e-01  8.80484534e-01  8.81017396e-01
  8.82499373e-01  8.85374312e-01  8.90436649e-01  8.90580567e-01
  8.91728254e-01  8.93057807e-01  8.95309636e-01  8.96479131e-01
  8.97491851e-01  8.98080497e-01  8.98444257e-01  8.98616303e-01
  8.98754056e-01  8.99100195e-01  9.04543182e-01  9.07612779e-01
  9.07763843e-01  9.08116696e-01  9.11718001e-01  9.15666544e-01
  9.15957537e-01  9.16430520e-01  9.16574112e-01  9.18679770e-01
  9.19480195e-01  9.23933416e-01  9.26280379e-01  9.27196816e-01
  9.28108917e-01  9.28613696e-01  9.31033895e-01  9.32121018e-01
  9.32197922e-01  9.32433541e-01  9.33727184e-01  9.34685823e-01
  9.35220681e-01  9.37322375e-01  9.37867974e-01  9.40144714e-01
  9.41756642e-01  9.45093319e-01  9.47067851e-01  9.47413990e-01
  9.47523845e-01  9.50158606e-01  9.51377689e-01  9.52118845e-01
  9.56821175e-01  9.58892949e-01  9.62979035e-01  9.64324757e-01
  9.64416041e-01  9.66391645e-01  9.66962325e-01  9.67719695e-01
  9.68691109e-01  9.70165282e-01  9.70845954e-01  9.71780344e-01
  9.72263699e-01  9.73628557e-01  9.73931255e-01  9.75915753e-01
  9.76861045e-01  9.77408898e-01  9.77952994e-01  9.78755255e-01
  9.79284507e-01  9.79554559e-01  9.81316976e-01  9.81336453e-01
  9.82192128e-01  9.83011967e-01  9.83487364e-01  9.83561853e-01
  9.84792972e-01  9.85184580e-01  9.85395085e-01  9.86017855e-01
  9.86832049e-01  9.87311326e-01  9.87389543e-01  9.87405879e-01
  9.88284756e-01  9.88595190e-01  9.88824518e-01  9.90229044e-01
  9.90467632e-01  9.90976141e-01  9.92426655e-01  9.92628681e-01
  9.93466892e-01  9.93836406e-01  9.95521003e-01  9.96266761e-01
  9.98068105e-01  1.00012417e+00  1.00162144e+00  1.00174078e+00
  1.00386215e+00  1.00414781e+00  1.00476852e+00  1.00772434e+00
  1.01310101e+00  1.01368967e+00  1.01522084e+00  1.01720398e+00
  1.02921063e+00  1.03490911e+00  1.04265323e+00  1.04527678e+00
  1.07169575e+00  1.07497814e+00  1.08193111e+00  1.08631059e+00
  1.09630845e+00  1.10401850e+00  1.20921189e+00  1.24291648e+00]

  UserWarning,

2022-10-31 11:03:58,734:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [-0.08801748 -0.08553762 -0.08311123 -0.06628425 -0.06526849 -0.04823493
 -0.04541334 -0.04243311 -0.04075135 -0.03961086 -0.03617548 -0.03542051
 -0.03258018 -0.0309516  -0.02884819 -0.02216673 -0.0189442  -0.01771032
 -0.01691894 -0.01653723 -0.013533   -0.01253745 -0.01241667 -0.01213356
 -0.00978924 -0.00904612 -0.00823484 -0.00561879 -0.00361771 -0.00299146
 -0.002937    0.00606448  0.00636894  0.01008776  0.01075267  0.01755309
  0.02071952  0.02202813  0.02734906  0.0302474   0.03101099  0.03121003
  0.03190817  0.03339337  0.03446593  0.03807093  0.03911556  0.04337924
  0.04346521  0.04580302  0.05166463  0.05221032  0.05441982  0.05456023
  0.05742594  0.06008588  0.0617947   0.06333441  0.06357105  0.06430252
  0.06571263  0.06716023  0.06746866  0.06828827  0.0684648   0.06983372
  0.06999698  0.07021398  0.07077574  0.07267427  0.07367298  0.07368089
  0.07491475  0.07665438  0.07701718  0.07708101  0.07794814  0.07821313
  0.07909283  0.08163926  0.08276324  0.08451171  0.08683796  0.08779315
  0.08794584  0.08901455  0.09060596  0.10217318  0.10403517  0.10416115
  0.10554745  0.10702145  0.10819917  0.10911702  0.10914454  0.10924217
  0.11129427  0.11281088  0.11971706  0.12010429  0.12086032  0.12113815
  0.12189593  0.12237097  0.12407589  0.12553389  0.1268837   0.12783505
  0.12885817  0.12928436  0.12996532  0.13097893  0.13289463  0.13424447
  0.13638866  0.13759831  0.13796292  0.14285224  0.14532515  0.14553755
  0.1494713   0.15271986  0.15376189  0.15577904  0.1573503   0.15844301
  0.15975061  0.15988703  0.15994733  0.16082523  0.16174464  0.1625201
  0.16312874  0.16566199  0.16689254  0.16697394  0.16882048  0.17154631
  0.17333082  0.17526541  0.17809243  0.17885772  0.18020873  0.18088493
  0.1830886   0.18452729  0.18647655  0.18685032  0.18794636  0.18799253
  0.19084439  0.1912402   0.19130149  0.19281669  0.19377051  0.19585316
  0.1970279   0.19759947  0.19845054  0.199057    0.20010984  0.20025488
  0.20092207  0.20276063  0.20308305  0.2031052   0.20348989  0.20401901
  0.20444433  0.2062242   0.20751446  0.20957269  0.21641646  0.22263339
  0.2246281   0.22532253  0.22764559  0.22791367  0.22848416  0.22939484
  0.23777412  0.23832264  0.2441062   0.24430554  0.24505176  0.25046768
  0.25230973  0.25396452  0.25577861  0.25645348  0.25835535  0.26089286
  0.26463704  0.26687987  0.26696048  0.26937288  0.27354968  0.27417059
  0.27599179  0.2794445   0.28369735  0.28392558  0.2866109   0.28947233
  0.29449832  0.29730737  0.30067472  0.30174894  0.30218962  0.30279137
  0.30282291  0.30664592  0.30668173  0.31214371  0.31439887  0.31617435
  0.31727052  0.31920922  0.3200462   0.32277686  0.32289665  0.32852767
  0.32913299  0.3297012   0.33327152  0.33558642  0.33655009  0.34097197
  0.34266234  0.34271688  0.34578869  0.35200756  0.35649283  0.35726361
  0.35758341  0.36172646  0.36710915  0.37790875  0.38040558  0.39135313
  0.39310928  0.40034193  0.40438985  0.4062422   0.40830396  0.40859954
  0.40951255  0.4108737   0.41264869  0.41479624  0.41529402  0.41543816
  0.41749074  0.41895686  0.41895808  0.4209066   0.4239815   0.42507318
  0.42596358  0.42921586  0.4327519   0.43941334  0.44234203  0.44623803
  0.44980461  0.45904534  0.46230683  0.46339729  0.47350523  0.47553112
  0.47692945  0.48007169  0.48304126  0.48509934  0.48576322  0.4887637
  0.48903932  0.49067782  0.492173    0.49323664  0.49703867  0.49769052
  0.5001129   0.5045352   0.51283221  0.51663382  0.52074635  0.52188658
  0.5222903   0.52295272  0.52722176  0.52777764  0.5308659   0.53230255
  0.53425422  0.54138252  0.54153598  0.54501554  0.54590252  0.54850651
  0.5567404   0.55812738  0.55862725  0.56146743  0.56400808  0.56632153
  0.56712356  0.56716667  0.56757799  0.56985622  0.57141889  0.57172184
  0.57237624  0.57244129  0.57397095  0.57513148  0.57708119  0.57794418
  0.57886787  0.57996013  0.58066212  0.58153299  0.58270634  0.58504635
  0.58693381  0.58733482  0.58788359  0.59094921  0.59160482  0.59251755
  0.59277327  0.59328535  0.59431134  0.597824    0.59892273  0.59948953
  0.59991749  0.60149118  0.60184987  0.60237649  0.60267742  0.60620367
  0.60644527  0.60752234  0.61141514  0.61248162  0.61642843  0.62406428
  0.62532637  0.62587364  0.62787603  0.63455853  0.6351212   0.63700725
  0.63750695  0.63755537  0.64362916  0.64904855  0.65265417  0.65541046
  0.65551421  0.65652822  0.65960295  0.66191043  0.66219847  0.66339425
  0.66424657  0.66530191  0.67033177  0.67157827  0.67238731  0.675331
  0.67588997  0.67601528  0.67694778  0.67838961  0.67970131  0.67990177
  0.68036687  0.68186687  0.68318245  0.68462447  0.68610066  0.68925732
  0.69185028  0.69789153  0.69981026  0.70577763  0.70723449  0.70925192
  0.71202496  0.71277282  0.7133213   0.71351707  0.7143448   0.71438472
  0.71676354  0.71845053  0.71891595  0.71947651  0.71993014  0.72004114
  0.72218483  0.72740185  0.72747712  0.72753817  0.73254373  0.74225462
  0.74246929  0.74660496  0.74888318  0.74955848  0.75013679  0.75262055
  0.76280499  0.76455741  0.76570808  0.77467615  0.77568983  0.787125
  0.79430679  0.8013077   0.8036049   0.81362497  0.81632506  0.83359647
  0.83470824  0.85481577  0.86074841  0.86163788  0.86438596  0.87312708
  0.87321247  0.87848862  0.88041977  0.88474034  0.88552558  0.885828
  0.89368857  0.89382832  0.8939543   0.89703908  0.89772073  0.90073702
  0.90199633  0.9027191   0.90703761  0.90711734  0.9080245   0.90812269
  0.90855856  0.90949526  0.91142738  0.91360929  0.91382451  0.91648519
  0.91676855  0.91952316  0.92515442  0.92630006  0.92631098  0.92758183
  0.92922948  0.93080474  0.95105538  0.95159681  0.95365936  0.95520357
  0.95539854  0.96493168  0.9655697   0.9659156   0.96882017  0.96970548
  0.97286537  0.97349167  0.9757438   0.97606031  0.97729196  0.97783429
  0.97797632  0.97857747  0.97961504  0.98017044  0.98027765  0.98126519
  0.98145938  0.98185956  0.98209838  0.98216816  0.98296497  0.98386997
  0.98494776  0.98530111  0.98557567  0.9860716   0.98663527  0.98729883
  0.98819786  0.98893002  0.98918914  0.99076303  0.9907661   0.99242643
  0.9936965   0.99403928  0.99450739  0.99527577  0.99557214  0.99586569
  0.9967075   0.99835258  0.99880096  0.99904131  0.9991537   1.00133827
  1.00164381  1.00256471  1.00293291  1.0034538   1.00357333  1.0047074
  1.00656124  1.00687096  1.00713222  1.00891465  1.01475712  1.02243533
  1.02442135  1.02571907  1.02693857  1.03344019  1.0349128   1.03800929
  1.03888603  1.04699918  1.05033271  1.05215659  1.06503463  1.06646883
  1.0762837   1.07764039  1.09339114  1.10476586  1.14262567  1.15325065]

  UserWarning,

2022-10-31 11:03:58,734:INFO:Calculating mean and std
2022-10-31 11:03:58,734:INFO:Creating metrics dataframe
2022-10-31 11:03:58,749:INFO:Uploading results into container
2022-10-31 11:03:58,749:INFO:Uploading model into container now
2022-10-31 11:03:58,749:INFO:master_model_container: 34
2022-10-31 11:03:58,749:INFO:display_container: 2
2022-10-31 11:03:58,749:INFO:LGBMRegressor(random_state=3360)
2022-10-31 11:03:58,749:INFO:create_model() successfully completed......................................
2022-10-31 11:03:58,880:ERROR:create_model() for LGBMRegressor(random_state=3360) raised an exception or returned all 0.0:
2022-10-31 11:03:58,880:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:03:58,880:INFO:Initializing Dummy Regressor
2022-10-31 11:03:58,881:INFO:Total runtime is 2.4315139651298523 minutes
2022-10-31 11:03:58,881:INFO:SubProcess create_model() called ==================================
2022-10-31 11:03:58,881:INFO:Initializing create_model()
2022-10-31 11:03:58,881:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:03:58,881:INFO:Checking exceptions
2022-10-31 11:03:58,884:INFO:Importing libraries
2022-10-31 11:03:58,885:INFO:Copying training dataset
2022-10-31 11:03:58,887:INFO:Defining folds
2022-10-31 11:03:58,887:INFO:Declaring metric variables
2022-10-31 11:03:58,887:INFO:Importing untrained model
2022-10-31 11:03:58,887:INFO:Dummy Regressor Imported successfully
2022-10-31 11:03:58,887:INFO:Starting cross validation
2022-10-31 11:03:58,887:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:04:01,117:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50328707]

  UserWarning,

2022-10-31 11:04:01,211:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.49579985]

  UserWarning,

2022-10-31 11:04:01,273:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50292184]

  UserWarning,

2022-10-31 11:04:01,302:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50584368]

  UserWarning,

2022-10-31 11:04:01,318:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50356034]

  UserWarning,

2022-10-31 11:04:01,333:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50264743]

  UserWarning,

2022-10-31 11:04:01,333:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50465583]

  UserWarning,

2022-10-31 11:04:02,271:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50045645]

  UserWarning,

2022-10-31 11:04:02,286:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50063904]

  UserWarning,

2022-10-31 11:04:02,286:INFO:Calculating mean and std
2022-10-31 11:04:02,286:INFO:Creating metrics dataframe
2022-10-31 11:04:02,286:INFO:Uploading results into container
2022-10-31 11:04:02,286:INFO:Uploading model into container now
2022-10-31 11:04:02,286:INFO:master_model_container: 35
2022-10-31 11:04:02,286:INFO:display_container: 2
2022-10-31 11:04:02,286:INFO:DummyRegressor()
2022-10-31 11:04:02,286:INFO:create_model() successfully completed......................................
2022-10-31 11:04:02,395:WARNING:create_model() for DummyRegressor() raised an exception or returned all 0.0, trying without fit_kwargs:
2022-10-31 11:04:02,395:WARNING:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

2022-10-31 11:04:02,395:INFO:Initializing create_model()
2022-10-31 11:04:02,395:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1BA17C88>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC27E32388>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:04:02,395:INFO:Checking exceptions
2022-10-31 11:04:02,395:INFO:Importing libraries
2022-10-31 11:04:02,395:INFO:Copying training dataset
2022-10-31 11:04:02,411:INFO:Defining folds
2022-10-31 11:04:02,411:INFO:Declaring metric variables
2022-10-31 11:04:02,411:INFO:Importing untrained model
2022-10-31 11:04:02,411:INFO:Dummy Regressor Imported successfully
2022-10-31 11:04:02,411:INFO:Starting cross validation
2022-10-31 11:04:02,411:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:04:04,716:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50401753]

  UserWarning,

2022-10-31 11:04:04,716:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50292184]

  UserWarning,

2022-10-31 11:04:04,716:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50328707]

  UserWarning,

2022-10-31 11:04:04,778:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50584368]

  UserWarning,

2022-10-31 11:04:04,778:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.49579985]

  UserWarning,

2022-10-31 11:04:04,851:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50356034]

  UserWarning,

2022-10-31 11:04:04,882:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50264743]

  UserWarning,

2022-10-31 11:04:04,884:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50465583]

  UserWarning,

2022-10-31 11:04:05,778:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50045645]

  UserWarning,

2022-10-31 11:04:05,794:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py:774: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to 0. Details: 
Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 71, in _cached_call
    return cache[method]
KeyError: 'predict'

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\model_selection\_validation.py", line 761, in _score
    scores = scorer(estimator, X_test, y_test)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 103, in __call__
    score = scorer._score(cached_call, estimator, *args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 258, in _score
    y_pred = method_caller(estimator, "predict", X)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\metrics\_scorer.py", line 73, in _cached_call
    result = getattr(estimator, method)(*args, **kwargs)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\utils\metaestimators.py", line 113, in <lambda>
    out = lambda *args, **kwargs: self.fn(obj, *args, **kwargs)  # noqa
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 251, in predict
    y = _inverse_transform_one(transformer, y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pipeline.py", line 82, in _inverse_transform_one
    return transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\preprocess\transformers.py", line 262, in inverse_transform
    output = self.transformer.inverse_transform(y)
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\preprocessing\_label.py", line 161, in inverse_transform
    raise ValueError("y contains previously unseen labels: %s" % str(diff))
ValueError: y contains previously unseen labels: [0.50063904]

  UserWarning,

2022-10-31 11:04:05,794:INFO:Calculating mean and std
2022-10-31 11:04:05,794:INFO:Creating metrics dataframe
2022-10-31 11:04:05,794:INFO:Uploading results into container
2022-10-31 11:04:05,809:INFO:Uploading model into container now
2022-10-31 11:04:05,809:INFO:master_model_container: 36
2022-10-31 11:04:05,809:INFO:display_container: 2
2022-10-31 11:04:05,809:INFO:DummyRegressor()
2022-10-31 11:04:05,809:INFO:create_model() successfully completed......................................
2022-10-31 11:04:05,916:ERROR:create_model() for DummyRegressor() raised an exception or returned all 0.0:
2022-10-31 11:04:05,916:ERROR:Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 801, in compare_models
    != 0.0
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 817, in compare_models
    != 0.0
AssertionError

2022-10-31 11:04:05,916:INFO:master_model_container: 36
2022-10-31 11:04:05,916:INFO:display_container: 2
2022-10-31 11:04:05,916:INFO:[]
2022-10-31 11:04:05,916:INFO:compare_models() successfully completed......................................
2022-10-31 11:04:05,947:INFO:Initializing save_model()
2022-10-31 11:04:05,947:INFO:save_model(model=[], model_name=best_model, prep_pipe_=Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Age', 'RoomService', 'FoodCourt',
                                             'ShoppingMall', 'Spa', 'VRDeck'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(inclu...
                                    transformer=OneHotEncoder(cols=['HomePlanet',
                                                                    'Destination'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['PassengerId', 'Cabin', 'Name'],
                                    transformer=LeaveOneOutEncoder(cols=['PassengerId',
                                                                         'Cabin',
                                                                         'Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=3360))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-31 11:04:05,947:INFO:Adding model into prep_pipe
2022-10-31 11:04:05,968:INFO:best_model.pkl saved in current working directory
2022-10-31 11:04:05,989:INFO:Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('label_encoding',
                 TransformerWrapperWithInverse(transformer=LabelEncoder())),
                ('numerical_imputer',
                 TransformerWrapper(include=['Age', 'RoomService', 'FoodCourt',
                                             'ShoppingMall', 'Spa', 'VRDeck'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(inclu...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['PassengerId', 'Cabin', 'Name'],
                                    transformer=LeaveOneOutEncoder(cols=['PassengerId',
                                                                         'Cabin',
                                                                         'Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=3360))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model', [])])
2022-10-31 11:04:05,989:INFO:save_model() successfully completed......................................
2022-10-31 11:08:06,779:INFO:PyCaret RegressionExperiment
2022-10-31 11:08:06,779:INFO:Logging name: reg-default-name
2022-10-31 11:08:06,780:INFO:ML Usecase: MLUsecase.REGRESSION
2022-10-31 11:08:06,780:INFO:version 3.0.0.rc4
2022-10-31 11:08:06,780:INFO:Initializing setup()
2022-10-31 11:08:06,780:INFO:self.USI: c4ce
2022-10-31 11:08:06,780:INFO:self.variable_keys: {'X', '_ml_usecase', 'fold_groups_param', 'y_test', 'memory', 'variable_keys', 'exp_name_log', 'target_param', 'X_test', 'fold_generator', '_all_metrics', 'display_container', 'transform_target_param', '_gpu_n_jobs_param', '_all_models_internal', '_all_models', 'fold_shuffle_param', 'y_train', 'data', 'transform_target_method_param', 'log_plots_param', 'master_model_container', 'exp_id', 'seed', 'html_param', 'X_train', 'pipeline', '_available_plots', 'USI', 'idx', 'n_jobs_param', 'y', 'logging_param', 'gpu_param'}
2022-10-31 11:08:06,780:INFO:Checking environment
2022-10-31 11:08:06,780:INFO:python_version: 3.7.13
2022-10-31 11:08:06,780:INFO:python_build: ('default', 'Oct 19 2022 10:19:43')
2022-10-31 11:08:06,780:INFO:machine: AMD64
2022-10-31 11:08:06,780:INFO:platform: Windows-10-10.0.19041-SP0
2022-10-31 11:08:06,780:INFO:Memory: svmem(total=17052463104, available=9129795584, percent=46.5, used=7922667520, free=9129795584)
2022-10-31 11:08:06,780:INFO:Physical Core: 4
2022-10-31 11:08:06,780:INFO:Logical Core: 8
2022-10-31 11:08:06,780:INFO:Checking libraries
2022-10-31 11:08:06,780:INFO:System:
2022-10-31 11:08:06,780:INFO:    python: 3.7.13 (default, Oct 19 2022, 10:19:43) [MSC v.1916 64 bit (AMD64)]
2022-10-31 11:08:06,780:INFO:executable: C:\Users\Raghuram\anaconda3\envs\py37\python.exe
2022-10-31 11:08:06,780:INFO:   machine: Windows-10-10.0.19041-SP0
2022-10-31 11:08:06,780:INFO:PyCaret required dependencies:
2022-10-31 11:08:06,781:INFO:                 pip: 22.2.2
2022-10-31 11:08:06,781:INFO:          setuptools: 65.4.0
2022-10-31 11:08:06,781:INFO:             pycaret: 3.0.0rc4
2022-10-31 11:08:06,781:INFO:             IPython: 7.34.0
2022-10-31 11:08:06,781:INFO:          ipywidgets: 8.0.2
2022-10-31 11:08:06,781:INFO:                tqdm: 4.64.1
2022-10-31 11:08:06,781:INFO:               numpy: 1.21.6
2022-10-31 11:08:06,781:INFO:              pandas: 1.3.5
2022-10-31 11:08:06,781:INFO:              jinja2: 3.1.2
2022-10-31 11:08:06,781:INFO:               scipy: 1.5.4
2022-10-31 11:08:06,781:INFO:              joblib: 1.2.0
2022-10-31 11:08:06,781:INFO:             sklearn: 1.0.2
2022-10-31 11:08:06,781:INFO:                pyod: 1.0.6
2022-10-31 11:08:06,781:INFO:            imblearn: 0.9.0
2022-10-31 11:08:06,781:INFO:   category_encoders: 2.5.1.post0
2022-10-31 11:08:06,781:INFO:            lightgbm: 3.3.3
2022-10-31 11:08:06,781:INFO:               numba: 0.55.2
2022-10-31 11:08:06,781:INFO:            requests: 2.28.1
2022-10-31 11:08:06,781:INFO:          matplotlib: 3.5.3
2022-10-31 11:08:06,781:INFO:          scikitplot: 0.3.7
2022-10-31 11:08:06,782:INFO:         yellowbrick: 1.5
2022-10-31 11:08:06,782:INFO:              plotly: 5.11.0
2022-10-31 11:08:06,782:INFO:             kaleido: 0.2.1
2022-10-31 11:08:06,782:INFO:         statsmodels: 0.13.2
2022-10-31 11:08:06,782:INFO:              sktime: 0.13.4
2022-10-31 11:08:06,782:INFO:               tbats: 1.1.1
2022-10-31 11:08:06,782:INFO:            pmdarima: 1.8.5
2022-10-31 11:08:06,782:INFO:              psutil: 5.9.3
2022-10-31 11:08:06,782:INFO:PyCaret optional dependencies:
2022-10-31 11:08:06,782:INFO:                shap: Not installed
2022-10-31 11:08:06,782:INFO:           interpret: Not installed
2022-10-31 11:08:06,782:INFO:                umap: 0.5.3
2022-10-31 11:08:06,782:INFO:    pandas_profiling: 3.4.0
2022-10-31 11:08:06,782:INFO:  explainerdashboard: Not installed
2022-10-31 11:08:06,782:INFO:             autoviz: Not installed
2022-10-31 11:08:06,782:INFO:           fairlearn: Not installed
2022-10-31 11:08:06,782:INFO:             xgboost: Not installed
2022-10-31 11:08:06,782:INFO:            catboost: Not installed
2022-10-31 11:08:06,782:INFO:              kmodes: 0.12.2
2022-10-31 11:08:06,782:INFO:             mlxtend: 0.19.0
2022-10-31 11:08:06,782:INFO:       statsforecast: Not installed
2022-10-31 11:08:06,782:INFO:        tune_sklearn: Not installed
2022-10-31 11:08:06,783:INFO:                 ray: Not installed
2022-10-31 11:08:06,783:INFO:            hyperopt: Not installed
2022-10-31 11:08:06,783:INFO:              optuna: Not installed
2022-10-31 11:08:06,783:INFO:               skopt: Not installed
2022-10-31 11:08:06,783:INFO:              mlflow: 1.30.0
2022-10-31 11:08:06,783:INFO:              gradio: Not installed
2022-10-31 11:08:06,783:INFO:             fastapi: Not installed
2022-10-31 11:08:06,783:INFO:             uvicorn: Not installed
2022-10-31 11:08:06,783:INFO:              m2cgen: Not installed
2022-10-31 11:08:06,783:INFO:           evidently: Not installed
2022-10-31 11:08:06,783:INFO:                nltk: 3.7
2022-10-31 11:08:06,783:INFO:            pyLDAvis: 3.2.2
2022-10-31 11:08:06,783:INFO:              gensim: 3.8.3
2022-10-31 11:08:06,783:INFO:               spacy: 2.3.8
2022-10-31 11:08:06,783:INFO:           wordcloud: 1.8.2.2
2022-10-31 11:08:06,783:INFO:            textblob: 0.17.1
2022-10-31 11:08:06,783:INFO:               fugue: Not installed
2022-10-31 11:08:06,783:INFO:           streamlit: 1.14.0
2022-10-31 11:08:06,783:INFO:             prophet: Not installed
2022-10-31 11:08:06,784:INFO:None
2022-10-31 11:08:06,784:INFO:Set up data.
2022-10-31 11:08:06,791:INFO:Set up train/test split.
2022-10-31 11:08:06,798:INFO:Set up index.
2022-10-31 11:08:06,799:INFO:Set up folding strategy.
2022-10-31 11:08:06,799:INFO:Assigning column types.
2022-10-31 11:08:06,805:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2022-10-31 11:08:06,805:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,810:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,813:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,873:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,907:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,907:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:06,907:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:06,907:INFO:Engine for model 'lasso' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,924:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,924:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:06,974:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,029:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,030:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,030:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,031:INFO:Engine successfully changes for model 'lasso' to 'sklearn'.
2022-10-31 11:08:07,035:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,041:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,098:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,143:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,144:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,144:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,149:INFO:Engine for model 'ridge' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,153:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,211:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,256:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,257:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,257:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,257:INFO:Engine successfully changes for model 'ridge' to 'sklearn'.
2022-10-31 11:08:07,266:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,324:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,355:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,371:INFO:Engine for model 'en' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,433:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,482:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,483:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,483:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,484:INFO:Engine successfully changes for model 'en' to 'sklearn'.
2022-10-31 11:08:07,538:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,585:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,585:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,585:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,648:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,702:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,702:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,702:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,703:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2022-10-31 11:08:07,754:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,813:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,814:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,881:INFO:Engine for model 'svm' has not been set explicitly, hence returning None.
2022-10-31 11:08:07,925:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,926:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:07,926:INFO:Engine successfully changes for model 'svm' to 'sklearn'.
2022-10-31 11:08:08,034:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:08,034:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:08,138:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:08,138:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:08,138:INFO:Preparing preprocessing pipeline...
2022-10-31 11:08:08,138:INFO:Set up simple imputation.
2022-10-31 11:08:08,138:INFO:Set up encoding of ordinal features.
2022-10-31 11:08:08,138:INFO:Set up encoding of categorical features.
2022-10-31 11:08:08,138:INFO:Set up variance threshold.
2022-10-31 11:08:08,385:INFO:Finished creating preprocessing pipeline.
2022-10-31 11:08:08,401:INFO:Pipeline: Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Year', 'Present_Price',
                                             'Kms_Driven', 'Owner'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car_Name', 'Fuel_Type',
                                             'Seller_Type', 'Transmission'],
                                    transformer=SimpleImputer(fill_value='...
                 TransformerWrapper(include=['Fuel_Type'],
                                    transformer=OneHotEncoder(cols=['Fuel_Type'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car_Name'],
                                    transformer=LeaveOneOutEncoder(cols=['Car_Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=8328))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))])
2022-10-31 11:08:08,401:INFO:Creating final display dataframe.
2022-10-31 11:08:09,214:INFO:Setup display_container:                  Description             Value
0                 Session id              8328
1                     Target     Selling_Price
2                Target type        Regression
3                 Data shape         (301, 11)
4           Train data shape         (210, 11)
5            Test data shape          (91, 11)
6           Ordinal features                 2
7           Numeric features                 4
8       Categorical features                 4
9                 Preprocess              True
10           Imputation type            simple
11        Numeric imputation              mean
12    Categorical imputation          constant
13  Maximum one-hot encoding                 5
14           Encoding method              None
15    Low variance threshold                 0
16            Fold Generator             KFold
17               Fold Number                10
18                  CPU Jobs                -1
19                   Use GPU             False
20            Log Experiment             False
21           Experiment Name  reg-default-name
22                       USI              c4ce
2022-10-31 11:08:09,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:09,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:09,447:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:09,447:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2022-10-31 11:08:09,447:INFO:setup() successfully completed in 2.67s...............
2022-10-31 11:08:09,447:INFO:Initializing compare_models()
2022-10-31 11:08:09,447:INFO:compare_models(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, include=None, fold=None, round=4, cross_validation=True, sort=R2, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'R2', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.regression.oop.RegressionExperiment'>}, exclude=None)
2022-10-31 11:08:09,447:INFO:Checking exceptions
2022-10-31 11:08:09,447:INFO:Preparing display monitor
2022-10-31 11:08:09,447:INFO:Initializing Linear Regression
2022-10-31 11:08:09,447:INFO:Total runtime is 0.0 minutes
2022-10-31 11:08:09,462:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:09,462:INFO:Initializing create_model()
2022-10-31 11:08:09,462:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=lr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:09,462:INFO:Checking exceptions
2022-10-31 11:08:09,465:INFO:Importing libraries
2022-10-31 11:08:09,465:INFO:Copying training dataset
2022-10-31 11:08:09,467:INFO:Defining folds
2022-10-31 11:08:09,467:INFO:Declaring metric variables
2022-10-31 11:08:09,468:INFO:Importing untrained model
2022-10-31 11:08:09,468:INFO:Linear Regression Imported successfully
2022-10-31 11:08:09,468:INFO:Starting cross validation
2022-10-31 11:08:09,470:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:11,304:INFO:Calculating mean and std
2022-10-31 11:08:11,304:INFO:Creating metrics dataframe
2022-10-31 11:08:11,308:INFO:Uploading results into container
2022-10-31 11:08:11,309:INFO:Uploading model into container now
2022-10-31 11:08:11,309:INFO:master_model_container: 1
2022-10-31 11:08:11,309:INFO:display_container: 2
2022-10-31 11:08:11,309:INFO:LinearRegression(n_jobs=-1)
2022-10-31 11:08:11,309:INFO:create_model() successfully completed......................................
2022-10-31 11:08:11,417:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:11,417:INFO:Creating metrics dataframe
2022-10-31 11:08:11,432:INFO:Initializing Lasso Regression
2022-10-31 11:08:11,432:INFO:Total runtime is 0.03309024175008138 minutes
2022-10-31 11:08:11,432:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:11,432:INFO:Initializing create_model()
2022-10-31 11:08:11,432:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=lasso, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:11,432:INFO:Checking exceptions
2022-10-31 11:08:11,432:INFO:Importing libraries
2022-10-31 11:08:11,432:INFO:Copying training dataset
2022-10-31 11:08:11,432:INFO:Defining folds
2022-10-31 11:08:11,432:INFO:Declaring metric variables
2022-10-31 11:08:11,432:INFO:Importing untrained model
2022-10-31 11:08:11,432:INFO:Lasso Regression Imported successfully
2022-10-31 11:08:11,432:INFO:Starting cross validation
2022-10-31 11:08:11,432:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:12,075:INFO:Calculating mean and std
2022-10-31 11:08:12,076:INFO:Creating metrics dataframe
2022-10-31 11:08:12,080:INFO:Uploading results into container
2022-10-31 11:08:12,081:INFO:Uploading model into container now
2022-10-31 11:08:12,081:INFO:master_model_container: 2
2022-10-31 11:08:12,081:INFO:display_container: 2
2022-10-31 11:08:12,082:INFO:Lasso(random_state=8328)
2022-10-31 11:08:12,082:INFO:create_model() successfully completed......................................
2022-10-31 11:08:12,214:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:12,214:INFO:Creating metrics dataframe
2022-10-31 11:08:12,214:INFO:Initializing Ridge Regression
2022-10-31 11:08:12,214:INFO:Total runtime is 0.04612192710240682 minutes
2022-10-31 11:08:12,214:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:12,214:INFO:Initializing create_model()
2022-10-31 11:08:12,214:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=ridge, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:12,214:INFO:Checking exceptions
2022-10-31 11:08:12,214:INFO:Importing libraries
2022-10-31 11:08:12,214:INFO:Copying training dataset
2022-10-31 11:08:12,230:INFO:Defining folds
2022-10-31 11:08:12,230:INFO:Declaring metric variables
2022-10-31 11:08:12,230:INFO:Importing untrained model
2022-10-31 11:08:12,230:INFO:Ridge Regression Imported successfully
2022-10-31 11:08:12,230:INFO:Starting cross validation
2022-10-31 11:08:12,230:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:12,504:INFO:Calculating mean and std
2022-10-31 11:08:12,504:INFO:Creating metrics dataframe
2022-10-31 11:08:12,520:INFO:Uploading results into container
2022-10-31 11:08:12,520:INFO:Uploading model into container now
2022-10-31 11:08:12,520:INFO:master_model_container: 3
2022-10-31 11:08:12,520:INFO:display_container: 2
2022-10-31 11:08:12,520:INFO:Ridge(random_state=8328)
2022-10-31 11:08:12,520:INFO:create_model() successfully completed......................................
2022-10-31 11:08:12,629:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:12,629:INFO:Creating metrics dataframe
2022-10-31 11:08:12,629:INFO:Initializing Elastic Net
2022-10-31 11:08:12,629:INFO:Total runtime is 0.0530456821123759 minutes
2022-10-31 11:08:12,645:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:12,645:INFO:Initializing create_model()
2022-10-31 11:08:12,645:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=en, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:12,645:INFO:Checking exceptions
2022-10-31 11:08:12,645:INFO:Importing libraries
2022-10-31 11:08:12,645:INFO:Copying training dataset
2022-10-31 11:08:12,645:INFO:Defining folds
2022-10-31 11:08:12,645:INFO:Declaring metric variables
2022-10-31 11:08:12,645:INFO:Importing untrained model
2022-10-31 11:08:12,645:INFO:Elastic Net Imported successfully
2022-10-31 11:08:12,645:INFO:Starting cross validation
2022-10-31 11:08:12,645:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:12,951:INFO:Calculating mean and std
2022-10-31 11:08:12,951:INFO:Creating metrics dataframe
2022-10-31 11:08:12,951:INFO:Uploading results into container
2022-10-31 11:08:12,951:INFO:Uploading model into container now
2022-10-31 11:08:12,951:INFO:master_model_container: 4
2022-10-31 11:08:12,951:INFO:display_container: 2
2022-10-31 11:08:12,951:INFO:ElasticNet(random_state=8328)
2022-10-31 11:08:12,951:INFO:create_model() successfully completed......................................
2022-10-31 11:08:13,079:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:13,079:INFO:Creating metrics dataframe
2022-10-31 11:08:13,086:INFO:Initializing Least Angle Regression
2022-10-31 11:08:13,087:INFO:Total runtime is 0.06065578858057658 minutes
2022-10-31 11:08:13,087:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:13,087:INFO:Initializing create_model()
2022-10-31 11:08:13,087:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=lar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:13,087:INFO:Checking exceptions
2022-10-31 11:08:13,089:INFO:Importing libraries
2022-10-31 11:08:13,090:INFO:Copying training dataset
2022-10-31 11:08:13,093:INFO:Defining folds
2022-10-31 11:08:13,093:INFO:Declaring metric variables
2022-10-31 11:08:13,093:INFO:Importing untrained model
2022-10-31 11:08:13,094:INFO:Least Angle Regression Imported successfully
2022-10-31 11:08:13,095:INFO:Starting cross validation
2022-10-31 11:08:13,096:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:13,151:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 4 iterations, i.e. alpha=6.552e-02, with an active set of 4 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 5 iterations, i.e. alpha=4.974e-02, with an active set of 5 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=1.599e-02, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 6 iterations, i.e. alpha=4.303e-03, with an active set of 6 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=5.500e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,167:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_least_angle.py:660: ConvergenceWarning: Regressors in active set degenerate. Dropping a regressor, after 7 iterations, i.e. alpha=1.823e-04, with an active set of 7 regressors, and the smallest cholesky pivot element being 2.220e-16. Reduce max_iter or increase eps parameters.
  ConvergenceWarning,

2022-10-31 11:08:13,182:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,214:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,229:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,245:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,280:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,305:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,305:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,336:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), Lars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:13,352:INFO:Calculating mean and std
2022-10-31 11:08:13,367:INFO:Creating metrics dataframe
2022-10-31 11:08:13,367:INFO:Uploading results into container
2022-10-31 11:08:13,367:INFO:Uploading model into container now
2022-10-31 11:08:13,367:INFO:master_model_container: 5
2022-10-31 11:08:13,367:INFO:display_container: 2
2022-10-31 11:08:13,367:INFO:Lars(random_state=8328)
2022-10-31 11:08:13,367:INFO:create_model() successfully completed......................................
2022-10-31 11:08:13,508:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:13,509:INFO:Creating metrics dataframe
2022-10-31 11:08:13,516:INFO:Initializing Lasso Least Angle Regression
2022-10-31 11:08:13,517:INFO:Total runtime is 0.06783056656519572 minutes
2022-10-31 11:08:13,517:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:13,517:INFO:Initializing create_model()
2022-10-31 11:08:13,517:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=llar, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:13,518:INFO:Checking exceptions
2022-10-31 11:08:13,520:INFO:Importing libraries
2022-10-31 11:08:13,520:INFO:Copying training dataset
2022-10-31 11:08:13,521:INFO:Defining folds
2022-10-31 11:08:13,521:INFO:Declaring metric variables
2022-10-31 11:08:13,521:INFO:Importing untrained model
2022-10-31 11:08:13,521:INFO:Lasso Least Angle Regression Imported successfully
2022-10-31 11:08:13,521:INFO:Starting cross validation
2022-10-31 11:08:13,521:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:13,612:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,643:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,668:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,701:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,716:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,736:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,767:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,783:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,799:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,814:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), LassoLars())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)

Set parameter alpha to: original_alpha * np.sqrt(n_samples). 
  FutureWarning,

2022-10-31 11:08:13,845:INFO:Calculating mean and std
2022-10-31 11:08:13,845:INFO:Creating metrics dataframe
2022-10-31 11:08:13,863:INFO:Uploading results into container
2022-10-31 11:08:13,864:INFO:Uploading model into container now
2022-10-31 11:08:13,864:INFO:master_model_container: 6
2022-10-31 11:08:13,864:INFO:display_container: 2
2022-10-31 11:08:13,865:INFO:LassoLars(random_state=8328)
2022-10-31 11:08:13,865:INFO:create_model() successfully completed......................................
2022-10-31 11:08:13,967:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:13,967:INFO:Creating metrics dataframe
2022-10-31 11:08:13,983:INFO:Initializing Orthogonal Matching Pursuit
2022-10-31 11:08:13,983:INFO:Total runtime is 0.07560451428095499 minutes
2022-10-31 11:08:13,983:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:13,983:INFO:Initializing create_model()
2022-10-31 11:08:13,983:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=omp, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:13,983:INFO:Checking exceptions
2022-10-31 11:08:13,983:INFO:Importing libraries
2022-10-31 11:08:13,983:INFO:Copying training dataset
2022-10-31 11:08:13,983:INFO:Defining folds
2022-10-31 11:08:13,983:INFO:Declaring metric variables
2022-10-31 11:08:13,983:INFO:Importing untrained model
2022-10-31 11:08:13,983:INFO:Orthogonal Matching Pursuit Imported successfully
2022-10-31 11:08:13,983:INFO:Starting cross validation
2022-10-31 11:08:13,983:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:14,073:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,084:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,101:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,119:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,150:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,166:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,182:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,197:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,229:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,244:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_base.py:138: FutureWarning: The default of 'normalize' will be set to False in version 1.2 and deprecated in version 1.4.
If you wish to scale the data, use Pipeline with a StandardScaler in a preprocessing stage. To reproduce the previous behavior:

from sklearn.pipeline import make_pipeline

model = make_pipeline(StandardScaler(with_mean=False), OrthogonalMatchingPursuit())

If you wish to pass a sample_weight parameter, you need to pass it as a fit parameter to each step of the pipeline as follows:

kwargs = {s[0] + '__sample_weight': sample_weight for s in model.steps}
model.fit(X, y, **kwargs)


  FutureWarning,

2022-10-31 11:08:14,270:INFO:Calculating mean and std
2022-10-31 11:08:14,270:INFO:Creating metrics dataframe
2022-10-31 11:08:14,270:INFO:Uploading results into container
2022-10-31 11:08:14,270:INFO:Uploading model into container now
2022-10-31 11:08:14,270:INFO:master_model_container: 7
2022-10-31 11:08:14,270:INFO:display_container: 2
2022-10-31 11:08:14,270:INFO:OrthogonalMatchingPursuit()
2022-10-31 11:08:14,270:INFO:create_model() successfully completed......................................
2022-10-31 11:08:14,398:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:14,398:INFO:Creating metrics dataframe
2022-10-31 11:08:14,398:INFO:Initializing Bayesian Ridge
2022-10-31 11:08:14,398:INFO:Total runtime is 0.08252833286921182 minutes
2022-10-31 11:08:14,398:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:14,398:INFO:Initializing create_model()
2022-10-31 11:08:14,398:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=br, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:14,398:INFO:Checking exceptions
2022-10-31 11:08:14,398:INFO:Importing libraries
2022-10-31 11:08:14,398:INFO:Copying training dataset
2022-10-31 11:08:14,398:INFO:Defining folds
2022-10-31 11:08:14,398:INFO:Declaring metric variables
2022-10-31 11:08:14,398:INFO:Importing untrained model
2022-10-31 11:08:14,398:INFO:Bayesian Ridge Imported successfully
2022-10-31 11:08:14,414:INFO:Starting cross validation
2022-10-31 11:08:14,414:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:14,688:INFO:Calculating mean and std
2022-10-31 11:08:14,689:INFO:Creating metrics dataframe
2022-10-31 11:08:14,693:INFO:Uploading results into container
2022-10-31 11:08:14,693:INFO:Uploading model into container now
2022-10-31 11:08:14,693:INFO:master_model_container: 8
2022-10-31 11:08:14,693:INFO:display_container: 2
2022-10-31 11:08:14,694:INFO:BayesianRidge()
2022-10-31 11:08:14,694:INFO:create_model() successfully completed......................................
2022-10-31 11:08:14,798:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:14,798:INFO:Creating metrics dataframe
2022-10-31 11:08:14,814:INFO:Initializing Passive Aggressive Regressor
2022-10-31 11:08:14,814:INFO:Total runtime is 0.08945079644521077 minutes
2022-10-31 11:08:14,814:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:14,814:INFO:Initializing create_model()
2022-10-31 11:08:14,814:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=par, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:14,814:INFO:Checking exceptions
2022-10-31 11:08:14,814:INFO:Importing libraries
2022-10-31 11:08:14,814:INFO:Copying training dataset
2022-10-31 11:08:14,814:INFO:Defining folds
2022-10-31 11:08:14,814:INFO:Declaring metric variables
2022-10-31 11:08:14,814:INFO:Importing untrained model
2022-10-31 11:08:14,814:INFO:Passive Aggressive Regressor Imported successfully
2022-10-31 11:08:14,814:INFO:Starting cross validation
2022-10-31 11:08:14,814:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:15,091:INFO:Calculating mean and std
2022-10-31 11:08:15,091:INFO:Creating metrics dataframe
2022-10-31 11:08:15,095:INFO:Uploading results into container
2022-10-31 11:08:15,095:INFO:Uploading model into container now
2022-10-31 11:08:15,095:INFO:master_model_container: 9
2022-10-31 11:08:15,096:INFO:display_container: 2
2022-10-31 11:08:15,096:INFO:PassiveAggressiveRegressor(random_state=8328)
2022-10-31 11:08:15,096:INFO:create_model() successfully completed......................................
2022-10-31 11:08:15,197:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:15,197:INFO:Creating metrics dataframe
2022-10-31 11:08:15,213:INFO:Initializing Huber Regressor
2022-10-31 11:08:15,213:INFO:Total runtime is 0.09609625736872354 minutes
2022-10-31 11:08:15,213:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:15,213:INFO:Initializing create_model()
2022-10-31 11:08:15,213:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=huber, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:15,213:INFO:Checking exceptions
2022-10-31 11:08:15,213:INFO:Importing libraries
2022-10-31 11:08:15,213:INFO:Copying training dataset
2022-10-31 11:08:15,213:INFO:Defining folds
2022-10-31 11:08:15,213:INFO:Declaring metric variables
2022-10-31 11:08:15,213:INFO:Importing untrained model
2022-10-31 11:08:15,213:INFO:Huber Regressor Imported successfully
2022-10-31 11:08:15,213:INFO:Starting cross validation
2022-10-31 11:08:15,228:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:15,469:WARNING:C:\Users\Raghuram\anaconda3\envs\py37\lib\site-packages\sklearn\linear_model\_huber.py:332: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
  self.n_iter_ = _check_optimize_result("lbfgs", opt_res, self.max_iter)

2022-10-31 11:08:15,521:INFO:Calculating mean and std
2022-10-31 11:08:15,521:INFO:Creating metrics dataframe
2022-10-31 11:08:15,521:INFO:Uploading results into container
2022-10-31 11:08:15,521:INFO:Uploading model into container now
2022-10-31 11:08:15,521:INFO:master_model_container: 10
2022-10-31 11:08:15,521:INFO:display_container: 2
2022-10-31 11:08:15,521:INFO:HuberRegressor()
2022-10-31 11:08:15,521:INFO:create_model() successfully completed......................................
2022-10-31 11:08:15,630:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:15,630:INFO:Creating metrics dataframe
2022-10-31 11:08:15,646:INFO:Initializing K Neighbors Regressor
2022-10-31 11:08:15,646:INFO:Total runtime is 0.1033135970433553 minutes
2022-10-31 11:08:15,646:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:15,646:INFO:Initializing create_model()
2022-10-31 11:08:15,646:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=knn, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:15,646:INFO:Checking exceptions
2022-10-31 11:08:15,646:INFO:Importing libraries
2022-10-31 11:08:15,646:INFO:Copying training dataset
2022-10-31 11:08:15,646:INFO:Defining folds
2022-10-31 11:08:15,646:INFO:Declaring metric variables
2022-10-31 11:08:15,646:INFO:Importing untrained model
2022-10-31 11:08:15,646:INFO:K Neighbors Regressor Imported successfully
2022-10-31 11:08:15,646:INFO:Starting cross validation
2022-10-31 11:08:15,646:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:16,066:INFO:Calculating mean and std
2022-10-31 11:08:16,068:INFO:Creating metrics dataframe
2022-10-31 11:08:16,070:INFO:Uploading results into container
2022-10-31 11:08:16,070:INFO:Uploading model into container now
2022-10-31 11:08:16,070:INFO:master_model_container: 11
2022-10-31 11:08:16,070:INFO:display_container: 2
2022-10-31 11:08:16,070:INFO:KNeighborsRegressor(n_jobs=-1)
2022-10-31 11:08:16,070:INFO:create_model() successfully completed......................................
2022-10-31 11:08:16,198:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:16,198:INFO:Creating metrics dataframe
2022-10-31 11:08:16,198:INFO:Initializing Decision Tree Regressor
2022-10-31 11:08:16,198:INFO:Total runtime is 0.11251436869303384 minutes
2022-10-31 11:08:16,198:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:16,198:INFO:Initializing create_model()
2022-10-31 11:08:16,198:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=dt, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:16,198:INFO:Checking exceptions
2022-10-31 11:08:16,213:INFO:Importing libraries
2022-10-31 11:08:16,213:INFO:Copying training dataset
2022-10-31 11:08:16,213:INFO:Defining folds
2022-10-31 11:08:16,213:INFO:Declaring metric variables
2022-10-31 11:08:16,213:INFO:Importing untrained model
2022-10-31 11:08:16,213:INFO:Decision Tree Regressor Imported successfully
2022-10-31 11:08:16,213:INFO:Starting cross validation
2022-10-31 11:08:16,213:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:16,494:INFO:Calculating mean and std
2022-10-31 11:08:16,494:INFO:Creating metrics dataframe
2022-10-31 11:08:16,498:INFO:Uploading results into container
2022-10-31 11:08:16,498:INFO:Uploading model into container now
2022-10-31 11:08:16,499:INFO:master_model_container: 12
2022-10-31 11:08:16,499:INFO:display_container: 2
2022-10-31 11:08:16,499:INFO:DecisionTreeRegressor(random_state=8328)
2022-10-31 11:08:16,499:INFO:create_model() successfully completed......................................
2022-10-31 11:08:16,612:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:16,613:INFO:Creating metrics dataframe
2022-10-31 11:08:16,620:INFO:Initializing Random Forest Regressor
2022-10-31 11:08:16,620:INFO:Total runtime is 0.11955078840255737 minutes
2022-10-31 11:08:16,621:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:16,621:INFO:Initializing create_model()
2022-10-31 11:08:16,621:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=rf, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:16,621:INFO:Checking exceptions
2022-10-31 11:08:16,623:INFO:Importing libraries
2022-10-31 11:08:16,623:INFO:Copying training dataset
2022-10-31 11:08:16,626:INFO:Defining folds
2022-10-31 11:08:16,626:INFO:Declaring metric variables
2022-10-31 11:08:16,627:INFO:Importing untrained model
2022-10-31 11:08:16,627:INFO:Random Forest Regressor Imported successfully
2022-10-31 11:08:16,628:INFO:Starting cross validation
2022-10-31 11:08:16,628:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:17,518:INFO:Calculating mean and std
2022-10-31 11:08:17,518:INFO:Creating metrics dataframe
2022-10-31 11:08:17,534:INFO:Uploading results into container
2022-10-31 11:08:17,534:INFO:Uploading model into container now
2022-10-31 11:08:17,534:INFO:master_model_container: 13
2022-10-31 11:08:17,534:INFO:display_container: 2
2022-10-31 11:08:17,534:INFO:RandomForestRegressor(n_jobs=-1, random_state=8328)
2022-10-31 11:08:17,534:INFO:create_model() successfully completed......................................
2022-10-31 11:08:17,660:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:17,660:INFO:Creating metrics dataframe
2022-10-31 11:08:17,671:INFO:Initializing Extra Trees Regressor
2022-10-31 11:08:17,672:INFO:Total runtime is 0.13708276748657225 minutes
2022-10-31 11:08:17,672:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:17,672:INFO:Initializing create_model()
2022-10-31 11:08:17,672:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=et, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:17,672:INFO:Checking exceptions
2022-10-31 11:08:17,674:INFO:Importing libraries
2022-10-31 11:08:17,674:INFO:Copying training dataset
2022-10-31 11:08:17,678:INFO:Defining folds
2022-10-31 11:08:17,678:INFO:Declaring metric variables
2022-10-31 11:08:17,678:INFO:Importing untrained model
2022-10-31 11:08:17,679:INFO:Extra Trees Regressor Imported successfully
2022-10-31 11:08:17,680:INFO:Starting cross validation
2022-10-31 11:08:17,681:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:18,564:INFO:Calculating mean and std
2022-10-31 11:08:18,564:INFO:Creating metrics dataframe
2022-10-31 11:08:18,564:INFO:Uploading results into container
2022-10-31 11:08:18,564:INFO:Uploading model into container now
2022-10-31 11:08:18,580:INFO:master_model_container: 14
2022-10-31 11:08:18,580:INFO:display_container: 2
2022-10-31 11:08:18,580:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8328)
2022-10-31 11:08:18,580:INFO:create_model() successfully completed......................................
2022-10-31 11:08:18,699:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:18,699:INFO:Creating metrics dataframe
2022-10-31 11:08:18,701:INFO:Initializing AdaBoost Regressor
2022-10-31 11:08:18,701:INFO:Total runtime is 0.1542434811592102 minutes
2022-10-31 11:08:18,701:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:18,701:INFO:Initializing create_model()
2022-10-31 11:08:18,701:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=ada, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:18,701:INFO:Checking exceptions
2022-10-31 11:08:18,701:INFO:Importing libraries
2022-10-31 11:08:18,701:INFO:Copying training dataset
2022-10-31 11:08:18,701:INFO:Defining folds
2022-10-31 11:08:18,701:INFO:Declaring metric variables
2022-10-31 11:08:18,701:INFO:Importing untrained model
2022-10-31 11:08:18,701:INFO:AdaBoost Regressor Imported successfully
2022-10-31 11:08:18,701:INFO:Starting cross validation
2022-10-31 11:08:18,701:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:19,132:INFO:Calculating mean and std
2022-10-31 11:08:19,132:INFO:Creating metrics dataframe
2022-10-31 11:08:19,132:INFO:Uploading results into container
2022-10-31 11:08:19,132:INFO:Uploading model into container now
2022-10-31 11:08:19,132:INFO:master_model_container: 15
2022-10-31 11:08:19,132:INFO:display_container: 2
2022-10-31 11:08:19,132:INFO:AdaBoostRegressor(random_state=8328)
2022-10-31 11:08:19,132:INFO:create_model() successfully completed......................................
2022-10-31 11:08:19,242:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:19,242:INFO:Creating metrics dataframe
2022-10-31 11:08:19,258:INFO:Initializing Gradient Boosting Regressor
2022-10-31 11:08:19,258:INFO:Total runtime is 0.1635242978731791 minutes
2022-10-31 11:08:19,258:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:19,258:INFO:Initializing create_model()
2022-10-31 11:08:19,258:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=gbr, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:19,258:INFO:Checking exceptions
2022-10-31 11:08:19,268:INFO:Importing libraries
2022-10-31 11:08:19,268:INFO:Copying training dataset
2022-10-31 11:08:19,271:INFO:Defining folds
2022-10-31 11:08:19,272:INFO:Declaring metric variables
2022-10-31 11:08:19,272:INFO:Importing untrained model
2022-10-31 11:08:19,273:INFO:Gradient Boosting Regressor Imported successfully
2022-10-31 11:08:19,273:INFO:Starting cross validation
2022-10-31 11:08:19,275:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:19,627:INFO:Calculating mean and std
2022-10-31 11:08:19,627:INFO:Creating metrics dataframe
2022-10-31 11:08:19,627:INFO:Uploading results into container
2022-10-31 11:08:19,627:INFO:Uploading model into container now
2022-10-31 11:08:19,627:INFO:master_model_container: 16
2022-10-31 11:08:19,627:INFO:display_container: 2
2022-10-31 11:08:19,627:INFO:GradientBoostingRegressor(random_state=8328)
2022-10-31 11:08:19,627:INFO:create_model() successfully completed......................................
2022-10-31 11:08:19,749:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:19,749:INFO:Creating metrics dataframe
2022-10-31 11:08:19,749:INFO:Initializing Light Gradient Boosting Machine
2022-10-31 11:08:19,749:INFO:Total runtime is 0.17169946034749348 minutes
2022-10-31 11:08:19,749:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:19,749:INFO:Initializing create_model()
2022-10-31 11:08:19,749:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=lightgbm, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:19,749:INFO:Checking exceptions
2022-10-31 11:08:19,764:INFO:Importing libraries
2022-10-31 11:08:19,764:INFO:Copying training dataset
2022-10-31 11:08:19,764:INFO:Defining folds
2022-10-31 11:08:19,764:INFO:Declaring metric variables
2022-10-31 11:08:19,764:INFO:Importing untrained model
2022-10-31 11:08:19,764:INFO:Light Gradient Boosting Machine Imported successfully
2022-10-31 11:08:19,764:INFO:Starting cross validation
2022-10-31 11:08:19,764:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:20,067:INFO:Calculating mean and std
2022-10-31 11:08:20,067:INFO:Creating metrics dataframe
2022-10-31 11:08:20,067:INFO:Uploading results into container
2022-10-31 11:08:20,067:INFO:Uploading model into container now
2022-10-31 11:08:20,067:INFO:master_model_container: 17
2022-10-31 11:08:20,067:INFO:display_container: 2
2022-10-31 11:08:20,067:INFO:LGBMRegressor(random_state=8328)
2022-10-31 11:08:20,067:INFO:create_model() successfully completed......................................
2022-10-31 11:08:20,179:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:20,179:INFO:Creating metrics dataframe
2022-10-31 11:08:20,194:INFO:Initializing Dummy Regressor
2022-10-31 11:08:20,194:INFO:Total runtime is 0.17912898461023966 minutes
2022-10-31 11:08:20,194:INFO:SubProcess create_model() called ==================================
2022-10-31 11:08:20,194:INFO:Initializing create_model()
2022-10-31 11:08:20,194:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=dummy, fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000002DC271F3108>, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:20,194:INFO:Checking exceptions
2022-10-31 11:08:20,194:INFO:Importing libraries
2022-10-31 11:08:20,194:INFO:Copying training dataset
2022-10-31 11:08:20,194:INFO:Defining folds
2022-10-31 11:08:20,194:INFO:Declaring metric variables
2022-10-31 11:08:20,194:INFO:Importing untrained model
2022-10-31 11:08:20,194:INFO:Dummy Regressor Imported successfully
2022-10-31 11:08:20,194:INFO:Starting cross validation
2022-10-31 11:08:20,194:INFO:Cross validating with KFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2022-10-31 11:08:20,459:INFO:Calculating mean and std
2022-10-31 11:08:20,459:INFO:Creating metrics dataframe
2022-10-31 11:08:20,459:INFO:Uploading results into container
2022-10-31 11:08:20,459:INFO:Uploading model into container now
2022-10-31 11:08:20,459:INFO:master_model_container: 18
2022-10-31 11:08:20,459:INFO:display_container: 2
2022-10-31 11:08:20,459:INFO:DummyRegressor()
2022-10-31 11:08:20,459:INFO:create_model() successfully completed......................................
2022-10-31 11:08:20,563:INFO:SubProcess create_model() end ==================================
2022-10-31 11:08:20,563:INFO:Creating metrics dataframe
2022-10-31 11:08:20,579:INFO:Initializing create_model()
2022-10-31 11:08:20,579:INFO:create_model(self=<pycaret.regression.oop.RegressionExperiment object at 0x000002DC1CEEC548>, estimator=ExtraTreesRegressor(n_jobs=-1, random_state=8328), fold=KFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, kwargs={})
2022-10-31 11:08:20,579:INFO:Checking exceptions
2022-10-31 11:08:20,579:INFO:Importing libraries
2022-10-31 11:08:20,579:INFO:Copying training dataset
2022-10-31 11:08:20,594:INFO:Defining folds
2022-10-31 11:08:20,594:INFO:Declaring metric variables
2022-10-31 11:08:20,594:INFO:Importing untrained model
2022-10-31 11:08:20,594:INFO:Declaring custom model
2022-10-31 11:08:20,594:INFO:Extra Trees Regressor Imported successfully
2022-10-31 11:08:20,594:INFO:Cross validation set to False
2022-10-31 11:08:20,594:INFO:Fitting Model
2022-10-31 11:08:20,858:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8328)
2022-10-31 11:08:20,858:INFO:create_model() successfully completed......................................
2022-10-31 11:08:20,994:INFO:master_model_container: 18
2022-10-31 11:08:20,994:INFO:display_container: 2
2022-10-31 11:08:20,994:INFO:ExtraTreesRegressor(n_jobs=-1, random_state=8328)
2022-10-31 11:08:20,994:INFO:compare_models() successfully completed......................................
2022-10-31 11:08:21,010:INFO:Initializing save_model()
2022-10-31 11:08:21,010:INFO:save_model(model=ExtraTreesRegressor(n_jobs=-1, random_state=8328), model_name=best_model, prep_pipe_=Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Year', 'Present_Price',
                                             'Kms_Driven', 'Owner'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car_Name', 'Fuel_Type',
                                             'Seller_Type', 'Transmission'],
                                    transformer=SimpleImputer(fill_value='...
                 TransformerWrapper(include=['Fuel_Type'],
                                    transformer=OneHotEncoder(cols=['Fuel_Type'],
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car_Name'],
                                    transformer=LeaveOneOutEncoder(cols=['Car_Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=8328))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0)))]), verbose=True, use_case=MLUsecase.REGRESSION, kwargs={})
2022-10-31 11:08:21,010:INFO:Adding model into prep_pipe
2022-10-31 11:08:21,069:INFO:best_model.pkl saved in current working directory
2022-10-31 11:08:21,107:INFO:Pipeline(memory=Memory(location=C:\Users\Raghuram\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Year', 'Present_Price',
                                             'Kms_Driven', 'Owner'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=['Car_Name', 'Fuel_Type',
                                             'Seller_Type', 'Transmission'],
                                    transformer=SimpleImputer(fill_value='...
                                                              handle_missing='return_nan',
                                                              use_cat_names=True))),
                ('rest_encoding',
                 TransformerWrapper(include=['Car_Name'],
                                    transformer=LeaveOneOutEncoder(cols=['Car_Name'],
                                                                   handle_missing='return_nan',
                                                                   random_state=8328))),
                ('low_variance',
                 TransformerWrapper(exclude=[],
                                    transformer=VarianceThreshold(threshold=0))),
                ('trained_model',
                 ExtraTreesRegressor(n_jobs=-1, random_state=8328))])
2022-10-31 11:08:21,107:INFO:save_model() successfully completed......................................
